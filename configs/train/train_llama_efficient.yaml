defaults:
  - train_llama.yaml
  - override model: llama2
  - _self_

args:
  name: train-llama-efficient
  batch_size: 16
  seq_len: 1024

common:
  distributed:
    tp_size: 2

optimization:
  clip_grad_norm: null

# Add activation checkpointing to the model transforms
# This wraps LlamaBlock modules with a checkpointing wrapper to reduce memory usage
# allowing for larger batch sizes or training larger models (up to ~8B on single node A100s)
model_transforms:
  - _name: tensor_parallel
  - _name: activation_checkpoint
    layer_classes: ["LlamaAttention"]
    use_reentrant: false
  - _name: fully_shard
    reshard_after_forward: true
