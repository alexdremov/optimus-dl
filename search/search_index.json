{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Optimus-DL","text":"<p>Optimus-DL is a modular, high-performance research framework for training Large Language Models (LLMs) and other deep learning models. It leverages modern PyTorch features (AMP, DDP, Compile) and a flexible, composition-based architecture.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Modular \"Recipe\" Architecture: Clean separation between model definitions, data pipelines, and training logic.</li> <li>Hydra-based Configuration: Hierarchical, type-safe, and easily conveniently override-able configurations.</li> <li>Universal Metrics System: Lazy evaluation and automatic distributed aggregation of metrics.</li> <li>Modern PyTorch: Built-in support for Mixed Precision (AMP), FSDP2, Tensor Parallelism, Sequence Parallelism, and <code>torch.compile</code>.</li> <li>Efficient Kernels: Integrated support for Liger-Kernel for memory-efficient and fast RMSNorm, SwiGLU, and CrossEntropy.</li> <li>Registry System: easy dependency injection and component swapping via a centralized registry.</li> </ul> <p>The core idea of making everything modular and replacable is to make research experiments easy to implement cleanly.</p>"},{"location":"#supported-models","title":"Supported Models","text":"<p>Optimus-DL includes highly optimized implementations of:</p> <ul> <li>Llama 2 / 3: Full support for GQA, RoPE, and various sharding strategies.</li> <li>Qwen: Support for Qwen-style attention (Q/K Norm) and architectures.</li> <li>GPT-2: Classic architecture for baselining.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone &lt;repository-url&gt;\ncd optimus-dl\n\n# Install in editable mode with dependencies\npip install -e .\n</code></pre>"},{"location":"#training","title":"Training","text":"<p>Training is orchestrated via <code>scripts/train.py</code> using Hydra configs.</p> <pre><code># Run with default configuration\npython scripts/train.py\n\n# Override specific parameters\npython scripts/train.py model=gpt2 optimization.batch_size=64 common.use_gpu=true\n\n# Your own config\npython scripts/train.py --config-name=train_llama\n</code></pre>"},{"location":"#writing-train-configs","title":"Writing Train Configs","text":"<p>This project uses Hydra and OmegaConf for configuration management. Configurations are hierarchical and composable, allowing you to mix and match models, datasets, and training strategies.</p>"},{"location":"#structure-interpolation","title":"Structure &amp; Interpolation","text":"<p>Configs are located in <code>configs/train/</code>. A typical training config composes defaults (model, optimizer, scheduler) and then overrides specific parameters.</p> <p>We use a special <code>args</code> section as a \"scratch space\" for high-level variables. These are referenced throughout the config using OmegaConf's interpolation syntax <code>${...}</code>. This ensures consistency (e.g., setting <code>seq_len</code> in one place updates both the model and the data pipeline).</p> <pre><code>_name: base\nargs:\n  name: my-experiment\n  batch_size: 64\n  seq_len: 1024\n\n# ... later in the config ...\noptimization:\n  iterations: ${args.iterations}\n\ndata:\n  scratch:\n    base_transforms:\n      _name: compose\n      transforms:\n        # ...\n        - _name: flat_batcher\n          batch_size: ${args.batch_size} # Interpolated from args\n          seq_len: ${args.seq_len}\n</code></pre>"},{"location":"#data-pipelines-datascratch","title":"Data Pipelines &amp; <code>data.scratch</code>","text":"<p>The <code>data</code> section typically defines <code>train_datasets</code> and <code>eval_datasets</code>. To avoid repeating complex transform chains, we define them in <code>data.scratch</code> and reference them via interpolation.</p> <pre><code>data:\n  scratch:\n    # Define the transform chain once\n    my_transform:\n      _name: compose\n      transforms:\n        - _name: tokenize\n          tokenizer_config: {_name: tiktoken, name: gpt2}\n        - _name: to_device\n\n  train_datasets:\n    source:\n      _name: loop\n      inner: {_name: preset_dataset, split: train}\n    # Reference the transform\n    transform: ${data.scratch.my_transform}\n</code></pre>"},{"location":"#hydra-omegaconf-extra-quick-guide","title":"Hydra &amp; Omegaconf Extra Quick Guide","text":"<p>Here are some power-user features you'll likely use:</p> <ul> <li>Overriding Defaults: You can swap out entire components from the command line.</li> </ul> <pre><code># Switch the model to GPT-2 and optimizer to SGD\npython scripts/train.py model=gpt2 optimization/optimizer=sgd\n</code></pre> <ul> <li>Multirun (<code>-m</code>): Run multiple experiments sequentially with a sweep.</li> </ul> <pre><code># Run 3 experiments with different learning rates\npython scripts/train.py -m optimization.optimizer.lr=1e-3,1e-4,1e-5\n</code></pre> <ul> <li> <p>Interpolation: Reference other config values dynamically.</p> <ul> <li><code>${layout.param}</code>: Standard interpolation.</li> <li><code>${oc.env:VAR_NAME}</code>: Read from environment variable <code>VAR_NAME</code>.</li> <li><code>${.relative_param}</code>: Relative path interpolation.</li> <li><code>${eval:expression}</code>: Evaluate a Python expression. For example, <code>${eval:\"'string' + '_suffix'\"}</code> or <code>${eval:\"int(100 * 0.5)\"}</code>. This is defined in <code>optimus_dl/core/omegaconf.py</code>.</li> </ul> </li> <li> <p>Debugging: See the resolved configuration without running the code.</p> </li> </ul> <pre><code># Print the full config structure\npython scripts/train.py --config-name=train_llama -c job\n</code></pre>"},{"location":"#framework-internals","title":"Framework Internals","text":"<p>Understanding these core components is crucial for advanced usage and research extensions.</p>"},{"location":"#registry-system","title":"Registry System","text":"<p>The framework relies heavily on a registry pattern to decouple configuration from implementation. This allows you to swap components (models, optimizers, schedulers) just by changing the <code>_name</code> field in the config.</p> <ul> <li>Location: <code>optimus_dl/core/registry.py</code></li> <li>Usage:</li> </ul> <pre><code>from optimus_dl.core.registry import make_registry\n\n# Create a new registry\nregistry, register, build = make_registry(\"my_component\")\n\n@register(\"my_impl\")\nclass MyImplementation:\n    def __init__(self, param): ...\n\n# Build from config\nobj = build(RegistryConfig(_name=\"my_impl\", param=1))\n</code></pre>"},{"location":"#data-pipeline","title":"Data Pipeline","text":"<p>Data loading is split into Sources and Transforms.</p> <ul> <li>Source: Yields raw items (e.g., text, examples).</li> <li>Transforms: A chain of operations (Tokenize -&gt; Chunk -&gt; Shuffle -&gt; Batch -&gt; ToDevice).</li> </ul> <p>This design allows for highly reusable data processing pipelines. Complex transform chains are often defined in <code>data.scratch</code> and referenced in dataset configs.</p>"},{"location":"#checkpointing","title":"Checkpointing","text":"<p>We use PyTorch's Distributed Checkpoint (DCP) API for efficient, sharded saving/loading of large models.</p> <ul> <li>Structure: Checkpoints are directories containing sharded tensor data and a metadata file.</li> <li>Manager: <code>CheckpointManager</code> handles the complexity of saving model, optimizer, scheduler, and dataloader states.</li> <li>Auto-Resume: The training loop automatically detects the latest checkpoint in the output directory and resumes from it.</li> </ul> <p>LoadStrategy: For fine-tuning or experiments, you might want to load only parts of a checkpoint. The <code>LoadStrategy</code> class (<code>optimus_dl/modules/checkpoint/load_strategy.py</code>) controls this.</p> <ul> <li><code>load_model</code> (bool): Load model weights.</li> <li><code>load_optimizer</code> (bool): Load optimizer state.</li> <li><code>load_scheduler</code> (bool): Load learning rate scheduler state.</li> <li><code>load_data_sources</code> (bool): Load data source state (e.g. readers position).</li> <li><code>load_dataloaders</code> (bool): Load full dataloader state.</li> <li><code>load_metrics</code> (bool): Load accumulated metrics.</li> <li><code>load_iteration</code> (bool): Resume iteration count.</li> <li><code>extra_ignore_keys</code> (list): Specific keys to ignore in the checkpoint state dict.</li> </ul>"},{"location":"#advanced-usage","title":"Advanced Usage","text":""},{"location":"#model-transforms","title":"Model Transforms","text":"<p>Optimus-DL applies transformations to the model after initialization but before training. This is where distributed wrappers and compilation happen.</p> <ul> <li>Config: <code>model_transforms</code> list in <code>train.yaml</code>.</li> <li>Common Transforms:<ul> <li><code>ddp</code>: Standard DistributedDataParallel.</li> <li><code>fully_shard</code>: PyTorch FSDP2 (Fully Sharded Data Parallel). Supports mixed precision, CPU offloading, and mesh sharding.</li> <li><code>compile</code>: <code>torch.compile</code> for graph optimization.</li> </ul> </li> </ul> <pre><code>model_transforms:\n  - _name: fully_shard\n    mixed_precision:\n      param_dtype: bfloat16\n      reduce_dtype: float32\n  - _name: compile\n</code></pre>"},{"location":"#evaluation-with-lm_eval","title":"Evaluation with <code>lm_eval</code>","text":"<p>The framework integrates with the Language Model Evaluation Harness for standardized benchmarks.</p> <ul> <li>Script: <code>scripts/eval.py</code></li> <li>Config: <code>configs/eval/default.yaml</code></li> </ul> <pre><code># Evaluate a checkpoint on Hellaswag and MMLU\npython scripts/eval.py \\\n    common.checkpoint_path=outputs/my-run/checkpoint_00010000 \\\n    lm_eval.tasks=[hellaswag,mmlu] \\\n    lm_eval.batch_size=8\n</code></pre> <p>More advanced: <pre><code>python scripts/eval.py --config-name quick_pretrained \\\n          common.checkpoint_path=null ++common.model._name=preset_hfllama2 ++common.model.hf_model_name=TinyLlama/TinyLlama-1.1B-Chat-v1.0 \\\n          lm_eval.tasks=[hellaswag,mmlu] \\\n          lm_eval.batch_size=4\n</code></pre></p>"},{"location":"#serving-models","title":"Serving Models","text":"<p>Optimus-DL provides a simple serving script for deploying trained models as an OpenAI-compatible API endpoint. This uses <code>scripts/serve.py</code>.</p> <ul> <li>Script: <code>scripts/serve.py</code></li> <li>Config: <code>configs/serve/</code></li> </ul> <pre><code># Serve a TinyLlama model\npython scripts/serve.py --config-name=tinyllama\n</code></pre> <p>Make requests: <pre><code>curl -X POST http://127.0.0.1:8000//v1/chat/completions \\\n-d '{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"}], \"max_tokens\": 100, \"temperature\": 0.01}'\n</code></pre></p> <pre><code>curl -X POST http://localhost:8000/v1/completions -d '{\"prompt\": \"All:\", \"max_tokens\": 50, \"temperature\": 0.01}'\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"<ul> <li><code>optimus_dl/</code>: Main package source code.<ul> <li><code>core/</code>: Fundamental utilities (logging, registry, device management).</li> <li><code>modules/</code>: Pluggable components (models, optimizers, data loaders).</li> <li><code>recipe/</code>: Orchestration logic (training loops, evaluation).</li> </ul> </li> <li><code>configs/</code>: Hierarchical Hydra configuration files.</li> <li><code>scripts/</code>: Entry points.</li> </ul>"},{"location":"#development","title":"Development","text":"<p>The project enforces strict code quality standards.</p> <pre><code># Run tests\npytest\n\n# Format code\nblack .\nisort .\nruff check --fix .\n</code></pre>"},{"location":"#license","title":"License","text":"<p>MIT License.</p>"},{"location":"reference/","title":"Index","text":""},{"location":"reference/#optimus_dl","title":"<code>optimus_dl</code>","text":"<p>Optimus-DL: A modular, high-performance framework for training Large Language Models.</p> <p>Optimus-DL is a research framework built on PyTorch that provides:</p> <ul> <li>Modular \"Recipe\" architecture for clean separation of concerns</li> <li>Hydra-based configuration management</li> <li>Universal metrics system with distributed aggregation</li> <li>Modern PyTorch features (AMP, FSDP2, Tensor Parallelism, torch.compile)</li> <li>Efficient kernels via Liger-Kernel integration</li> <li>Registry system for easy component swapping</li> </ul> Example <p>Basic training:</p> <pre><code>from optimus_dl.core.registry import build\nfrom optimus_dl.recipe.train.config import TrainConfig\n\nconfig = TrainConfig(...)\nrecipe = build(\"train_recipe\", config)\nrecipe.run()\n</code></pre>"},{"location":"reference/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>core</code>: </li> <li><code>modules</code>: </li> <li><code>recipe</code>: </li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>core<ul> <li>bootstrap</li> <li>device</li> <li>env</li> <li>log</li> <li>model_utils</li> <li>omegaconf</li> <li>profile</li> <li>registry</li> <li>seed</li> </ul> </li> <li>modules<ul> <li>checkpoint<ul> <li>checkpoint_manager</li> <li>load_strategy</li> </ul> </li> <li>criterion<ul> <li>base</li> <li>config</li> <li>cross_entropy</li> </ul> </li> <li>data<ul> <li>config</li> <li>datasets<ul> <li>base</li> <li>composite</li> <li>huggingface</li> <li>loop_dataset</li> <li>strategies<ul> <li>base</li> <li>concat_random</li> <li>document</li> </ul> </li> <li>tokenized_dataset</li> <li>tokenized_flat_dataset</li> <li>txt_lines</li> </ul> </li> <li>presets<ul> <li>fineweb-edu</li> <li>slimpajama</li> <li>tinyshakespeare</li> </ul> </li> <li>transforms<ul> <li>base</li> <li>basic_batcher</li> <li>chunk_tokens</li> <li>composite</li> <li>flat_tokens_batcher</li> <li>prefetch</li> <li>shuffle</li> <li>to_device</li> <li>tokenize</li> </ul> </li> </ul> </li> <li>distributed<ul> <li>base</li> <li>config</li> <li>fake</li> <li>mesh</li> </ul> </li> <li>eval<ul> <li>model</li> </ul> </li> <li>loggers<ul> <li>base</li> <li>config</li> <li>jsonl</li> <li>wandb</li> </ul> </li> <li>lr_scheduler<ul> <li>base</li> <li>cosine_annealing</li> <li>linear_warmup</li> <li>wsd_scheduler</li> </ul> </li> <li>metrics<ul> <li>base</li> <li>common</li> <li>engine</li> <li>metrics</li> <li>source</li> <li>sources<ul> <li>causal_lm</li> <li>generation</li> </ul> </li> </ul> </li> <li>model<ul> <li>base</li> <li>blocks<ul> <li>attention</li> <li>layer_norms</li> <li>mlp</li> <li>rope</li> <li>transformer</li> </ul> </li> <li>config</li> <li>gpt2</li> <li>llama2</li> <li>olmo3</li> <li>presets<ul> <li>hf_llama</li> <li>hf_olmo3</li> <li>hf_qwen3</li> <li>utils</li> </ul> </li> <li>qwen3</li> </ul> </li> <li>model_transforms<ul> <li>base</li> <li>checkpoint</li> <li>compile</li> <li>config</li> <li>distributed</li> <li>load_weights</li> <li>tensor_parallel</li> </ul> </li> <li>optim<ul> <li>adamw</li> <li>config</li> <li>muon</li> <li>soap</li> </ul> </li> <li>tokenizer<ul> <li>base</li> <li>config</li> <li>implementations<ul> <li>char</li> <li>huggingface</li> <li>inline_tokens</li> <li>tiktoken</li> </ul> </li> </ul> </li> </ul> </li> <li>recipe<ul> <li>eval<ul> <li>base</li> <li>config</li> </ul> </li> <li>metrics<ul> <li>base</li> <li>config</li> </ul> </li> <li>mixins<ul> <li>model_builder</li> </ul> </li> <li>pretokenize<ul> <li>checkpoint</li> <li>config</li> <li>processor</li> <li>recipe</li> <li>sharder</li> <li>source</li> </ul> </li> <li>serve<ul> <li>base</li> <li>config</li> <li>models</li> </ul> </li> <li>train<ul> <li>base</li> <li>builders<ul> <li>criterion_builder</li> <li>data_builder</li> <li>optimizer_builder</li> <li>scheduler_builder</li> </ul> </li> <li>config</li> <li>mixins<ul> <li>execution<ul> <li>context_mixin</li> <li>interruption_mixin</li> <li>iteration_mixin</li> </ul> </li> <li>managers<ul> <li>evaluation_manager</li> <li>logger_manager</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/core/","title":"Index","text":""},{"location":"reference/core/#optimus_dl.core","title":"<code>optimus_dl.core</code>","text":""},{"location":"reference/core/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>bootstrap</code>: Automatically imports all files and nested submodules of a given module.</li> <li><code>device</code>: Device and distributed setup utilities.</li> <li><code>env</code>: Return the value of an environment variable as <code>int</code>.</li> <li><code>log</code>: Set up beautiful Python logging with colors and optional file output.</li> <li><code>model_utils</code>: Model utility functions.</li> <li><code>omegaconf</code>: OmegaConf custom resolvers for configuration.</li> <li><code>profile</code>: Performance profiling utilities.</li> <li><code>registry</code>: Registry system for dependency injection and component management.</li> <li><code>seed</code>: Utilities for setting random seeds for reproducibility.</li> </ul>"},{"location":"reference/core/bootstrap/","title":"bootstrap","text":""},{"location":"reference/core/bootstrap/#optimus_dl.core.bootstrap","title":"<code>optimus_dl.core.bootstrap</code>","text":""},{"location":"reference/core/bootstrap/#optimus_dl.core.bootstrap.bootstrap_module","title":"<code>bootstrap_module(module_name, path=None, exclude_patterns=None, silent_errors=False)</code>","text":"<p>Automatically imports all files and nested submodules of a given module.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>The full module name to import submodules for.</p> required <code>path</code> <code>str | None</code> <p>The filesystem path for the module. If None, derived from module_name.</p> <code>None</code> <code>exclude_patterns</code> <code>list[str] | None</code> <p>List of patterns to exclude from import.</p> <code>None</code> <code>silent_errors</code> <code>bool</code> <p>If True, log errors instead of raising them.</p> <code>False</code> Source code in <code>optimus_dl/core/bootstrap.py</code> <pre><code>def bootstrap_module(\n    module_name: str,\n    path: str | None = None,\n    exclude_patterns: list[str] | None = None,\n    silent_errors: bool = False,\n) -&gt; None:\n    \"\"\"Automatically imports all files and nested submodules of a given module.\n\n    Args:\n        module_name: The full module name to import submodules for.\n        path: The filesystem path for the module. If None, derived from module_name.\n        exclude_patterns: List of patterns to exclude from import.\n        silent_errors: If True, log errors instead of raising them.\n    \"\"\"\n    if exclude_patterns is None:\n        exclude_patterns = [\"test_\", \"tests\", \"__pycache__\", \".pyc\"]\n\n    logger = logging.getLogger(__name__)\n\n    try:\n        if path is None:\n            spec = importlib.util.find_spec(module_name)\n            if spec is None or not spec.submodule_search_locations:\n                raise ImportError(f\"Module '{module_name}' not found or not a package.\")\n            paths = spec.submodule_search_locations\n        else:\n            paths = [path]\n\n        for package_path in paths:\n            if not os.path.exists(package_path):\n                continue\n\n            for entry in os.listdir(package_path):\n                # Skip private files and excluded patterns\n                if entry.startswith(\"_\") or any(\n                    pattern in entry for pattern in exclude_patterns\n                ):\n                    continue\n\n                full_path = os.path.join(package_path, entry)\n\n                try:\n                    if os.path.isdir(full_path):\n                        # Check if it's a valid Python package\n                        if os.path.exists(os.path.join(full_path, \"__init__.py\")):\n                            submodule_name = f\"{module_name}.{entry}\"\n                            importlib.import_module(submodule_name)\n                            logger.debug(f\"Imported submodule: {submodule_name}\")\n                    elif entry.endswith(\".py\") and entry != \"__init__.py\":\n                        submodule_name = f\"{module_name}.{entry[:-3]}\"\n                        importlib.import_module(submodule_name)\n                        logger.debug(f\"Imported module: {submodule_name}\")\n\n                except ImportError as e:\n                    if silent_errors:\n                        logger.warning(f\"Failed to import {entry}: {e}\")\n                    else:\n                        raise\n\n    except Exception as e:\n        if silent_errors:\n            logger.error(f\"Bootstrap failed for {module_name}: {e}\")\n        else:\n            raise\n</code></pre>"},{"location":"reference/core/device/","title":"device","text":""},{"location":"reference/core/device/#optimus_dl.core.device","title":"<code>optimus_dl.core.device</code>","text":"<p>Device and distributed setup utilities.</p> <p>This module provides functions for automatically detecting and setting up the best available compute device (CUDA, MPS, XPU, or CPU) and initializing distributed training collectives.</p>"},{"location":"reference/core/device/#optimus_dl.core.device.DeviceSetup","title":"<code>DeviceSetup</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Container for device and collective setup results.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <code>None</code> <code>collective</code> <code>Any</code> <code>None</code> Source code in <code>optimus_dl/core/device.py</code> <pre><code>class DeviceSetup(NamedTuple):\n    \"\"\"Container for device and collective setup results.\n\n    Attributes:\n        device: The PyTorch device to use for computation.\n        collective: The distributed collective object for multi-GPU/multi-node training.\n    \"\"\"\n\n    device: torch.device\n    collective: Any\n</code></pre>"},{"location":"reference/core/device/#optimus_dl.core.device.get_best_device","title":"<code>get_best_device()</code>","text":"<p>Detect and return the best available compute device.</p> <p>Checks for available devices in order of preference: 1. CUDA (NVIDIA GPUs) 2. MPS (Apple Silicon GPUs) 3. XPU (Intel GPUs) 4. CPU (fallback)</p> <p>Returns:</p> Type Description <code>device</code> <p>The best available torch.device. Always returns a valid device,</p> <code>device</code> <p>defaulting to CPU if no accelerators are available.</p> Example <pre><code>device = get_best_device()\nprint(device)  # cuda, mps, xpu, or cpu\n</code></pre> Source code in <code>optimus_dl/core/device.py</code> <pre><code>def get_best_device() -&gt; torch.device:\n    \"\"\"Detect and return the best available compute device.\n\n    Checks for available devices in order of preference:\n    1. CUDA (NVIDIA GPUs)\n    2. MPS (Apple Silicon GPUs)\n    3. XPU (Intel GPUs)\n    4. CPU (fallback)\n\n    Returns:\n        The best available torch.device. Always returns a valid device,\n        defaulting to CPU if no accelerators are available.\n\n    Example:\n        ```python\n        device = get_best_device()\n        print(device)  # cuda, mps, xpu, or cpu\n        ```\n    \"\"\"\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    if torch.mps.is_available():\n        return torch.device(\"mps\")\n    if torch.xpu.is_available():\n        return torch.device(\"xpu\")\n    return torch.device(\"cpu\")\n</code></pre>"},{"location":"reference/core/device/#optimus_dl.core.device.setup_device_and_collective","title":"<code>setup_device_and_collective(use_gpu, config)</code>","text":"<p>Setup compute device and distributed training collective.</p> <p>This function initializes the training environment by: 1. Selecting the appropriate compute device (GPU or CPU) 2. Setting up distributed communication if multiple devices are available 3. Returning both the device and collective for use in training</p> <p>Parameters:</p> Name Type Description Default <code>use_gpu</code> <code>bool</code> <p>If True, attempts to use GPU if available. If False, uses CPU.</p> required <code>config</code> <code>DistributedConfig</code> <p>Distributed configuration specifying how to set up multi-GPU mesh.</p> required <p>Returns:</p> Type Description <code>DeviceSetup</code> <p>DeviceSetup namedtuple containing:</p> <code>DeviceSetup</code> <ul> <li>device: The PyTorch device to use for computation</li> </ul> <code>DeviceSetup</code> <ul> <li>collective: Distributed collective object for multi-GPU coordination</li> </ul> Example <pre><code>from optimus_dl.modules.distributed.config import DistributedConfig\nconfig = DistributedConfig()\nsetup = setup_device_and_collective(use_gpu=True, config=config)\nmodel = model.to(setup.device)\n# Use setup.collective for distributed operations\n</code></pre> Source code in <code>optimus_dl/core/device.py</code> <pre><code>def setup_device_and_collective(\n    use_gpu: bool, config: DistributedConfig\n) -&gt; DeviceSetup:\n    \"\"\"Setup compute device and distributed training collective.\n\n    This function initializes the training environment by:\n    1. Selecting the appropriate compute device (GPU or CPU)\n    2. Setting up distributed communication if multiple devices are available\n    3. Returning both the device and collective for use in training\n\n    Args:\n        use_gpu: If True, attempts to use GPU if available. If False, uses CPU.\n        config: Distributed configuration specifying how to set up multi-GPU mesh.\n\n    Returns:\n        DeviceSetup namedtuple containing:\n\n        - device: The PyTorch device to use for computation\n        - collective: Distributed collective object for multi-GPU coordination\n\n    Example:\n        ```python\n        from optimus_dl.modules.distributed.config import DistributedConfig\n        config = DistributedConfig()\n        setup = setup_device_and_collective(use_gpu=True, config=config)\n        model = model.to(setup.device)\n        # Use setup.collective for distributed operations\n        ```\n    \"\"\"\n    from optimus_dl.modules.distributed import build_best_collective\n\n    device = torch.device(\"cpu\")\n    if use_gpu:\n        device = get_best_device()\n    collective = build_best_collective(config=config, device=device)\n    device = collective.default_device\n    return DeviceSetup(device=device, collective=collective)\n</code></pre>"},{"location":"reference/core/env/","title":"env","text":""},{"location":"reference/core/env/#optimus_dl.core.env","title":"<code>optimus_dl.core.env</code>","text":""},{"location":"reference/core/env/#optimus_dl.core.env.get_device_from_env","title":"<code>get_device_from_env(env, name)</code>","text":"<p>Return the value of an environment variable as :class:<code>torch.device</code>.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>Mapping[str, str]</code> <p>The environment mapping.</p> required <code>name</code> <code>str</code> <p>The name of the environment variable.</p> required <p>Returns:</p> Type Description <code>device | None</code> <p>The torch.device, or None if the variable is not present.</p> <p>Raises:</p> Type Description <code>InvalidEnvironmentVariableError</code> <p>If the value is not a valid device string.</p> Source code in <code>optimus_dl/core/env.py</code> <pre><code>def get_device_from_env(env: Mapping[str, str], name: str) -&gt; torch.device | None:\n    \"\"\"Return the value of an environment variable as :class:`torch.device`.\n\n    Args:\n        env: The environment mapping.\n        name: The name of the environment variable.\n\n    Returns:\n        The torch.device, or None if the variable is not present.\n\n    Raises:\n        InvalidEnvironmentVariableError: If the value is not a valid device string.\n    \"\"\"\n    device_str = env.get(name)\n    if device_str is None:\n        return None\n\n    try:\n        return torch.device(device_str)\n    except (RuntimeError, ValueError):\n        raise InvalidEnvironmentVariableError(\n            name, f\"The value of the `{name}` environment variable is expected to specify a PyTorch device, but is '{device_str}' instead.\"  # fmt: skip\n        ) from None\n</code></pre>"},{"location":"reference/core/env/#optimus_dl.core.env.get_int_from_env","title":"<code>get_int_from_env(env, name, allow_zero=False)</code>","text":"<p>Return the value of an environment variable as <code>int</code>.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>Mapping[str, str]</code> <p>The environment mapping (e.g. os.environ).</p> required <code>name</code> <code>str</code> <p>The name of the environment variable.</p> required <code>allow_zero</code> <code>bool</code> <p>If <code>True</code>, returns the value if it equals to zero; otherwise, raises a :class:<code>InvalidEnvironmentVariableError</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>int | None</code> <p>The integer value, or None if the variable is not present.</p> <p>Raises:</p> Type Description <code>InvalidEnvironmentVariableError</code> <p>If the value is not a valid integer or violates constraints (e.g. negative when allow_zero is False).</p> Source code in <code>optimus_dl/core/env.py</code> <pre><code>def get_int_from_env(\n    env: Mapping[str, str], name: str, allow_zero: bool = False\n) -&gt; int | None:\n    \"\"\"Return the value of an environment variable as ``int``.\n\n    Args:\n        env: The environment mapping (e.g. os.environ).\n        name: The name of the environment variable.\n        allow_zero: If ``True``, returns the value if it equals to zero;\n            otherwise, raises a :class:`InvalidEnvironmentVariableError`.\n\n    Returns:\n        The integer value, or None if the variable is not present.\n\n    Raises:\n        InvalidEnvironmentVariableError: If the value is not a valid integer\n            or violates constraints (e.g. negative when allow_zero is False).\n    \"\"\"\n    s = env.get(name)\n    if s is None:\n        return None\n\n    try:\n        value = int(s)\n    except ValueError:\n        raise InvalidEnvironmentVariableError(\n            name, f\"The value of the `{name}` environment variable is expected to be an integer, but is '{s}' instead.\"  # fmt: skip\n        ) from None\n\n    if not allow_zero:\n        if not value &gt;= 1:\n            raise InvalidEnvironmentVariableError(\n                name, f\"The value of the `{name}` environment variable is expected to be a positive integer, but is {value} instead.\"  # fmt: skip\n            )\n    else:\n        if not value &gt;= 0:\n            raise InvalidEnvironmentVariableError(\n                name, f\"The value of the `{name}` environment variable is expected to be greater than or equal to 0, but is {value} instead.\"  # fmt: skip\n            )\n\n    return value\n</code></pre>"},{"location":"reference/core/env/#optimus_dl.core.env.get_path_from_env","title":"<code>get_path_from_env(env, name)</code>","text":"<p>Return the value of an environment variable as :class:<code>~pathlib.Path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>Mapping[str, str]</code> <p>The environment mapping.</p> required <code>name</code> <code>str</code> <p>The name of the environment variable.</p> required <p>Returns:</p> Type Description <code>Path | None</code> <p>The path, or None if the variable is not present.</p> <p>Raises:</p> Type Description <code>InvalidEnvironmentVariableError</code> <p>If the value cannot be converted to a Path.</p> Source code in <code>optimus_dl/core/env.py</code> <pre><code>def get_path_from_env(env: Mapping[str, str], name: str) -&gt; Path | None:\n    \"\"\"Return the value of an environment variable as :class:`~pathlib.Path`.\n\n    Args:\n        env: The environment mapping.\n        name: The name of the environment variable.\n\n    Returns:\n        The path, or None if the variable is not present.\n\n    Raises:\n        InvalidEnvironmentVariableError: If the value cannot be converted to a Path.\n    \"\"\"\n    pathname = env.get(name)\n    if not pathname:\n        return None\n\n    try:\n        return Path(pathname)\n    except ValueError:\n        raise InvalidEnvironmentVariableError(\n            name, f\"The value of the `{name}` environment variable is expected to be a pathname, but is '{pathname}' instead.\"  # fmt: skip\n        ) from None\n</code></pre>"},{"location":"reference/core/log/","title":"log","text":""},{"location":"reference/core/log/#optimus_dl.core.log","title":"<code>optimus_dl.core.log</code>","text":""},{"location":"reference/core/log/#optimus_dl.core.log.setup_logging","title":"<code>setup_logging(level=logging.INFO, log_file=None, use_rich=True, use_colors=True, format_string=None, date_format='%Y-%m-%d %H:%M:%S')</code>","text":"<p>Set up beautiful Python logging with colors and optional file output.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int | str</code> <p>Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)</p> <code>INFO</code> <code>log_file</code> <code>str | Path | None</code> <p>Optional path to log file for persistent logging</p> <code>None</code> <code>use_rich</code> <code>bool</code> <p>Use Rich library for enhanced console output</p> <code>True</code> <code>use_colors</code> <code>bool</code> <p>Enable colored output (ignored if use_rich=True)</p> <code>True</code> <code>format_string</code> <code>str | None</code> <p>Custom format string (uses default if None)</p> <code>None</code> <code>date_format</code> <code>str</code> <p>Date format for timestamps</p> <code>'%Y-%m-%d %H:%M:%S'</code> <p>Returns:</p> Type Description <code>Logger</code> <p>Configured logger instance</p> Example <pre><code>logger = setup_logging(level=\"DEBUG\", log_file=\"app.log\")\nlogger.info(\"Application started\")\nlogger.error(\"Something went wrong!\")\n</code></pre> Source code in <code>optimus_dl/core/log.py</code> <pre><code>def setup_logging(\n    level: int | str = logging.INFO,\n    log_file: str | Path | None = None,\n    use_rich: bool = True,\n    use_colors: bool = True,\n    format_string: str | None = None,\n    date_format: str = \"%Y-%m-%d %H:%M:%S\",\n) -&gt; logging.Logger:\n    \"\"\"\n    Set up beautiful Python logging with colors and optional file output.\n\n    Args:\n        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n        log_file: Optional path to log file for persistent logging\n        use_rich: Use Rich library for enhanced console output\n        use_colors: Enable colored output (ignored if use_rich=True)\n        format_string: Custom format string (uses default if None)\n        date_format: Date format for timestamps\n\n    Returns:\n        Configured logger instance\n\n    Example:\n        ```python\n        logger = setup_logging(level=\"DEBUG\", log_file=\"app.log\")\n        logger.info(\"Application started\")\n        logger.error(\"Something went wrong!\")\n        ```\n    \"\"\"\n    warnings.filterwarnings(\n        \"ignore\",\n        message=\".*Subclassing `Dict` in Structured Config classes is deprecated.*\",\n    )\n\n    httpx_logger = logging.getLogger(\"httpx\")\n    # Set its level to WARNING (this suppresses INFO and DEBUG logs)\n    httpx_logger.setLevel(logging.WARNING)\n\n    # Clear any existing handlers\n    root_logger = logging.getLogger()\n    root_logger.handlers.clear()\n\n    # Set the logging level\n    if isinstance(level, str):\n        level = getattr(logging, level.upper())\n    root_logger.setLevel(level)\n\n    handlers = []\n\n    is_interactive = sys.stdout.isatty()\n\n    # Override use_rich if we are not in a real terminal\n    if not is_interactive:\n        use_rich = False\n\n    # Console handler setup\n    if use_rich:\n        # Rich handler for beautiful console output\n        console = Console()\n        rich_handler = RichHandler(\n            console=console,\n            show_time=True,\n            show_level=True,\n            show_path=True,\n            markup=True,\n            rich_tracebacks=True,\n        )\n        rich_handler.setLevel(level)\n        handlers.append(rich_handler)\n\n    else:\n        # Standard console handler with optional colors\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setLevel(level)\n\n        if use_colors:\n            # Colorlog for colored output\n            color_formatter = colorlog.ColoredFormatter(\n                format_string\n                or \"%(log_color)s%(asctime)s | %(name)s | %(levelname)-8s | %(message)s\",\n                datefmt=date_format,\n                log_colors={\n                    \"DEBUG\": \"cyan\",\n                    \"INFO\": \"green\",\n                    \"WARNING\": \"yellow\",\n                    \"ERROR\": \"red\",\n                    \"CRITICAL\": \"red,bg_white\",\n                },\n            )\n            console_handler.setFormatter(color_formatter)\n        else:\n            # Standard formatter\n            formatter = logging.Formatter(\n                format_string\n                or \"%(asctime)s | %(name)s | %(levelname)-8s | %(message)s\",\n                datefmt=date_format,\n            )\n            console_handler.setFormatter(formatter)\n\n        handlers.append(console_handler)\n\n    # File handler setup (if specified)\n    if log_file:\n        log_path = Path(log_file)\n        log_path.parent.mkdir(parents=True, exist_ok=True)\n\n        file_handler = logging.FileHandler(log_path)\n        file_handler.setLevel(level)\n\n        # File output doesn't need colors\n        file_formatter = logging.Formatter(\n            \"%(asctime)s | %(name)s | %(levelname)-8s | %(funcName)s:%(lineno)d | %(message)s\",\n            datefmt=date_format,\n        )\n        file_handler.setFormatter(file_formatter)\n        handlers.append(file_handler)\n\n    # Add all handlers to the root logger\n    for handler in handlers:\n        root_logger.addHandler(handler)\n\n    # Return a logger instance\n    return logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/core/log/#optimus_dl.core.log.warn_once","title":"<code>warn_once(logger, message)</code>","text":"<p>Log a warning message only once, even if called multiple times.</p> <p>This is useful for deprecation warnings or other messages that should only appear once per program execution, even if the code path is executed multiple times.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>Logger instance to log the warning to.</p> required <code>message</code> <code>str</code> <p>Warning message to log (only logged once).</p> required Example <pre><code>logger = logging.getLogger(__name__)\nwarn_once(logger, \"This feature is deprecated\")\nwarn_once(logger, \"This feature is deprecated\")  # Won't log again\n</code></pre> Source code in <code>optimus_dl/core/log.py</code> <pre><code>def warn_once(logger: logging.Logger, message: str) -&gt; None:\n    \"\"\"Log a warning message only once, even if called multiple times.\n\n    This is useful for deprecation warnings or other messages that should\n    only appear once per program execution, even if the code path is\n    executed multiple times.\n\n    Args:\n        logger: Logger instance to log the warning to.\n        message: Warning message to log (only logged once).\n\n    Example:\n        ```python\n        logger = logging.getLogger(__name__)\n        warn_once(logger, \"This feature is deprecated\")\n        warn_once(logger, \"This feature is deprecated\")  # Won't log again\n        ```\n    \"\"\"\n    if not hasattr(warn_once, \"logged_messages\"):\n        warn_once.logged_messages = set()\n    if message not in warn_once.logged_messages:\n        logger.warning(message)\n        warn_once.logged_messages.add(message)\n</code></pre>"},{"location":"reference/core/model_utils/","title":"model_utils","text":""},{"location":"reference/core/model_utils/#optimus_dl.core.model_utils","title":"<code>optimus_dl.core.model_utils</code>","text":"<p>Model utility functions.</p> <p>This module provides helper functions for working with PyTorch models, such as counting parameters, analyzing model structure, and other common operations.</p>"},{"location":"reference/core/model_utils/#optimus_dl.core.model_utils.get_num_parameters","title":"<code>get_num_parameters(model)</code>","text":"<p>Count the total number of parameters in a model.</p> <p>This function counts all parameters in the model, including both trainable and non-trainable parameters. It uses a set to handle cases where parameters might be shared across modules (though this is rare in practice).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>PyTorch model to count parameters for.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total number of parameters (sum of all parameter tensor sizes).</p> Example <pre><code>model = Llama(LlamaConfig(n_embd=512, n_head=8, n_layer=12))\nnum_params = get_num_parameters(model)\nprint(f\"Model has {num_params:,} parameters\")\n# Model has 123,456,789 parameters\n</code></pre> Source code in <code>optimus_dl/core/model_utils.py</code> <pre><code>def get_num_parameters(model: torch.nn.Module) -&gt; int:\n    \"\"\"Count the total number of parameters in a model.\n\n    This function counts all parameters in the model, including both trainable\n    and non-trainable parameters. It uses a set to handle cases where parameters\n    might be shared across modules (though this is rare in practice).\n\n    Args:\n        model: PyTorch model to count parameters for.\n\n    Returns:\n        Total number of parameters (sum of all parameter tensor sizes).\n\n    Example:\n        ```python\n        model = Llama(LlamaConfig(n_embd=512, n_head=8, n_layer=12))\n        num_params = get_num_parameters(model)\n        print(f\"Model has {num_params:,} parameters\")\n        # Model has 123,456,789 parameters\n        ```\n    \"\"\"\n    params = set()\n    for param in model.parameters():\n        params.add(param)\n    return sum(param.numel() for param in params)\n</code></pre>"},{"location":"reference/core/omegaconf/","title":"omegaconf","text":""},{"location":"reference/core/omegaconf/#optimus_dl.core.omegaconf","title":"<code>optimus_dl.core.omegaconf</code>","text":"<p>OmegaConf custom resolvers for configuration.</p> <p>This module registers custom resolvers for OmegaConf that enable advanced configuration features like Python expression evaluation and environment variable access.</p>"},{"location":"reference/core/omegaconf/#optimus_dl.core.omegaconf.conf_hash_resolver","title":"<code>conf_hash_resolver(*args, _root_)</code>","text":"<p>Resolver for computing hash of a root config.</p> Source code in <code>optimus_dl/core/omegaconf.py</code> <pre><code>def conf_hash_resolver(*args, _root_):\n    \"\"\"Resolver for computing hash of a root config.\"\"\"\n    max_len = 16\n    if len(args) &gt; 0:\n        assert len(args) == 1, \"Only one argument is allowed\"\n        max_len = int(args[0])\n    return hash_resolver(_root_, max_len=max_len)\n</code></pre>"},{"location":"reference/core/omegaconf/#optimus_dl.core.omegaconf.hash_resolver","title":"<code>hash_resolver(x, max_len=16)</code>","text":"<p>Resolver for computing hash of a value repr.</p> Source code in <code>optimus_dl/core/omegaconf.py</code> <pre><code>def hash_resolver(x, max_len=16):\n    \"\"\"Resolver for computing hash of a value repr.\"\"\"\n    x = repr(x)\n    return hashlib.sha256(x.encode(\"utf-8\")).hexdigest()[:max_len]\n</code></pre>"},{"location":"reference/core/profile/","title":"profile","text":""},{"location":"reference/core/profile/#optimus_dl.core.profile","title":"<code>optimus_dl.core.profile</code>","text":"<p>Performance profiling utilities.</p> <p>This module provides functions for measuring execution time of code blocks, iterators, and function calls. Supports both CPU timing (using perf_counter) and GPU timing (using CUDA events for accurate GPU kernel timing).</p>"},{"location":"reference/core/profile/#optimus_dl.core.profile.measured_iter","title":"<code>measured_iter(itr)</code>","text":"<p>Measure time between iterations of an iterator.</p> <p>Yields tuples of (elapsed_time_ms, element) for each element in the iterator. The elapsed time is measured from the start of one iteration to the start of the next, providing per-iteration timing.</p> <p>Parameters:</p> Name Type Description Default <code>itr</code> <code>Iterator[T]</code> <p>Iterator to measure.</p> required <p>Yields:</p> Type Description <code>float</code> <p>Tuples of (elapsed_time_ms, element) where elapsed_time_ms is the</p> <code>T</code> <p>time in milliseconds since the previous iteration.</p> Example <pre><code>data = [1, 2, 3, 4, 5]\nfor elapsed, item in measured_iter(iter(data)):\n    print(f\"Item {item} took {elapsed:.2f}ms\")\n</code></pre> Source code in <code>optimus_dl/core/profile.py</code> <pre><code>def measured_iter(itr: Iterator[T]) -&gt; Iterator[tuple[float, T]]:\n    \"\"\"Measure time between iterations of an iterator.\n\n    Yields tuples of (elapsed_time_ms, element) for each element in the iterator.\n    The elapsed time is measured from the start of one iteration to the start\n    of the next, providing per-iteration timing.\n\n    Args:\n        itr: Iterator to measure.\n\n    Yields:\n        Tuples of (elapsed_time_ms, element) where elapsed_time_ms is the\n        time in milliseconds since the previous iteration.\n\n    Example:\n        ```python\n        data = [1, 2, 3, 4, 5]\n        for elapsed, item in measured_iter(iter(data)):\n            print(f\"Item {item} took {elapsed:.2f}ms\")\n        ```\n    \"\"\"\n    start = time.perf_counter_ns()\n    for elem in itr:\n        elapsed = (time.perf_counter_ns() - start) / 1e6  # Convert to milliseconds\n        yield elapsed, elem\n        start = time.perf_counter_ns()\n</code></pre>"},{"location":"reference/core/profile/#optimus_dl.core.profile.measured_lambda","title":"<code>measured_lambda(f, cuda_events=False, enabled=True)</code>","text":"<p>Measure execution time of a callable function.</p> <p>Supports both CPU timing (using perf_counter) and GPU timing (using CUDA events). CUDA events provide more accurate timing for GPU operations as they measure actual GPU kernel execution time rather than wall-clock time.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[], T]</code> <p>Callable function to measure (takes no arguments).</p> required <code>cuda_events</code> <code>bool</code> <p>If True, use CUDA events for timing (more accurate for GPU operations). If False, use CPU perf_counter. Requires CUDA to be available.</p> <code>False</code> <code>enabled</code> <code>bool</code> <p>If False, skip timing and return (0, result). Useful for disabling profiling in production code.</p> <code>True</code> <p>Returns:</p> Type Description <code>float</code> <p>Tuple of (elapsed_time_ms, result) where:</p> <code>T</code> <ul> <li>elapsed_time_ms: Execution time in milliseconds</li> </ul> <code>tuple[float, T]</code> <ul> <li>result: Return value of the function call</li> </ul> Example <pre><code>def forward_pass():\n    return model(input_ids)\n\n# CPU timing\nelapsed, output = measured_lambda(forward_pass, cuda_events=False)\n\n# GPU timing (more accurate for GPU ops)\nelapsed, output = measured_lambda(forward_pass, cuda_events=True)\n</code></pre> Source code in <code>optimus_dl/core/profile.py</code> <pre><code>def measured_lambda(\n    f: Callable[[], T],\n    cuda_events: bool = False,\n    enabled: bool = True,\n) -&gt; tuple[float, T]:\n    \"\"\"Measure execution time of a callable function.\n\n    Supports both CPU timing (using perf_counter) and GPU timing (using CUDA\n    events). CUDA events provide more accurate timing for GPU operations as\n    they measure actual GPU kernel execution time rather than wall-clock time.\n\n    Args:\n        f: Callable function to measure (takes no arguments).\n        cuda_events: If True, use CUDA events for timing (more accurate for\n            GPU operations). If False, use CPU perf_counter. Requires CUDA\n            to be available.\n        enabled: If False, skip timing and return (0, result). Useful for\n            disabling profiling in production code.\n\n    Returns:\n        Tuple of (elapsed_time_ms, result) where:\n\n        - elapsed_time_ms: Execution time in milliseconds\n        - result: Return value of the function call\n\n    Example:\n        ```python\n        def forward_pass():\n            return model(input_ids)\n\n        # CPU timing\n        elapsed, output = measured_lambda(forward_pass, cuda_events=False)\n\n        # GPU timing (more accurate for GPU ops)\n        elapsed, output = measured_lambda(forward_pass, cuda_events=True)\n        ```\n    \"\"\"\n    if not enabled:\n        return 0, f()\n    if cuda_events:\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        start.record()\n        elem = f()\n        end.record()\n        end.synchronize()\n        elapsed = start.elapsed_time(end)\n        return elapsed, elem\n    else:\n        start = time.perf_counter_ns()\n        elem = f()\n        elapsed = (time.perf_counter_ns() - start) / 1e6  # Convert to milliseconds\n        return elapsed, elem\n</code></pre>"},{"location":"reference/core/profile/#optimus_dl.core.profile.measured_next","title":"<code>measured_next(itr)</code>","text":"<p>Measure time to get the next element from an iterator.</p> <p>This is useful for measuring data loading time, as it measures the time to fetch a single batch from a data iterator.</p> <p>Parameters:</p> Name Type Description Default <code>itr</code> <code>Iterator[T]</code> <p>Iterator to get next element from.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Tuple of (elapsed_time_ms, element) where elapsed_time_ms is the</p> <code>T</code> <p>time in milliseconds to get the next element.</p> Example <pre><code>data_iter = iter(dataloader)\nelapsed, batch = measured_next(data_iter)\nprint(f\"Data loading took {elapsed:.2f}ms\")\n</code></pre> Source code in <code>optimus_dl/core/profile.py</code> <pre><code>def measured_next(itr: Iterator[T]) -&gt; tuple[float, T]:\n    \"\"\"Measure time to get the next element from an iterator.\n\n    This is useful for measuring data loading time, as it measures the time\n    to fetch a single batch from a data iterator.\n\n    Args:\n        itr: Iterator to get next element from.\n\n    Returns:\n        Tuple of (elapsed_time_ms, element) where elapsed_time_ms is the\n        time in milliseconds to get the next element.\n\n    Example:\n        ```python\n        data_iter = iter(dataloader)\n        elapsed, batch = measured_next(data_iter)\n        print(f\"Data loading took {elapsed:.2f}ms\")\n        ```\n    \"\"\"\n    start = time.perf_counter_ns()\n    elem = next(itr)\n    elapsed = (time.perf_counter_ns() - start) / 1e6  # Convert to milliseconds\n    return elapsed, elem\n</code></pre>"},{"location":"reference/core/registry/","title":"registry","text":""},{"location":"reference/core/registry/#optimus_dl.core.registry","title":"<code>optimus_dl.core.registry</code>","text":"<p>Registry system for dependency injection and component management.</p> <p>This module provides a flexible registry pattern that allows components (models, optimizers, data loaders, etc.) to be registered and instantiated from configuration. The registry system is the foundation of Optimus-DL's modular architecture, enabling easy component swapping and configuration-driven instantiation.</p> Example <p>Basic usage:</p> <pre><code># Create a registry\nregistry, register, build = make_registry(\"my_component\")\n\n# Register a component\n@register(\"my_impl\", MyConfig)\nclass MyImplementation:\n    def __init__(self, cfg: MyConfig):\n        self.cfg = cfg\n\n# Build from config\nconfig = MyConfig(_name=\"my_impl\", param=1)\nobj = build(config)\n</code></pre>"},{"location":"reference/core/registry/#optimus_dl.core.registry.RegistryConfig","title":"<code>RegistryConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>dict[str, Any]</code></p> <p>Flexible configuration base class for registry components.</p> <p>This extends RegistryConfigStrict to allow arbitrary additional fields via dictionary inheritance. Use this when you need custom configuration parameters beyond static fields or for dynamic configurations.</p> <p>Attributes:</p> Name Type Description <code>_name</code> <code>str | None</code> <p>The registered name of the component to instantiate.</p> <code>extra_fields</code> <code>str | None</code> <p>Additional fields can be added as dictionary keys.</p> Source code in <code>optimus_dl/core/registry.py</code> <pre><code>@dataclass\nclass RegistryConfig(dict[str, Any]):\n    \"\"\"Flexible configuration base class for registry components.\n\n    This extends RegistryConfigStrict to allow arbitrary additional fields\n    via dictionary inheritance. Use this when you need custom configuration\n    parameters beyond static fields or for dynamic configurations.\n\n    Attributes:\n        _name: The registered name of the component to instantiate.\n        extra_fields: Additional fields can be added as dictionary keys.\n    \"\"\"\n\n    _name: str | None = None\n</code></pre>"},{"location":"reference/core/registry/#optimus_dl.core.registry.RegistryConfigStrict","title":"<code>RegistryConfigStrict</code>  <code>dataclass</code>","text":"<p>Strict configuration base class for registry components.</p> <p>This is a minimal configuration class that only requires a <code>_name</code> field. Use this when you don't need additional configuration fields.</p> <p>Attributes:</p> Name Type Description <code>_name</code> <code>str | None</code> <p>The registered name of the component to instantiate.</p> Source code in <code>optimus_dl/core/registry.py</code> <pre><code>@dataclass\nclass RegistryConfigStrict:\n    \"\"\"Strict configuration base class for registry components.\n\n    This is a minimal configuration class that only requires a `_name` field.\n    Use this when you don't need additional configuration fields.\n\n    Attributes:\n        _name: The registered name of the component to instantiate.\n    \"\"\"\n\n    _name: str | None = None\n</code></pre>"},{"location":"reference/core/registry/#optimus_dl.core.registry.build","title":"<code>build(registry_name, cfg, cast_to=None, **kwargs)</code>","text":"<pre><code>build(registry_name: str, cfg: CorrectCfg, cast_to: type[T], **kwargs: Any) -&gt; T\n</code></pre><pre><code>build(registry_name: str, cfg: CorrectCfg, cast_to: None = None, **kwargs: Any) -&gt; Any\n</code></pre> <p>Build a component from a named registry.</p> <p>This is a convenience function that builds a component from a registry by name. It's useful when you know the registry name but don't have the registry functions directly available.</p> <p>Parameters:</p> Name Type Description Default <code>registry_name</code> <code>str</code> <p>Name of the registry to build from (e.g., \"model\", \"optimizer\").</p> required <code>cfg</code> <code>CorrectCfg | None</code> <p>Configuration object with <code>_name</code> field specifying the component.</p> required <code>cast_to</code> <code>type[T] | None</code> <p>Optional type to cast the result to.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the component constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T | Any | None</code> <p>An instance of the registered component.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the registry doesn't exist, the component name is not found, or if cast_to is provided and the object is not of that type.</p> Example <pre><code>config = RegistryConfig(_name=\"llama\", n_embd=512)\nmodel = build(\"model\", config, cast_to=BaseModel)\n</code></pre> Source code in <code>optimus_dl/core/registry.py</code> <pre><code>def build(\n    registry_name: str,\n    cfg: CorrectCfg | None,\n    cast_to: type[T] | None = None,\n    **kwargs: Any,\n) -&gt; T | Any | None:\n    \"\"\"Build a component from a named registry.\n\n    This is a convenience function that builds a component from a registry by name.\n    It's useful when you know the registry name but don't have the registry\n    functions directly available.\n\n    Args:\n        registry_name: Name of the registry to build from (e.g., \"model\", \"optimizer\").\n        cfg: Configuration object with `_name` field specifying the component.\n        cast_to: Optional type to cast the result to.\n        **kwargs: Additional arguments passed to the component constructor.\n\n    Returns:\n        An instance of the registered component.\n\n    Raises:\n        AssertionError: If the registry doesn't exist, the component name is not\n            found, or if cast_to is provided and the object is not of that type.\n\n    Example:\n        ```python\n        config = RegistryConfig(_name=\"llama\", n_embd=512)\n        model = build(\"model\", config, cast_to=BaseModel)\n\n        ```\"\"\"\n    assert registry_name in registries, f\"Unknown registry {registry_name}\"\n    _, _, build_fn = make_registry(registry_name)\n    obj = build_fn(cfg, **kwargs)\n    if cast_to is not None:\n        assert isinstance(obj, cast_to), f\"Expected {cast_to}, got {type(obj)}\"\n    return obj\n</code></pre>"},{"location":"reference/core/registry/#optimus_dl.core.registry.make_registry","title":"<code>make_registry(registry_name, base_class=None)</code>","text":"<p>Create or retrieve a component registry.</p> <p>This function creates a new registry or retrieves an existing one. Each registry maintains a mapping of component names to their classes and configuration classes. The registry pattern enables dependency injection and configuration-driven component instantiation.</p> <p>Parameters:</p> Name Type Description Default <code>registry_name</code> <code>str</code> <p>Unique name for the registry (e.g., \"model\", \"optimizer\").</p> required <code>base_class</code> <code>type | type[Any] | None</code> <p>Optional base class that all registered components must inherit from. Used for type checking. If None, any class can be registered.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>registry_dict</code> <code>dict</code> <p>The actual registry dictionary (for inspection/debugging)</p> <code>register</code> <code>callable</code> <p>Decorator function to register components</p> <code>build</code> <code>callable</code> <p>Function to build instances from configuration</p> Example <pre><code>registry, register, build = make_registry(\"model\", BaseModel)\n\n@register(\"llama\", LlamaConfig)\nclass Llama(BaseModel):\n    def __init__(self, cfg: LlamaConfig):\n        self.cfg = cfg\n\n# Later, build from config\nconfig = LlamaConfig(_name=\"llama\", n_embd=512)\nmodel = build(config)\n</code></pre> <p>Note:     The registry is stored globally. Multiple calls with the same <code>registry_name</code>     will return the same registry instance.</p> Source code in <code>optimus_dl/core/registry.py</code> <pre><code>def make_registry(registry_name: str, base_class: type | type[Any] | None = None):\n    \"\"\"Create or retrieve a component registry.\n\n    This function creates a new registry or retrieves an existing one. Each registry\n    maintains a mapping of component names to their classes and configuration classes.\n    The registry pattern enables dependency injection and configuration-driven\n    component instantiation.\n\n    Args:\n        registry_name: Unique name for the registry (e.g., \"model\", \"optimizer\").\n        base_class: Optional base class that all registered components must inherit\n            from. Used for type checking. If None, any class can be registered.\n\n    Returns:\n        registry_dict (dict): The actual registry dictionary (for inspection/debugging)\n        register (callable): Decorator function to register components\n        build (callable): Function to build instances from configuration\n\n    Example:\n        ```python\n        registry, register, build = make_registry(\"model\", BaseModel)\n\n        @register(\"llama\", LlamaConfig)\n        class Llama(BaseModel):\n            def __init__(self, cfg: LlamaConfig):\n                self.cfg = cfg\n\n        # Later, build from config\n        config = LlamaConfig(_name=\"llama\", n_embd=512)\n        model = build(config)\n\n        ```\n    Note:\n        The registry is stored globally. Multiple calls with the same `registry_name`\n        will return the same registry instance.\n    \"\"\"\n    if registry_name in registries:\n        registry = registries[registry_name]\n    else:\n        registries[registry_name] = {}\n        registry = registries[registry_name]\n\n    def register_arch(name: str, class_name: str, registered_class: type):\n        \"\"\"Register an architecture variant of a base class.\n\n        This allows registering multiple variants (architectures) of a base class.\n        For example, you might have a base \"llama\" model with variants like\n        \"llama-7b\", \"llama-13b\", \"llama-70b\".\n\n        Args:\n            name: Name of the architecture variant.\n            class_name: Name of the base class this variant belongs to.\n            registered_class: The class implementing this architecture.\n\n        Returns:\n            A decorator function that takes a config factory method.\n\n        Raises:\n            AssertionError: If the architecture is already registered or if the\n                base class doesn't exist or doesn't have a structured config.\n\n        Example:\n            ```python\n            @register(\"llama\", LlamaConfig)\n            class Llama(BaseModel):\n                pass\n\n            @Llama.register_arch(\"7b\")\n            def llama_7b_config():\n                return LlamaConfig(n_layers=32, n_embd=4096)\n\n            ```\"\"\"\n        full_name = f\"{class_name}-{name}\"\n\n        assert (\n            full_name not in registry\n        ), f\"Double registering of {full_name} in {registry_name} registry\"\n\n        base_registry = registry.get(class_name, None)\n        assert (\n            base_registry is not None\n        ), f\"Base class {class_name} not found in {registry_name} registry\"\n        assert (\n            base_registry[1] is not None\n        ), f\"Base class {class_name} must have a structured config in {registry_name} registry to register architectures\"\n\n        def wrapper(method):\n            cfg = method()\n            assert (\n                cfg is None\n                or isinstance(cfg, RegistryConfig)\n                or isinstance(cfg, RegistryConfigStrict)\n            ), \"Configs must be subclasses of RegistryConfig\"\n            registry[full_name] = (registered_class, cfg)\n            return method\n\n        return wrapper\n\n    def register(name: str, cfg: type | None = None):\n        \"\"\"Register a component in the registry.\n\n        This is the main decorator for registering components. It associates a\n        component class with a name and optional configuration class.\n\n        Args:\n            name: Unique name to register the component under (e.g., \"llama\", \"adamw\").\n            cfg: Optional configuration class that must be a subclass of\n                RegistryConfig or RegistryConfigStrict. If None, the component\n                will be instantiated without configuration.\n\n        Returns:\n            A decorator function that registers the decorated class.\n\n        Raises:\n            AssertionError: If the name is already registered or if cfg is not\n                a valid config class.\n\n        Example:\n            ```python\n            @register(\"my_model\", MyModelConfig)\n            class MyModel:\n                def __init__(self, cfg: MyModelConfig):\n                    self.cfg = cfg\n\n            ```\"\"\"\n        assert (\n            name not in registry\n        ), f\"Double registering of {name} in {registry_name} registry\"\n        assert (\n            cfg is None\n            or issubclass(cfg, RegistryConfig)\n            or issubclass(cfg, RegistryConfigStrict)\n        ), \"Configs must be subclasses of RegistryConfig or RegistryConfigStrict\"\n\n        def wrapper(registered_class: C) -&gt; C:\n            \"\"\"Decorator that registers a class in the registry.\n\n            Args:\n                registered_class: The class to register.\n\n            Returns:\n                The same class (for use as a decorator).\n            \"\"\"\n            registry[name] = (registered_class, cfg)\n            registered_class.register_arch = functools.partial(\n                register_arch,\n                class_name=name,\n                registered_class=registered_class,\n            )\n            return registered_class\n\n        return wrapper\n\n    @overload\n    def build(\n        cfg: CorrectCfg,\n        cast_to: type[T],\n        **kwargs: Any,\n    ) -&gt; T: ...\n\n    @overload\n    def build(\n        cfg: CorrectCfg,\n        cast_to: None = None,\n        **kwargs: Any,\n    ) -&gt; Any: ...\n\n    def build(\n        cfg: CorrectCfg | None,\n        cast_to: type[T] | None = None,\n        **kwargs: Any,\n    ) -&gt; T | Any | None:\n        \"\"\"Build a component instance from configuration.\n\n        This function instantiates a registered component based on its configuration.\n        It handles merging default configuration values with provided overrides,\n        and supports both structured configs and plain dictionaries.\n\n        Args:\n            cfg: Configuration object containing `_name` field specifying which\n                component to build. Can be a RegistryConfig, dict, or None.\n                If None, returns None. If a string, treats it as the component name.\n            cast_to: Optional type to cast the result to. If provided, raises\n                AssertionError if the built object is not an instance of this type.\n            **kwargs: Additional keyword arguments passed to the component constructor.\n\n        Returns:\n            An instance of the registered component, or None if cfg is None.\n\n        Raises:\n            AssertionError: If the component name is not found in the registry, or\n                if cast_to is provided and the built object is not of that type.\n\n        Example:\n            ```python\n            config = RegistryConfig(_name=\"llama\", n_embd=512)\n            model = build(config, cast_to=BaseModel)\n            assert isinstance(model, Llama)\n\n            ```\"\"\"\n        if cfg is None:\n            return None\n        cfg_orig = cfg\n        if isinstance(cfg, str):\n            name: str = cfg\n            cfg = {}\n        else:\n            if not omegaconf.OmegaConf.is_config(cfg):\n                cfg = omegaconf.OmegaConf.structured(cfg)\n            name: str = cfg[\"_name\"]\n        assert name in registry, f\"Unknown {name} in {registry_name} registry\"\n        registered_class, structured_cfg = registry[name]\n        structured_cfg_original = structured_cfg\n        is_strict = False\n        if type(structured_cfg) is type:\n            is_strict = issubclass(structured_cfg, RegistryConfigStrict)\n            structured_cfg = omegaconf.OmegaConf.merge(\n                omegaconf.OmegaConf.structured(structured_cfg), structured_cfg()\n            )\n        if structured_cfg is not None:\n            if is_strict:\n                expected_keys = set(structured_cfg.keys())\n                try:\n                    actual_keys = set(cfg.keys())\n                except AttributeError as e:\n                    raise ValueError(\n                        f\"Cannot get true keys for config {type(cfg)}: {cfg}\"\n                    ) from e\n\n                maybe_path = \".\".join(_get_cfg_path(cfg_orig) or [\"&lt;root&gt;\"])\n                assert actual_keys.issubset(expected_keys), (\n                    f\"For {maybe_path} {structured_cfg_original} expected keys {expected_keys}, \"\n                    f\"got {actual_keys},\\n\"\n                    f\"diff: {actual_keys - expected_keys}\"\n                )\n            try:\n                cfg = omegaconf.OmegaConf.merge(\n                    structured_cfg,\n                    omegaconf.OmegaConf.to_container(cfg=cfg, resolve=True),\n                )\n            except omegaconf.errors.ConfigTypeError:\n                logger.error(f\"{structured_cfg = }\\n====\\n{cfg = }\")\n                raise\n            obj = registered_class(cfg, **kwargs)\n        else:\n            obj = registered_class(**kwargs)\n\n        if cast_to is not None:\n            assert isinstance(obj, cast_to), f\"Expected {cast_to}, got {type(obj)}\"\n        if base_class is not None:\n            assert isinstance(\n                obj, base_class\n            ), f\"Expected {base_class}, got {type(obj)}\"\n        return obj\n\n    return registry, register, build\n</code></pre>"},{"location":"reference/core/seed/","title":"seed","text":""},{"location":"reference/core/seed/#optimus_dl.core.seed","title":"<code>optimus_dl.core.seed</code>","text":"<p>Utilities for setting random seeds for reproducibility.</p>"},{"location":"reference/core/seed/#optimus_dl.core.seed.set_seed","title":"<code>set_seed(seed, deterministic=False)</code>","text":"<p>Set random seeds for reproducibility across different libraries.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>The integer seed to set.</p> required <code>deterministic</code> <code>bool</code> <p>If True, makes CUDA operations deterministic. Note: This can sometimes come with a performance penalty.</p> <code>False</code> Source code in <code>optimus_dl/core/seed.py</code> <pre><code>def set_seed(seed: int, deterministic: bool = False) -&gt; None:\n    \"\"\"Set random seeds for reproducibility across different libraries.\n\n    Args:\n        seed: The integer seed to set.\n        deterministic: If True, makes CUDA operations deterministic.\n            Note: This can sometimes come with a performance penalty.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    if deterministic:\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\n\n        # Force determinism in PyTorch operations\n        torch.use_deterministic_algorithms(True, warn_only=True)\n</code></pre>"},{"location":"reference/modules/","title":"Index","text":""},{"location":"reference/modules/#optimus_dl.modules","title":"<code>optimus_dl.modules</code>","text":""},{"location":"reference/modules/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>checkpoint</code>: </li> <li><code>criterion</code>: </li> <li><code>data</code>: </li> <li><code>distributed</code>: </li> <li><code>eval</code>: </li> <li><code>loggers</code>: </li> <li><code>lr_scheduler</code>: </li> <li><code>metrics</code>: </li> <li><code>model</code>: </li> <li><code>model_transforms</code>: </li> <li><code>optim</code>: </li> <li><code>tokenizer</code>: </li> </ul>"},{"location":"reference/modules/checkpoint/","title":"Index","text":""},{"location":"reference/modules/checkpoint/#optimus_dl.modules.checkpoint","title":"<code>optimus_dl.modules.checkpoint</code>","text":""},{"location":"reference/modules/checkpoint/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>checkpoint_manager</code>: Checkpoint management system for distributed training.</li> <li><code>load_strategy</code>: Checkpoint loading strategy configuration.</li> </ul>"},{"location":"reference/modules/checkpoint/checkpoint_manager/","title":"checkpoint_manager","text":""},{"location":"reference/modules/checkpoint/checkpoint_manager/#optimus_dl.modules.checkpoint.checkpoint_manager","title":"<code>optimus_dl.modules.checkpoint.checkpoint_manager</code>","text":"<p>Checkpoint management system for distributed training.</p> <p>This module provides the CheckpointManager which handles saving and loading sharded model and optimizer states using PyTorch's Distributed Checkpoint (DCP) API. It also manages metadata, learning rate scheduler states, and data loader positions.</p>"},{"location":"reference/modules/checkpoint/checkpoint_manager/#optimus_dl.modules.checkpoint.checkpoint_manager.CheckpointManager","title":"<code>CheckpointManager</code>","text":"<p>Manages saving and loading of distributed checkpoints.</p> <p>This class provides high-level orchestration for training checkpoints. It integrates with PyTorch DCP for efficient sharded I/O and handles the complexity of synchronizing metadata and per-rank states (like dataloaders).</p> Source code in <code>optimus_dl/modules/checkpoint/checkpoint_manager.py</code> <pre><code>class CheckpointManager:\n    \"\"\"Manages saving and loading of distributed checkpoints.\n\n    This class provides high-level orchestration for training checkpoints. It\n    integrates with PyTorch DCP for efficient sharded I/O and handles the\n    complexity of synchronizing metadata and per-rank states (like dataloaders).\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: CheckpointManagerConfig,\n        **kwargs: Any,\n    ):\n        \"\"\"Initialize CheckpointManager.\n\n        Args:\n            cfg: Configuration object.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        self.cfg = cfg\n\n    def is_restart(self, checkpoint_path):\n        \"\"\"Check if a checkpoint exists in the given directory.\n        Such case corresponds to the case when run was started, stopped and then restarted again.\n\n        Args:\n            checkpoint_path: Path for output checkpoints\n        \"\"\"\n        return self.get_checkpoint(checkpoint_path) is not None\n\n    def get_checkpoint(self, path: str | pathlib.Path) -&gt; CheckpointPath | None:\n        \"\"\"Get a checkpoint from a path.\n        Path can be a directory, then the latest checkpoint is selected or a specific checkpoint directory or metadata file.\n        \"\"\"\n        if not isinstance(path, pathlib.Path):\n            path = pathlib.Path(str(path))\n\n        path = path.expanduser().resolve()\n\n        if not path.exists():\n            return None\n\n        if path.name.startswith(\"checkpoint_\"):\n            # this is exact checkpoint directory\n            metadata_name = path.name.replace(\"checkpoint_\", \"metadata_\") + \".pt\"\n            metadata_path = path.parent / metadata_name\n            if metadata_path.exists():\n                return CheckpointPath(metadata=str(metadata_path), checkpoint=str(path))\n\n        if (\n            path.name.startswith(\"metadata_\")\n            and path.name.endswith(\".pt\")\n            and path.is_file()\n        ):\n            # this is exact metadata file\n            checkpoint_name = pathlib.Path(\n                path.name.replace(\"metadata_\", \"checkpoint_\")\n            ).stem\n            checkpoint_path = path.parent / checkpoint_name\n            if checkpoint_path.exists():\n                return CheckpointPath(\n                    metadata=str(path), checkpoint=str(checkpoint_path)\n                )\n\n            # maybe not DCP?\n            checkpoint_path = path.parent / (checkpoint_name + \".pt\")\n            if checkpoint_path.exists():\n                return CheckpointPath(\n                    metadata=str(path), checkpoint=str(checkpoint_path)\n                )\n\n        # this is a directory, find latest checkpoint\n        if path.is_dir():\n            latest_checkpoint = path / \"metadata_latest.pt\"\n            return self.get_checkpoint(latest_checkpoint)\n        else:\n            return None\n\n    def load_checkpoint_if_exists(\n        self,\n        checkpoint_path: str,\n        model: BaseModel,\n        collective: Collective,\n        optimizer: Optimizer | None = None,\n        lr_scheduler: BaseLRScheduler | None = None,\n        data_loaders: dict | None = None,\n        load_strategy: LoadStrategy | None = None,\n        **kwargs: Any,\n    ) -&gt; tuple[int, dict | None]:\n        \"\"\"Attempt to find and load the latest checkpoint from a directory.\n\n        Args:\n            checkpoint_path: Directory to search for checkpoints.\n            model: Model to load weights into.\n            optimizer: Optional optimizer to restore state.\n            collective: Collective for distributed coordination.\n            lr_scheduler: Optional LR scheduler to restore.\n            data_loaders: Optional dict of dataloaders to restore state.\n            load_strategy: Strategy defining what components to load.\n            **kwargs: Passed to load_checkpoint.\n\n        Returns:\n            Tuple of (start_iteration, metadata). start_iteration defaults to 1 if no\n            checkpoint is found.\n        \"\"\"\n        latest_checkpoint = self.get_checkpoint(checkpoint_path)\n        if not latest_checkpoint:\n            return 1, None\n\n        try:\n            metadata = self.load_checkpoint(\n                checkpoint_path=latest_checkpoint.checkpoint,\n                model=model,\n                optimizer=optimizer,\n                lr_scheduler=lr_scheduler,\n                data_loaders=data_loaders,\n                collective=collective,\n                load_strategy=load_strategy,\n                **kwargs,\n            )\n            start_iteration = metadata[\"iteration\"] + 1\n            logger.info(f\"Starting with iteration = {start_iteration}\")\n            return start_iteration, metadata\n        except Exception as e:\n            logger.warning(f\"Failed to load checkpoint: {e}\")\n            raise\n\n    def save_checkpoint_if_needed(\n        self,\n        iteration: int,\n        collective: Collective,\n        checkpoint_path: str | Path,\n        save_freq: int,\n        **kwargs: Any,\n    ) -&gt; bool:\n        \"\"\"Save checkpoint if iteration matches save_freq.\"\"\"\n        if save_freq &lt;= 0 or iteration % save_freq != 0:\n            return False\n\n        try:\n            self.save_checkpoint(\n                checkpoint_path=checkpoint_path,\n                iteration=iteration,\n                collective=collective,\n                **kwargs,\n            )\n            return True\n        except Exception as e:\n            logger.error(f\"Checkpoint saving failed: {e}\")\n            raise\n\n    def save_checkpoint(\n        self,\n        checkpoint_path: str | Path,\n        model: BaseModel,\n        optimizer: Optimizer | None,\n        collective: Collective,\n        full_config: Any,\n        lr_scheduler=None,\n        iteration: int = 0,\n        data_loaders: dict | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Save training checkpoint using distributed checkpoint API.\n\n        Args:\n            checkpoint_path: Directory to save checkpoint\n            model: Model to save\n            optimizer: Optimizer to save\n            collective: Collective for distributed operations\n            full_config: Full configuration object for metadata\n            lr_scheduler: Optional LR scheduler to save\n            iteration: Current training iteration\n            data_loaders: Optional data loaders to save state\n            **kwargs: Additional metadata to save\n        \"\"\"\n        if not isinstance(checkpoint_path, Path):\n            checkpoint_path = Path(checkpoint_path)\n        checkpoint_path.mkdir(parents=True, exist_ok=True)\n\n        logger.info(f\"Saving state for model and optimizer at iteration {iteration}\")\n        model_state_dict = dcp_state_dict.get_model_state_dict(\n            model, options=dcp_state_dict.StateDictOptions()\n        )\n\n        state_dict = {\n            \"model\": model_state_dict,\n        }\n        if optimizer is not None:\n            state_dict[\"optimizer\"] = dcp_state_dict.get_optimizer_state_dict(\n                model, optimizer, options=dcp_state_dict.StateDictOptions()\n            )\n\n        # Add metadata\n        kwargs_states = {}\n        for key, value in kwargs.items():\n            kwargs_states[key] = value\n            if hasattr(value, \"state_dict\"):\n                logger.info(f\"Saving state for {key}\")\n                kwargs_states[key] = value.state_dict()\n            else:\n                logger.error(\n                    f\"Could not save state for {key} ({value}) as no state_dict() method found\"\n                )\n        metadata = {\n            \"iteration\": iteration,\n            \"config\": full_config,\n            \"world_size\": collective.world_size,\n        }\n\n        if lr_scheduler is not None:\n            logger.info(\"Saving lr_scheduler\")\n            metadata[\"lr_scheduler\"] = lr_scheduler.state_dict()\n\n        # Save using distributed checkpoint API\n        checkpoint_id = str(checkpoint_path / f\"checkpoint_{iteration:09d}\")\n        dcp_save(\n            state_dict=state_dict,\n            storage_writer=FileSystemWriter(checkpoint_id),\n            process_group=collective.process_group,\n        )\n\n        metadata_path = None\n        if collective.is_master:\n            # Save metadata separately\n            metadata_path = checkpoint_path / f\"metadata_{iteration:09d}.pt\"\n            torch.save(metadata, metadata_path)\n            logger.info(f\"Checkpoint saved to {checkpoint_id} / {metadata_path}\")\n\n        assert (\n            \"data_loaders\" not in kwargs_states\n        ), \"Data loaders should be passed separately\"\n        assert \"metrics\" not in kwargs_states, \"Metrics should be passed separately\"\n        logger.info(\"Saving data loaders and metrics\")\n\n        from optimus_dl.modules.metrics import (\n            state_dict as metrics_state_dict,\n        )\n\n        per_rank_metadata = {\n            \"data_loaders\": {\n                k: v.state_dict() for k, v in (data_loaders or {}).items()\n            },\n            \"metrics\": metrics_state_dict(),\n            **kwargs_states,\n        }\n\n        # Save per-rank metadata\n        rank = collective.rank\n        per_rank_metadata_path = (\n            checkpoint_path / f\"per_rank_metadata_{rank}_{iteration:09d}.pt\"\n        )\n        torch.save(per_rank_metadata, per_rank_metadata_path)\n\n        # Create symlink to latest\n        if collective.is_master:\n            latest_checkpoint = checkpoint_path / \"checkpoint_latest\"\n            latest_metadata = checkpoint_path / \"metadata_latest.pt\"\n\n            if latest_checkpoint.exists() or latest_checkpoint.is_symlink():\n                latest_checkpoint.unlink()\n            if latest_metadata.exists():\n                latest_metadata.unlink()\n\n            latest_checkpoint.symlink_to(f\"checkpoint_{iteration:09d}\")\n            latest_metadata.symlink_to(f\"metadata_{iteration:09d}.pt\")\n\n        logger.info(\n            f\"Checkpoint saved successfully, {checkpoint_id}, {per_rank_metadata_path}, {metadata_path}\"\n        )\n        logger.info(\n            f\"{per_rank_metadata.keys() = } {metadata.keys() = } {state_dict.keys() = }\"\n        )\n\n    def load_checkpoint(\n        self,\n        checkpoint_path: str | Path,\n        model: BaseModel | None,\n        optimizer: Optimizer | None,\n        collective: Collective,\n        lr_scheduler=None,\n        data_loaders: dict | None = None,\n        data_sources=None,\n        load_strategy: LoadStrategy | None = None,\n        **kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Load training checkpoint using distributed checkpoint API.\"\"\"\n        load_strategy = load_strategy or LoadStrategy()\n        checkpoint = self.get_checkpoint(checkpoint_path)\n\n        if checkpoint is None:\n            raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n\n        logger.info(f\"Loading checkpoint with restore strategy {load_strategy}\")\n\n        if not load_strategy.load_model:\n            model = None\n            if load_strategy.load_optimizer:\n                load_strategy.load_optimizer = False\n                logger.warning(\"Not restoring optimizer as model is not loaded\")\n\n        if not load_strategy.load_optimizer:\n            optimizer = None\n\n        if not load_strategy.load_scheduler:\n            lr_scheduler = None\n\n        if load_strategy.load_data_sources and load_strategy.load_dataloaders:\n            data_sources = None\n            load_strategy.load_data_sources = False\n            logger.warning(\n                \"Not restoring data sources directly as they will be restored with dataloaders restoration\"\n            )\n        elif not load_strategy.load_data_sources:\n            data_sources = None\n            if load_strategy.load_dataloaders:\n                load_strategy.load_dataloaders = False\n                logger.warning(\n                    \"Not restoring dataloaders as data sources are not loaded\"\n                )\n\n        if not load_strategy.load_dataloaders:\n            data_loaders = None\n\n        logger.info(f\"Loading checkpoint from {checkpoint_path}\")\n\n        # Get state dicts for loading\n        state_dict = {}\n        if model is not None:\n            state_dict[\"model\"] = dcp_state_dict.get_model_state_dict(\n                model, options=dcp_state_dict.StateDictOptions()\n            )\n        else:\n            optimizer = None\n\n        if optimizer is not None:\n            state_dict[\"optimizer\"] = dcp_state_dict.get_optimizer_state_dict(\n                model, optimizer, options=dcp_state_dict.StateDictOptions()\n            )\n\n        # Load using distributed checkpoint API\n        if len(state_dict) &gt; 0:\n            dcp_load(\n                state_dict=state_dict,\n                storage_reader=FileSystemReader(checkpoint.checkpoint),\n                process_group=collective.process_group,\n            )\n\n        # Set the loaded state dicts\n        if model is not None:\n            dcp_state_dict.set_model_state_dict(\n                model, state_dict[\"model\"], options=dcp_state_dict.StateDictOptions()\n            )\n        if optimizer is not None:\n            dcp_state_dict.set_optimizer_state_dict(\n                model,\n                optimizer,\n                state_dict[\"optimizer\"],\n                options=dcp_state_dict.StateDictOptions(),\n            )\n\n        # Load metadata\n        if collective.is_master:\n            metadata = torch.load(\n                checkpoint.metadata, map_location=\"cpu\", weights_only=False\n            )\n            metadatas = [metadata]\n            collective.broadcast_objects(metadatas, source_rank=0)\n        else:\n            metadatas = [None]\n            collective.broadcast_objects(\n                metadatas, source_rank=0\n            )  # pyright: ignore[reportArgumentType]\n            metadata = metadatas[0]\n        assert metadata is not None, \"Metadata not loaded correctly\"\n\n        if lr_scheduler is not None and \"lr_scheduler\" in metadata:\n            lr_scheduler.load_state_dict(metadata[\"lr_scheduler\"])\n            logger.info(\"Restored lr_scheduler\")\n        else:\n            logger.info(\"Did not restore lr_scheduler\")\n\n        if not load_strategy.load_iteration:\n            metadata[\"iteration\"] = 0\n\n        iteration = metadata[\"iteration\"]\n\n        rank = collective.rank\n        per_rank_metadata_path = checkpoint.per_rank_metadata(rank)\n        per_rank_metadata = torch.load(\n            per_rank_metadata_path, map_location=\"cpu\", weights_only=False\n        )\n        for key in load_strategy.extra_ignore_keys or []:\n            if key in per_rank_metadata:\n                per_rank_metadata.pop(key)\n\n        data_loaders = data_loaders or {}\n        data_loaders_states = [per_rank_metadata.get(\"data_loaders\", {})]\n\n        # make sure all tp ranks have the same data_loaders_states\n        tp_world = collective.tp_world\n        tp_world.broadcast_objects(data_loaders_states, source_rank=0)\n        for k, v in data_loaders_states[0].items():\n            if k in data_loaders:\n                logger.info(f\"Restoring {k}\")\n                data_loaders[k].load_state_dict(v)\n            else:\n                logger.warning(f\"Data loader {k} not found in current configuration\")\n\n        if \"data_sources\" in per_rank_metadata and data_sources is not None:\n            sources_states = [per_rank_metadata[\"data_sources\"]]\n            tp_world.broadcast_objects(sources_states, source_rank=0)\n            data_sources.load_state_dict(sources_states[0])\n            logger.info(\n                \"Restoring data sources indipendently (without the full dataloader pipeline)\"\n            )\n\n        if \"metrics\" in per_rank_metadata and load_strategy.load_metrics:\n            from optimus_dl.modules.metrics import (\n                load_state_dict as metrics_load_state_dict,\n            )\n\n            metrics_load_state_dict(per_rank_metadata[\"metrics\"])\n            logger.info(\"Restoring metrics\")\n        else:\n            logger.info(\"Metrics not restored\")\n\n        for key, value in kwargs.items():\n            assert hasattr(\n                value, \"load_state_dict\"\n            ), f\"Do not how to restore {key} = {value}\"\n            if key not in per_rank_metadata:\n                logger.warning(f\"Not restoring {key} = {value} as no state found\")\n            value.load_state_dict(per_rank_metadata[key])\n\n        logger.info(f\"Checkpoint has {iteration = }\")\n        return metadata\n\n    def build_model_from_checkpoint(\n        self,\n        checkpoint_path: str | Path,\n        device: str | torch.device,\n        model_key=\"model\",\n        **kwargs: Any,\n    ) -&gt; tuple[BaseModel, dict]:\n        \"\"\"Build model and load from checkpoint.\n\n        Args:\n            checkpoint_path: Path to checkpoint directory or metadata file\n            device: Device to load model on\n            **kwargs: Additional arguments passed to model building\n\n        Returns:\n            Tuple of (model, config) where config is the training config from checkpoint\n        \"\"\"\n        checkpoint = self.get_checkpoint(checkpoint_path)\n        if checkpoint is None:\n            raise FileNotFoundError(f\"Metadata file not found: {checkpoint}\")\n\n        # Load metadata\n        metadata_path = checkpoint.metadata\n        metadata = torch.load(metadata_path, map_location=\"cpu\", weights_only=False)\n        config = metadata[\"config\"]\n\n        logger.info(f\"Loading model with config: {config[model_key]}\")\n\n        # Build model using the config\n        model = build(\"model\", config[model_key], **kwargs)\n        assert isinstance(model, BaseModel)\n\n        self.load_model_state_dict(model, checkpoint.checkpoint)\n\n        # Move model to device\n        model = model.to(device)\n        logger.info(f\"Loaded model from {checkpoint_path} on {device}\")\n        return model, config\n\n    def load_model_state_dict(self, model: BaseModel, checkpoint_path: str) -&gt; None:\n        \"\"\"Load model state dict from checkpoint, handling both DCP and regular checkpoints.\"\"\"\n        checkpoint = self.get_checkpoint(checkpoint_path)\n        if checkpoint is None:\n            if not Path(checkpoint_path).exists():\n                raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n            else:\n                checkpoint = Path(checkpoint_path)\n                is_dcp = Path(checkpoint_path).is_dir()\n        else:\n            is_dcp = checkpoint.is_dcp_checkpoint()\n\n        if is_dcp:\n            logger.info(f\"Detected DCP checkpoint: {checkpoint}\")\n            self._load_dcp_checkpoint(model, checkpoint)\n        else:\n            # Try to load regular PyTorch checkpoint\n            self._load_regular_checkpoint(model, checkpoint)\n\n    def _load_dcp_checkpoint(\n        self, model: BaseModel, checkpoint: CheckpointPath | Path\n    ) -&gt; None:\n        \"\"\"Convert and load DCP checkpoint using dcp_to_torch_save.\"\"\"\n        if isinstance(checkpoint, CheckpointPath):\n            assert checkpoint.is_dcp_checkpoint()\n            checkpoint_path = checkpoint.checkpoint\n        else:\n            assert isinstance(checkpoint, Path)\n            assert checkpoint.exists() and checkpoint.is_dir()\n            checkpoint_path = checkpoint\n\n        from torch.distributed.checkpoint.format_utils import dcp_to_torch_save\n\n        # Create temporary file for converted checkpoint\n        with tempfile.NamedTemporaryFile(suffix=\".pt\", delete=False) as tmp_file:\n            temp_path = tmp_file.name\n\n        try:\n            # Convert DCP checkpoint to regular torch format\n            dcp_to_torch_save(\n                dcp_checkpoint_dir=str(checkpoint_path),\n                torch_save_path=temp_path,\n            )\n\n            # Load the converted checkpoint\n            logger.info(f\"Loading converted checkpoint from: {temp_path}\")\n            state_dict = torch.load(temp_path, map_location=\"cpu\", weights_only=False)\n\n            if \"model\" in state_dict:\n                model.load_state_dict(state_dict[\"model\"], strict=True)\n                logger.info(\"Successfully loaded model weights from DCP checkpoint\")\n            else:\n                # Try to load the state dict directly if no \"model\" key\n                model.load_state_dict(state_dict, strict=True)\n                logger.info(\n                    \"Successfully loaded model weights from DCP checkpoint (direct)\"\n                )\n\n        finally:\n            # Clean up temporary file\n            if Path(temp_path).exists():\n                Path(temp_path).unlink()\n\n    def _load_regular_checkpoint(\n        self, model: BaseModel, checkpoint: CheckpointPath | Path\n    ) -&gt; None:\n        \"\"\"Load regular PyTorch checkpoint files.\"\"\"\n        if isinstance(checkpoint, CheckpointPath):\n            assert not checkpoint.is_dcp_checkpoint()\n            checkpoint_path = checkpoint.checkpoint\n        else:\n            assert isinstance(checkpoint, Path)\n            assert checkpoint.exists() and checkpoint.is_file()\n            checkpoint_path = checkpoint\n\n        # Look for regular .pt files\n        logger.info(f\"Attempting to load: {checkpoint}\")\n        state_dict = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=False)\n\n        if \"model\" in state_dict:\n            model.load_state_dict(state_dict[\"model\"], strict=False)\n            logger.info(f\"Loaded model weights from {checkpoint_path}\")\n            return\n        elif isinstance(state_dict, dict) and any(\n            key.startswith((\"module.\", \"model.\", \"_orig_mod.\")) or \".\" in key\n            for key in state_dict.keys()\n        ):\n            # This looks like a model state dict\n            model.load_state_dict(state_dict, strict=False)\n            logger.info(f\"Loaded model weights from {checkpoint_path} (direct)\")\n</code></pre>"},{"location":"reference/modules/checkpoint/checkpoint_manager/#optimus_dl.modules.checkpoint.checkpoint_manager.CheckpointManager.__init__","title":"<code>__init__(cfg, **kwargs)</code>","text":"<p>Initialize CheckpointManager.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>CheckpointManagerConfig</code> <p>Configuration object.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>optimus_dl/modules/checkpoint/checkpoint_manager.py</code> <pre><code>def __init__(\n    self,\n    cfg: CheckpointManagerConfig,\n    **kwargs: Any,\n):\n    \"\"\"Initialize CheckpointManager.\n\n    Args:\n        cfg: Configuration object.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    self.cfg = cfg\n</code></pre>"},{"location":"reference/modules/checkpoint/checkpoint_manager/#optimus_dl.modules.checkpoint.checkpoint_manager.CheckpointManager.build_model_from_checkpoint","title":"<code>build_model_from_checkpoint(checkpoint_path, device, model_key='model', **kwargs)</code>","text":"<p>Build model and load from checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>str | Path</code> <p>Path to checkpoint directory or metadata file</p> required <code>device</code> <code>str | device</code> <p>Device to load model on</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to model building</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[BaseModel, dict]</code> <p>Tuple of (model, config) where config is the training config from checkpoint</p> Source code in <code>optimus_dl/modules/checkpoint/checkpoint_manager.py</code> <pre><code>def build_model_from_checkpoint(\n    self,\n    checkpoint_path: str | Path,\n    device: str | torch.device,\n    model_key=\"model\",\n    **kwargs: Any,\n) -&gt; tuple[BaseModel, dict]:\n    \"\"\"Build model and load from checkpoint.\n\n    Args:\n        checkpoint_path: Path to checkpoint directory or metadata file\n        device: Device to load model on\n        **kwargs: Additional arguments passed to model building\n\n    Returns:\n        Tuple of (model, config) where config is the training config from checkpoint\n    \"\"\"\n    checkpoint = self.get_checkpoint(checkpoint_path)\n    if checkpoint is None:\n        raise FileNotFoundError(f\"Metadata file not found: {checkpoint}\")\n\n    # Load metadata\n    metadata_path = checkpoint.metadata\n    metadata = torch.load(metadata_path, map_location=\"cpu\", weights_only=False)\n    config = metadata[\"config\"]\n\n    logger.info(f\"Loading model with config: {config[model_key]}\")\n\n    # Build model using the config\n    model = build(\"model\", config[model_key], **kwargs)\n    assert isinstance(model, BaseModel)\n\n    self.load_model_state_dict(model, checkpoint.checkpoint)\n\n    # Move model to device\n    model = model.to(device)\n    logger.info(f\"Loaded model from {checkpoint_path} on {device}\")\n    return model, config\n</code></pre>"},{"location":"reference/modules/checkpoint/checkpoint_manager/#optimus_dl.modules.checkpoint.checkpoint_manager.CheckpointManager.get_checkpoint","title":"<code>get_checkpoint(path)</code>","text":"<p>Get a checkpoint from a path. Path can be a directory, then the latest checkpoint is selected or a specific checkpoint directory or metadata file.</p> Source code in <code>optimus_dl/modules/checkpoint/checkpoint_manager.py</code> <pre><code>def get_checkpoint(self, path: str | pathlib.Path) -&gt; CheckpointPath | None:\n    \"\"\"Get a checkpoint from a path.\n    Path can be a directory, then the latest checkpoint is selected or a specific checkpoint directory or metadata file.\n    \"\"\"\n    if not isinstance(path, pathlib.Path):\n        path = pathlib.Path(str(path))\n\n    path = path.expanduser().resolve()\n\n    if not path.exists():\n        return None\n\n    if path.name.startswith(\"checkpoint_\"):\n        # this is exact checkpoint directory\n        metadata_name = path.name.replace(\"checkpoint_\", \"metadata_\") + \".pt\"\n        metadata_path = path.parent / metadata_name\n        if metadata_path.exists():\n            return CheckpointPath(metadata=str(metadata_path), checkpoint=str(path))\n\n    if (\n        path.name.startswith(\"metadata_\")\n        and path.name.endswith(\".pt\")\n        and path.is_file()\n    ):\n        # this is exact metadata file\n        checkpoint_name = pathlib.Path(\n            path.name.replace(\"metadata_\", \"checkpoint_\")\n        ).stem\n        checkpoint_path = path.parent / checkpoint_name\n        if checkpoint_path.exists():\n            return CheckpointPath(\n                metadata=str(path), checkpoint=str(checkpoint_path)\n            )\n\n        # maybe not DCP?\n        checkpoint_path = path.parent / (checkpoint_name + \".pt\")\n        if checkpoint_path.exists():\n            return CheckpointPath(\n                metadata=str(path), checkpoint=str(checkpoint_path)\n            )\n\n    # this is a directory, find latest checkpoint\n    if path.is_dir():\n        latest_checkpoint = path / \"metadata_latest.pt\"\n        return self.get_checkpoint(latest_checkpoint)\n    else:\n        return None\n</code></pre>"},{"location":"reference/modules/checkpoint/checkpoint_manager/#optimus_dl.modules.checkpoint.checkpoint_manager.CheckpointManager.is_restart","title":"<code>is_restart(checkpoint_path)</code>","text":"<p>Check if a checkpoint exists in the given directory. Such case corresponds to the case when run was started, stopped and then restarted again.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <p>Path for output checkpoints</p> required Source code in <code>optimus_dl/modules/checkpoint/checkpoint_manager.py</code> <pre><code>def is_restart(self, checkpoint_path):\n    \"\"\"Check if a checkpoint exists in the given directory.\n    Such case corresponds to the case when run was started, stopped and then restarted again.\n\n    Args:\n        checkpoint_path: Path for output checkpoints\n    \"\"\"\n    return self.get_checkpoint(checkpoint_path) is not None\n</code></pre>"},{"location":"reference/modules/checkpoint/checkpoint_manager/#optimus_dl.modules.checkpoint.checkpoint_manager.CheckpointManager.load_checkpoint","title":"<code>load_checkpoint(checkpoint_path, model, optimizer, collective, lr_scheduler=None, data_loaders=None, data_sources=None, load_strategy=None, **kwargs)</code>","text":"<p>Load training checkpoint using distributed checkpoint API.</p> Source code in <code>optimus_dl/modules/checkpoint/checkpoint_manager.py</code> <pre><code>def load_checkpoint(\n    self,\n    checkpoint_path: str | Path,\n    model: BaseModel | None,\n    optimizer: Optimizer | None,\n    collective: Collective,\n    lr_scheduler=None,\n    data_loaders: dict | None = None,\n    data_sources=None,\n    load_strategy: LoadStrategy | None = None,\n    **kwargs: Any,\n) -&gt; dict:\n    \"\"\"Load training checkpoint using distributed checkpoint API.\"\"\"\n    load_strategy = load_strategy or LoadStrategy()\n    checkpoint = self.get_checkpoint(checkpoint_path)\n\n    if checkpoint is None:\n        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n\n    logger.info(f\"Loading checkpoint with restore strategy {load_strategy}\")\n\n    if not load_strategy.load_model:\n        model = None\n        if load_strategy.load_optimizer:\n            load_strategy.load_optimizer = False\n            logger.warning(\"Not restoring optimizer as model is not loaded\")\n\n    if not load_strategy.load_optimizer:\n        optimizer = None\n\n    if not load_strategy.load_scheduler:\n        lr_scheduler = None\n\n    if load_strategy.load_data_sources and load_strategy.load_dataloaders:\n        data_sources = None\n        load_strategy.load_data_sources = False\n        logger.warning(\n            \"Not restoring data sources directly as they will be restored with dataloaders restoration\"\n        )\n    elif not load_strategy.load_data_sources:\n        data_sources = None\n        if load_strategy.load_dataloaders:\n            load_strategy.load_dataloaders = False\n            logger.warning(\n                \"Not restoring dataloaders as data sources are not loaded\"\n            )\n\n    if not load_strategy.load_dataloaders:\n        data_loaders = None\n\n    logger.info(f\"Loading checkpoint from {checkpoint_path}\")\n\n    # Get state dicts for loading\n    state_dict = {}\n    if model is not None:\n        state_dict[\"model\"] = dcp_state_dict.get_model_state_dict(\n            model, options=dcp_state_dict.StateDictOptions()\n        )\n    else:\n        optimizer = None\n\n    if optimizer is not None:\n        state_dict[\"optimizer\"] = dcp_state_dict.get_optimizer_state_dict(\n            model, optimizer, options=dcp_state_dict.StateDictOptions()\n        )\n\n    # Load using distributed checkpoint API\n    if len(state_dict) &gt; 0:\n        dcp_load(\n            state_dict=state_dict,\n            storage_reader=FileSystemReader(checkpoint.checkpoint),\n            process_group=collective.process_group,\n        )\n\n    # Set the loaded state dicts\n    if model is not None:\n        dcp_state_dict.set_model_state_dict(\n            model, state_dict[\"model\"], options=dcp_state_dict.StateDictOptions()\n        )\n    if optimizer is not None:\n        dcp_state_dict.set_optimizer_state_dict(\n            model,\n            optimizer,\n            state_dict[\"optimizer\"],\n            options=dcp_state_dict.StateDictOptions(),\n        )\n\n    # Load metadata\n    if collective.is_master:\n        metadata = torch.load(\n            checkpoint.metadata, map_location=\"cpu\", weights_only=False\n        )\n        metadatas = [metadata]\n        collective.broadcast_objects(metadatas, source_rank=0)\n    else:\n        metadatas = [None]\n        collective.broadcast_objects(\n            metadatas, source_rank=0\n        )  # pyright: ignore[reportArgumentType]\n        metadata = metadatas[0]\n    assert metadata is not None, \"Metadata not loaded correctly\"\n\n    if lr_scheduler is not None and \"lr_scheduler\" in metadata:\n        lr_scheduler.load_state_dict(metadata[\"lr_scheduler\"])\n        logger.info(\"Restored lr_scheduler\")\n    else:\n        logger.info(\"Did not restore lr_scheduler\")\n\n    if not load_strategy.load_iteration:\n        metadata[\"iteration\"] = 0\n\n    iteration = metadata[\"iteration\"]\n\n    rank = collective.rank\n    per_rank_metadata_path = checkpoint.per_rank_metadata(rank)\n    per_rank_metadata = torch.load(\n        per_rank_metadata_path, map_location=\"cpu\", weights_only=False\n    )\n    for key in load_strategy.extra_ignore_keys or []:\n        if key in per_rank_metadata:\n            per_rank_metadata.pop(key)\n\n    data_loaders = data_loaders or {}\n    data_loaders_states = [per_rank_metadata.get(\"data_loaders\", {})]\n\n    # make sure all tp ranks have the same data_loaders_states\n    tp_world = collective.tp_world\n    tp_world.broadcast_objects(data_loaders_states, source_rank=0)\n    for k, v in data_loaders_states[0].items():\n        if k in data_loaders:\n            logger.info(f\"Restoring {k}\")\n            data_loaders[k].load_state_dict(v)\n        else:\n            logger.warning(f\"Data loader {k} not found in current configuration\")\n\n    if \"data_sources\" in per_rank_metadata and data_sources is not None:\n        sources_states = [per_rank_metadata[\"data_sources\"]]\n        tp_world.broadcast_objects(sources_states, source_rank=0)\n        data_sources.load_state_dict(sources_states[0])\n        logger.info(\n            \"Restoring data sources indipendently (without the full dataloader pipeline)\"\n        )\n\n    if \"metrics\" in per_rank_metadata and load_strategy.load_metrics:\n        from optimus_dl.modules.metrics import (\n            load_state_dict as metrics_load_state_dict,\n        )\n\n        metrics_load_state_dict(per_rank_metadata[\"metrics\"])\n        logger.info(\"Restoring metrics\")\n    else:\n        logger.info(\"Metrics not restored\")\n\n    for key, value in kwargs.items():\n        assert hasattr(\n            value, \"load_state_dict\"\n        ), f\"Do not how to restore {key} = {value}\"\n        if key not in per_rank_metadata:\n            logger.warning(f\"Not restoring {key} = {value} as no state found\")\n        value.load_state_dict(per_rank_metadata[key])\n\n    logger.info(f\"Checkpoint has {iteration = }\")\n    return metadata\n</code></pre>"},{"location":"reference/modules/checkpoint/checkpoint_manager/#optimus_dl.modules.checkpoint.checkpoint_manager.CheckpointManager.load_checkpoint_if_exists","title":"<code>load_checkpoint_if_exists(checkpoint_path, model, collective, optimizer=None, lr_scheduler=None, data_loaders=None, load_strategy=None, **kwargs)</code>","text":"<p>Attempt to find and load the latest checkpoint from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>str</code> <p>Directory to search for checkpoints.</p> required <code>model</code> <code>BaseModel</code> <p>Model to load weights into.</p> required <code>optimizer</code> <code>Optimizer | None</code> <p>Optional optimizer to restore state.</p> <code>None</code> <code>collective</code> <code>Collective</code> <p>Collective for distributed coordination.</p> required <code>lr_scheduler</code> <code>BaseLRScheduler | None</code> <p>Optional LR scheduler to restore.</p> <code>None</code> <code>data_loaders</code> <code>dict | None</code> <p>Optional dict of dataloaders to restore state.</p> <code>None</code> <code>load_strategy</code> <code>LoadStrategy | None</code> <p>Strategy defining what components to load.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Passed to load_checkpoint.</p> <code>{}</code> <p>Returns:</p> Type Description <code>int</code> <p>Tuple of (start_iteration, metadata). start_iteration defaults to 1 if no</p> <code>dict | None</code> <p>checkpoint is found.</p> Source code in <code>optimus_dl/modules/checkpoint/checkpoint_manager.py</code> <pre><code>def load_checkpoint_if_exists(\n    self,\n    checkpoint_path: str,\n    model: BaseModel,\n    collective: Collective,\n    optimizer: Optimizer | None = None,\n    lr_scheduler: BaseLRScheduler | None = None,\n    data_loaders: dict | None = None,\n    load_strategy: LoadStrategy | None = None,\n    **kwargs: Any,\n) -&gt; tuple[int, dict | None]:\n    \"\"\"Attempt to find and load the latest checkpoint from a directory.\n\n    Args:\n        checkpoint_path: Directory to search for checkpoints.\n        model: Model to load weights into.\n        optimizer: Optional optimizer to restore state.\n        collective: Collective for distributed coordination.\n        lr_scheduler: Optional LR scheduler to restore.\n        data_loaders: Optional dict of dataloaders to restore state.\n        load_strategy: Strategy defining what components to load.\n        **kwargs: Passed to load_checkpoint.\n\n    Returns:\n        Tuple of (start_iteration, metadata). start_iteration defaults to 1 if no\n        checkpoint is found.\n    \"\"\"\n    latest_checkpoint = self.get_checkpoint(checkpoint_path)\n    if not latest_checkpoint:\n        return 1, None\n\n    try:\n        metadata = self.load_checkpoint(\n            checkpoint_path=latest_checkpoint.checkpoint,\n            model=model,\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            data_loaders=data_loaders,\n            collective=collective,\n            load_strategy=load_strategy,\n            **kwargs,\n        )\n        start_iteration = metadata[\"iteration\"] + 1\n        logger.info(f\"Starting with iteration = {start_iteration}\")\n        return start_iteration, metadata\n    except Exception as e:\n        logger.warning(f\"Failed to load checkpoint: {e}\")\n        raise\n</code></pre>"},{"location":"reference/modules/checkpoint/checkpoint_manager/#optimus_dl.modules.checkpoint.checkpoint_manager.CheckpointManager.load_model_state_dict","title":"<code>load_model_state_dict(model, checkpoint_path)</code>","text":"<p>Load model state dict from checkpoint, handling both DCP and regular checkpoints.</p> Source code in <code>optimus_dl/modules/checkpoint/checkpoint_manager.py</code> <pre><code>def load_model_state_dict(self, model: BaseModel, checkpoint_path: str) -&gt; None:\n    \"\"\"Load model state dict from checkpoint, handling both DCP and regular checkpoints.\"\"\"\n    checkpoint = self.get_checkpoint(checkpoint_path)\n    if checkpoint is None:\n        if not Path(checkpoint_path).exists():\n            raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n        else:\n            checkpoint = Path(checkpoint_path)\n            is_dcp = Path(checkpoint_path).is_dir()\n    else:\n        is_dcp = checkpoint.is_dcp_checkpoint()\n\n    if is_dcp:\n        logger.info(f\"Detected DCP checkpoint: {checkpoint}\")\n        self._load_dcp_checkpoint(model, checkpoint)\n    else:\n        # Try to load regular PyTorch checkpoint\n        self._load_regular_checkpoint(model, checkpoint)\n</code></pre>"},{"location":"reference/modules/checkpoint/checkpoint_manager/#optimus_dl.modules.checkpoint.checkpoint_manager.CheckpointManager.save_checkpoint","title":"<code>save_checkpoint(checkpoint_path, model, optimizer, collective, full_config, lr_scheduler=None, iteration=0, data_loaders=None, **kwargs)</code>","text":"<p>Save training checkpoint using distributed checkpoint API.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>str | Path</code> <p>Directory to save checkpoint</p> required <code>model</code> <code>BaseModel</code> <p>Model to save</p> required <code>optimizer</code> <code>Optimizer | None</code> <p>Optimizer to save</p> required <code>collective</code> <code>Collective</code> <p>Collective for distributed operations</p> required <code>full_config</code> <code>Any</code> <p>Full configuration object for metadata</p> required <code>lr_scheduler</code> <p>Optional LR scheduler to save</p> <code>None</code> <code>iteration</code> <code>int</code> <p>Current training iteration</p> <code>0</code> <code>data_loaders</code> <code>dict | None</code> <p>Optional data loaders to save state</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional metadata to save</p> <code>{}</code> Source code in <code>optimus_dl/modules/checkpoint/checkpoint_manager.py</code> <pre><code>def save_checkpoint(\n    self,\n    checkpoint_path: str | Path,\n    model: BaseModel,\n    optimizer: Optimizer | None,\n    collective: Collective,\n    full_config: Any,\n    lr_scheduler=None,\n    iteration: int = 0,\n    data_loaders: dict | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save training checkpoint using distributed checkpoint API.\n\n    Args:\n        checkpoint_path: Directory to save checkpoint\n        model: Model to save\n        optimizer: Optimizer to save\n        collective: Collective for distributed operations\n        full_config: Full configuration object for metadata\n        lr_scheduler: Optional LR scheduler to save\n        iteration: Current training iteration\n        data_loaders: Optional data loaders to save state\n        **kwargs: Additional metadata to save\n    \"\"\"\n    if not isinstance(checkpoint_path, Path):\n        checkpoint_path = Path(checkpoint_path)\n    checkpoint_path.mkdir(parents=True, exist_ok=True)\n\n    logger.info(f\"Saving state for model and optimizer at iteration {iteration}\")\n    model_state_dict = dcp_state_dict.get_model_state_dict(\n        model, options=dcp_state_dict.StateDictOptions()\n    )\n\n    state_dict = {\n        \"model\": model_state_dict,\n    }\n    if optimizer is not None:\n        state_dict[\"optimizer\"] = dcp_state_dict.get_optimizer_state_dict(\n            model, optimizer, options=dcp_state_dict.StateDictOptions()\n        )\n\n    # Add metadata\n    kwargs_states = {}\n    for key, value in kwargs.items():\n        kwargs_states[key] = value\n        if hasattr(value, \"state_dict\"):\n            logger.info(f\"Saving state for {key}\")\n            kwargs_states[key] = value.state_dict()\n        else:\n            logger.error(\n                f\"Could not save state for {key} ({value}) as no state_dict() method found\"\n            )\n    metadata = {\n        \"iteration\": iteration,\n        \"config\": full_config,\n        \"world_size\": collective.world_size,\n    }\n\n    if lr_scheduler is not None:\n        logger.info(\"Saving lr_scheduler\")\n        metadata[\"lr_scheduler\"] = lr_scheduler.state_dict()\n\n    # Save using distributed checkpoint API\n    checkpoint_id = str(checkpoint_path / f\"checkpoint_{iteration:09d}\")\n    dcp_save(\n        state_dict=state_dict,\n        storage_writer=FileSystemWriter(checkpoint_id),\n        process_group=collective.process_group,\n    )\n\n    metadata_path = None\n    if collective.is_master:\n        # Save metadata separately\n        metadata_path = checkpoint_path / f\"metadata_{iteration:09d}.pt\"\n        torch.save(metadata, metadata_path)\n        logger.info(f\"Checkpoint saved to {checkpoint_id} / {metadata_path}\")\n\n    assert (\n        \"data_loaders\" not in kwargs_states\n    ), \"Data loaders should be passed separately\"\n    assert \"metrics\" not in kwargs_states, \"Metrics should be passed separately\"\n    logger.info(\"Saving data loaders and metrics\")\n\n    from optimus_dl.modules.metrics import (\n        state_dict as metrics_state_dict,\n    )\n\n    per_rank_metadata = {\n        \"data_loaders\": {\n            k: v.state_dict() for k, v in (data_loaders or {}).items()\n        },\n        \"metrics\": metrics_state_dict(),\n        **kwargs_states,\n    }\n\n    # Save per-rank metadata\n    rank = collective.rank\n    per_rank_metadata_path = (\n        checkpoint_path / f\"per_rank_metadata_{rank}_{iteration:09d}.pt\"\n    )\n    torch.save(per_rank_metadata, per_rank_metadata_path)\n\n    # Create symlink to latest\n    if collective.is_master:\n        latest_checkpoint = checkpoint_path / \"checkpoint_latest\"\n        latest_metadata = checkpoint_path / \"metadata_latest.pt\"\n\n        if latest_checkpoint.exists() or latest_checkpoint.is_symlink():\n            latest_checkpoint.unlink()\n        if latest_metadata.exists():\n            latest_metadata.unlink()\n\n        latest_checkpoint.symlink_to(f\"checkpoint_{iteration:09d}\")\n        latest_metadata.symlink_to(f\"metadata_{iteration:09d}.pt\")\n\n    logger.info(\n        f\"Checkpoint saved successfully, {checkpoint_id}, {per_rank_metadata_path}, {metadata_path}\"\n    )\n    logger.info(\n        f\"{per_rank_metadata.keys() = } {metadata.keys() = } {state_dict.keys() = }\"\n    )\n</code></pre>"},{"location":"reference/modules/checkpoint/checkpoint_manager/#optimus_dl.modules.checkpoint.checkpoint_manager.CheckpointManager.save_checkpoint_if_needed","title":"<code>save_checkpoint_if_needed(iteration, collective, checkpoint_path, save_freq, **kwargs)</code>","text":"<p>Save checkpoint if iteration matches save_freq.</p> Source code in <code>optimus_dl/modules/checkpoint/checkpoint_manager.py</code> <pre><code>def save_checkpoint_if_needed(\n    self,\n    iteration: int,\n    collective: Collective,\n    checkpoint_path: str | Path,\n    save_freq: int,\n    **kwargs: Any,\n) -&gt; bool:\n    \"\"\"Save checkpoint if iteration matches save_freq.\"\"\"\n    if save_freq &lt;= 0 or iteration % save_freq != 0:\n        return False\n\n    try:\n        self.save_checkpoint(\n            checkpoint_path=checkpoint_path,\n            iteration=iteration,\n            collective=collective,\n            **kwargs,\n        )\n        return True\n    except Exception as e:\n        logger.error(f\"Checkpoint saving failed: {e}\")\n        raise\n</code></pre>"},{"location":"reference/modules/checkpoint/checkpoint_manager/#optimus_dl.modules.checkpoint.checkpoint_manager.CheckpointManagerConfig","title":"<code>CheckpointManagerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfig</code></p> <p>Configuration for CheckpointManager.</p> Source code in <code>optimus_dl/modules/checkpoint/checkpoint_manager.py</code> <pre><code>@dataclass\nclass CheckpointManagerConfig(RegistryConfig):\n    \"\"\"Configuration for CheckpointManager.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/modules/checkpoint/checkpoint_manager/#optimus_dl.modules.checkpoint.checkpoint_manager.CheckpointPath","title":"<code>CheckpointPath</code>  <code>dataclass</code>","text":"<p>Represents comprehensive checkpoint information. This class holds paths to both the metadata and the actual checkpoint files.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>str</code> required <code>checkpoint</code> <code>str</code> required Source code in <code>optimus_dl/modules/checkpoint/checkpoint_manager.py</code> <pre><code>@dataclass\nclass CheckpointPath:\n    \"\"\"Represents comprehensive checkpoint information.\n    This class holds paths to both the metadata and the actual checkpoint files.\n\n    Attributes:\n        metadata: Path to the metadata file (always single file)\n        checkpoint: Path to the actual checkpoint file or directory (can be sharded)\n    \"\"\"\n\n    metadata: str\n    checkpoint: str\n\n    def per_rank_metadata(self, rank) -&gt; str:\n        return self.metadata.replace(\"metadata_\", f\"per_rank_metadata_{rank}_\")\n\n    def is_dcp_checkpoint(self):\n        return pathlib.Path(self.checkpoint).is_dir()\n</code></pre>"},{"location":"reference/modules/checkpoint/load_strategy/","title":"load_strategy","text":""},{"location":"reference/modules/checkpoint/load_strategy/#optimus_dl.modules.checkpoint.load_strategy","title":"<code>optimus_dl.modules.checkpoint.load_strategy</code>","text":"<p>Checkpoint loading strategy configuration.</p> <p>This module defines the LoadStrategy class that controls which components of a checkpoint are loaded. This is useful for fine-tuning scenarios where you might want to load model weights but not optimizer state, or for resuming training with different configurations.</p>"},{"location":"reference/modules/checkpoint/load_strategy/#optimus_dl.modules.checkpoint.load_strategy.LoadStrategy","title":"<code>LoadStrategy</code>  <code>dataclass</code>","text":"<p>Configuration for selective checkpoint loading.</p> <p>This class controls which components are loaded from a checkpoint. It's particularly useful for:</p> <ul> <li>Fine-tuning: Load model weights but not optimizer state</li> <li>Resuming with different configs: Load model but reset optimizer</li> <li>Evaluation: Load only model weights</li> <li>Debugging: Load specific components to isolate issues</li> </ul> <p>All fields default to True, meaning by default everything is loaded.</p> Example <pre><code># Fine-tuning: load model only\nstrategy = LoadStrategy(\n    load_model=True,\n    load_optimizer=False,\n    load_scheduler=False,\n    load_iteration=False,\n)\n\n# Resume training: load everything\nstrategy = LoadStrategy()  # All defaults to True\n\n# Evaluation: model only\nstrategy = LoadStrategy(\n    load_model=True,\n    load_optimizer=False,\n    load_scheduler=False,\n    load_data_sources=False,\n    load_dataloaders=False,\n    load_metrics=False,\n    load_iteration=False,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>load_model</code> <code>bool</code> <p>Whether to load model weights.</p> <code>True</code> <code>load_optimizer</code> <code>bool</code> <p>Whether to load optimizer state.</p> <code>True</code> <code>load_scheduler</code> <code>bool</code> <p>Whether to load learning rate scheduler state.</p> <code>True</code> <code>load_data_sources</code> <code>bool</code> <p>Whether to load data source state (e.g. dataset position).</p> <code>True</code> <code>load_dataloaders</code> <code>bool</code> <p>Whether to load full dataloader state.</p> <code>True</code> <code>load_metrics</code> <code>bool</code> <p>Whether to load accumulated metrics.</p> <code>True</code> <code>load_iteration</code> <code>bool</code> <p>Whether to resume the iteration count.</p> <code>True</code> <code>extra_ignore_keys</code> <code>list[str] | None</code> <p>List of specific keys to ignore in the checkpoint state dict.</p> <code>None</code> Source code in <code>optimus_dl/modules/checkpoint/load_strategy.py</code> <pre><code>@dataclass\nclass LoadStrategy:\n    \"\"\"Configuration for selective checkpoint loading.\n\n    This class controls which components are loaded from a checkpoint. It's\n    particularly useful for:\n\n    - Fine-tuning: Load model weights but not optimizer state\n    - Resuming with different configs: Load model but reset optimizer\n    - Evaluation: Load only model weights\n    - Debugging: Load specific components to isolate issues\n\n    All fields default to True, meaning by default everything is loaded.\n\n    Example:\n        ```python\n        # Fine-tuning: load model only\n        strategy = LoadStrategy(\n            load_model=True,\n            load_optimizer=False,\n            load_scheduler=False,\n            load_iteration=False,\n        )\n\n        # Resume training: load everything\n        strategy = LoadStrategy()  # All defaults to True\n\n        # Evaluation: model only\n        strategy = LoadStrategy(\n            load_model=True,\n            load_optimizer=False,\n            load_scheduler=False,\n            load_data_sources=False,\n            load_dataloaders=False,\n            load_metrics=False,\n            load_iteration=False,\n        )\n\n        ```\"\"\"\n\n    load_model: bool = field(\n        default=True, metadata={\"description\": \"Whether to load model weights.\"}\n    )\n    load_optimizer: bool = field(\n        default=True, metadata={\"description\": \"Whether to load optimizer state.\"}\n    )\n    load_scheduler: bool = field(\n        default=True,\n        metadata={\"description\": \"Whether to load learning rate scheduler state.\"},\n    )\n    load_data_sources: bool = field(\n        default=True,\n        metadata={\n            \"description\": \"Whether to load data source state (e.g. dataset position).\"\n        },\n    )\n    load_dataloaders: bool = field(\n        default=True, metadata={\"description\": \"Whether to load full dataloader state.\"}\n    )\n    load_metrics: bool = field(\n        default=True, metadata={\"description\": \"Whether to load accumulated metrics.\"}\n    )\n    load_iteration: bool = field(\n        default=True, metadata={\"description\": \"Whether to resume the iteration count.\"}\n    )\n    extra_ignore_keys: list[str] | None = field(\n        default=None,\n        metadata={\n            \"description\": \"List of specific keys to ignore in the checkpoint state dict.\"\n        },\n    )\n</code></pre>"},{"location":"reference/modules/criterion/","title":"Index","text":""},{"location":"reference/modules/criterion/#optimus_dl.modules.criterion","title":"<code>optimus_dl.modules.criterion</code>","text":""},{"location":"reference/modules/criterion/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>base</code>: Base criterion (loss function) class.</li> <li><code>config</code>: </li> <li><code>cross_entropy</code>: Standard Cross Entropy loss with distributed and kernel optimizations.</li> </ul>"},{"location":"reference/modules/criterion/base/","title":"base","text":""},{"location":"reference/modules/criterion/base/#optimus_dl.modules.criterion.base","title":"<code>optimus_dl.modules.criterion.base</code>","text":"<p>Base criterion (loss function) class.</p> <p>This module defines the BaseCriterion class that all loss functions must inherit from. Criteria compute the loss given a model and a batch of data.</p>"},{"location":"reference/modules/criterion/base/#optimus_dl.modules.criterion.base.BaseCriterion","title":"<code>BaseCriterion</code>","text":"<p>Base class for all loss criteria (loss functions).</p> <p>All loss functions in Optimus-DL should inherit from this class. The criterion is responsible for computing the loss given a model's output and the target data.</p> <p>Subclasses should implement:</p> <ul> <li><code>__call__()</code>: Compute the loss given model and batch</li> </ul> Example <pre><code>@register_criterion(\"cross_entropy\", CrossEntropyConfig)\nclass CrossEntropyCriterion(BaseCriterion):\n    def __init__(self, cfg: CrossEntropyConfig):\n        self.cfg = cfg\n\n    def __call__(self, model: BaseModel, batch: dict) -&gt; torch.Tensor:\n        logits = model(batch[\"input_ids\"])\n        return F.cross_entropy(logits.view(-1, logits.size(-1)),\n                              batch[\"labels\"].view(-1))\n</code></pre> Source code in <code>optimus_dl/modules/criterion/base.py</code> <pre><code>class BaseCriterion:\n    \"\"\"Base class for all loss criteria (loss functions).\n\n    All loss functions in Optimus-DL should inherit from this class. The criterion\n    is responsible for computing the loss given a model's output and the target data.\n\n    Subclasses should implement:\n\n    - `__call__()`: Compute the loss given model and batch\n\n    Example:\n        ```python\n        @register_criterion(\"cross_entropy\", CrossEntropyConfig)\n        class CrossEntropyCriterion(BaseCriterion):\n            def __init__(self, cfg: CrossEntropyConfig):\n                self.cfg = cfg\n\n            def __call__(self, model: BaseModel, batch: dict) -&gt; torch.Tensor:\n                logits = model(batch[\"input_ids\"])\n                return F.cross_entropy(logits.view(-1, logits.size(-1)),\n                                      batch[\"labels\"].view(-1))\n\n        ```\"\"\"\n\n    def __call__(\n        self, model: BaseModel, batch: dict, requested_protocols: set[str] | None = None\n    ) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n        \"\"\"Compute the loss for a given model and batch.\n\n        Args:\n            model: The model to compute loss for. Should be called with the batch\n                to get model outputs.\n            batch: Dictionary containing input data and targets. Typically includes:\n                - \"input_ids\": Token IDs for the input sequence\n                - \"labels\": Target token IDs for computing loss\n                - Other model-specific fields\n            requested_protocols: Optional set of protocol strings (e.g., {'logits', 'classification'})\n                that are requested by the metrics system. Subclasses can use this to avoid\n                computing data that won't be used.\n\n        Returns:\n            A tuple of (loss, exposed_protocols), where:\n                - loss: Scalar tensor containing the loss value.\n                - exposed_protocols: Dictionary mapping protocol names (e.g., 'logits')\n                  to their computed values for reuse in metrics.\n\n        Raises:\n            NotImplementedError: Must be implemented by subclasses.\n\n        Example:\n            ```python\n            criterion = CrossEntropyCriterion(cfg)\n            loss, exposed = criterion(model, batch, requested_protocols={'logits'})\n            loss.backward()\n            ```\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/modules/criterion/base/#optimus_dl.modules.criterion.base.BaseCriterion.__call__","title":"<code>__call__(model, batch, requested_protocols=None)</code>","text":"<p>Compute the loss for a given model and batch.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel</code> <p>The model to compute loss for. Should be called with the batch to get model outputs.</p> required <code>batch</code> <code>dict</code> <p>Dictionary containing input data and targets. Typically includes: - \"input_ids\": Token IDs for the input sequence - \"labels\": Target token IDs for computing loss - Other model-specific fields</p> required <code>requested_protocols</code> <code>set[str] | None</code> <p>Optional set of protocol strings (e.g., {'logits', 'classification'}) that are requested by the metrics system. Subclasses can use this to avoid computing data that won't be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, dict[str, Any]]</code> <p>A tuple of (loss, exposed_protocols), where: - loss: Scalar tensor containing the loss value. - exposed_protocols: Dictionary mapping protocol names (e.g., 'logits')   to their computed values for reuse in metrics.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by subclasses.</p> Example <pre><code>criterion = CrossEntropyCriterion(cfg)\nloss, exposed = criterion(model, batch, requested_protocols={'logits'})\nloss.backward()\n</code></pre> Source code in <code>optimus_dl/modules/criterion/base.py</code> <pre><code>def __call__(\n    self, model: BaseModel, batch: dict, requested_protocols: set[str] | None = None\n) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"Compute the loss for a given model and batch.\n\n    Args:\n        model: The model to compute loss for. Should be called with the batch\n            to get model outputs.\n        batch: Dictionary containing input data and targets. Typically includes:\n            - \"input_ids\": Token IDs for the input sequence\n            - \"labels\": Target token IDs for computing loss\n            - Other model-specific fields\n        requested_protocols: Optional set of protocol strings (e.g., {'logits', 'classification'})\n            that are requested by the metrics system. Subclasses can use this to avoid\n            computing data that won't be used.\n\n    Returns:\n        A tuple of (loss, exposed_protocols), where:\n            - loss: Scalar tensor containing the loss value.\n            - exposed_protocols: Dictionary mapping protocol names (e.g., 'logits')\n              to their computed values for reuse in metrics.\n\n    Raises:\n        NotImplementedError: Must be implemented by subclasses.\n\n    Example:\n        ```python\n        criterion = CrossEntropyCriterion(cfg)\n        loss, exposed = criterion(model, batch, requested_protocols={'logits'})\n        loss.backward()\n        ```\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/modules/criterion/config/","title":"config","text":""},{"location":"reference/modules/criterion/config/#optimus_dl.modules.criterion.config","title":"<code>optimus_dl.modules.criterion.config</code>","text":""},{"location":"reference/modules/criterion/config/#optimus_dl.modules.criterion.config.CriterionConfig","title":"<code>CriterionConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfig</code></p> <p>CriterionConfig(_name: str | None = None)</p> Source code in <code>optimus_dl/modules/criterion/config.py</code> <pre><code>@dataclass\nclass CriterionConfig(RegistryConfig):\n    pass\n</code></pre>"},{"location":"reference/modules/criterion/cross_entropy/","title":"cross_entropy","text":""},{"location":"reference/modules/criterion/cross_entropy/#optimus_dl.modules.criterion.cross_entropy","title":"<code>optimus_dl.modules.criterion.cross_entropy</code>","text":""},{"location":"reference/modules/criterion/cross_entropy/#optimus_dl.modules.criterion.cross_entropy.CrossEntropyCriterion","title":"<code>CrossEntropyCriterion</code>","text":"<p>               Bases: <code>BaseCriterion</code></p> <p>Standard Cross Entropy loss with distributed and kernel optimizations.</p> <p>This criterion implements standard Cross Entropy but adds support for:</p> <ul> <li>Loss Parallelism: Computes loss directly on sharded logits (DTensors) to   save memory and communication.</li> <li>Liger Kernel: Optional high-performance kernel for faster computation   and lower memory usage on GPUs.</li> <li>Metrics: Automatically logs accuracy, perplexity, and token counts.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>CrossEntropyCriterionConfig</code> <p>Configuration for cross entropy.</p> required <code>collective</code> <code>Collective</code> <p>Collective object for distributed operations.</p> required Source code in <code>optimus_dl/modules/criterion/cross_entropy.py</code> <pre><code>@register_criterion(\"cross_entropy\", CrossEntropyCriterionConfig)\nclass CrossEntropyCriterion(BaseCriterion):\n    \"\"\"Standard Cross Entropy loss with distributed and kernel optimizations.\n\n    This criterion implements standard Cross Entropy but adds support for:\n\n    - **Loss Parallelism**: Computes loss directly on sharded logits (DTensors) to\n      save memory and communication.\n    - **Liger Kernel**: Optional high-performance kernel for faster computation\n      and lower memory usage on GPUs.\n    - **Metrics**: Automatically logs accuracy, perplexity, and token counts.\n\n    Args:\n        cfg: Configuration for cross entropy.\n        collective: Collective object for distributed operations.\n    \"\"\"\n\n    def __init__(\n        self, cfg: CrossEntropyCriterionConfig, collective: Collective, **kwargs\n    ):\n        self.cfg = cfg\n        self.collective = collective\n        self.padding_token_id = cfg.padding_token_id\n        self._liger_available = False\n        if self.cfg.use_liger_kernel or self.cfg.use_liger_kernel is None:\n            try:\n                from liger_kernel.transformers.functional import liger_cross_entropy\n\n                self._liger_cross_entropy = liger_cross_entropy\n                self._liger_available = True\n                if self.cfg.use_liger_kernel is None:\n                    logger.info(\"Using liger-kernel for cross-entropy.\")\n            except ImportError:\n                if self.cfg.use_liger_kernel is not None:\n                    logger.warning(\n                        \"use_liger_kernel=True but liger-kernel is not installed. Falling back to PyTorch.\"\n                    )\n\n    def __call__(self, model, batch, requested_protocols: set[str] | None = None):\n        \"\"\"Compute the cross entropy loss.\n\n        Automatically handles target shifting (labels = inputs[1:]) and manages\n        distributed loss computation if the model output is a DTensor.\n\n        Args:\n            model: The language model.\n            batch: Dictionary containing 'input_ids' and optional 'labels'.\n            requested_protocols: Optional set of requested protocols.\n\n        Returns:\n            Tuple of (loss tensor, exposed_protocols dictionary).\n        \"\"\"\n        requested_protocols = requested_protocols or set()\n        batch = copy.copy(batch)\n        input_ids = batch.pop(\"input_ids\")\n        labels = batch.pop(\"labels\", None)\n\n        B, T = input_ids.shape\n\n        if labels is not None:\n            # Batcher already performed causal shifting (input_ids and labels are aligned)\n            targets = labels\n            batch[\"input_ids\"] = input_ids\n        else:\n            assert (\n                \"cu_seqlens\" not in batch\n            ), \"If input is flat, we cannot generate labels and inputs efficiently\"\n            # Perform standard causal shifting: targets = inputs[1:], inputs = inputs[:-1]\n            targets = input_ids[:, 1:]\n            batch[\"input_ids\"] = input_ids[:, :-1]\n\n            # Metadata tensors that match the sequence length must also be sliced\n            for k in list(batch.keys()):\n                v = batch[k]\n                if isinstance(v, torch.Tensor) and v.ndim &gt;= 2 and v.shape[1] == T:\n                    batch[k] = v[:, :-1]\n\n        # Log sequence statistics accurately for all schemes\n        if \"cu_seqlens\" in batch:\n            # Packed/Flat batch: metadata already adjusted for shifting\n            cu = batch[\"cu_seqlens\"]\n            doc_lens = (cu[1:] - cu[:-1]).float()\n            log_averaged(\"input_max_seq_len\", doc_lens.max().item(), round=2)\n            log_averaged(\n                \"input_mean_seq_len\",\n                lambda: doc_lens.mean().item(),\n                weight=len(doc_lens),\n                round=2,\n            )\n        elif \"seq_lens\" in batch:\n            # Padded batch: lengths represent un-shifted sequences, we adjust here\n            sl = (batch[\"seq_lens\"] - 1).float()\n            log_averaged(\"input_max_seq_len\", sl.max().item(), round=2)\n            log_averaged(\n                \"input_mean_seq_len\",\n                lambda: sl.mean().item(),\n                weight=sl.shape[0],\n                round=2,\n            )\n        else:\n            # Fixed-size batch\n            current_T = batch[\"input_ids\"].shape[1]\n            log_averaged(\"input_max_seq_len\", current_T, round=2)\n            log_averaged(\"input_mean_seq_len\", current_T, weight=B, round=2)\n\n        model_out = model(**batch)\n        logits = model_out[\"logits\"]\n        is_dtensor = isinstance(logits, DTensor)\n\n        valid_tokens = cached_lambda(\n            lambda: ((targets &gt;= 0) &amp; (targets != self.padding_token_id)).sum().item()\n            / self.collective.tp_world_size\n        )\n        predictions = cached_lambda(lambda: self.gather_predictions(logits))\n\n        log_averaged(\n            \"accuracy\",\n            lambda: self.accuracy_metric(predictions(), targets),\n            weight=valid_tokens,\n            round=2,\n        )\n        log_summed(\n            \"batch_tokens\",\n            valid_tokens,\n        )\n        log_summed(\n            \"total_tokens\",\n            valid_tokens,\n            reset=False,\n        )\n\n        targets_flat = targets.reshape(-1)\n        enable_loss_parallel = False\n        if is_dtensor:\n            from torch.distributed.tensor.placement_types import Replicate\n\n            if not isinstance(targets_flat, DTensor):\n                targets_parallel = DTensor.from_local(\n                    targets_flat, logits.device_mesh, (Replicate(),)\n                )\n            else:\n                targets_parallel = targets_flat\n\n            # Only enable loss_parallel if logits are actually sharded\n            for placement in logits.placements:\n                if isinstance(placement, Shard):\n                    enable_loss_parallel = True\n                    break\n        else:\n            targets_parallel = targets_flat\n\n        if (\n            self._liger_available\n            and targets_parallel.device.type != \"cpu\"\n            and not is_dtensor\n        ):\n            # Liger kernel handles mixed precision internally, no need to cast to float\n            loss = self._liger_cross_entropy(\n                input=logits.view(-1, logits.size(-1)),\n                target=targets_parallel,\n                label_smoothing=self.cfg.label_smoothing,\n                ignore_index=self.padding_token_id,\n            )\n        else:\n            with (\n                torch.autocast(targets_parallel.device.type, enabled=False),\n                loss_parallel() if enable_loss_parallel else nullcontext(),\n            ):\n                loss = torch.nn.functional.cross_entropy(\n                    input=logits.view(-1, logits.size(-1)).float(),\n                    target=targets_parallel,\n                    label_smoothing=self.cfg.label_smoothing,\n                    ignore_index=self.padding_token_id,\n                )\n\n        log_averaged(\n            \"loss\",\n            value=lambda: loss.item(),\n            weight=valid_tokens,\n        )\n        log_averaged_exponent(\n            \"perplexity\",\n            value=lambda: loss.item(),\n            weight=valid_tokens,\n        )\n\n        exposed = {}\n        if (\n            StandardProtocols.LOGITS in requested_protocols\n            or StandardProtocols.CLASSIFICATION in requested_protocols\n        ):\n            with torch.no_grad():\n                is_flat = B == 1 and \"cu_seqlens\" in batch\n                current_seq_lens = batch.get(\"seq_lens\")\n\n                if StandardProtocols.LOGITS in requested_protocols:\n                    res_logits = logits\n                    if isinstance(res_logits, DTensor):\n                        res_logits = res_logits.full_tensor()\n\n                    if is_flat:\n                        res_logits = self._unflatten_flat(\n                            res_logits, batch[\"cu_seqlens\"], batch[\"max_seqlen\"]\n                        )\n                    exposed[StandardProtocols.LOGITS] = res_logits\n\n                if StandardProtocols.CLASSIFICATION in requested_protocols:\n                    res_preds = predictions()  # Already gathered by cached_lambda\n                    res_targets = targets\n\n                    # Base mask for valid tokens\n                    res_mask = res_targets != self.padding_token_id\n                    # Refine mask for padded batches if current_seq_lens is available\n                    if not is_flat and current_seq_lens is not None:\n                        res_mask = res_mask &amp; (\n                            torch.arange(res_mask.shape[1], device=res_mask.device)\n                            &lt; current_seq_lens[:, None]\n                        )\n\n                    if is_flat:\n                        cu = batch[\"cu_seqlens\"]\n                        ms = batch[\"max_seqlen\"]\n                        res_preds = self._unflatten_flat(res_preds, cu, ms)\n                        res_targets = self._unflatten_flat(\n                            res_targets, cu, ms, pad_val=self.padding_token_id\n                        )\n                        res_mask = self._unflatten_flat(res_mask, cu, ms, pad_val=False)\n\n                    exposed[StandardProtocols.CLASSIFICATION] = dict(\n                        predictions=res_preds,\n                        targets=res_targets,\n                        mask=res_mask,\n                    )\n\n        return loss, exposed\n\n    @staticmethod\n    def _unflatten_flat(t, cu_seqlens, max_seqlen, pad_val=0):\n        \"\"\"Helper to reconstruct (batch, time) layout from a flat (1, sum_T) batch.\"\"\"\n        # t is (1, T_sum, ...)\n        device = t.device\n        dtype = t.dtype\n        num_docs = len(cu_seqlens) - 1\n        total_tokens = int(cu_seqlens[-1].item())\n\n        # seqlens of each document\n        seqlens = cu_seqlens[1:] - cu_seqlens[:-1]\n\n        # Batch index for each token: [0,0,0, 1,1, 2,2,2,2, ...]\n        batch_idx = torch.repeat_interleave(\n            torch.arange(num_docs, device=device), seqlens.to(torch.long)\n        )\n\n        # Local index for each token: [0,1,2, 0,1, 0,1,2,3, ...]\n        # Global index minus sequence start index\n        local_idx = torch.arange(total_tokens, device=device) - torch.repeat_interleave(\n            cu_seqlens[:-1].to(torch.long), seqlens.to(torch.long)\n        )\n\n        # Prepare output buffer (batch, max_time, ...)\n        out_shape = (num_docs, max_seqlen, *t.shape[2:])\n        out = torch.full(out_shape, pad_val, device=device, dtype=dtype)\n\n        # Vectorized assignment: out[batch_idx, local_idx] = t[0, :total_tokens]\n        out[batch_idx, local_idx] = t[0]\n\n        return out\n\n    @torch.no_grad()\n    def gather_predictions(self, logits):\n        \"\"\"\n        Get predictions from logits.\n        \"\"\"\n        is_dtensor = isinstance(logits, DTensor)\n        if is_dtensor:\n            assert isinstance(self.collective, MeshCollective)\n            local_logits = logits.to_local()\n            maxes = torch.max(local_logits, -1)\n\n            maxes_values_distr = DTensor.from_local(\n                maxes.values,\n                device_mesh=self.collective.tp_mesh,\n                placements=(Shard(1),),\n            ).full_tensor()\n            tok_shift = self.collective.tp_rank * local_logits.size(-1)\n            maxes_index_distr = DTensor.from_local(\n                maxes.indices + tok_shift,\n                device_mesh=self.collective.tp_mesh,\n                placements=(Shard(1),),\n            ).full_tensor()\n\n            max_total = torch.max(maxes_values_distr, -1, keepdim=True)\n            predictions = torch.gather(\n                maxes_index_distr,\n                dim=1,\n                index=max_total.indices,\n            )\n        else:\n            predictions = torch.argmax(logits, dim=-1)\n        return predictions\n\n    @torch.no_grad()\n    def accuracy_metric(self, predictions, targets):\n        \"\"\"Compute top-1 accuracy.\n\n        Handles both standard Tensors and distributed DTensors. For DTensors, it\n        performs a distributed max across tensor-parallel ranks.\n        \"\"\"\n\n        correct = predictions == targets\n        valid = (targets &gt;= 0) &amp; (targets != self.padding_token_id)\n        correct = (correct &amp; valid).float()\n        return (correct.sum() / valid.sum()).item()\n</code></pre>"},{"location":"reference/modules/criterion/cross_entropy/#optimus_dl.modules.criterion.cross_entropy.CrossEntropyCriterion.__call__","title":"<code>__call__(model, batch, requested_protocols=None)</code>","text":"<p>Compute the cross entropy loss.</p> <p>Automatically handles target shifting (labels = inputs[1:]) and manages distributed loss computation if the model output is a DTensor.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The language model.</p> required <code>batch</code> <p>Dictionary containing 'input_ids' and optional 'labels'.</p> required <code>requested_protocols</code> <code>set[str] | None</code> <p>Optional set of requested protocols.</p> <code>None</code> <p>Returns:</p> Type Description <p>Tuple of (loss tensor, exposed_protocols dictionary).</p> Source code in <code>optimus_dl/modules/criterion/cross_entropy.py</code> <pre><code>def __call__(self, model, batch, requested_protocols: set[str] | None = None):\n    \"\"\"Compute the cross entropy loss.\n\n    Automatically handles target shifting (labels = inputs[1:]) and manages\n    distributed loss computation if the model output is a DTensor.\n\n    Args:\n        model: The language model.\n        batch: Dictionary containing 'input_ids' and optional 'labels'.\n        requested_protocols: Optional set of requested protocols.\n\n    Returns:\n        Tuple of (loss tensor, exposed_protocols dictionary).\n    \"\"\"\n    requested_protocols = requested_protocols or set()\n    batch = copy.copy(batch)\n    input_ids = batch.pop(\"input_ids\")\n    labels = batch.pop(\"labels\", None)\n\n    B, T = input_ids.shape\n\n    if labels is not None:\n        # Batcher already performed causal shifting (input_ids and labels are aligned)\n        targets = labels\n        batch[\"input_ids\"] = input_ids\n    else:\n        assert (\n            \"cu_seqlens\" not in batch\n        ), \"If input is flat, we cannot generate labels and inputs efficiently\"\n        # Perform standard causal shifting: targets = inputs[1:], inputs = inputs[:-1]\n        targets = input_ids[:, 1:]\n        batch[\"input_ids\"] = input_ids[:, :-1]\n\n        # Metadata tensors that match the sequence length must also be sliced\n        for k in list(batch.keys()):\n            v = batch[k]\n            if isinstance(v, torch.Tensor) and v.ndim &gt;= 2 and v.shape[1] == T:\n                batch[k] = v[:, :-1]\n\n    # Log sequence statistics accurately for all schemes\n    if \"cu_seqlens\" in batch:\n        # Packed/Flat batch: metadata already adjusted for shifting\n        cu = batch[\"cu_seqlens\"]\n        doc_lens = (cu[1:] - cu[:-1]).float()\n        log_averaged(\"input_max_seq_len\", doc_lens.max().item(), round=2)\n        log_averaged(\n            \"input_mean_seq_len\",\n            lambda: doc_lens.mean().item(),\n            weight=len(doc_lens),\n            round=2,\n        )\n    elif \"seq_lens\" in batch:\n        # Padded batch: lengths represent un-shifted sequences, we adjust here\n        sl = (batch[\"seq_lens\"] - 1).float()\n        log_averaged(\"input_max_seq_len\", sl.max().item(), round=2)\n        log_averaged(\n            \"input_mean_seq_len\",\n            lambda: sl.mean().item(),\n            weight=sl.shape[0],\n            round=2,\n        )\n    else:\n        # Fixed-size batch\n        current_T = batch[\"input_ids\"].shape[1]\n        log_averaged(\"input_max_seq_len\", current_T, round=2)\n        log_averaged(\"input_mean_seq_len\", current_T, weight=B, round=2)\n\n    model_out = model(**batch)\n    logits = model_out[\"logits\"]\n    is_dtensor = isinstance(logits, DTensor)\n\n    valid_tokens = cached_lambda(\n        lambda: ((targets &gt;= 0) &amp; (targets != self.padding_token_id)).sum().item()\n        / self.collective.tp_world_size\n    )\n    predictions = cached_lambda(lambda: self.gather_predictions(logits))\n\n    log_averaged(\n        \"accuracy\",\n        lambda: self.accuracy_metric(predictions(), targets),\n        weight=valid_tokens,\n        round=2,\n    )\n    log_summed(\n        \"batch_tokens\",\n        valid_tokens,\n    )\n    log_summed(\n        \"total_tokens\",\n        valid_tokens,\n        reset=False,\n    )\n\n    targets_flat = targets.reshape(-1)\n    enable_loss_parallel = False\n    if is_dtensor:\n        from torch.distributed.tensor.placement_types import Replicate\n\n        if not isinstance(targets_flat, DTensor):\n            targets_parallel = DTensor.from_local(\n                targets_flat, logits.device_mesh, (Replicate(),)\n            )\n        else:\n            targets_parallel = targets_flat\n\n        # Only enable loss_parallel if logits are actually sharded\n        for placement in logits.placements:\n            if isinstance(placement, Shard):\n                enable_loss_parallel = True\n                break\n    else:\n        targets_parallel = targets_flat\n\n    if (\n        self._liger_available\n        and targets_parallel.device.type != \"cpu\"\n        and not is_dtensor\n    ):\n        # Liger kernel handles mixed precision internally, no need to cast to float\n        loss = self._liger_cross_entropy(\n            input=logits.view(-1, logits.size(-1)),\n            target=targets_parallel,\n            label_smoothing=self.cfg.label_smoothing,\n            ignore_index=self.padding_token_id,\n        )\n    else:\n        with (\n            torch.autocast(targets_parallel.device.type, enabled=False),\n            loss_parallel() if enable_loss_parallel else nullcontext(),\n        ):\n            loss = torch.nn.functional.cross_entropy(\n                input=logits.view(-1, logits.size(-1)).float(),\n                target=targets_parallel,\n                label_smoothing=self.cfg.label_smoothing,\n                ignore_index=self.padding_token_id,\n            )\n\n    log_averaged(\n        \"loss\",\n        value=lambda: loss.item(),\n        weight=valid_tokens,\n    )\n    log_averaged_exponent(\n        \"perplexity\",\n        value=lambda: loss.item(),\n        weight=valid_tokens,\n    )\n\n    exposed = {}\n    if (\n        StandardProtocols.LOGITS in requested_protocols\n        or StandardProtocols.CLASSIFICATION in requested_protocols\n    ):\n        with torch.no_grad():\n            is_flat = B == 1 and \"cu_seqlens\" in batch\n            current_seq_lens = batch.get(\"seq_lens\")\n\n            if StandardProtocols.LOGITS in requested_protocols:\n                res_logits = logits\n                if isinstance(res_logits, DTensor):\n                    res_logits = res_logits.full_tensor()\n\n                if is_flat:\n                    res_logits = self._unflatten_flat(\n                        res_logits, batch[\"cu_seqlens\"], batch[\"max_seqlen\"]\n                    )\n                exposed[StandardProtocols.LOGITS] = res_logits\n\n            if StandardProtocols.CLASSIFICATION in requested_protocols:\n                res_preds = predictions()  # Already gathered by cached_lambda\n                res_targets = targets\n\n                # Base mask for valid tokens\n                res_mask = res_targets != self.padding_token_id\n                # Refine mask for padded batches if current_seq_lens is available\n                if not is_flat and current_seq_lens is not None:\n                    res_mask = res_mask &amp; (\n                        torch.arange(res_mask.shape[1], device=res_mask.device)\n                        &lt; current_seq_lens[:, None]\n                    )\n\n                if is_flat:\n                    cu = batch[\"cu_seqlens\"]\n                    ms = batch[\"max_seqlen\"]\n                    res_preds = self._unflatten_flat(res_preds, cu, ms)\n                    res_targets = self._unflatten_flat(\n                        res_targets, cu, ms, pad_val=self.padding_token_id\n                    )\n                    res_mask = self._unflatten_flat(res_mask, cu, ms, pad_val=False)\n\n                exposed[StandardProtocols.CLASSIFICATION] = dict(\n                    predictions=res_preds,\n                    targets=res_targets,\n                    mask=res_mask,\n                )\n\n    return loss, exposed\n</code></pre>"},{"location":"reference/modules/criterion/cross_entropy/#optimus_dl.modules.criterion.cross_entropy.CrossEntropyCriterion.accuracy_metric","title":"<code>accuracy_metric(predictions, targets)</code>","text":"<p>Compute top-1 accuracy.</p> <p>Handles both standard Tensors and distributed DTensors. For DTensors, it performs a distributed max across tensor-parallel ranks.</p> Source code in <code>optimus_dl/modules/criterion/cross_entropy.py</code> <pre><code>@torch.no_grad()\ndef accuracy_metric(self, predictions, targets):\n    \"\"\"Compute top-1 accuracy.\n\n    Handles both standard Tensors and distributed DTensors. For DTensors, it\n    performs a distributed max across tensor-parallel ranks.\n    \"\"\"\n\n    correct = predictions == targets\n    valid = (targets &gt;= 0) &amp; (targets != self.padding_token_id)\n    correct = (correct &amp; valid).float()\n    return (correct.sum() / valid.sum()).item()\n</code></pre>"},{"location":"reference/modules/criterion/cross_entropy/#optimus_dl.modules.criterion.cross_entropy.CrossEntropyCriterion.gather_predictions","title":"<code>gather_predictions(logits)</code>","text":"<p>Get predictions from logits.</p> Source code in <code>optimus_dl/modules/criterion/cross_entropy.py</code> <pre><code>@torch.no_grad()\ndef gather_predictions(self, logits):\n    \"\"\"\n    Get predictions from logits.\n    \"\"\"\n    is_dtensor = isinstance(logits, DTensor)\n    if is_dtensor:\n        assert isinstance(self.collective, MeshCollective)\n        local_logits = logits.to_local()\n        maxes = torch.max(local_logits, -1)\n\n        maxes_values_distr = DTensor.from_local(\n            maxes.values,\n            device_mesh=self.collective.tp_mesh,\n            placements=(Shard(1),),\n        ).full_tensor()\n        tok_shift = self.collective.tp_rank * local_logits.size(-1)\n        maxes_index_distr = DTensor.from_local(\n            maxes.indices + tok_shift,\n            device_mesh=self.collective.tp_mesh,\n            placements=(Shard(1),),\n        ).full_tensor()\n\n        max_total = torch.max(maxes_values_distr, -1, keepdim=True)\n        predictions = torch.gather(\n            maxes_index_distr,\n            dim=1,\n            index=max_total.indices,\n        )\n    else:\n        predictions = torch.argmax(logits, dim=-1)\n    return predictions\n</code></pre>"},{"location":"reference/modules/criterion/cross_entropy/#optimus_dl.modules.criterion.cross_entropy.CrossEntropyCriterionConfig","title":"<code>CrossEntropyCriterionConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>CrossEntropyCriterionConfig(_name: str | None = None, label_smoothing: float = 0.0, use_liger_kernel: bool | None = None, padding_token_id: int = -100)</p> <p>Parameters:</p> Name Type Description Default <code>label_smoothing</code> <code>float</code> <code>0.0</code> <code>use_liger_kernel</code> <code>bool | None</code> <code>None</code> <code>padding_token_id</code> <code>int</code> <code>-100</code> Source code in <code>optimus_dl/modules/criterion/cross_entropy.py</code> <pre><code>@dataclass\nclass CrossEntropyCriterionConfig(RegistryConfigStrict):\n    label_smoothing: float = 0.0\n    use_liger_kernel: bool | None = None\n    padding_token_id: int = -100\n</code></pre>"},{"location":"reference/modules/data/","title":"Index","text":""},{"location":"reference/modules/data/#optimus_dl.modules.data","title":"<code>optimus_dl.modules.data</code>","text":""},{"location":"reference/modules/data/#optimus_dl.modules.data.DataPipeline","title":"<code>DataPipeline</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>DataPipeline(datasets, dataloader)</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>BaseNode</code> <code>None</code> <code>dataloader</code> <code>BaseNode | Loader</code> <code>None</code> Source code in <code>optimus_dl/modules/data/__init__.py</code> <pre><code>class DataPipeline(NamedTuple):\n    datasets: torchdata.nodes.BaseNode\n    dataloader: torchdata.nodes.BaseNode | torchdata.nodes.Loader\n</code></pre>"},{"location":"reference/modules/data/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>config</code>: </li> <li><code>datasets</code>: </li> <li><code>presets</code>: </li> <li><code>transforms</code>: </li> </ul>"},{"location":"reference/modules/data/config/","title":"config","text":""},{"location":"reference/modules/data/config/#optimus_dl.modules.data.config","title":"<code>optimus_dl.modules.data.config</code>","text":""},{"location":"reference/modules/data/config/#optimus_dl.modules.data.config.DataConfig","title":"<code>DataConfig</code>  <code>dataclass</code>","text":"<p>DataConfig(train_datasets: optimus_dl.modules.data.config.DataPipelineConfig = '???', eval_datasets: dict[str, optimus_dl.modules.data.config.DataPipelineConfig] = , scratch: Any = None) <p>Parameters:</p> Name Type Description Default <code>train_datasets</code> <code>DataPipelineConfig</code> <code>'???'</code> <code>eval_datasets</code> <code>dict[str, DataPipelineConfig]</code> <p>dict() -&gt; new empty dictionary dict(mapping) -&gt; new dictionary initialized from a mapping object's     (key, value) pairs dict(iterable) -&gt; new dictionary initialized as if via:     d = {}     for k, v in iterable:         d[k] = v dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs     in the keyword argument list.  For example:  dict(one=1, two=2)</p> <code>&lt;class 'dict'&gt;</code> <code>scratch</code> <code>Any</code> <code>None</code> Source code in <code>optimus_dl/modules/data/config.py</code> <pre><code>@dataclass\nclass DataConfig:\n    train_datasets: DataPipelineConfig = MISSING\n    eval_datasets: dict[str, DataPipelineConfig] = field(default_factory=dict)\n\n    scratch: Any = None\n</code></pre>"},{"location":"reference/modules/data/config/#optimus_dl.modules.data.config.DataPipelineConfig","title":"<code>DataPipelineConfig</code>  <code>dataclass</code>","text":"<p>DataPipelineConfig(source: optimus_dl.core.registry.RegistryConfig, transform: optimus_dl.core.registry.RegistryConfig | None = None)</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>RegistryConfig</code> required <code>transform</code> <code>RegistryConfig | None</code> <code>None</code> Source code in <code>optimus_dl/modules/data/config.py</code> <pre><code>@dataclass\nclass DataPipelineConfig:\n    source: RegistryConfig\n    transform: RegistryConfig | None = None\n</code></pre>"},{"location":"reference/modules/data/datasets/","title":"Index","text":""},{"location":"reference/modules/data/datasets/#optimus_dl.modules.data.datasets","title":"<code>optimus_dl.modules.data.datasets</code>","text":""},{"location":"reference/modules/data/datasets/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>base</code>: Base dataset class for data sources.</li> <li><code>composite</code>: Generate a unique seed for each rank based on the base seed, rank, and epoch.</li> <li><code>huggingface</code>: Configuration for Hugging Face datasets.</li> <li><code>loop_dataset</code>: Dataset that infinitely loops over an inner dataset.</li> <li><code>strategies</code>: </li> <li><code>tokenized_dataset</code>: Configuration for pre-tokenized sharded datasets.</li> <li><code>tokenized_flat_dataset</code>: Configuration for flat tokenized datasets.</li> <li><code>txt_lines</code>: Configuration for line-based text datasets.</li> </ul>"},{"location":"reference/modules/data/datasets/base/","title":"base","text":""},{"location":"reference/modules/data/datasets/base/#optimus_dl.modules.data.datasets.base","title":"<code>optimus_dl.modules.data.datasets.base</code>","text":"<p>Base dataset class for data sources.</p> <p>This module defines the BaseDataset class that all data sources must inherit from. It provides integration with torchdata's pipeline system and checkpointing support.</p>"},{"location":"reference/modules/data/datasets/base/#optimus_dl.modules.data.datasets.base.BaseDataset","title":"<code>BaseDataset</code>","text":"<p>               Bases: <code>BaseNode</code></p> <p>Base class for all dataset implementations.</p> <p>All data sources in Optimus-DL should inherit from this class. It provides:</p> <ul> <li>Integration with torchdata's pipeline system</li> <li>Checkpointing support for resuming data iteration</li> <li>Configuration storage</li> </ul> <p>Subclasses should implement:</p> <ul> <li>The data iteration logic (inherited from torchdata.nodes.BaseNode)</li> <li>Optionally override <code>load_state_dict()</code> for custom checkpointing</li> </ul> Example <pre><code>@register_dataset(\"my_dataset\", MyDatasetConfig)\nclass MyDataset(BaseDataset):\n    def __init__(self, cfg: MyDatasetConfig, **kwargs):\n        super().__init__(cfg, **kwargs)\n        self.data = load_data(cfg.data_path)\n\n    def __iter__(self):\n        for item in self.data:\n            yield item\n</code></pre> Source code in <code>optimus_dl/modules/data/datasets/base.py</code> <pre><code>class BaseDataset(torchdata.nodes.BaseNode):\n    \"\"\"Base class for all dataset implementations.\n\n    All data sources in Optimus-DL should inherit from this class. It provides:\n\n    - Integration with torchdata's pipeline system\n    - Checkpointing support for resuming data iteration\n    - Configuration storage\n\n    Subclasses should implement:\n\n    - The data iteration logic (inherited from torchdata.nodes.BaseNode)\n    - Optionally override `load_state_dict()` for custom checkpointing\n\n    Example:\n        ```python\n        @register_dataset(\"my_dataset\", MyDatasetConfig)\n        class MyDataset(BaseDataset):\n            def __init__(self, cfg: MyDatasetConfig, **kwargs):\n                super().__init__(cfg, **kwargs)\n                self.data = load_data(cfg.data_path)\n\n            def __iter__(self):\n                for item in self.data:\n                    yield item\n\n        ```\"\"\"\n\n    def __init__(self, cfg, **kwargs):\n        \"\"\"Initialize the base dataset.\n\n        Args:\n            cfg: Configuration object for this dataset.\n            **kwargs: Additional keyword arguments passed from the data builder.\n        \"\"\"\n        super().__init__()\n        self.cfg = cfg\n\n    def load_state_dict(self, state_dict: dict) -&gt; None:\n        \"\"\"Load dataset state from checkpoint.\n\n        This method restores the dataset's iteration state, allowing training\n        to resume from the same position in the dataset. The default implementation\n        uses torchdata's `reset()` method.\n\n        Args:\n            state_dict: Dictionary containing the dataset's saved state.\n                Typically includes iteration position, random state, etc.\n\n        Note:\n            Subclasses can override this to handle custom state restoration.\n            The state_dict is typically saved by the checkpoint manager.\n        \"\"\"\n        self.reset(state_dict)\n</code></pre>"},{"location":"reference/modules/data/datasets/base/#optimus_dl.modules.data.datasets.base.BaseDataset.__init__","title":"<code>__init__(cfg, **kwargs)</code>","text":"<p>Initialize the base dataset.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Configuration object for this dataset.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed from the data builder.</p> <code>{}</code> Source code in <code>optimus_dl/modules/data/datasets/base.py</code> <pre><code>def __init__(self, cfg, **kwargs):\n    \"\"\"Initialize the base dataset.\n\n    Args:\n        cfg: Configuration object for this dataset.\n        **kwargs: Additional keyword arguments passed from the data builder.\n    \"\"\"\n    super().__init__()\n    self.cfg = cfg\n</code></pre>"},{"location":"reference/modules/data/datasets/base/#optimus_dl.modules.data.datasets.base.BaseDataset.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Load dataset state from checkpoint.</p> <p>This method restores the dataset's iteration state, allowing training to resume from the same position in the dataset. The default implementation uses torchdata's <code>reset()</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict</code> <p>Dictionary containing the dataset's saved state. Typically includes iteration position, random state, etc.</p> required Note <p>Subclasses can override this to handle custom state restoration. The state_dict is typically saved by the checkpoint manager.</p> Source code in <code>optimus_dl/modules/data/datasets/base.py</code> <pre><code>def load_state_dict(self, state_dict: dict) -&gt; None:\n    \"\"\"Load dataset state from checkpoint.\n\n    This method restores the dataset's iteration state, allowing training\n    to resume from the same position in the dataset. The default implementation\n    uses torchdata's `reset()` method.\n\n    Args:\n        state_dict: Dictionary containing the dataset's saved state.\n            Typically includes iteration position, random state, etc.\n\n    Note:\n        Subclasses can override this to handle custom state restoration.\n        The state_dict is typically saved by the checkpoint manager.\n    \"\"\"\n    self.reset(state_dict)\n</code></pre>"},{"location":"reference/modules/data/datasets/composite/","title":"composite","text":""},{"location":"reference/modules/data/datasets/composite/#optimus_dl.modules.data.datasets.composite","title":"<code>optimus_dl.modules.data.datasets.composite</code>","text":""},{"location":"reference/modules/data/datasets/composite/#optimus_dl.modules.data.datasets.composite.CompositeDataset","title":"<code>CompositeDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset that combines multiple sub-datasets with weighted sampling.</p> <p>This class orchestrates a collection of datasets, sampling from them according to specified weights. It handles:</p> <ul> <li>Weighted Sampling: Using a rank-safe multinomial sampler.</li> <li>Exhaustion Policies: Can stop training when one/all datasets are   exhausted or cycle through them indefinitely.</li> <li>Hierarchical Checkpointing: Correctly saves and restores the state   of all sub-datasets and the global sampling state.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>CompositeDatasetConfig</code> <p>Composite dataset configuration.</p> required <code>rank</code> <code>int</code> <p>Distributed rank.</p> required <code>world_size</code> <code>int</code> <p>Total number of ranks.</p> required Source code in <code>optimus_dl/modules/data/datasets/composite.py</code> <pre><code>@register_dataset(\"composite\", CompositeDatasetConfig)\nclass CompositeDataset(BaseDataset):\n    \"\"\"Dataset that combines multiple sub-datasets with weighted sampling.\n\n    This class orchestrates a collection of datasets, sampling from them according\n    to specified weights. It handles:\n\n    - **Weighted Sampling**: Using a rank-safe multinomial sampler.\n    - **Exhaustion Policies**: Can stop training when one/all datasets are\n      exhausted or cycle through them indefinitely.\n    - **Hierarchical Checkpointing**: Correctly saves and restores the state\n      of all sub-datasets and the global sampling state.\n\n    Args:\n        cfg: Composite dataset configuration.\n        rank: Distributed rank.\n        world_size: Total number of ranks.\n    \"\"\"\n\n    DATASET_NODE_STATES_KEY = \"dataset_node_states\"\n    DATASETS_EXHAUSTED_KEY = \"datasets_exhausted\"\n    EPOCH_KEY = \"epoch\"\n    NUM_YIELDED_KEY = \"num_yielded\"\n    WEIGHTED_SAMPLER_STATE_KEY = \"weighted_sampler_state\"\n\n    def __init__(\n        self,\n        cfg: CompositeDatasetConfig,\n        rank: int,\n        world_size: int,\n        seed: int,\n        **kwargs,\n    ):\n        super().__init__(cfg)\n        self.rank = rank\n        self.world_size = world_size\n\n        self.datasets = {}\n        self.weights = {}\n        self.cycle_flags = {}\n\n        for name, ds_cfg in cfg.datasets.items():\n            logger.info(f\"Initializing sub-dataset {name} with weight {ds_cfg.weight}\")\n            # Sub-datasets are likely BaseNodes themselves\n            self.datasets[name] = build_dataset(\n                ds_cfg.dataset, rank=rank, world_size=world_size, **kwargs\n            )\n            self.weights[name] = ds_cfg.weight\n            self.cycle_flags[name] = ds_cfg.cycle\n\n        self.stop_criteria = cfg.stop_criteria\n        self.seed = seed\n        self.strict_load = cfg.strict_load\n\n        self._validate()\n\n        self._epoch = 0\n        self._num_yielded = 0\n        self._weighted_sampler = None\n        self._datasets_exhausted = {}\n\n    def _validate(self):\n        for weight in self.weights.values():\n            if weight &lt; 0:\n                raise ValueError(\"Weights must be non-negative\")\n\n    def reset(self, initial_state: dict[str, Any] | None = None):\n        \"\"\"Reset or restore the composite dataset state.\n\n        Restores global epoch/yield counters, the weighted sampler state, and\n        recursively calls reset() on all sub-datasets.\n        \"\"\"\n        super().reset(initial_state)\n\n        config_datasets = self.datasets.keys()\n\n        if initial_state is not None:\n            # Handle strict_load\n            state_datasets = initial_state.get(self.DATASET_NODE_STATES_KEY, {}).keys()\n\n            if self.strict_load:\n                if set(state_datasets) != set(config_datasets):\n                    raise ValueError(\n                        f\"Strict load enabled. Mismatch in datasets.\\n\"\n                        f\"Config: {list(config_datasets)}\\nState: {list(state_datasets)}\"\n                    )\n\n            self._num_yielded = initial_state.get(self.NUM_YIELDED_KEY, 0)\n            self._epoch = initial_state.get(self.EPOCH_KEY, 0)\n            self._datasets_exhausted = initial_state.get(\n                self.DATASETS_EXHAUSTED_KEY, dict.fromkeys(config_datasets, False)\n            )\n\n            # If config matches state datasets, we can load sampler state\n            if set(state_datasets) == set(config_datasets):\n                sampler_state = initial_state.get(self.WEIGHTED_SAMPLER_STATE_KEY)\n                self._weighted_sampler = self._get_new_weighted_sampler(sampler_state)\n            else:\n                # Mismatch and strict_load=False: Reset sampler\n                logger.warning(\n                    \"Dataset configuration changed (or strict_load=False), resetting weighted sampler state.\"\n                )\n                self._weighted_sampler = self._get_new_weighted_sampler(None)\n\n            # Load sub-datasets\n            for name, dataset in self.datasets.items():\n                if name in initial_state.get(self.DATASET_NODE_STATES_KEY, {}):\n                    dataset.reset(initial_state[self.DATASET_NODE_STATES_KEY][name])\n                else:\n                    if self.strict_load:\n                        # Should have been caught above, but safety check\n                        raise ValueError(f\"Missing state for dataset {name}\")\n                    logger.info(f\"Resetting dataset {name} (not found in state).\")\n                    dataset.reset()\n        else:\n            # Fresh start\n            self._num_yielded = 0\n            self._epoch = 0\n\n            self._weighted_sampler = self._get_new_weighted_sampler()\n            self._datasets_exhausted = dict.fromkeys(self.datasets, False)\n            for dataset in self.datasets.values():\n                dataset.reset()\n\n    def _get_new_weighted_sampler(self, initial_state=None):\n        return _WeightedSampler(\n            weights=self.weights,\n            seed=self.seed,\n            rank=self.rank,\n            world_size=self.world_size,\n            epoch=self._epoch,\n            initial_state=initial_state,\n        )\n\n    def _check_for_stop_iteration(self) -&gt; None:\n        if self.stop_criteria == StopCriteria.CYCLE_FOREVER:\n            return\n\n        if all(self._datasets_exhausted.values()):\n            raise StopIteration()\n\n        if self.stop_criteria == StopCriteria.FIRST_DATASET_EXHAUSTED and any(\n            self._datasets_exhausted.values()\n        ):\n            raise StopIteration()\n\n    def next(self) -&gt; Any:\n        \"\"\"Sample the next item from one of the sub-datasets.\n\n        Uses the internal weighted sampler to choose a dataset, then delegates\n        to that dataset's next() method. If a dataset is exhausted, it is either\n        reset (cycled) or removed from sampling depending on configuration.\n        \"\"\"\n        while True:\n            self._check_for_stop_iteration()\n\n            # Get next dataset to sample from\n            try:\n                ds_name = next(self._weighted_sampler)\n            except StopIteration as err:\n                # If sampler is empty (all weights 0), we should have caught it in check_for_stop_iteration\n                # unless there's a sync issue. Treat as end of data.\n                raise RuntimeError(\n                    \"Exhausted all datasets and cannot cycle throug\"\n                ) from err\n            try:\n                assert not self._datasets_exhausted[ds_name]\n                item = next(self.datasets[ds_name])\n                self._num_yielded += 1\n                return item\n\n            except StopIteration:\n                self._datasets_exhausted[ds_name] = True\n\n                if self.cycle_flags[ds_name]:\n                    # Reset this dataset\n                    logger.debug(f\"Cycling dataset {ds_name}\")\n                    self.datasets[ds_name].reset()\n                    self._datasets_exhausted[ds_name] = False\n                    try:\n                        item = next(self.datasets[ds_name])\n                        self._num_yielded += 1\n                        return item\n                    except StopIteration as err:\n                        raise RuntimeError(\n                            \"Cannot yield at least one item from dataset after resetting and trying to cycle\"\n                        ) from err\n                else:\n                    # Not cycling: Disable this dataset in sampler to avoid polling it again\n                    self._weighted_sampler.set_active(ds_name, False)\n\n                self._check_for_stop_iteration()\n\n    def get_state(self) -&gt; dict[str, Any]:\n        \"\"\"Collect current state for checkpointing.\"\"\"\n        return {\n            self.DATASETS_EXHAUSTED_KEY: copy.deepcopy(self._datasets_exhausted),\n            self.DATASET_NODE_STATES_KEY: {\n                k: v.state_dict() for k, v in self.datasets.items()\n            },\n            self.EPOCH_KEY: self._epoch,\n            self.NUM_YIELDED_KEY: self._num_yielded,\n            self.WEIGHTED_SAMPLER_STATE_KEY: (\n                self._weighted_sampler.state_dict() if self._weighted_sampler else None\n            ),\n        }\n</code></pre>"},{"location":"reference/modules/data/datasets/composite/#optimus_dl.modules.data.datasets.composite.CompositeDataset.get_state","title":"<code>get_state()</code>","text":"<p>Collect current state for checkpointing.</p> Source code in <code>optimus_dl/modules/data/datasets/composite.py</code> <pre><code>def get_state(self) -&gt; dict[str, Any]:\n    \"\"\"Collect current state for checkpointing.\"\"\"\n    return {\n        self.DATASETS_EXHAUSTED_KEY: copy.deepcopy(self._datasets_exhausted),\n        self.DATASET_NODE_STATES_KEY: {\n            k: v.state_dict() for k, v in self.datasets.items()\n        },\n        self.EPOCH_KEY: self._epoch,\n        self.NUM_YIELDED_KEY: self._num_yielded,\n        self.WEIGHTED_SAMPLER_STATE_KEY: (\n            self._weighted_sampler.state_dict() if self._weighted_sampler else None\n        ),\n    }\n</code></pre>"},{"location":"reference/modules/data/datasets/composite/#optimus_dl.modules.data.datasets.composite.CompositeDataset.next","title":"<code>next()</code>","text":"<p>Sample the next item from one of the sub-datasets.</p> <p>Uses the internal weighted sampler to choose a dataset, then delegates to that dataset's next() method. If a dataset is exhausted, it is either reset (cycled) or removed from sampling depending on configuration.</p> Source code in <code>optimus_dl/modules/data/datasets/composite.py</code> <pre><code>def next(self) -&gt; Any:\n    \"\"\"Sample the next item from one of the sub-datasets.\n\n    Uses the internal weighted sampler to choose a dataset, then delegates\n    to that dataset's next() method. If a dataset is exhausted, it is either\n    reset (cycled) or removed from sampling depending on configuration.\n    \"\"\"\n    while True:\n        self._check_for_stop_iteration()\n\n        # Get next dataset to sample from\n        try:\n            ds_name = next(self._weighted_sampler)\n        except StopIteration as err:\n            # If sampler is empty (all weights 0), we should have caught it in check_for_stop_iteration\n            # unless there's a sync issue. Treat as end of data.\n            raise RuntimeError(\n                \"Exhausted all datasets and cannot cycle throug\"\n            ) from err\n        try:\n            assert not self._datasets_exhausted[ds_name]\n            item = next(self.datasets[ds_name])\n            self._num_yielded += 1\n            return item\n\n        except StopIteration:\n            self._datasets_exhausted[ds_name] = True\n\n            if self.cycle_flags[ds_name]:\n                # Reset this dataset\n                logger.debug(f\"Cycling dataset {ds_name}\")\n                self.datasets[ds_name].reset()\n                self._datasets_exhausted[ds_name] = False\n                try:\n                    item = next(self.datasets[ds_name])\n                    self._num_yielded += 1\n                    return item\n                except StopIteration as err:\n                    raise RuntimeError(\n                        \"Cannot yield at least one item from dataset after resetting and trying to cycle\"\n                    ) from err\n            else:\n                # Not cycling: Disable this dataset in sampler to avoid polling it again\n                self._weighted_sampler.set_active(ds_name, False)\n\n            self._check_for_stop_iteration()\n</code></pre>"},{"location":"reference/modules/data/datasets/composite/#optimus_dl.modules.data.datasets.composite.CompositeDataset.reset","title":"<code>reset(initial_state=None)</code>","text":"<p>Reset or restore the composite dataset state.</p> <p>Restores global epoch/yield counters, the weighted sampler state, and recursively calls reset() on all sub-datasets.</p> Source code in <code>optimus_dl/modules/data/datasets/composite.py</code> <pre><code>def reset(self, initial_state: dict[str, Any] | None = None):\n    \"\"\"Reset or restore the composite dataset state.\n\n    Restores global epoch/yield counters, the weighted sampler state, and\n    recursively calls reset() on all sub-datasets.\n    \"\"\"\n    super().reset(initial_state)\n\n    config_datasets = self.datasets.keys()\n\n    if initial_state is not None:\n        # Handle strict_load\n        state_datasets = initial_state.get(self.DATASET_NODE_STATES_KEY, {}).keys()\n\n        if self.strict_load:\n            if set(state_datasets) != set(config_datasets):\n                raise ValueError(\n                    f\"Strict load enabled. Mismatch in datasets.\\n\"\n                    f\"Config: {list(config_datasets)}\\nState: {list(state_datasets)}\"\n                )\n\n        self._num_yielded = initial_state.get(self.NUM_YIELDED_KEY, 0)\n        self._epoch = initial_state.get(self.EPOCH_KEY, 0)\n        self._datasets_exhausted = initial_state.get(\n            self.DATASETS_EXHAUSTED_KEY, dict.fromkeys(config_datasets, False)\n        )\n\n        # If config matches state datasets, we can load sampler state\n        if set(state_datasets) == set(config_datasets):\n            sampler_state = initial_state.get(self.WEIGHTED_SAMPLER_STATE_KEY)\n            self._weighted_sampler = self._get_new_weighted_sampler(sampler_state)\n        else:\n            # Mismatch and strict_load=False: Reset sampler\n            logger.warning(\n                \"Dataset configuration changed (or strict_load=False), resetting weighted sampler state.\"\n            )\n            self._weighted_sampler = self._get_new_weighted_sampler(None)\n\n        # Load sub-datasets\n        for name, dataset in self.datasets.items():\n            if name in initial_state.get(self.DATASET_NODE_STATES_KEY, {}):\n                dataset.reset(initial_state[self.DATASET_NODE_STATES_KEY][name])\n            else:\n                if self.strict_load:\n                    # Should have been caught above, but safety check\n                    raise ValueError(f\"Missing state for dataset {name}\")\n                logger.info(f\"Resetting dataset {name} (not found in state).\")\n                dataset.reset()\n    else:\n        # Fresh start\n        self._num_yielded = 0\n        self._epoch = 0\n\n        self._weighted_sampler = self._get_new_weighted_sampler()\n        self._datasets_exhausted = dict.fromkeys(self.datasets, False)\n        for dataset in self.datasets.values():\n            dataset.reset()\n</code></pre>"},{"location":"reference/modules/data/datasets/composite/#optimus_dl.modules.data.datasets.composite.CompositeDatasetConfig","title":"<code>CompositeDatasetConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>CompositeDatasetConfig(_name: str | None = None, datasets: dict[str, optimus_dl.modules.data.datasets.composite.DatasetConfig] = , strict_load: bool = True, stop_criteria: optimus_dl.modules.data.datasets.composite.StopCriteria = ) <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>dict[str, DatasetConfig]</code> <p>Datasets to load: name -&gt; config</p> <code>&lt;class 'dict'&gt;</code> <code>strict_load</code> <code>bool</code> <p>Whether to raise an error if state dict does not contain all required keys</p> <code>True</code> <code>stop_criteria</code> <code>StopCriteria</code> <p>Stop criteria for the composite dataset</p> <code>&lt;StopCriteria.CYCLE_FOREVER: 'CYCLE_FOREVER'&gt;</code> Source code in <code>optimus_dl/modules/data/datasets/composite.py</code> <pre><code>@dataclass\nclass CompositeDatasetConfig(RegistryConfigStrict):\n    datasets: dict[str, DatasetConfig] = field(\n        default_factory=dict,\n        metadata={\"description\": \"Datasets to load: name -&gt; config\"},\n    )\n    strict_load: bool = field(\n        default=True,\n        metadata={\n            \"description\": \"Whether to raise an error if state dict does not contain all required keys\"\n        },\n    )\n    stop_criteria: StopCriteria = field(\n        default=StopCriteria.CYCLE_FOREVER,\n        metadata={\"description\": \"Stop criteria for the composite dataset\"},\n    )\n</code></pre>"},{"location":"reference/modules/data/datasets/composite/#optimus_dl.modules.data.datasets.composite.DatasetConfig","title":"<code>DatasetConfig</code>  <code>dataclass</code>","text":"<p>DatasetConfig(dataset: optimus_dl.core.registry.RegistryConfig = '???', weight: float = 1.0, cycle: bool = True)</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>RegistryConfig</code> <p>Dataset config to load</p> <code>'???'</code> <code>weight</code> <code>float</code> <p>Weight of the dataset for sampling</p> <code>1.0</code> <code>cycle</code> <code>bool</code> <p>Whether to cycle through the dataset after it is exhausted</p> <code>True</code> Source code in <code>optimus_dl/modules/data/datasets/composite.py</code> <pre><code>@dataclass\nclass DatasetConfig:\n    dataset: RegistryConfig = field(\n        default=MISSING, metadata={\"description\": \"Dataset config to load\"}\n    )\n    weight: float = field(\n        default=1.0, metadata={\"description\": \"Weight of the dataset for sampling\"}\n    )\n    cycle: bool = field(\n        default=True,\n        metadata={\n            \"description\": \"Whether to cycle through the dataset after it is exhausted\"\n        },\n    )\n</code></pre>"},{"location":"reference/modules/data/datasets/huggingface/","title":"huggingface","text":""},{"location":"reference/modules/data/datasets/huggingface/#optimus_dl.modules.data.datasets.huggingface","title":"<code>optimus_dl.modules.data.datasets.huggingface</code>","text":""},{"location":"reference/modules/data/datasets/huggingface/#optimus_dl.modules.data.datasets.huggingface.HuggingFaceDataset","title":"<code>HuggingFaceDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset wrapper for Hugging Face Hub datasets.</p> <p>This class integrates with the Hugging Face <code>datasets</code> library, supporting:</p> <ul> <li>Streaming: Automatically enables streaming for efficient loading of   large datasets without downloading everything.</li> <li>Distributed Sharding: Uses <code>split_dataset_by_node</code> to ensure each rank   sees a unique portion of the data.</li> <li>Checkpointing: Tracks current position to allow resuming from the middle   of a stream.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Hugging Face dataset configuration.</p> required <code>rank</code> <code>int</code> <p>Distributed rank.</p> required <code>world_size</code> <code>int</code> <p>Total number of ranks.</p> required Source code in <code>optimus_dl/modules/data/datasets/huggingface.py</code> <pre><code>@register_dataset(\"huggingface_dataset\", HuggingFaceDatasetConfig)\nclass HuggingFaceDataset(BaseDataset):\n    \"\"\"Dataset wrapper for Hugging Face Hub datasets.\n\n    This class integrates with the Hugging Face `datasets` library, supporting:\n\n    - **Streaming**: Automatically enables streaming for efficient loading of\n      large datasets without downloading everything.\n    - **Distributed Sharding**: Uses `split_dataset_by_node` to ensure each rank\n      sees a unique portion of the data.\n    - **Checkpointing**: Tracks current position to allow resuming from the middle\n      of a stream.\n\n    Args:\n        cfg: Hugging Face dataset configuration.\n        rank: Distributed rank.\n        world_size: Total number of ranks.\n    \"\"\"\n\n    def __init__(self, cfg, rank: int, world_size: int, **kwargs):\n        super().__init__(cfg)\n        self.rank = rank\n        self.world_size = world_size\n        self.position = 0\n\n    def get_state(self):\n        \"\"\"Return the current position and configuration for checkpointing.\"\"\"\n        return {\n            \"cfg\": self.cfg,\n            \"dataset_state\": (\n                self.dataset.state_dict()\n                if hasattr(self.dataset, \"state_dict\")\n                else None\n            ),\n            \"world_size\": self.world_size,\n            \"rank\": self.rank,\n            \"position\": self.position,\n        }\n\n    def reset(self, initial_state: dict | None = None):\n        \"\"\"Initialize or restore the dataset stream.\n\n        Configures streaming, performs distributed sharding, and skips to the\n        saved position if restoring from a checkpoint.\n        \"\"\"\n        super().reset(initial_state)\n        if initial_state is not None:\n            self.cfg = initial_state.get(\"cfg\", self.cfg)\n            self.cfg = OmegaConf.merge(\n                OmegaConf.structured(HuggingFaceDatasetConfig), self.cfg\n            )\n            self.position = initial_state[\"position\"]\n\n            assert self.rank == initial_state.get(\"rank\", self.rank)\n            assert self.world_size == initial_state.get(\"world_size\", self.world_size)\n\n        if (\n            \"streaming\" in self.cfg.dataset_load_kwargs\n            and not self.cfg.dataset_load_kwargs[\"streaming\"]\n        ):\n            logger.info(\"streaming=False is not recommended\")\n        else:\n            self.cfg.dataset_load_kwargs[\"streaming\"] = True\n\n        if not self.cfg.dataset_load_kwargs.get(\"streaming\"):\n            self.cfg.dataset_load_kwargs.setdefault(\"num_proc\", 4)\n        self.dataset = load_dataset(**self.cfg.dataset_load_kwargs)\n\n        if self.world_size &gt; 1:\n            logger.info(\n                f\"Sharding dataset... (num_shards={self.world_size}, index={self.rank})\"\n            )\n            self.dataset = datasets.distributed.split_dataset_by_node(\n                dataset=self.dataset,\n                rank=self.rank,\n                world_size=self.world_size,\n            )\n\n        if (\n            initial_state is not None\n            and \"dataset_state\" in initial_state\n            and initial_state[\"dataset_state\"] is not None\n        ):\n            self.dataset.load_state_dict(initial_state[\"dataset_state\"])\n\n        if not isinstance(self.dataset, datasets.IterableDataset):\n            self.dataset = self.dataset.skip(self.position)\n        self.iter = iter(self.dataset)\n\n    def next(self):\n        \"\"\"Yield the next example from the Hugging Face dataset.\"\"\"\n        self.position += 1\n        return next(self.iter)\n</code></pre>"},{"location":"reference/modules/data/datasets/huggingface/#optimus_dl.modules.data.datasets.huggingface.HuggingFaceDataset.get_state","title":"<code>get_state()</code>","text":"<p>Return the current position and configuration for checkpointing.</p> Source code in <code>optimus_dl/modules/data/datasets/huggingface.py</code> <pre><code>def get_state(self):\n    \"\"\"Return the current position and configuration for checkpointing.\"\"\"\n    return {\n        \"cfg\": self.cfg,\n        \"dataset_state\": (\n            self.dataset.state_dict()\n            if hasattr(self.dataset, \"state_dict\")\n            else None\n        ),\n        \"world_size\": self.world_size,\n        \"rank\": self.rank,\n        \"position\": self.position,\n    }\n</code></pre>"},{"location":"reference/modules/data/datasets/huggingface/#optimus_dl.modules.data.datasets.huggingface.HuggingFaceDataset.next","title":"<code>next()</code>","text":"<p>Yield the next example from the Hugging Face dataset.</p> Source code in <code>optimus_dl/modules/data/datasets/huggingface.py</code> <pre><code>def next(self):\n    \"\"\"Yield the next example from the Hugging Face dataset.\"\"\"\n    self.position += 1\n    return next(self.iter)\n</code></pre>"},{"location":"reference/modules/data/datasets/huggingface/#optimus_dl.modules.data.datasets.huggingface.HuggingFaceDataset.reset","title":"<code>reset(initial_state=None)</code>","text":"<p>Initialize or restore the dataset stream.</p> <p>Configures streaming, performs distributed sharding, and skips to the saved position if restoring from a checkpoint.</p> Source code in <code>optimus_dl/modules/data/datasets/huggingface.py</code> <pre><code>def reset(self, initial_state: dict | None = None):\n    \"\"\"Initialize or restore the dataset stream.\n\n    Configures streaming, performs distributed sharding, and skips to the\n    saved position if restoring from a checkpoint.\n    \"\"\"\n    super().reset(initial_state)\n    if initial_state is not None:\n        self.cfg = initial_state.get(\"cfg\", self.cfg)\n        self.cfg = OmegaConf.merge(\n            OmegaConf.structured(HuggingFaceDatasetConfig), self.cfg\n        )\n        self.position = initial_state[\"position\"]\n\n        assert self.rank == initial_state.get(\"rank\", self.rank)\n        assert self.world_size == initial_state.get(\"world_size\", self.world_size)\n\n    if (\n        \"streaming\" in self.cfg.dataset_load_kwargs\n        and not self.cfg.dataset_load_kwargs[\"streaming\"]\n    ):\n        logger.info(\"streaming=False is not recommended\")\n    else:\n        self.cfg.dataset_load_kwargs[\"streaming\"] = True\n\n    if not self.cfg.dataset_load_kwargs.get(\"streaming\"):\n        self.cfg.dataset_load_kwargs.setdefault(\"num_proc\", 4)\n    self.dataset = load_dataset(**self.cfg.dataset_load_kwargs)\n\n    if self.world_size &gt; 1:\n        logger.info(\n            f\"Sharding dataset... (num_shards={self.world_size}, index={self.rank})\"\n        )\n        self.dataset = datasets.distributed.split_dataset_by_node(\n            dataset=self.dataset,\n            rank=self.rank,\n            world_size=self.world_size,\n        )\n\n    if (\n        initial_state is not None\n        and \"dataset_state\" in initial_state\n        and initial_state[\"dataset_state\"] is not None\n    ):\n        self.dataset.load_state_dict(initial_state[\"dataset_state\"])\n\n    if not isinstance(self.dataset, datasets.IterableDataset):\n        self.dataset = self.dataset.skip(self.position)\n    self.iter = iter(self.dataset)\n</code></pre>"},{"location":"reference/modules/data/datasets/huggingface/#optimus_dl.modules.data.datasets.huggingface.HuggingFaceDatasetConfig","title":"<code>HuggingFaceDatasetConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for Hugging Face datasets.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>dataset_load_kwargs</code> <code>dict</code> <code>'???'</code> Source code in <code>optimus_dl/modules/data/datasets/huggingface.py</code> <pre><code>@dataclass\nclass HuggingFaceDatasetConfig(RegistryConfigStrict):\n    \"\"\"Configuration for Hugging Face datasets.\n\n    Attributes:\n        dataset_load_kwargs: Dictionary of arguments passed to `datasets.load_dataset`.\n            e.g., {\"path\": \"wikitext\", \"name\": \"wikitext-2-raw-v1\", \"split\": \"train\"}.\n    \"\"\"\n\n    dataset_load_kwargs: dict = MISSING\n</code></pre>"},{"location":"reference/modules/data/datasets/loop_dataset/","title":"loop_dataset","text":""},{"location":"reference/modules/data/datasets/loop_dataset/#optimus_dl.modules.data.datasets.loop_dataset","title":"<code>optimus_dl.modules.data.datasets.loop_dataset</code>","text":""},{"location":"reference/modules/data/datasets/loop_dataset/#optimus_dl.modules.data.datasets.loop_dataset.LoopDataset","title":"<code>LoopDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset that infinitely loops over an inner dataset.</p> <p>When the inner dataset is exhausted, it is automatically re-initialized, creating an endless stream of data. This is useful for training loops where the model needs to see the data multiple times.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>LoopDatasetConfig</code> <p>Loop dataset configuration.</p> required <code>rank</code> <code>int</code> <p>Distributed rank for sharding.</p> required <code>world_size</code> <code>int</code> <p>Total number of ranks.</p> required Source code in <code>optimus_dl/modules/data/datasets/loop_dataset.py</code> <pre><code>@register_dataset(\"loop\", LoopDatasetConfig)\nclass LoopDataset(BaseDataset):\n    \"\"\"Dataset that infinitely loops over an inner dataset.\n\n    When the inner dataset is exhausted, it is automatically re-initialized,\n    creating an endless stream of data. This is useful for training loops where\n    the model needs to see the data multiple times.\n\n    Args:\n        cfg: Loop dataset configuration.\n        rank: Distributed rank for sharding.\n        world_size: Total number of ranks.\n    \"\"\"\n\n    def __init__(self, cfg: LoopDatasetConfig, rank: int, world_size: int, **kwargs):\n        super().__init__(cfg)\n        self.rank = rank\n        self.world_size = world_size\n        self.kwargs = kwargs\n\n        self.inner_dataset: torchdata.nodes.BaseNode | None = None\n\n    def _build_inner(self):\n        \"\"\"Build the inner dataset from configuration.\"\"\"\n        self.inner_dataset = build_dataset(\n            self.cfg.inner, rank=self.rank, world_size=self.world_size, **self.kwargs\n        )\n\n    def next(self):\n        \"\"\"Yield the next item from the inner dataset, resetting it if exhausted.\"\"\"\n        if self.inner_dataset is None:\n            raise ValueError(\"Inner dataset not initialized\")\n\n        try:\n            return next(self.inner_dataset)\n        except StopIteration:\n            logger.info(\"Inner dataset exhausted, recreating loop...\")\n            self._build_inner()\n            return next(self.inner_dataset)\n\n    def reset(self, initial_state: dict | None = None):\n        \"\"\"Reset or restore the loop dataset state.\n\n        Args:\n            initial_state: Optional state dictionary for resuming.\n        \"\"\"\n        super().reset(initial_state)\n\n        inner_state = None\n        if initial_state is not None:\n            self.rank = initial_state.get(\"rank\", self.rank)\n            self.world_size = initial_state.get(\"world_size\", self.world_size)\n            inner_state = initial_state.get(\"inner_state\")\n\n        if self.inner_dataset is None:\n            self._build_inner()\n\n        assert self.inner_dataset is not None, \"Inner dataset not initialized\"\n        self.inner_dataset.reset(inner_state)\n\n    def get_state(self):\n        \"\"\"Return the current state for checkpointing.\"\"\"\n        state = {\n            \"rank\": self.rank,\n            \"world_size\": self.world_size,\n        }\n        if self.inner_dataset:\n            state[\"inner_state\"] = self.inner_dataset.state_dict()\n        return state\n</code></pre>"},{"location":"reference/modules/data/datasets/loop_dataset/#optimus_dl.modules.data.datasets.loop_dataset.LoopDataset.get_state","title":"<code>get_state()</code>","text":"<p>Return the current state for checkpointing.</p> Source code in <code>optimus_dl/modules/data/datasets/loop_dataset.py</code> <pre><code>def get_state(self):\n    \"\"\"Return the current state for checkpointing.\"\"\"\n    state = {\n        \"rank\": self.rank,\n        \"world_size\": self.world_size,\n    }\n    if self.inner_dataset:\n        state[\"inner_state\"] = self.inner_dataset.state_dict()\n    return state\n</code></pre>"},{"location":"reference/modules/data/datasets/loop_dataset/#optimus_dl.modules.data.datasets.loop_dataset.LoopDataset.next","title":"<code>next()</code>","text":"<p>Yield the next item from the inner dataset, resetting it if exhausted.</p> Source code in <code>optimus_dl/modules/data/datasets/loop_dataset.py</code> <pre><code>def next(self):\n    \"\"\"Yield the next item from the inner dataset, resetting it if exhausted.\"\"\"\n    if self.inner_dataset is None:\n        raise ValueError(\"Inner dataset not initialized\")\n\n    try:\n        return next(self.inner_dataset)\n    except StopIteration:\n        logger.info(\"Inner dataset exhausted, recreating loop...\")\n        self._build_inner()\n        return next(self.inner_dataset)\n</code></pre>"},{"location":"reference/modules/data/datasets/loop_dataset/#optimus_dl.modules.data.datasets.loop_dataset.LoopDataset.reset","title":"<code>reset(initial_state=None)</code>","text":"<p>Reset or restore the loop dataset state.</p> <p>Parameters:</p> Name Type Description Default <code>initial_state</code> <code>dict | None</code> <p>Optional state dictionary for resuming.</p> <code>None</code> Source code in <code>optimus_dl/modules/data/datasets/loop_dataset.py</code> <pre><code>def reset(self, initial_state: dict | None = None):\n    \"\"\"Reset or restore the loop dataset state.\n\n    Args:\n        initial_state: Optional state dictionary for resuming.\n    \"\"\"\n    super().reset(initial_state)\n\n    inner_state = None\n    if initial_state is not None:\n        self.rank = initial_state.get(\"rank\", self.rank)\n        self.world_size = initial_state.get(\"world_size\", self.world_size)\n        inner_state = initial_state.get(\"inner_state\")\n\n    if self.inner_dataset is None:\n        self._build_inner()\n\n    assert self.inner_dataset is not None, \"Inner dataset not initialized\"\n    self.inner_dataset.reset(inner_state)\n</code></pre>"},{"location":"reference/modules/data/datasets/loop_dataset/#optimus_dl.modules.data.datasets.loop_dataset.LoopDatasetConfig","title":"<code>LoopDatasetConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>LoopDatasetConfig(_name: str | None = None, inner: Any = '???')</p> <p>Parameters:</p> Name Type Description Default <code>inner</code> <code>Any</code> <code>'???'</code> Source code in <code>optimus_dl/modules/data/datasets/loop_dataset.py</code> <pre><code>@dataclass\nclass LoopDatasetConfig(RegistryConfigStrict):\n    inner: Any = MISSING\n</code></pre>"},{"location":"reference/modules/data/datasets/tokenized_dataset/","title":"tokenized_dataset","text":""},{"location":"reference/modules/data/datasets/tokenized_dataset/#optimus_dl.modules.data.datasets.tokenized_dataset","title":"<code>optimus_dl.modules.data.datasets.tokenized_dataset</code>","text":""},{"location":"reference/modules/data/datasets/tokenized_dataset/#optimus_dl.modules.data.datasets.tokenized_dataset.TokenizedDataset","title":"<code>TokenizedDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset that streams full tokenized documents from numpy shards.</p> <p>This dataset expects data prepared by <code>scripts/prepare_data.py</code>, consisting of multiple <code>.npy</code> shards and a global <code>index.json</code>. It provides:</p> <ul> <li>Memory Mapping: Efficiently reads shards using <code>mmap_mode=\"r\"</code>.</li> <li>Pluggable Strategies: Supports different sampling strategies (sequential, random chunking, etc.).</li> <li>Precise Seeking: Can jump to any document index globally for resuming.</li> </ul> <p>Yields:</p> Name Type Description <code>Dictionary</code> <p>{\"input_ids\": np.array([...]), \"document_id\": int | np.array}</p> Source code in <code>optimus_dl/modules/data/datasets/tokenized_dataset.py</code> <pre><code>@register_dataset(\"tokenized_dataset\", TokenizedDatasetConfig)\nclass TokenizedDataset(BaseDataset):\n    \"\"\"Dataset that streams full tokenized documents from numpy shards.\n\n    This dataset expects data prepared by `scripts/prepare_data.py`, consisting\n    of multiple `.npy` shards and a global `index.json`. It provides:\n\n    - **Memory Mapping**: Efficiently reads shards using `mmap_mode=\"r\"`.\n    - **Pluggable Strategies**: Supports different sampling strategies (sequential, random chunking, etc.).\n    - **Precise Seeking**: Can jump to any document index globally for resuming.\n\n    Yields:\n        Dictionary: {\"input_ids\": np.array([...]), \"document_id\": int | np.array}\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: TokenizedDatasetConfig,\n        rank: int,\n        world_size: int,\n        seed: int,\n        **kwargs,\n    ):\n        super().__init__(cfg)\n        self.data_dir = Path(cfg.data_dir)\n        self.index_file = cfg.index_file\n        self.rank = rank\n        self.world_size = world_size\n        self.limit = cfg.limit\n\n        # Internal State\n        self.shards = []\n        self.shard_num_docs = []\n        self.total_docs = 0\n        self.doc_lengths: np.ndarray | None = None\n        self.doc_to_shard_map: np.ndarray | None = None\n\n        # Strategy\n        self.strategy = build_dataset_sampling_strategy(\n            cfg.strategy,\n            rank=rank,\n            world_size=world_size,\n            seed=seed,\n        )\n\n        # Current Shard State\n        self.current_shard_idx = -1\n        self.current_shard_tokens: np.ndarray | None = None\n        self.current_shard_doc_lens: np.ndarray | None = None\n        self.shard_doc_start_idx = 0  # Global doc index where current shard starts\n\n    def _resolve_dtype(self, type_str: str):\n        \"\"\"Map string dtype names to numpy dtypes.\"\"\"\n        dtype_map = {\n            \"np.uint8\": np.uint8,\n            \"np.uint16\": np.uint16,\n            \"np.uint32\": np.uint32,\n            \"np.int32\": np.int32,\n            \"np.int64\": np.int64,\n            \"uint8\": np.uint8,\n            \"uint16\": np.uint16,\n            \"uint32\": np.uint32,\n            \"int32\": np.int32,\n            \"int64\": np.int64,\n        }\n        return dtype_map.get(type_str, np.uint16)\n\n    def _load_index(self):\n        \"\"\"Load metadata and calculate rank-specific document boundaries.\"\"\"\n        index_path = self.data_dir / self.index_file\n        if not index_path.exists():\n            raise FileNotFoundError(f\"Index file not found: {index_path}\")\n\n        with open(index_path) as f:\n            data = json.load(f)\n\n        self.dtype = self._resolve_dtype(data[\"config\"][\"dtype\"])\n\n        files_meta = data.get(\"files\", [])\n        files_meta.sort(key=lambda x: x[\"shard_idx\"])\n\n        self.shards = []\n        self.shard_num_docs = []\n        all_lengths = []\n        self.total_docs = 0\n\n        # Pre-load lengths and build shard map\n        shard_indices = []\n\n        for meta in files_meta:\n            token_file = self.data_dir / meta[\"file\"]\n            lens_file = self.data_dir / meta[\"lens_file\"]\n            num_docs = meta.get(\"num_docs\", 0)\n\n            if not token_file.exists():\n                raise FileNotFoundError(f\"Token file not found: {token_file}\")\n\n            if not lens_file.exists():\n                raise FileNotFoundError(f\"Lens file not found: {lens_file}\")\n\n            self.shards.append((token_file, lens_file))\n            self.shard_num_docs.append(num_docs)\n\n            # Load lengths (cast to int64 to prevent overflow)\n            shard_lens = np.load(lens_file, mmap_mode=\"r\").astype(np.int64)\n            all_lengths.append(shard_lens)\n\n            # Create mapping: [shard_idx] * num_docs\n            shard_indices.append(\n                np.full(num_docs, len(self.shards) - 1, dtype=np.int32)\n            )\n\n            self.total_docs += num_docs\n\n        # Concatenate all lengths\n        if all_lengths:\n            self.doc_lengths = np.concatenate(all_lengths)\n            self.doc_to_shard_map = np.concatenate(shard_indices)\n        else:\n            self.doc_lengths = np.array([], dtype=np.int64)\n            self.doc_to_shard_map = np.array([], dtype=np.int32)\n\n        # Apply limit\n        if self.limit is not None:\n            self.total_docs = min(self.total_docs, self.limit)\n            self.doc_lengths = self.doc_lengths[: self.limit]\n            self.doc_to_shard_map = self.doc_to_shard_map[: self.limit]\n\n        # Initialize strategy\n        self.strategy.initialize(self.doc_lengths)\n\n    def _load_shard_for_doc(self, doc_idx: int):\n        \"\"\"Ensure the shard containing doc_idx is loaded.\"\"\"\n        shard_idx = self.doc_to_shard_map[doc_idx]\n\n        if shard_idx != self.current_shard_idx:\n            # Load new shard\n            token_path, lens_path = self.shards[shard_idx]\n            self.current_shard_tokens = np.load(token_path, mmap_mode=\"r\")\n            self.current_shard_doc_lens = np.load(lens_path, mmap_mode=\"r\")\n            self.current_shard_idx = shard_idx\n\n            # Calculate where this shard starts in global doc indices\n            count = 0\n            for i in range(shard_idx):\n                count += self.shard_num_docs[i]\n            self.shard_doc_start_idx = count\n\n    def _fetch_segment(self, doc_idx: int, start: int, end: int) -&gt; np.ndarray:\n        \"\"\"Fetch a specific segment of tokens from a document.\"\"\"\n        self._load_shard_for_doc(doc_idx)\n\n        # Local document index within the shard\n        local_doc_idx = doc_idx - self.shard_doc_start_idx\n\n        # Cache cumulative offsets for the current shard to enable O(1) lookups\n        if (\n            not hasattr(self, \"_current_shard_offsets\")\n            or self._current_shard_offsets_shard_idx != self.current_shard_idx\n        ):\n            self._current_shard_offsets = np.concatenate(\n                ([0], np.cumsum(self.current_shard_doc_lens))\n            ).astype(np.int64)\n            self._current_shard_offsets_shard_idx = self.current_shard_idx\n\n        doc_start_token = self._current_shard_offsets[local_doc_idx]\n\n        # Extract\n        abs_start = int(doc_start_token) + start\n        abs_end = int(doc_start_token) + end\n\n        if abs_end &gt; len(self.current_shard_tokens):\n            logger.error(\n                f\"Shard {self.current_shard_idx} mismatch: expected end {abs_end} &gt; len {len(self.current_shard_tokens)}\"\n            )\n            raise RuntimeError(\"Data corruption: lens file does not match token file.\")\n\n        return self.current_shard_tokens[abs_start:abs_end]\n\n    def reset(self, initial_state: dict[str, Any] | None = None):\n        \"\"\"Restore dataset state.\"\"\"\n        super().reset(initial_state)\n\n        # Reload index and lengths\n        self._load_index()\n\n        # Pass state to strategy\n        strategy_state = initial_state.get(\"strategy_state\") if initial_state else None\n        self.strategy.reset(strategy_state)\n\n    def next(self):\n        \"\"\"Yield the next sample.\"\"\"\n        try:\n            segments = self.strategy.next_sample()\n        except StopIteration:\n            raise\n\n        if not segments:\n            raise StopIteration\n\n        # Collect data\n        data_parts = []\n        seq_lens = []\n        doc_ids_parts = []\n\n        is_multi_doc = len(segments) &gt; 1\n\n        for doc_idx, (start, end) in segments:\n            part = self._fetch_segment(doc_idx, start, end)\n            data_parts.append(part)\n            length = len(part)\n            seq_lens.append(length)\n\n            if is_multi_doc:\n                doc_ids_parts.append(np.full(length, doc_idx, dtype=np.int64))\n\n        if len(data_parts) == 1:\n            input_ids = data_parts[0]\n            document_id = segments[0][0]\n        else:\n            input_ids = np.concatenate(data_parts)\n            document_id = np.concatenate(doc_ids_parts)\n\n        # Ensure correct dtype\n        if input_ids.dtype != self.dtype:\n            input_ids = input_ids.astype(self.dtype)\n\n        item = {\n            \"input_ids\": input_ids,\n            \"document_id\": document_id,\n            \"seq_lens\": np.array(seq_lens, dtype=np.int32),\n        }\n\n        return item\n\n    def get_state(self):\n        \"\"\"Return state for checkpointing.\"\"\"\n        return {\n            \"rank\": self.rank,\n            \"world_size\": self.world_size,\n            \"strategy_state\": self.strategy.get_state(),\n        }\n</code></pre>"},{"location":"reference/modules/data/datasets/tokenized_dataset/#optimus_dl.modules.data.datasets.tokenized_dataset.TokenizedDataset.get_state","title":"<code>get_state()</code>","text":"<p>Return state for checkpointing.</p> Source code in <code>optimus_dl/modules/data/datasets/tokenized_dataset.py</code> <pre><code>def get_state(self):\n    \"\"\"Return state for checkpointing.\"\"\"\n    return {\n        \"rank\": self.rank,\n        \"world_size\": self.world_size,\n        \"strategy_state\": self.strategy.get_state(),\n    }\n</code></pre>"},{"location":"reference/modules/data/datasets/tokenized_dataset/#optimus_dl.modules.data.datasets.tokenized_dataset.TokenizedDataset.next","title":"<code>next()</code>","text":"<p>Yield the next sample.</p> Source code in <code>optimus_dl/modules/data/datasets/tokenized_dataset.py</code> <pre><code>def next(self):\n    \"\"\"Yield the next sample.\"\"\"\n    try:\n        segments = self.strategy.next_sample()\n    except StopIteration:\n        raise\n\n    if not segments:\n        raise StopIteration\n\n    # Collect data\n    data_parts = []\n    seq_lens = []\n    doc_ids_parts = []\n\n    is_multi_doc = len(segments) &gt; 1\n\n    for doc_idx, (start, end) in segments:\n        part = self._fetch_segment(doc_idx, start, end)\n        data_parts.append(part)\n        length = len(part)\n        seq_lens.append(length)\n\n        if is_multi_doc:\n            doc_ids_parts.append(np.full(length, doc_idx, dtype=np.int64))\n\n    if len(data_parts) == 1:\n        input_ids = data_parts[0]\n        document_id = segments[0][0]\n    else:\n        input_ids = np.concatenate(data_parts)\n        document_id = np.concatenate(doc_ids_parts)\n\n    # Ensure correct dtype\n    if input_ids.dtype != self.dtype:\n        input_ids = input_ids.astype(self.dtype)\n\n    item = {\n        \"input_ids\": input_ids,\n        \"document_id\": document_id,\n        \"seq_lens\": np.array(seq_lens, dtype=np.int32),\n    }\n\n    return item\n</code></pre>"},{"location":"reference/modules/data/datasets/tokenized_dataset/#optimus_dl.modules.data.datasets.tokenized_dataset.TokenizedDataset.reset","title":"<code>reset(initial_state=None)</code>","text":"<p>Restore dataset state.</p> Source code in <code>optimus_dl/modules/data/datasets/tokenized_dataset.py</code> <pre><code>def reset(self, initial_state: dict[str, Any] | None = None):\n    \"\"\"Restore dataset state.\"\"\"\n    super().reset(initial_state)\n\n    # Reload index and lengths\n    self._load_index()\n\n    # Pass state to strategy\n    strategy_state = initial_state.get(\"strategy_state\") if initial_state else None\n    self.strategy.reset(strategy_state)\n</code></pre>"},{"location":"reference/modules/data/datasets/tokenized_dataset/#optimus_dl.modules.data.datasets.tokenized_dataset.TokenizedDatasetConfig","title":"<code>TokenizedDatasetConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for pre-tokenized sharded datasets.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <code>'???'</code> <code>index_file</code> <code>str</code> <code>'index.json'</code> <code>limit</code> <code>int | None</code> <code>None</code> <code>strategy</code> <code>RegistryConfig</code> <code>DocumentStrategyConfig(_name='document', shuffle=False)</code> Source code in <code>optimus_dl/modules/data/datasets/tokenized_dataset.py</code> <pre><code>@dataclass\nclass TokenizedDatasetConfig(RegistryConfigStrict):\n    \"\"\"Configuration for pre-tokenized sharded datasets.\n\n    Attributes:\n        data_dir: Path to the directory containing shards and index file.\n        index_file: Name of the JSON index file (defaults to index.json).\n        limit: Optional maximum number of documents to read.\n        strategy: Sampling strategy configuration.\n    \"\"\"\n\n    data_dir: str = MISSING\n    index_file: str = \"index.json\"\n    limit: int | None = None  # Optional limit on number of documents\n    strategy: RegistryConfig = field(\n        default_factory=lambda: DocumentStrategyConfig(_name=\"document\")\n    )\n</code></pre>"},{"location":"reference/modules/data/datasets/tokenized_flat_dataset/","title":"tokenized_flat_dataset","text":""},{"location":"reference/modules/data/datasets/tokenized_flat_dataset/#optimus_dl.modules.data.datasets.tokenized_flat_dataset","title":"<code>optimus_dl.modules.data.datasets.tokenized_flat_dataset</code>","text":""},{"location":"reference/modules/data/datasets/tokenized_flat_dataset/#optimus_dl.modules.data.datasets.tokenized_flat_dataset.TokenizedFlatDataset","title":"<code>TokenizedFlatDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset that treats multiple token files as a single contiguous stream.</p> <p>This dataset memory-maps all provided files and calculates a global token offset. It then partitions this global stream into equal segments for each distributed rank. This is ideal for pre-training on very large corpora where data is stored as raw token IDs.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>TokenizedFlatDatasetConfig</code> <p>Flat tokenized dataset configuration.</p> required <code>rank</code> <code>int</code> <p>Distributed rank.</p> required <code>world_size</code> <code>int</code> <p>Total number of ranks.</p> required Source code in <code>optimus_dl/modules/data/datasets/tokenized_flat_dataset.py</code> <pre><code>@register_dataset(\"tokenized_flat\", TokenizedFlatDatasetConfig)\nclass TokenizedFlatDataset(BaseDataset):\n    \"\"\"Dataset that treats multiple token files as a single contiguous stream.\n\n    This dataset memory-maps all provided files and calculates a global token\n    offset. It then partitions this global stream into equal segments for each\n    distributed rank. This is ideal for pre-training on very large corpora\n    where data is stored as raw token IDs.\n\n    Args:\n        cfg: Flat tokenized dataset configuration.\n        rank: Distributed rank.\n        world_size: Total number of ranks.\n    \"\"\"\n\n    def __init__(\n        self, cfg: TokenizedFlatDatasetConfig, rank: int, world_size: int, **kwargs\n    ):\n        super().__init__(cfg)\n        self.files = cfg.files\n        self.seq_len = cfg.seq_len\n        self.batch_size = cfg.batch_size\n        self.dtype = cfg.dtype\n        self.rank = rank\n        self.world_size = world_size\n\n    def _remap_files(self):\n        \"\"\"Memory-map all files and calculate rank-specific token boundaries.\"\"\"\n        # Safe dtype conversion without eval()\n        dtype_map = {\n            \"np.uint8\": np.uint8,\n            \"np.uint16\": np.uint16,\n            \"np.uint32\": np.uint32,\n            \"np.int8\": np.int8,\n            \"np.int16\": np.int16,\n            \"np.int32\": np.int32,\n            \"np.float32\": np.float32,\n            \"np.float64\": np.float64,\n            \"uint8\": np.uint8,\n            \"uint16\": np.uint16,\n            \"uint32\": np.uint32,\n            \"int8\": np.int8,\n            \"int16\": np.int16,\n            \"int32\": np.int32,\n            \"float32\": np.float32,\n            \"float64\": np.float64,\n        }\n\n        dtype = dtype_map.get(self.dtype, np.uint16)\n        if self.dtype not in dtype_map:\n            import logging\n\n            logger = logging.getLogger(__name__)\n            logger.warning(f\"Unknown dtype '{self.dtype}', defaulting to np.uint16\")\n\n        self.files_mapped = [np.memmap(i, dtype=dtype, mode=\"r\") for i in self.files]\n        self.cumlens = np.cumsum([len(i) for i in self.files_mapped])\n\n        total_tokens = self.cumlens[-1]\n        tokens_per_rank = math.floor(total_tokens / self.world_size)\n\n        self.index = tokens_per_rank * (self.rank)\n        self.limit = tokens_per_rank * (self.rank + 1)\n\n    @property\n    def file_index(self):\n        \"\"\"Find the index of the file containing the current token offset.\"\"\"\n        if self.index &gt;= self.cumlens[-1]:\n            return None\n        return np.min(np.arange(len(self.files_mapped))[self.index &lt; self.cumlens])\n\n    def reset(self, initial_state: dict | None = None):\n        \"\"\"Restore dataset state or recalculate boundaries for a fresh start.\"\"\"\n        super().reset(initial_state)\n\n        initial_state = initial_state or {}\n\n        old_files = self.files\n        self.files = initial_state.get(\"files\", self.files)\n        self.seq_len = initial_state.get(\"seq_len\", self.seq_len)\n        self.batch_size = initial_state.get(\"batch_size\", self.batch_size)\n\n        assert initial_state.get(\"world_size\", self.world_size) == self.world_size\n        assert initial_state.get(\"rank\", self.rank) == self.rank\n\n        if self.files != old_files or not hasattr(self, \"index\"):\n            self._remap_files()\n\n        self.index = initial_state.get(\"index\", self.index)\n        self.limit = initial_state.get(\"limit\", self.limit)\n\n    def get_state(self):\n        \"\"\"Return current token offset for checkpointing.\"\"\"\n        return {\n            \"files\": self.files,\n            \"index\": self.index,\n            \"limit\": self.limit,\n            \"seq_len\": self.seq_len,\n            \"batch_size\": self.batch_size,\n            \"rank\": self.rank,\n            \"world_size\": self.world_size,\n        }\n\n    def _take_at_most(self, size):\n        \"\"\"Read at most `size` tokens from the current file, advancing the pointer.\"\"\"\n        file_index = self.file_index\n        if file_index is None:\n            raise StopIteration\n\n        infile_index = self.index\n        if file_index &gt; 0:\n            infile_index = self.index - self.cumlens[file_index - 1]\n\n        to_take = min(size, len(self.files_mapped[file_index]) - infile_index)\n        chunk = self.files_mapped[file_index][infile_index : infile_index + to_take]\n        self.index += to_take\n        return chunk\n\n    def next(self):\n        \"\"\"Yield the next batch of sequences.\"\"\"\n        target_size = self.batch_size * self.seq_len\n        result = self._take_at_most(target_size)\n\n        while len(result) != target_size:\n            left = target_size - len(result)\n            result = np.concatenate((result, self._take_at_most(left)))\n\n        if self.index &gt; self.limit:\n            raise StopIteration\n\n        return {\"input_ids\": result.reshape(self.batch_size, self.seq_len)}\n</code></pre>"},{"location":"reference/modules/data/datasets/tokenized_flat_dataset/#optimus_dl.modules.data.datasets.tokenized_flat_dataset.TokenizedFlatDataset.file_index","title":"<code>file_index</code>  <code>property</code>","text":"<p>Find the index of the file containing the current token offset.</p>"},{"location":"reference/modules/data/datasets/tokenized_flat_dataset/#optimus_dl.modules.data.datasets.tokenized_flat_dataset.TokenizedFlatDataset.get_state","title":"<code>get_state()</code>","text":"<p>Return current token offset for checkpointing.</p> Source code in <code>optimus_dl/modules/data/datasets/tokenized_flat_dataset.py</code> <pre><code>def get_state(self):\n    \"\"\"Return current token offset for checkpointing.\"\"\"\n    return {\n        \"files\": self.files,\n        \"index\": self.index,\n        \"limit\": self.limit,\n        \"seq_len\": self.seq_len,\n        \"batch_size\": self.batch_size,\n        \"rank\": self.rank,\n        \"world_size\": self.world_size,\n    }\n</code></pre>"},{"location":"reference/modules/data/datasets/tokenized_flat_dataset/#optimus_dl.modules.data.datasets.tokenized_flat_dataset.TokenizedFlatDataset.next","title":"<code>next()</code>","text":"<p>Yield the next batch of sequences.</p> Source code in <code>optimus_dl/modules/data/datasets/tokenized_flat_dataset.py</code> <pre><code>def next(self):\n    \"\"\"Yield the next batch of sequences.\"\"\"\n    target_size = self.batch_size * self.seq_len\n    result = self._take_at_most(target_size)\n\n    while len(result) != target_size:\n        left = target_size - len(result)\n        result = np.concatenate((result, self._take_at_most(left)))\n\n    if self.index &gt; self.limit:\n        raise StopIteration\n\n    return {\"input_ids\": result.reshape(self.batch_size, self.seq_len)}\n</code></pre>"},{"location":"reference/modules/data/datasets/tokenized_flat_dataset/#optimus_dl.modules.data.datasets.tokenized_flat_dataset.TokenizedFlatDataset.reset","title":"<code>reset(initial_state=None)</code>","text":"<p>Restore dataset state or recalculate boundaries for a fresh start.</p> Source code in <code>optimus_dl/modules/data/datasets/tokenized_flat_dataset.py</code> <pre><code>def reset(self, initial_state: dict | None = None):\n    \"\"\"Restore dataset state or recalculate boundaries for a fresh start.\"\"\"\n    super().reset(initial_state)\n\n    initial_state = initial_state or {}\n\n    old_files = self.files\n    self.files = initial_state.get(\"files\", self.files)\n    self.seq_len = initial_state.get(\"seq_len\", self.seq_len)\n    self.batch_size = initial_state.get(\"batch_size\", self.batch_size)\n\n    assert initial_state.get(\"world_size\", self.world_size) == self.world_size\n    assert initial_state.get(\"rank\", self.rank) == self.rank\n\n    if self.files != old_files or not hasattr(self, \"index\"):\n        self._remap_files()\n\n    self.index = initial_state.get(\"index\", self.index)\n    self.limit = initial_state.get(\"limit\", self.limit)\n</code></pre>"},{"location":"reference/modules/data/datasets/tokenized_flat_dataset/#optimus_dl.modules.data.datasets.tokenized_flat_dataset.TokenizedFlatDatasetConfig","title":"<code>TokenizedFlatDatasetConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for flat tokenized datasets.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>str</code> <code>'np.uint16'</code> <code>files</code> <code>list[str]</code> <p>Built-in mutable sequence.</p> <p>If no argument is given, the constructor creates a new empty list. The argument must be an iterable if specified.</p> <code>&lt;dynamic&gt;</code> <code>seq_len</code> <code>int</code> <code>'???'</code> <code>batch_size</code> <code>int</code> <code>'???'</code> Source code in <code>optimus_dl/modules/data/datasets/tokenized_flat_dataset.py</code> <pre><code>@dataclass\nclass TokenizedFlatDatasetConfig(RegistryConfigStrict):\n    \"\"\"Configuration for flat tokenized datasets.\n\n    Attributes:\n        dtype: Numpy dtype of the token files.\n        files: List of paths to tokenized `.npy` or raw binary files.\n        seq_len: Sequence length for each batch.\n        batch_size: Number of sequences per batch.\n    \"\"\"\n\n    dtype: str = \"np.uint16\"\n    files: list[str] = field(\n        default_factory=list,\n    )\n    seq_len: int = field(default=MISSING)\n    batch_size: int = field(default=MISSING)\n</code></pre>"},{"location":"reference/modules/data/datasets/txt_lines/","title":"txt_lines","text":""},{"location":"reference/modules/data/datasets/txt_lines/#optimus_dl.modules.data.datasets.txt_lines","title":"<code>optimus_dl.modules.data.datasets.txt_lines</code>","text":""},{"location":"reference/modules/data/datasets/txt_lines/#optimus_dl.modules.data.datasets.txt_lines.TxtLinesDataset","title":"<code>TxtLinesDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset that reads and shards a text file line-by-line.</p> <p>This dataset handles:</p> <ul> <li>Remote Loading: Automatically downloads files from URLs and caches them.</li> <li>Line Filtering: Optional removal of empty lines.</li> <li>Distributed Sharding: Partitions the total number of lines equally   across ranks.</li> </ul> Note <p>This dataset loads the entire file into memory on each rank. It is intended for small to medium-sized text files (e.g., TinyShakespeare).</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>TxtLinesDatasetConfig</code> <p>Text lines dataset configuration.</p> required <code>rank</code> <code>int</code> <p>Distributed rank.</p> required <code>world_size</code> <code>int</code> <p>Total number of ranks.</p> required Source code in <code>optimus_dl/modules/data/datasets/txt_lines.py</code> <pre><code>@register_dataset(\"txt_lines\", TxtLinesDatasetConfig)\nclass TxtLinesDataset(BaseDataset):\n    \"\"\"Dataset that reads and shards a text file line-by-line.\n\n    This dataset handles:\n\n    - **Remote Loading**: Automatically downloads files from URLs and caches them.\n    - **Line Filtering**: Optional removal of empty lines.\n    - **Distributed Sharding**: Partitions the total number of lines equally\n      across ranks.\n\n    Note:\n      This dataset loads the entire file into memory on each rank. It is intended\n      for small to medium-sized text files (e.g., TinyShakespeare).\n\n    Args:\n        cfg: Text lines dataset configuration.\n        rank: Distributed rank.\n        world_size: Total number of ranks.\n    \"\"\"\n\n    def __init__(\n        self, cfg: TxtLinesDatasetConfig, rank: int, world_size: int, **kwargs\n    ):\n        super().__init__(cfg)\n        self.file_link = cfg.file_link\n        self.cache_dir = cfg.cache_dir\n        self.skip_empty_lines = cfg.skip_empty_lines\n        self.rank = rank\n        self.world_size = world_size\n\n        self.lines = []\n        self.index = 0\n        self.limit = 0\n\n    def _prepare_data(self):\n        \"\"\"Download (if needed) and shard the text data into lines.\"\"\"\n        # 1. Resolve path / download\n        local_path = self.file_link\n        if self.file_link.startswith(\"http://\") or self.file_link.startswith(\n            \"https://\"\n        ):\n            # Encode URL to filename to avoid collisions\n            url_hash = hashlib.sha256(self.file_link.encode(\"utf-8\")).hexdigest()\n            # Try to keep the extension if present\n            _, ext = os.path.splitext(self.file_link)\n            if not ext:\n                ext = \".txt\"\n            # Sanitize extension (just in case)\n            if len(ext) &gt; 10:\n                ext = \".txt\"\n\n            filename = f\"{url_hash}{ext}\"\n            local_path = os.path.join(self.cache_dir, filename)\n\n            if not os.path.exists(local_path):\n                os.makedirs(self.cache_dir, exist_ok=True)\n                logger.info(f\"Downloading {self.file_link} to {local_path}\")\n                try:\n                    response = requests.get(self.file_link, stream=True)\n                    response.raise_for_status()\n                    with open(local_path, \"wb\") as f:\n                        for chunk in response.iter_content(chunk_size=8192):\n                            f.write(chunk)\n                except Exception as e:\n                    logger.error(f\"Failed to download {self.file_link}: {e}\")\n                    raise\n\n        if not os.path.exists(local_path):\n            raise FileNotFoundError(f\"File not found: {local_path}\")\n\n        # 2. Read and filter\n        logger.info(f\"Loading data from {local_path}\")\n        with open(local_path, encoding=\"utf-8\") as f:\n            raw_lines = f.readlines()\n\n        # Strip trailing newlines\n        self.lines = [i.rstrip(\"\\n\") for i in raw_lines]\n\n        if self.skip_empty_lines:\n            self.lines = [i for i in self.lines if i.strip()]\n\n        # 3. Shard\n        total_lines = len(self.lines)\n        if total_lines == 0:\n            logger.warning(f\"Dataset at {local_path} is empty after filtering.\")\n            self.limit = 0\n            self.index = 0\n            return\n\n        lines_per_rank = total_lines // self.world_size\n        self.index = lines_per_rank * self.rank\n        self.limit = lines_per_rank * (self.rank + 1)\n\n        # Ensure index is within bounds (just in case)\n        self.index = max(0, min(self.index, total_lines))\n        self.limit = max(0, min(self.limit, total_lines))\n\n    def next(self):\n        \"\"\"Yield the next line of text.\"\"\"\n        if self.index &gt;= self.limit:\n            raise StopIteration\n\n        line = self.lines[self.index]\n        self.index += 1\n        return {\"text\": line}\n\n    def reset(self, initial_state: dict | None = None):\n        \"\"\"Restore dataset state or prepare the file for a fresh start.\"\"\"\n        super().reset(initial_state)\n        initial_state = initial_state or {}\n\n        if \"file_link\" in initial_state:\n            self.file_link = initial_state[\"file_link\"]\n\n        if \"skip_empty_lines\" in initial_state:\n            self.skip_empty_lines = initial_state[\"skip_empty_lines\"]\n\n        self._prepare_data()\n\n        self.index = initial_state.get(\"index\", self.index)\n        assert initial_state.get(\"rank\", self.rank) == self.rank\n        assert initial_state.get(\"world_size\", self.world_size) == self.world_size\n\n    def get_state(self):\n        \"\"\"Return current line index for checkpointing.\"\"\"\n        return {\n            \"index\": self.index,\n            \"file_link\": self.file_link,\n            \"skip_empty_lines\": self.skip_empty_lines,\n            \"rank\": self.rank,\n            \"world_size\": self.world_size,\n        }\n</code></pre>"},{"location":"reference/modules/data/datasets/txt_lines/#optimus_dl.modules.data.datasets.txt_lines.TxtLinesDataset.get_state","title":"<code>get_state()</code>","text":"<p>Return current line index for checkpointing.</p> Source code in <code>optimus_dl/modules/data/datasets/txt_lines.py</code> <pre><code>def get_state(self):\n    \"\"\"Return current line index for checkpointing.\"\"\"\n    return {\n        \"index\": self.index,\n        \"file_link\": self.file_link,\n        \"skip_empty_lines\": self.skip_empty_lines,\n        \"rank\": self.rank,\n        \"world_size\": self.world_size,\n    }\n</code></pre>"},{"location":"reference/modules/data/datasets/txt_lines/#optimus_dl.modules.data.datasets.txt_lines.TxtLinesDataset.next","title":"<code>next()</code>","text":"<p>Yield the next line of text.</p> Source code in <code>optimus_dl/modules/data/datasets/txt_lines.py</code> <pre><code>def next(self):\n    \"\"\"Yield the next line of text.\"\"\"\n    if self.index &gt;= self.limit:\n        raise StopIteration\n\n    line = self.lines[self.index]\n    self.index += 1\n    return {\"text\": line}\n</code></pre>"},{"location":"reference/modules/data/datasets/txt_lines/#optimus_dl.modules.data.datasets.txt_lines.TxtLinesDataset.reset","title":"<code>reset(initial_state=None)</code>","text":"<p>Restore dataset state or prepare the file for a fresh start.</p> Source code in <code>optimus_dl/modules/data/datasets/txt_lines.py</code> <pre><code>def reset(self, initial_state: dict | None = None):\n    \"\"\"Restore dataset state or prepare the file for a fresh start.\"\"\"\n    super().reset(initial_state)\n    initial_state = initial_state or {}\n\n    if \"file_link\" in initial_state:\n        self.file_link = initial_state[\"file_link\"]\n\n    if \"skip_empty_lines\" in initial_state:\n        self.skip_empty_lines = initial_state[\"skip_empty_lines\"]\n\n    self._prepare_data()\n\n    self.index = initial_state.get(\"index\", self.index)\n    assert initial_state.get(\"rank\", self.rank) == self.rank\n    assert initial_state.get(\"world_size\", self.world_size) == self.world_size\n</code></pre>"},{"location":"reference/modules/data/datasets/txt_lines/#optimus_dl.modules.data.datasets.txt_lines.TxtLinesDatasetConfig","title":"<code>TxtLinesDatasetConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for line-based text datasets.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>file_link</code> <code>str</code> <code>'???'</code> <code>cache_dir</code> <code>str</code> <p>Returns tempfile.tempdir as str.</p> <code>'/tmp'</code> <code>skip_empty_lines</code> <code>bool</code> <code>True</code> Source code in <code>optimus_dl/modules/data/datasets/txt_lines.py</code> <pre><code>@dataclass\nclass TxtLinesDatasetConfig(RegistryConfigStrict):\n    \"\"\"Configuration for line-based text datasets.\n\n    Attributes:\n        file_link: Local path or HTTP(S) URL to the text file.\n        cache_dir: Directory to cache downloaded files.\n        skip_empty_lines: If True, lines that are empty or only whitespace are ignored.\n    \"\"\"\n\n    file_link: str = MISSING\n    cache_dir: str = field(default_factory=tempfile.gettempdir)\n    skip_empty_lines: bool = True\n</code></pre>"},{"location":"reference/modules/data/datasets/strategies/","title":"Index","text":""},{"location":"reference/modules/data/datasets/strategies/#optimus_dl.modules.data.datasets.strategies","title":"<code>optimus_dl.modules.data.datasets.strategies</code>","text":""},{"location":"reference/modules/data/datasets/strategies/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>base</code>: Base class for dataset sampling strategies.</li> <li><code>concat_random</code>: Treats the dataset as a single concatenated stream of tokens, splits it into</li> <li><code>document</code>: Yields full documents from the dataset.</li> </ul>"},{"location":"reference/modules/data/datasets/strategies/base/","title":"base","text":""},{"location":"reference/modules/data/datasets/strategies/base/#optimus_dl.modules.data.datasets.strategies.base","title":"<code>optimus_dl.modules.data.datasets.strategies.base</code>","text":""},{"location":"reference/modules/data/datasets/strategies/base/#optimus_dl.modules.data.datasets.strategies.base.BaseStrategy","title":"<code>BaseStrategy</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for dataset sampling strategies.</p> Source code in <code>optimus_dl/modules/data/datasets/strategies/base.py</code> <pre><code>class BaseStrategy(ABC):\n    \"\"\"Base class for dataset sampling strategies.\"\"\"\n\n    def __init__(self, cfg: BaseStrategyConfig, rank: int, world_size: int):\n        self.cfg = cfg\n        self.rank = rank\n        self.world_size = world_size\n        self.doc_lengths: np.ndarray | None = None\n\n    def initialize(self, doc_lengths: np.ndarray):\n        \"\"\"Initialize the strategy with document lengths.\"\"\"\n        self.doc_lengths = doc_lengths\n\n    @abstractmethod\n    def next_sample(self) -&gt; list[tuple[int, tuple[int, int]]]:\n        \"\"\"Yield the next sample.\n\n        Returns:\n            A list of segments required to construct the sample.\n            Each segment is a tuple: (doc_id, (start_offset, end_offset)).\n            - doc_id: Global document index.\n            - start_offset: Start token index within the document (inclusive).\n            - end_offset: End token index within the document (exclusive).\n\n        Raises:\n            StopIteration: When the strategy is exhausted.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def reset(self, initial_state: dict[str, Any] | None = None):\n        \"\"\"Reset state to initial or checkpointed state.\n\n        Args:\n            initial_state: State dictionary to restore from (optional).\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_state(self) -&gt; dict[str, Any]:\n        \"\"\"Get state for checkpointing.\n\n        Returns:\n            Dictionary containing the current state.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/modules/data/datasets/strategies/base/#optimus_dl.modules.data.datasets.strategies.base.BaseStrategy.get_state","title":"<code>get_state()</code>  <code>abstractmethod</code>","text":"<p>Get state for checkpointing.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing the current state.</p> Source code in <code>optimus_dl/modules/data/datasets/strategies/base.py</code> <pre><code>@abstractmethod\ndef get_state(self) -&gt; dict[str, Any]:\n    \"\"\"Get state for checkpointing.\n\n    Returns:\n        Dictionary containing the current state.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/modules/data/datasets/strategies/base/#optimus_dl.modules.data.datasets.strategies.base.BaseStrategy.initialize","title":"<code>initialize(doc_lengths)</code>","text":"<p>Initialize the strategy with document lengths.</p> Source code in <code>optimus_dl/modules/data/datasets/strategies/base.py</code> <pre><code>def initialize(self, doc_lengths: np.ndarray):\n    \"\"\"Initialize the strategy with document lengths.\"\"\"\n    self.doc_lengths = doc_lengths\n</code></pre>"},{"location":"reference/modules/data/datasets/strategies/base/#optimus_dl.modules.data.datasets.strategies.base.BaseStrategy.next_sample","title":"<code>next_sample()</code>  <code>abstractmethod</code>","text":"<p>Yield the next sample.</p> <p>Returns:</p> Type Description <code>list[tuple[int, tuple[int, int]]]</code> <p>A list of segments required to construct the sample.</p> <code>list[tuple[int, tuple[int, int]]]</code> <p>Each segment is a tuple: (doc_id, (start_offset, end_offset)).</p> <code>list[tuple[int, tuple[int, int]]]</code> <ul> <li>doc_id: Global document index.</li> </ul> <code>list[tuple[int, tuple[int, int]]]</code> <ul> <li>start_offset: Start token index within the document (inclusive).</li> </ul> <code>list[tuple[int, tuple[int, int]]]</code> <ul> <li>end_offset: End token index within the document (exclusive).</li> </ul> <p>Raises:</p> Type Description <code>StopIteration</code> <p>When the strategy is exhausted.</p> Source code in <code>optimus_dl/modules/data/datasets/strategies/base.py</code> <pre><code>@abstractmethod\ndef next_sample(self) -&gt; list[tuple[int, tuple[int, int]]]:\n    \"\"\"Yield the next sample.\n\n    Returns:\n        A list of segments required to construct the sample.\n        Each segment is a tuple: (doc_id, (start_offset, end_offset)).\n        - doc_id: Global document index.\n        - start_offset: Start token index within the document (inclusive).\n        - end_offset: End token index within the document (exclusive).\n\n    Raises:\n        StopIteration: When the strategy is exhausted.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/modules/data/datasets/strategies/base/#optimus_dl.modules.data.datasets.strategies.base.BaseStrategy.reset","title":"<code>reset(initial_state=None)</code>  <code>abstractmethod</code>","text":"<p>Reset state to initial or checkpointed state.</p> <p>Parameters:</p> Name Type Description Default <code>initial_state</code> <code>dict[str, Any] | None</code> <p>State dictionary to restore from (optional).</p> <code>None</code> Source code in <code>optimus_dl/modules/data/datasets/strategies/base.py</code> <pre><code>@abstractmethod\ndef reset(self, initial_state: dict[str, Any] | None = None):\n    \"\"\"Reset state to initial or checkpointed state.\n\n    Args:\n        initial_state: State dictionary to restore from (optional).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/modules/data/datasets/strategies/base/#optimus_dl.modules.data.datasets.strategies.base.BaseStrategyConfig","title":"<code>BaseStrategyConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> Source code in <code>optimus_dl/modules/data/datasets/strategies/base.py</code> <pre><code>class BaseStrategyConfig(RegistryConfigStrict):\n    pass\n</code></pre>"},{"location":"reference/modules/data/datasets/strategies/concat_random/","title":"concat_random","text":""},{"location":"reference/modules/data/datasets/strategies/concat_random/#optimus_dl.modules.data.datasets.strategies.concat_random","title":"<code>optimus_dl.modules.data.datasets.strategies.concat_random</code>","text":""},{"location":"reference/modules/data/datasets/strategies/concat_random/#optimus_dl.modules.data.datasets.strategies.concat_random.ConcatAndChunkFullRandom","title":"<code>ConcatAndChunkFullRandom</code>","text":"<p>               Bases: <code>BaseStrategy</code></p> <p>Treats the dataset as a single concatenated stream of tokens, splits it into fixed-size chunks, and yields them in a globally random order.</p> <p>Algorithm: 1. Virtual Stream: [Doc0][Doc1] [Doc2] ... 2. Apply Global Offset (if random_offset): Skip first K tokens. 3. Define Chunks: Chunk i = Stream[Offset + iSize : Offset + (i+1)Size] 4. Shuffle Chunks: Permute indices 0..NumChunks. 5. Partition: Rank r takes indices {p | p % world_size == r}.</p> Source code in <code>optimus_dl/modules/data/datasets/strategies/concat_random.py</code> <pre><code>@register_dataset_sampling_strategy(\"concat_random\", ConcatAndChunkFullRandomConfig)\nclass ConcatAndChunkFullRandom(BaseStrategy):\n    \"\"\"\n    Treats the dataset as a single concatenated stream of tokens, splits it into\n    fixed-size chunks, and yields them in a globally random order.\n\n    Algorithm:\n    1. Virtual Stream: [Doc0] [Doc1] [Doc2] ...\n    2. Apply Global Offset (if random_offset): Skip first K tokens.\n    3. Define Chunks: Chunk i = Stream[Offset + i*Size : Offset + (i+1)*Size]\n    4. Shuffle Chunks: Permute indices 0..NumChunks.\n    5. Partition: Rank r takes indices {p | p % world_size == r}.\n    \"\"\"\n\n    def __init__(\n        self, cfg: ConcatAndChunkFullRandomConfig, rank: int, world_size: int, seed: int\n    ):\n        super().__init__(cfg, rank, world_size)\n        self.cfg = cfg\n        self.seed = seed\n\n        # Schedule state\n        self.my_chunk_indices: np.ndarray | None = None\n        self.chunk_ptr = 0\n        self.global_offset = 0\n\n        # Indexing for seek\n        self.cumulative_lengths: np.ndarray | None = None\n\n    def initialize(self, doc_lengths: np.ndarray):\n        super().initialize(doc_lengths)\n        # We need cumulative lengths to map GlobalTokenIdx -&gt; (DocIdx, Offset)\n        # Prepending 0 simplifies the binary search logic:\n        # intervals are [cum[i], cum[i+1])\n        self.cumulative_lengths = np.concatenate(([0], np.cumsum(doc_lengths)))\n        self._setup_schedule()\n\n    def _setup_schedule(self):\n        \"\"\"Create the chunk permutation and assign to this rank.\"\"\"\n        if self.cumulative_lengths is None:\n            raise RuntimeError(\"Strategy not initialized with doc_lengths\")\n\n        total_tokens = self.cumulative_lengths[-1]\n\n        # 1. Determine Global Offset\n        # We use a seed-based RNG to ensure all ranks agree on the offset\n        rng = np.random.default_rng(seed=self.seed)\n\n        if self.cfg.random_offset:\n            # Shift can be anything in [0, chunk_size).\n            # This ensures we hit different \"phases\" of the documents across epochs/seeds.\n            self.global_offset = rng.integers(0, self.cfg.chunk_size)\n        else:\n            self.global_offset = 0\n\n        # 2. Calculate Number of Chunks\n        available_tokens = total_tokens - self.global_offset\n        if available_tokens &lt;= 0:\n            self.my_chunk_indices = np.array([], dtype=np.int64)\n            return\n\n        num_chunks = available_tokens // self.cfg.chunk_size\n\n        # 3. Permute Chunks\n        # We shuffle the *indices* of the chunks (0 to num_chunks-1)\n        perm = rng.permutation(num_chunks)\n\n        # 4. Partition (Stride)\n        # Rank k gets perm[k, k+ws, k+2ws, ...]\n        self.my_chunk_indices = perm[self.rank :: self.world_size]\n\n    def _get_segments_for_range(\n        self, start_token: int, end_token: int\n    ) -&gt; list[tuple[int, tuple[int, int]]]:\n        \"\"\"Finds which documents cover the global token range [start, end).\"\"\"\n        segments = []\n\n        # Binary search to find the document containing 'start_token'\n        # searchsorted returns i such that cum[i-1] &lt;= val &lt; cum[i]\n        start_doc_idx = (\n            np.searchsorted(self.cumulative_lengths, start_token, side=\"right\") - 1\n        )\n\n        current_doc_idx = start_doc_idx\n        current_global_pos = start_token\n\n        while current_global_pos &lt; end_token:\n            if current_doc_idx &gt;= len(self.doc_lengths):\n                break\n\n            doc_start_global = self.cumulative_lengths[current_doc_idx]\n            doc_end_global = self.cumulative_lengths[current_doc_idx + 1]\n\n            seg_start_global = max(current_global_pos, doc_start_global)\n            seg_end_global = min(end_token, doc_end_global)\n\n            if seg_end_global &gt; seg_start_global:\n                local_start = seg_start_global - doc_start_global\n                local_end = seg_end_global - doc_start_global\n\n                segments.append(\n                    (int(current_doc_idx), (int(local_start), int(local_end)))\n                )\n\n                current_global_pos = seg_end_global\n\n            current_doc_idx += 1\n\n        return segments\n\n    def next_sample(self) -&gt; list[tuple[int, tuple[int, int]]]:\n        if self.my_chunk_indices is None:\n            raise RuntimeError(\"Strategy not initialized\")\n\n        if self.chunk_ptr &gt;= len(self.my_chunk_indices):\n            raise StopIteration\n\n        # 1. Get the abstract chunk index\n        chunk_idx = self.my_chunk_indices[self.chunk_ptr]\n\n        # 2. Map to global token coordinates\n        global_start = self.global_offset + (chunk_idx * self.cfg.chunk_size)\n        global_end = global_start + self.cfg.chunk_size\n\n        # 3. Retrieve segments\n        segments = self._get_segments_for_range(global_start, global_end)\n\n        self.chunk_ptr += 1\n        return segments\n\n    def reset(self, initial_state: dict[str, Any] | None = None):\n        if initial_state:\n            # Restore state\n            self.seed = initial_state.get(\"seed\", self.seed)\n            self.chunk_ptr = initial_state.get(\"chunk_ptr\", 0)\n\n            # Re-run setup to regenerate permutation and offset\n            if self.cumulative_lengths is not None:\n                self._setup_schedule()\n\n        else:\n            self.chunk_ptr = 0\n            if self.cumulative_lengths is not None:\n                self._setup_schedule()\n\n    def get_state(self) -&gt; dict[str, Any]:\n        return {\n            \"chunk_ptr\": self.chunk_ptr,\n            \"seed\": self.seed,\n            \"rank\": self.rank,\n        }\n</code></pre>"},{"location":"reference/modules/data/datasets/strategies/concat_random/#optimus_dl.modules.data.datasets.strategies.concat_random.ConcatAndChunkFullRandomConfig","title":"<code>ConcatAndChunkFullRandomConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseStrategyConfig</code></p> <p>ConcatAndChunkFullRandomConfig(_name: str | None = None, chunk_size: int = 2048, random_offset: bool = True)</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <code>2048</code> <code>random_offset</code> <code>bool</code> <code>True</code> Source code in <code>optimus_dl/modules/data/datasets/strategies/concat_random.py</code> <pre><code>@dataclass\nclass ConcatAndChunkFullRandomConfig(BaseStrategyConfig):\n    chunk_size: int = 2048\n    random_offset: bool = True  # Shift global start by random(0, chunk_size)\n</code></pre>"},{"location":"reference/modules/data/datasets/strategies/document/","title":"document","text":""},{"location":"reference/modules/data/datasets/strategies/document/#optimus_dl.modules.data.datasets.strategies.document","title":"<code>optimus_dl.modules.data.datasets.strategies.document</code>","text":""},{"location":"reference/modules/data/datasets/strategies/document/#optimus_dl.modules.data.datasets.strategies.document.DocumentStrategy","title":"<code>DocumentStrategy</code>","text":"<p>               Bases: <code>BaseStrategy</code></p> <p>Yields full documents from the dataset. Supports both sequential and random ordering via shuffling.</p> Source code in <code>optimus_dl/modules/data/datasets/strategies/document.py</code> <pre><code>@register_dataset_sampling_strategy(\"document\", DocumentStrategyConfig)\nclass DocumentStrategy(BaseStrategy):\n    \"\"\"\n    Yields full documents from the dataset.\n    Supports both sequential and random ordering via shuffling.\n    \"\"\"\n\n    def __init__(\n        self, cfg: DocumentStrategyConfig, rank: int, world_size: int, seed: int\n    ):\n        super().__init__(cfg, rank, world_size)\n        self.cfg = cfg\n        self.seed = seed\n        self.indices: np.ndarray | None = None\n        self.ptr = 0\n\n    def initialize(self, doc_lengths: np.ndarray):\n        super().initialize(doc_lengths)\n        self._setup_indices()\n\n    def _setup_indices(self):\n        total_docs = len(self.doc_lengths)\n\n        if self.cfg.shuffle:\n            rng = np.random.default_rng(seed=self.seed)\n            perm = rng.permutation(total_docs)\n            # Use striding for uniform distribution across ranks when shuffled\n            self.indices = perm[self.rank :: self.world_size]\n        else:\n            # Use contiguous blocks for sequential mode\n            docs_per_rank = total_docs // self.world_size\n            start = docs_per_rank * self.rank\n            end = (\n                docs_per_rank * (self.rank + 1)\n                if self.rank &lt; self.world_size - 1\n                else total_docs\n            )\n            self.indices = np.arange(start, end, dtype=np.int64)\n\n    def next_sample(self) -&gt; list[tuple[int, tuple[int, int]]]:\n        if self.indices is None:\n            raise RuntimeError(\"Strategy not initialized\")\n\n        if self.ptr &gt;= len(self.indices):\n            raise StopIteration\n\n        doc_idx = self.indices[self.ptr]\n        doc_len = self.doc_lengths[doc_idx]\n\n        self.ptr += 1\n        return [(int(doc_idx), (0, int(doc_len)))]\n\n    def reset(self, initial_state: dict[str, Any] | None = None):\n        if self.doc_lengths is not None:\n            self._setup_indices()\n\n        if initial_state:\n            self.ptr = initial_state.get(\"ptr\", 0)\n        else:\n            self.ptr = 0\n\n    def get_state(self) -&gt; dict[str, Any]:\n        return {\n            \"ptr\": self.ptr,\n            \"rank\": self.rank,\n        }\n</code></pre>"},{"location":"reference/modules/data/datasets/strategies/document/#optimus_dl.modules.data.datasets.strategies.document.DocumentStrategyConfig","title":"<code>DocumentStrategyConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseStrategyConfig</code></p> <p>DocumentStrategyConfig(_name: str | None = None, shuffle: bool = False)</p> <p>Parameters:</p> Name Type Description Default <code>shuffle</code> <code>bool</code> <code>False</code> Source code in <code>optimus_dl/modules/data/datasets/strategies/document.py</code> <pre><code>@dataclass\nclass DocumentStrategyConfig(BaseStrategyConfig):\n    shuffle: bool = False\n</code></pre>"},{"location":"reference/modules/data/presets/","title":"Index","text":""},{"location":"reference/modules/data/presets/#optimus_dl.modules.data.presets","title":"<code>optimus_dl.modules.data.presets</code>","text":""},{"location":"reference/modules/data/presets/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>fineweb-edu</code>: </li> <li><code>slimpajama</code>: </li> <li><code>tinyshakespeare</code>: </li> </ul>"},{"location":"reference/modules/data/presets/fineweb-edu/","title":"fineweb-edu","text":""},{"location":"reference/modules/data/presets/fineweb-edu/#optimus_dl.modules.data.presets.fineweb-edu","title":"<code>optimus_dl.modules.data.presets.fineweb-edu</code>","text":""},{"location":"reference/modules/data/presets/fineweb-edu/#optimus_dl.modules.data.presets.fineweb-edu.Config","title":"<code>Config</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Config(_name: str | None = None, subset: str = 'sample-10BT')</p> <p>Parameters:</p> Name Type Description Default <code>subset</code> <code>str</code> <code>'sample-10BT'</code> Source code in <code>optimus_dl/modules/data/presets/fineweb-edu.py</code> <pre><code>@dataclass\nclass Config(RegistryConfigStrict):\n    subset: str = \"sample-10BT\"\n</code></pre>"},{"location":"reference/modules/data/presets/slimpajama/","title":"slimpajama","text":""},{"location":"reference/modules/data/presets/slimpajama/#optimus_dl.modules.data.presets.slimpajama","title":"<code>optimus_dl.modules.data.presets.slimpajama</code>","text":""},{"location":"reference/modules/data/presets/slimpajama/#optimus_dl.modules.data.presets.slimpajama.Config","title":"<code>Config</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Config(_name: str | None = None, split: str = 'train', streaming: bool = True)</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <code>'train'</code> <code>streaming</code> <code>bool</code> <code>True</code> Source code in <code>optimus_dl/modules/data/presets/slimpajama.py</code> <pre><code>@dataclass\nclass Config(RegistryConfigStrict):\n    split: str = \"train\"\n    streaming: bool = True\n</code></pre>"},{"location":"reference/modules/data/presets/tinyshakespeare/","title":"tinyshakespeare","text":""},{"location":"reference/modules/data/presets/tinyshakespeare/#optimus_dl.modules.data.presets.tinyshakespeare","title":"<code>optimus_dl.modules.data.presets.tinyshakespeare</code>","text":""},{"location":"reference/modules/data/presets/tinyshakespeare/#optimus_dl.modules.data.presets.tinyshakespeare.Config","title":"<code>Config</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TxtLinesDatasetConfig</code></p> <p>Config(_name: str | None = None, file_link: str = '???', cache_dir: str = , skip_empty_lines: bool = True) Source code in <code>optimus_dl/modules/data/presets/tinyshakespeare.py</code> <pre><code>@dataclass\nclass Config(TxtLinesDatasetConfig):\n    def __post_init__(self):\n        self.file_link = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n</code></pre>"},{"location":"reference/modules/data/transforms/","title":"Index","text":""},{"location":"reference/modules/data/transforms/#optimus_dl.modules.data.transforms","title":"<code>optimus_dl.modules.data.transforms</code>","text":""},{"location":"reference/modules/data/transforms/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>base</code>: Base transform classes for data pipeline.</li> <li><code>basic_batcher</code>: Configuration for basic token batching with dynamic padding.</li> <li><code>chunk_tokens</code>: Configuration for chunking token sequences.</li> <li><code>composite</code>: Configuration for a chain of transforms.</li> <li><code>flat_tokens_batcher</code>: Configuration for token aggregation and batching.</li> <li><code>prefetch</code>: Configuration for prefetching.</li> <li><code>shuffle</code>: Configuration for data shuffling.</li> <li><code>to_device</code>: Configuration for device transfers.</li> <li><code>tokenize</code>: Configuration for text tokenization.</li> </ul>"},{"location":"reference/modules/data/transforms/base/","title":"base","text":""},{"location":"reference/modules/data/transforms/base/#optimus_dl.modules.data.transforms.base","title":"<code>optimus_dl.modules.data.transforms.base</code>","text":"<p>Base transform classes for data pipeline.</p> <p>This module defines the base classes for data transforms, which are components that process data as it flows through the pipeline. Transforms can be chained together to create complex data processing pipelines.</p>"},{"location":"reference/modules/data/transforms/base/#optimus_dl.modules.data.transforms.base.BaseTransform","title":"<code>BaseTransform</code>","text":"<p>Base class for all data transforms.</p> <p>All data transforms in Optimus-DL should inherit from this class. Transforms take a data source (BaseNode) and return a new BaseNode that applies the transformation. Transforms can be chained together using CompositeTransform.</p> <p>Subclasses should implement:</p> <ul> <li><code>build()</code>: Apply the transform to a data source and return a new node</li> </ul> Example <pre><code>@register_transform(\"tokenize\", TokenizeConfig)\nclass TokenizeTransform(BaseTransform):\n    def __init__(self, cfg: TokenizeConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.tokenizer = build_tokenizer(cfg.tokenizer_config)\n\n    def build(self, source: BaseNode) -&gt; BaseNode:\n        def tokenize_fn(item):\n            return {\"input_ids\": self.tokenizer.encode(item[\"text\"])}\n        return source.map(tokenize_fn)\n</code></pre> Source code in <code>optimus_dl/modules/data/transforms/base.py</code> <pre><code>class BaseTransform:\n    \"\"\"Base class for all data transforms.\n\n    All data transforms in Optimus-DL should inherit from this class. Transforms\n    take a data source (BaseNode) and return a new BaseNode that applies the\n    transformation. Transforms can be chained together using CompositeTransform.\n\n    Subclasses should implement:\n\n    - `build()`: Apply the transform to a data source and return a new node\n\n    Example:\n        ```python\n        @register_transform(\"tokenize\", TokenizeConfig)\n        class TokenizeTransform(BaseTransform):\n            def __init__(self, cfg: TokenizeConfig, **kwargs):\n                super().__init__(**kwargs)\n                self.tokenizer = build_tokenizer(cfg.tokenizer_config)\n\n            def build(self, source: BaseNode) -&gt; BaseNode:\n                def tokenize_fn(item):\n                    return {\"input_ids\": self.tokenizer.encode(item[\"text\"])}\n                return source.map(tokenize_fn)\n\n        ```\"\"\"\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        \"\"\"Initialize the transform.\n\n        Args:\n            *args: Positional arguments (typically unused, for compatibility).\n            **kwargs: Keyword arguments passed from the data builder.\n        \"\"\"\n        pass\n\n    def build(self, source: torchdata.nodes.BaseNode) -&gt; torchdata.nodes.BaseNode:\n        \"\"\"Apply the transform to a data source.\n\n        This method takes a data source node and returns a new node that applies\n        the transformation. The transformation is applied lazily as data flows\n        through the pipeline.\n\n        Args:\n            source: The data source node to transform.\n\n        Returns:\n            A new BaseNode that applies the transformation.\n\n        Raises:\n            NotImplementedError: Must be implemented by subclasses.\n\n        Example:\n            ```python\n            transform = TokenizeTransform(cfg)\n            transformed_source = transform.build(raw_source)\n            # transformed_source now yields tokenized data\n\n            ```\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/modules/data/transforms/base/#optimus_dl.modules.data.transforms.base.BaseTransform.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the transform.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments (typically unused, for compatibility).</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed from the data builder.</p> <code>{}</code> Source code in <code>optimus_dl/modules/data/transforms/base.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    \"\"\"Initialize the transform.\n\n    Args:\n        *args: Positional arguments (typically unused, for compatibility).\n        **kwargs: Keyword arguments passed from the data builder.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/modules/data/transforms/base/#optimus_dl.modules.data.transforms.base.BaseTransform.build","title":"<code>build(source)</code>","text":"<p>Apply the transform to a data source.</p> <p>This method takes a data source node and returns a new node that applies the transformation. The transformation is applied lazily as data flows through the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>BaseNode</code> <p>The data source node to transform.</p> required <p>Returns:</p> Type Description <code>BaseNode</code> <p>A new BaseNode that applies the transformation.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by subclasses.</p> Example <pre><code>transform = TokenizeTransform(cfg)\ntransformed_source = transform.build(raw_source)\n# transformed_source now yields tokenized data\n</code></pre> Source code in <code>optimus_dl/modules/data/transforms/base.py</code> <pre><code>def build(self, source: torchdata.nodes.BaseNode) -&gt; torchdata.nodes.BaseNode:\n    \"\"\"Apply the transform to a data source.\n\n    This method takes a data source node and returns a new node that applies\n    the transformation. The transformation is applied lazily as data flows\n    through the pipeline.\n\n    Args:\n        source: The data source node to transform.\n\n    Returns:\n        A new BaseNode that applies the transformation.\n\n    Raises:\n        NotImplementedError: Must be implemented by subclasses.\n\n    Example:\n        ```python\n        transform = TokenizeTransform(cfg)\n        transformed_source = transform.build(raw_source)\n        # transformed_source now yields tokenized data\n\n        ```\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/modules/data/transforms/base/#optimus_dl.modules.data.transforms.base.MapperConfig","title":"<code>MapperConfig</code>  <code>dataclass</code>","text":"<p>Configuration for map operations in data transforms.</p> <p>This configuration is used by transforms that apply map operations to data. It controls parallelism, ordering, and batching behavior.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>num_workers</code> <code>int</code> <code>4</code> <code>in_order</code> <code>bool</code> <code>True</code> <code>method</code> <code>str</code> <code>'thread'</code> <code>snapshot_frequency</code> <code>int</code> <code>32</code> <code>prebatch</code> <code>int</code> <code>32</code> Source code in <code>optimus_dl/modules/data/transforms/base.py</code> <pre><code>@dataclass\nclass MapperConfig:\n    \"\"\"Configuration for map operations in data transforms.\n\n    This configuration is used by transforms that apply map operations to data.\n    It controls parallelism, ordering, and batching behavior.\n\n    Attributes:\n        num_workers: Number of worker processes/threads for parallel processing.\n        in_order: If True, preserve the order of items. If False, allow out-of-order\n            processing for better performance.\n        method: Parallelization method: \"thread\" (threading) or \"process\" (multiprocessing).\n        snapshot_frequency: How often to snapshot the iterator state for checkpointing.\n        prebatch: Number of items to batch together before processing (for efficiency).\n    \"\"\"\n\n    num_workers: int = 4\n    in_order: bool = True\n    method: str = \"thread\"\n    snapshot_frequency: int = 32\n    prebatch: int = 32\n</code></pre>"},{"location":"reference/modules/data/transforms/basic_batcher/","title":"basic_batcher","text":""},{"location":"reference/modules/data/transforms/basic_batcher/#optimus_dl.modules.data.transforms.basic_batcher","title":"<code>optimus_dl.modules.data.transforms.basic_batcher</code>","text":""},{"location":"reference/modules/data/transforms/basic_batcher/#optimus_dl.modules.data.transforms.basic_batcher.BasicBatcher","title":"<code>BasicBatcher</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Transform that aggregates sequences and dynamically pads them.</p> <p>Unlike the flat batcher which packs tokens to minimize padding, this batcher keeps documents separate and pads shorter documents with a designated <code>pad_token_id</code> to match the longest document in the batch. It yields a dictionary containing the batched items and a <code>seq_lens</code> tensor recording the actual unpadded lengths of the batch.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>BasicBatcherConfig</code> <p>Batching configuration.</p> required Source code in <code>optimus_dl/modules/data/transforms/basic_batcher.py</code> <pre><code>@register_transform(\"basic_batcher\", BasicBatcherConfig)\nclass BasicBatcher(BaseTransform):\n    \"\"\"Transform that aggregates sequences and dynamically pads them.\n\n    Unlike the flat batcher which packs tokens to minimize padding, this\n    batcher keeps documents separate and pads shorter documents with a\n    designated `pad_token_id` to match the longest document in the batch.\n    It yields a dictionary containing the batched items and a `seq_lens`\n    tensor recording the actual unpadded lengths of the batch.\n\n    Args:\n        cfg: Batching configuration.\n    \"\"\"\n\n    def __init__(self, cfg: BasicBatcherConfig, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.cfg = cfg\n\n    def build(self, source: BaseNode) -&gt; BaseNode:\n        \"\"\"Apply the batching transformation to a source node.\"\"\"\n        return BasicBatcherNode(source, self.cfg)\n</code></pre>"},{"location":"reference/modules/data/transforms/basic_batcher/#optimus_dl.modules.data.transforms.basic_batcher.BasicBatcher.build","title":"<code>build(source)</code>","text":"<p>Apply the batching transformation to a source node.</p> Source code in <code>optimus_dl/modules/data/transforms/basic_batcher.py</code> <pre><code>def build(self, source: BaseNode) -&gt; BaseNode:\n    \"\"\"Apply the batching transformation to a source node.\"\"\"\n    return BasicBatcherNode(source, self.cfg)\n</code></pre>"},{"location":"reference/modules/data/transforms/basic_batcher/#optimus_dl.modules.data.transforms.basic_batcher.BasicBatcherConfig","title":"<code>BasicBatcherConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for basic token batching with dynamic padding.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int | None</code> <code>None</code> <code>max_tokens</code> <code>int | None</code> <code>None</code> <code>pad_token_id</code> <code>int</code> <code>'???'</code> <code>field</code> <code>str</code> <code>'input_ids'</code> <code>flatten</code> <code>bool</code> <code>False</code> Source code in <code>optimus_dl/modules/data/transforms/basic_batcher.py</code> <pre><code>@dataclass\nclass BasicBatcherConfig(RegistryConfigStrict):\n    \"\"\"Configuration for basic token batching with dynamic padding.\n\n    Attributes:\n        batch_size: Maximum number of sequences per batch. If None, only max_tokens limit is used.\n        max_tokens: Maximum total number of tokens per batch. If None, only batch_size limit is used.\n        pad_token_id: The token ID used to pad sequences to the maximum length.\n        field: The dictionary key containing the tokens (defaults to input_ids).\n        flatten: If True, yields a single flat sequence of shape (1, sum(lengths)) instead of (B, max_len).\n    \"\"\"\n\n    batch_size: int | None = None\n    max_tokens: int | None = None\n    pad_token_id: int = MISSING\n    field: str = \"input_ids\"\n    flatten: bool = False\n</code></pre>"},{"location":"reference/modules/data/transforms/basic_batcher/#optimus_dl.modules.data.transforms.basic_batcher.BasicBatcherNode","title":"<code>BasicBatcherNode</code>","text":"<p>               Bases: <code>BaseNode</code></p> <p>Internal node for performing dynamic padding and batching.</p> <p>Accumulates sequences from the source node up to the specified batch size or token limit, finds the maximum sequence length within that batch, and pads all sequences to match that length (or flattens them).</p> Source code in <code>optimus_dl/modules/data/transforms/basic_batcher.py</code> <pre><code>class BasicBatcherNode(BaseNode):\n    \"\"\"Internal node for performing dynamic padding and batching.\n\n    Accumulates sequences from the source node up to the specified batch size\n    or token limit, finds the maximum sequence length within that batch,\n    and pads all sequences to match that length (or flattens them).\n    \"\"\"\n\n    def __init__(self, node: BaseNode, cfg: BasicBatcherConfig, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.cfg = cfg\n        self.node = node\n        self._peeked_item = None\n\n        if self.cfg.batch_size is None and self.cfg.max_tokens is None:\n            raise ValueError(\n                \"Either batch_size or max_tokens must be specified in BasicBatcherConfig.\"\n            )\n\n    def reset(self, initial_state: dict | None = None):\n        \"\"\"Restore source node state.\"\"\"\n        super().reset(initial_state)\n        self._peeked_item = None\n        if initial_state:\n            self.cfg = initial_state[\"cfg\"]\n            self.node.reset(initial_state[\"source_state\"])\n            self._peeked_item = initial_state.get(\"_peeked_item\")\n        else:\n            self.node.reset()\n\n    def get_state(self) -&gt; dict[str, Any]:\n        \"\"\"Collect source state for checkpointing.\"\"\"\n        return {\n            \"cfg\": self.cfg,\n            \"source_state\": self.node.state_dict(),\n            \"_peeked_item\": self._peeked_item,\n        }\n\n    def next(self) -&gt; dict[str, Any]:\n        \"\"\"Yield the next complete batch of padded tokens and their original lengths.\"\"\"\n        batch_items = []\n        current_tokens = 0\n        current_count = 0\n\n        # Collect items from the source node\n        while True:\n            # Check if we reached batch_size limit\n            if self.cfg.batch_size is not None and current_count &gt;= self.cfg.batch_size:\n                break\n\n            # Get next item (either peeked or from source)\n            try:\n                if self._peeked_item is not None:\n                    item = self._peeked_item\n                    self._peeked_item = None\n                else:\n                    item = next(self.node)\n            except StopIteration:\n                if not batch_items:\n                    raise\n                break\n\n            seq_len = len(item[self.cfg.field])\n\n            # Check if we reached max_tokens limit\n            if self.cfg.max_tokens is not None:\n                # If adding this sequence exceeds max_tokens, but we already have items,\n                # save it for next batch and stop.\n                if current_tokens + seq_len &gt; self.cfg.max_tokens:\n                    if batch_items:\n                        self._peeked_item = item\n                        break\n                    else:\n                        # Single item exceeds max_tokens, yield it anyway as a single-item batch\n                        # (standard behavior for token-based batchers to avoid deadlocks)\n                        batch_items.append(item)\n                        break\n\n            batch_items.append(item)\n            current_tokens += seq_len\n            current_count += 1\n\n        # Extract sequence data\n        seqs = [item[self.cfg.field] for item in batch_items]\n\n        # Record original lengths before padding\n        lengths = [len(seq) for seq in seqs]\n\n        if self.cfg.flatten:\n            # Pack all sequences into one flat 1D sequence with causal shifting\n            # Each document s produces:\n            # - Input segment: s[:-1]\n            # - Target segment: s[1:]\n\n            if isinstance(seqs[0], torch.Tensor):\n                device = seqs[0].device\n                input_ids = torch.cat([s[:-1] for s in seqs])\n                labels = torch.cat([s[1:] for s in seqs])\n\n                # Align metadata with input_ids\n                position_ids = torch.cat(\n                    [torch.arange(len(s) - 1, device=device) for s in seqs]\n                )\n                document_ids = torch.cat(\n                    [\n                        torch.full((len(s) - 1,), i, device=device, dtype=torch.long)\n                        for i, s in enumerate(seqs)\n                    ]\n                )\n\n                # Adjusted lengths for cu_seqlens\n                shifted_lengths = [len(s) - 1 for s in seqs]\n\n                return {\n                    self.cfg.field: input_ids[None, :],\n                    \"labels\": labels[None, :],\n                    \"position_ids\": position_ids[None, :],\n                    \"document_ids\": document_ids[None, :],\n                    \"seq_lens\": torch.tensor([len(input_ids)], device=device),\n                    \"cu_seqlens\": torch.cumsum(\n                        torch.tensor([0] + shifted_lengths, device=device), dim=0\n                    ).to(torch.int32),\n                    \"max_seqlen\": int(max(shifted_lengths)),\n                }\n            else:\n                input_ids = np.concatenate([s[:-1] for s in seqs])\n                labels = np.concatenate([s[1:] for s in seqs])\n\n                position_ids = np.concatenate([np.arange(len(s) - 1) for s in seqs])\n                document_ids = np.concatenate(\n                    [np.full(len(s) - 1, i) for i, s in enumerate(seqs)]\n                )\n\n                shifted_lengths = [len(s) - 1 for s in seqs]\n\n                return {\n                    self.cfg.field: input_ids[None, :].astype(np.int64),\n                    \"labels\": labels[None, :].astype(np.int64),\n                    \"position_ids\": position_ids[None, :].astype(np.int64),\n                    \"document_ids\": document_ids[None, :].astype(np.int64),\n                    \"seq_lens\": np.array([len(input_ids)], dtype=np.int64),\n                    \"cu_seqlens\": np.cumsum([0] + shifted_lengths).astype(np.int32),\n                    \"max_seqlen\": int(max(shifted_lengths)),\n                }\n\n        # Determine the maximum sequence length in this specific batch\n        max_len = max(lengths)\n\n        # Pad sequences and generate position_ids/document_ids\n        padded_seqs = []\n        position_ids = []\n        document_ids = []\n\n        for i, (seq, seq_len) in enumerate(zip(seqs, lengths, strict=True)):\n            pad_len = max_len - seq_len\n\n            # Pad tokens\n            if isinstance(seq, torch.Tensor):\n                padded_seq = F.pad(seq, (0, pad_len), value=self.cfg.pad_token_id)\n            elif isinstance(seq, np.ndarray):\n                padded_seq = np.pad(\n                    seq, (0, pad_len), constant_values=self.cfg.pad_token_id\n                )\n            else:\n                padded_seq = list(seq) + [self.cfg.pad_token_id] * pad_len\n            padded_seqs.append(padded_seq)\n\n            # Generate position IDs (0..seq_len-1 then padded with 0 for safety with RoPE)\n            pos_ids = np.zeros(max_len, dtype=np.int64)\n            pos_ids[:seq_len] = np.arange(seq_len)\n            position_ids.append(pos_ids)\n\n            # Generate document IDs (each row is its own document)\n            doc_ids = np.full(max_len, i, dtype=np.int64)\n            document_ids.append(doc_ids)\n\n        # Build final batched outputs depending on the input type\n        if isinstance(padded_seqs[0], torch.Tensor):\n            batched_seqs = torch.stack(padded_seqs)\n            batched_lens = torch.tensor(lengths, dtype=torch.long)\n            batched_pos = torch.from_numpy(np.stack(position_ids))\n            batched_docs = torch.from_numpy(np.stack(document_ids))\n        else:\n            batched_seqs = np.array(padded_seqs, dtype=np.int64)\n            batched_lens = np.array(lengths, dtype=np.int64)\n            batched_pos = np.stack(position_ids)\n            batched_docs = np.stack(document_ids)\n\n        return {\n            self.cfg.field: batched_seqs,\n            \"seq_lens\": batched_lens,\n            \"position_ids\": batched_pos,\n            \"document_ids\": batched_docs,\n        }\n</code></pre>"},{"location":"reference/modules/data/transforms/basic_batcher/#optimus_dl.modules.data.transforms.basic_batcher.BasicBatcherNode.get_state","title":"<code>get_state()</code>","text":"<p>Collect source state for checkpointing.</p> Source code in <code>optimus_dl/modules/data/transforms/basic_batcher.py</code> <pre><code>def get_state(self) -&gt; dict[str, Any]:\n    \"\"\"Collect source state for checkpointing.\"\"\"\n    return {\n        \"cfg\": self.cfg,\n        \"source_state\": self.node.state_dict(),\n        \"_peeked_item\": self._peeked_item,\n    }\n</code></pre>"},{"location":"reference/modules/data/transforms/basic_batcher/#optimus_dl.modules.data.transforms.basic_batcher.BasicBatcherNode.next","title":"<code>next()</code>","text":"<p>Yield the next complete batch of padded tokens and their original lengths.</p> Source code in <code>optimus_dl/modules/data/transforms/basic_batcher.py</code> <pre><code>def next(self) -&gt; dict[str, Any]:\n    \"\"\"Yield the next complete batch of padded tokens and their original lengths.\"\"\"\n    batch_items = []\n    current_tokens = 0\n    current_count = 0\n\n    # Collect items from the source node\n    while True:\n        # Check if we reached batch_size limit\n        if self.cfg.batch_size is not None and current_count &gt;= self.cfg.batch_size:\n            break\n\n        # Get next item (either peeked or from source)\n        try:\n            if self._peeked_item is not None:\n                item = self._peeked_item\n                self._peeked_item = None\n            else:\n                item = next(self.node)\n        except StopIteration:\n            if not batch_items:\n                raise\n            break\n\n        seq_len = len(item[self.cfg.field])\n\n        # Check if we reached max_tokens limit\n        if self.cfg.max_tokens is not None:\n            # If adding this sequence exceeds max_tokens, but we already have items,\n            # save it for next batch and stop.\n            if current_tokens + seq_len &gt; self.cfg.max_tokens:\n                if batch_items:\n                    self._peeked_item = item\n                    break\n                else:\n                    # Single item exceeds max_tokens, yield it anyway as a single-item batch\n                    # (standard behavior for token-based batchers to avoid deadlocks)\n                    batch_items.append(item)\n                    break\n\n        batch_items.append(item)\n        current_tokens += seq_len\n        current_count += 1\n\n    # Extract sequence data\n    seqs = [item[self.cfg.field] for item in batch_items]\n\n    # Record original lengths before padding\n    lengths = [len(seq) for seq in seqs]\n\n    if self.cfg.flatten:\n        # Pack all sequences into one flat 1D sequence with causal shifting\n        # Each document s produces:\n        # - Input segment: s[:-1]\n        # - Target segment: s[1:]\n\n        if isinstance(seqs[0], torch.Tensor):\n            device = seqs[0].device\n            input_ids = torch.cat([s[:-1] for s in seqs])\n            labels = torch.cat([s[1:] for s in seqs])\n\n            # Align metadata with input_ids\n            position_ids = torch.cat(\n                [torch.arange(len(s) - 1, device=device) for s in seqs]\n            )\n            document_ids = torch.cat(\n                [\n                    torch.full((len(s) - 1,), i, device=device, dtype=torch.long)\n                    for i, s in enumerate(seqs)\n                ]\n            )\n\n            # Adjusted lengths for cu_seqlens\n            shifted_lengths = [len(s) - 1 for s in seqs]\n\n            return {\n                self.cfg.field: input_ids[None, :],\n                \"labels\": labels[None, :],\n                \"position_ids\": position_ids[None, :],\n                \"document_ids\": document_ids[None, :],\n                \"seq_lens\": torch.tensor([len(input_ids)], device=device),\n                \"cu_seqlens\": torch.cumsum(\n                    torch.tensor([0] + shifted_lengths, device=device), dim=0\n                ).to(torch.int32),\n                \"max_seqlen\": int(max(shifted_lengths)),\n            }\n        else:\n            input_ids = np.concatenate([s[:-1] for s in seqs])\n            labels = np.concatenate([s[1:] for s in seqs])\n\n            position_ids = np.concatenate([np.arange(len(s) - 1) for s in seqs])\n            document_ids = np.concatenate(\n                [np.full(len(s) - 1, i) for i, s in enumerate(seqs)]\n            )\n\n            shifted_lengths = [len(s) - 1 for s in seqs]\n\n            return {\n                self.cfg.field: input_ids[None, :].astype(np.int64),\n                \"labels\": labels[None, :].astype(np.int64),\n                \"position_ids\": position_ids[None, :].astype(np.int64),\n                \"document_ids\": document_ids[None, :].astype(np.int64),\n                \"seq_lens\": np.array([len(input_ids)], dtype=np.int64),\n                \"cu_seqlens\": np.cumsum([0] + shifted_lengths).astype(np.int32),\n                \"max_seqlen\": int(max(shifted_lengths)),\n            }\n\n    # Determine the maximum sequence length in this specific batch\n    max_len = max(lengths)\n\n    # Pad sequences and generate position_ids/document_ids\n    padded_seqs = []\n    position_ids = []\n    document_ids = []\n\n    for i, (seq, seq_len) in enumerate(zip(seqs, lengths, strict=True)):\n        pad_len = max_len - seq_len\n\n        # Pad tokens\n        if isinstance(seq, torch.Tensor):\n            padded_seq = F.pad(seq, (0, pad_len), value=self.cfg.pad_token_id)\n        elif isinstance(seq, np.ndarray):\n            padded_seq = np.pad(\n                seq, (0, pad_len), constant_values=self.cfg.pad_token_id\n            )\n        else:\n            padded_seq = list(seq) + [self.cfg.pad_token_id] * pad_len\n        padded_seqs.append(padded_seq)\n\n        # Generate position IDs (0..seq_len-1 then padded with 0 for safety with RoPE)\n        pos_ids = np.zeros(max_len, dtype=np.int64)\n        pos_ids[:seq_len] = np.arange(seq_len)\n        position_ids.append(pos_ids)\n\n        # Generate document IDs (each row is its own document)\n        doc_ids = np.full(max_len, i, dtype=np.int64)\n        document_ids.append(doc_ids)\n\n    # Build final batched outputs depending on the input type\n    if isinstance(padded_seqs[0], torch.Tensor):\n        batched_seqs = torch.stack(padded_seqs)\n        batched_lens = torch.tensor(lengths, dtype=torch.long)\n        batched_pos = torch.from_numpy(np.stack(position_ids))\n        batched_docs = torch.from_numpy(np.stack(document_ids))\n    else:\n        batched_seqs = np.array(padded_seqs, dtype=np.int64)\n        batched_lens = np.array(lengths, dtype=np.int64)\n        batched_pos = np.stack(position_ids)\n        batched_docs = np.stack(document_ids)\n\n    return {\n        self.cfg.field: batched_seqs,\n        \"seq_lens\": batched_lens,\n        \"position_ids\": batched_pos,\n        \"document_ids\": batched_docs,\n    }\n</code></pre>"},{"location":"reference/modules/data/transforms/basic_batcher/#optimus_dl.modules.data.transforms.basic_batcher.BasicBatcherNode.reset","title":"<code>reset(initial_state=None)</code>","text":"<p>Restore source node state.</p> Source code in <code>optimus_dl/modules/data/transforms/basic_batcher.py</code> <pre><code>def reset(self, initial_state: dict | None = None):\n    \"\"\"Restore source node state.\"\"\"\n    super().reset(initial_state)\n    self._peeked_item = None\n    if initial_state:\n        self.cfg = initial_state[\"cfg\"]\n        self.node.reset(initial_state[\"source_state\"])\n        self._peeked_item = initial_state.get(\"_peeked_item\")\n    else:\n        self.node.reset()\n</code></pre>"},{"location":"reference/modules/data/transforms/chunk_tokens/","title":"chunk_tokens","text":""},{"location":"reference/modules/data/transforms/chunk_tokens/#optimus_dl.modules.data.transforms.chunk_tokens","title":"<code>optimus_dl.modules.data.transforms.chunk_tokens</code>","text":""},{"location":"reference/modules/data/transforms/chunk_tokens/#optimus_dl.modules.data.transforms.chunk_tokens.ChunkTransform","title":"<code>ChunkTransform</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Transform that splits variable-length documents into fixed-size chunks.</p> <p>Useful when datasets yield full documents that are longer than the desired training sequence length.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>ChunkTransformConfig</code> <p>Chunking configuration.</p> required Source code in <code>optimus_dl/modules/data/transforms/chunk_tokens.py</code> <pre><code>@register_transform(\"chunk_tokens\", ChunkTransformConfig)\nclass ChunkTransform(BaseTransform):\n    \"\"\"Transform that splits variable-length documents into fixed-size chunks.\n\n    Useful when datasets yield full documents that are longer than the desired\n    training sequence length.\n\n    Args:\n        cfg: Chunking configuration.\n    \"\"\"\n\n    def __init__(self, cfg: ChunkTransformConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.cfg = cfg\n\n    def build(self, source: BaseNode) -&gt; BaseNode:\n        \"\"\"Apply the chunking transformation to a source node.\"\"\"\n        return ChunkTransformNode(source, self.cfg)\n</code></pre>"},{"location":"reference/modules/data/transforms/chunk_tokens/#optimus_dl.modules.data.transforms.chunk_tokens.ChunkTransform.build","title":"<code>build(source)</code>","text":"<p>Apply the chunking transformation to a source node.</p> Source code in <code>optimus_dl/modules/data/transforms/chunk_tokens.py</code> <pre><code>def build(self, source: BaseNode) -&gt; BaseNode:\n    \"\"\"Apply the chunking transformation to a source node.\"\"\"\n    return ChunkTransformNode(source, self.cfg)\n</code></pre>"},{"location":"reference/modules/data/transforms/chunk_tokens/#optimus_dl.modules.data.transforms.chunk_tokens.ChunkTransformConfig","title":"<code>ChunkTransformConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for chunking token sequences.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>max_seq_len</code> <code>int</code> <code>'???'</code> <code>add_one_for_shift</code> <code>bool</code> <code>True</code> Source code in <code>optimus_dl/modules/data/transforms/chunk_tokens.py</code> <pre><code>@dataclass\nclass ChunkTransformConfig(RegistryConfigStrict):\n    \"\"\"Configuration for chunking token sequences.\n\n    Attributes:\n        max_seq_len: Maximum length of each produced chunk.\n        add_one_for_shift: If True, adds 1 to max_seq_len (useful for causal LM training).\n    \"\"\"\n\n    max_seq_len: int = MISSING\n    add_one_for_shift: bool = True\n</code></pre>"},{"location":"reference/modules/data/transforms/chunk_tokens/#optimus_dl.modules.data.transforms.chunk_tokens.ChunkTransformNode","title":"<code>ChunkTransformNode</code>","text":"<p>               Bases: <code>BaseNode</code></p> <p>Internal node for performing sequence chunking.</p> <p>Maintains a buffer of tokens from the source node and yields segments of length <code>max_seq_len</code>.</p> Source code in <code>optimus_dl/modules/data/transforms/chunk_tokens.py</code> <pre><code>class ChunkTransformNode(BaseNode):\n    \"\"\"Internal node for performing sequence chunking.\n\n    Maintains a buffer of tokens from the source node and yields segments of\n    length `max_seq_len`.\n    \"\"\"\n\n    def __init__(self, node: BaseNode, cfg: ChunkTransformConfig, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.cfg = cfg\n        self.node = node\n        self.buffer = []\n\n    def reset(self, initial_state: dict | None = None):\n        \"\"\"Restore the buffer and source node state.\"\"\"\n        super().reset(initial_state)\n        self.buffer = []\n        if initial_state:\n            self.buffer = initial_state[\"buffer\"]\n            self.cfg = initial_state[\"cfg\"]\n\n            self.node.reset(initial_state[\"source_state\"])\n        else:\n            self.node.reset()\n\n    def get_state(self):\n        \"\"\"Collect current buffer and source state for checkpointing.\"\"\"\n        return {\n            \"buffer\": self.buffer,\n            \"cfg\": self.cfg,\n            \"source_state\": self.node.state_dict(),\n        }\n\n    def next(self):\n        \"\"\"Yield the next chunk of tokens, refilling the buffer if empty.\"\"\"\n        if len(self.buffer) == 0:\n            self.buffer = next(self.node)[\"input_ids\"]\n\n        taken = min(\n            self.cfg.max_seq_len + (1 if self.cfg.add_one_for_shift else 0),\n            len(self.buffer),\n        )\n        return_buff = self.buffer[:taken]\n        self.buffer = self.buffer[taken:]\n        return {\"input_ids\": return_buff}\n</code></pre>"},{"location":"reference/modules/data/transforms/chunk_tokens/#optimus_dl.modules.data.transforms.chunk_tokens.ChunkTransformNode.get_state","title":"<code>get_state()</code>","text":"<p>Collect current buffer and source state for checkpointing.</p> Source code in <code>optimus_dl/modules/data/transforms/chunk_tokens.py</code> <pre><code>def get_state(self):\n    \"\"\"Collect current buffer and source state for checkpointing.\"\"\"\n    return {\n        \"buffer\": self.buffer,\n        \"cfg\": self.cfg,\n        \"source_state\": self.node.state_dict(),\n    }\n</code></pre>"},{"location":"reference/modules/data/transforms/chunk_tokens/#optimus_dl.modules.data.transforms.chunk_tokens.ChunkTransformNode.next","title":"<code>next()</code>","text":"<p>Yield the next chunk of tokens, refilling the buffer if empty.</p> Source code in <code>optimus_dl/modules/data/transforms/chunk_tokens.py</code> <pre><code>def next(self):\n    \"\"\"Yield the next chunk of tokens, refilling the buffer if empty.\"\"\"\n    if len(self.buffer) == 0:\n        self.buffer = next(self.node)[\"input_ids\"]\n\n    taken = min(\n        self.cfg.max_seq_len + (1 if self.cfg.add_one_for_shift else 0),\n        len(self.buffer),\n    )\n    return_buff = self.buffer[:taken]\n    self.buffer = self.buffer[taken:]\n    return {\"input_ids\": return_buff}\n</code></pre>"},{"location":"reference/modules/data/transforms/chunk_tokens/#optimus_dl.modules.data.transforms.chunk_tokens.ChunkTransformNode.reset","title":"<code>reset(initial_state=None)</code>","text":"<p>Restore the buffer and source node state.</p> Source code in <code>optimus_dl/modules/data/transforms/chunk_tokens.py</code> <pre><code>def reset(self, initial_state: dict | None = None):\n    \"\"\"Restore the buffer and source node state.\"\"\"\n    super().reset(initial_state)\n    self.buffer = []\n    if initial_state:\n        self.buffer = initial_state[\"buffer\"]\n        self.cfg = initial_state[\"cfg\"]\n\n        self.node.reset(initial_state[\"source_state\"])\n    else:\n        self.node.reset()\n</code></pre>"},{"location":"reference/modules/data/transforms/composite/","title":"composite","text":""},{"location":"reference/modules/data/transforms/composite/#optimus_dl.modules.data.transforms.composite","title":"<code>optimus_dl.modules.data.transforms.composite</code>","text":""},{"location":"reference/modules/data/transforms/composite/#optimus_dl.modules.data.transforms.composite.CompositeTransform","title":"<code>CompositeTransform</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Transform that applies multiple transformations in sequence.</p> <p>This allows building complex data processing pipelines by composing simpler transforms (e.g., Tokenize -&gt; Chunk -&gt; Batch).</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>CompositeTransformConfig</code> <p>Composite transform configuration.</p> required Source code in <code>optimus_dl/modules/data/transforms/composite.py</code> <pre><code>@register_transform(\"compose\", CompositeTransformConfig)\nclass CompositeTransform(BaseTransform):\n    \"\"\"Transform that applies multiple transformations in sequence.\n\n    This allows building complex data processing pipelines by composing simpler\n    transforms (e.g., Tokenize -&gt; Chunk -&gt; Batch).\n\n    Args:\n        cfg: Composite transform configuration.\n    \"\"\"\n\n    def __init__(self, cfg: CompositeTransformConfig, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        transforms = []\n        for transform in cfg.transforms:\n            transforms.append(build_transform(transform, *args, **kwargs))\n        self.transforms = transforms\n\n    def build(self, source: BaseNode) -&gt; BaseNode:\n        \"\"\"Chain all internal transformations together starting from the source.\"\"\"\n        for transform in self.transforms:\n            source = transform.build(source)\n        return source\n</code></pre>"},{"location":"reference/modules/data/transforms/composite/#optimus_dl.modules.data.transforms.composite.CompositeTransform.build","title":"<code>build(source)</code>","text":"<p>Chain all internal transformations together starting from the source.</p> Source code in <code>optimus_dl/modules/data/transforms/composite.py</code> <pre><code>def build(self, source: BaseNode) -&gt; BaseNode:\n    \"\"\"Chain all internal transformations together starting from the source.\"\"\"\n    for transform in self.transforms:\n        source = transform.build(source)\n    return source\n</code></pre>"},{"location":"reference/modules/data/transforms/composite/#optimus_dl.modules.data.transforms.composite.CompositeTransformConfig","title":"<code>CompositeTransformConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for a chain of transforms.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>list[RegistryConfig]</code> <code>'???'</code> Source code in <code>optimus_dl/modules/data/transforms/composite.py</code> <pre><code>@dataclass\nclass CompositeTransformConfig(RegistryConfigStrict):\n    \"\"\"Configuration for a chain of transforms.\n\n    Attributes:\n        transforms: List of transformation configurations to apply in order.\n    \"\"\"\n\n    transforms: list[RegistryConfig] = MISSING\n</code></pre>"},{"location":"reference/modules/data/transforms/flat_tokens_batcher/","title":"flat_tokens_batcher","text":""},{"location":"reference/modules/data/transforms/flat_tokens_batcher/#optimus_dl.modules.data.transforms.flat_tokens_batcher","title":"<code>optimus_dl.modules.data.transforms.flat_tokens_batcher</code>","text":""},{"location":"reference/modules/data/transforms/flat_tokens_batcher/#optimus_dl.modules.data.transforms.flat_tokens_batcher.FlatTokensBatcher","title":"<code>FlatTokensBatcher</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Transform that aggregates token IDs and yields fixed-size batches.</p> <p>Unlike standard batchers that batch whole examples, this batcher pools all tokens from incoming documents and yields packed sequences, minimizing padding.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>FlatTokensBatcherConfig</code> <p>Batching configuration.</p> required Source code in <code>optimus_dl/modules/data/transforms/flat_tokens_batcher.py</code> <pre><code>@register_transform(\"flat_batcher\", FlatTokensBatcherConfig)\nclass FlatTokensBatcher(BaseTransform):\n    \"\"\"Transform that aggregates token IDs and yields fixed-size batches.\n\n    Unlike standard batchers that batch whole examples, this batcher pools all\n    tokens from incoming documents and yields packed sequences, minimizing\n    padding.\n\n    Args:\n        cfg: Batching configuration.\n    \"\"\"\n\n    def __init__(self, cfg: FlatTokensBatcherConfig, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.cfg = cfg\n\n    def build(self, source: BaseNode) -&gt; BaseNode:\n        \"\"\"Apply the batching transformation to a source node.\"\"\"\n        return FlatTokensBatcherNode(source, self.cfg)\n</code></pre>"},{"location":"reference/modules/data/transforms/flat_tokens_batcher/#optimus_dl.modules.data.transforms.flat_tokens_batcher.FlatTokensBatcher.build","title":"<code>build(source)</code>","text":"<p>Apply the batching transformation to a source node.</p> Source code in <code>optimus_dl/modules/data/transforms/flat_tokens_batcher.py</code> <pre><code>def build(self, source: BaseNode) -&gt; BaseNode:\n    \"\"\"Apply the batching transformation to a source node.\"\"\"\n    return FlatTokensBatcherNode(source, self.cfg)\n</code></pre>"},{"location":"reference/modules/data/transforms/flat_tokens_batcher/#optimus_dl.modules.data.transforms.flat_tokens_batcher.FlatTokensBatcherConfig","title":"<code>FlatTokensBatcherConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for token aggregation and batching.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int | None</code> <code>None</code> <code>seq_len</code> <code>int | None</code> <code>None</code> <code>max_tokens</code> <code>int | None</code> <code>None</code> <code>worker_cfg</code> <code>MapperConfig</code> <p>Configuration for map operations in data transforms.</p> <p>This configuration is used by transforms that apply map operations to data. It controls parallelism, ordering, and batching behavior.</p> <p>Attributes:     num_workers: Number of worker processes/threads for parallel processing.     in_order: If True, preserve the order of items. If False, allow out-of-order         processing for better performance.     method: Parallelization method: \"thread\" (threading) or \"process\" (multiprocessing).     snapshot_frequency: How often to snapshot the iterator state for checkpointing.     prebatch: Number of items to batch together before processing (for efficiency).</p> <code>&lt;dynamic&gt;</code> <code>field</code> <code>str</code> <code>'input_ids'</code> <code>mask_documents</code> <code>bool</code> <code>False</code> <code>flatten</code> <code>bool</code> <code>False</code> Source code in <code>optimus_dl/modules/data/transforms/flat_tokens_batcher.py</code> <pre><code>@dataclass\nclass FlatTokensBatcherConfig(RegistryConfigStrict):\n    \"\"\"Configuration for token aggregation and batching.\n\n    Attributes:\n        batch_size: Number of sequences per batch. Required if max_tokens is None.\n        seq_len: Sequence length for each sample. Required if max_tokens is None.\n        max_tokens: Total number of tokens per batch. If provided, overrides batch_size * seq_len.\n        worker_cfg: Configuration for map workers (not used directly by batcher).\n        field: The dictionary key containing the tokens (defaults to input_ids).\n        mask_documents: If True, tracks document boundaries and emits document_ids/position_ids.\n        flatten: If True, yields a single flat sequence of shape (1, sum_T) instead of (B, T).\n    \"\"\"\n\n    batch_size: int | None = None\n    seq_len: int | None = None\n    max_tokens: int | None = None\n    worker_cfg: MapperConfig = field(\n        default_factory=MapperConfig,\n    )\n    field: str = \"input_ids\"\n    mask_documents: bool = False\n    flatten: bool = False\n</code></pre>"},{"location":"reference/modules/data/transforms/flat_tokens_batcher/#optimus_dl.modules.data.transforms.flat_tokens_batcher.FlatTokensBatcherNode","title":"<code>FlatTokensBatcherNode</code>","text":"<p>               Bases: <code>BaseNode</code></p> <p>Internal node for performing token aggregation and batching.</p> <p>Accumulates pre-shifted segments from variable-length document sources into buffers until it has enough to form a complete batch of the target size. This ensures that document transitions are excluded from the sequence.</p> Source code in <code>optimus_dl/modules/data/transforms/flat_tokens_batcher.py</code> <pre><code>class FlatTokensBatcherNode(BaseNode):\n    \"\"\"Internal node for performing token aggregation and batching.\n\n    Accumulates pre-shifted segments from variable-length document sources\n    into buffers until it has enough to form a complete batch of the target size.\n    This ensures that document transitions are excluded from the sequence.\n    \"\"\"\n\n    def __init__(self, node: BaseNode, cfg: FlatTokensBatcherConfig, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.cfg = cfg\n        self.node = node\n        self.input_buffer = []\n        self.label_buffer = []\n        self.position_ids_buffer = []\n        self.document_ids_buffer = []\n        self._current_doc_id = 0\n\n        # Configuration Validation\n        if self.cfg.flatten:\n            if not self.cfg.mask_documents:\n                raise ValueError(\n                    \"FlatTokensBatcher: 'mask_documents' must be True when 'flatten' is True. \"\n                    \"Flat batches require document tracking to generate 'cu_seqlens' and 'position_ids' for sequence isolation.\"\n                )\n            if self.cfg.max_tokens is None and (\n                self.cfg.batch_size is None or self.cfg.seq_len is None\n            ):\n                raise ValueError(\n                    \"FlatTokensBatcher (flatten=True) requires either 'max_tokens' or both 'batch_size' and 'seq_len'.\"\n                )\n        else:\n            if self.cfg.batch_size is None or self.cfg.seq_len is None:\n                raise ValueError(\n                    \"FlatTokensBatcher (flatten=False) requires 'batch_size' and 'seq_len' to define the batch layout.\"\n                )\n\n    @property\n    def target_size(self):\n        \"\"\"Calculate total number of tokens needed for one batch of inputs.\"\"\"\n        if self.cfg.max_tokens is not None:\n            return self.cfg.max_tokens\n\n        return self.cfg.batch_size * self.cfg.seq_len\n\n    def reset(self, initial_state: dict | None = None):\n        \"\"\"Restore batcher buffer and source node state.\"\"\"\n        super().reset(initial_state)\n        self.input_buffer = []\n        self.label_buffer = []\n        self.position_ids_buffer = []\n        self.document_ids_buffer = []\n        self._current_doc_id = 0\n        if initial_state:\n            self.input_buffer = initial_state.get(\"input_buffer\", [])\n            self.label_buffer = initial_state.get(\"label_buffer\", [])\n            self.position_ids_buffer = initial_state.get(\"position_ids_buffer\", [])\n            self.document_ids_buffer = initial_state.get(\"document_ids_buffer\", [])\n            self._current_doc_id = initial_state.get(\"_current_doc_id\", 0)\n            self.cfg = initial_state[\"cfg\"]\n            self.node.reset(initial_state[\"source_state\"])\n        else:\n            self.node.reset()\n\n    def get_state(self) -&gt; dict[str, Any]:\n        \"\"\"Collect current buffer and source state for checkpointing.\"\"\"\n        return {\n            \"input_buffer\": self.input_buffer,\n            \"label_buffer\": self.label_buffer,\n            \"position_ids_buffer\": self.position_ids_buffer,\n            \"document_ids_buffer\": self.document_ids_buffer,\n            \"_current_doc_id\": self._current_doc_id,\n            \"cfg\": self.cfg,\n            \"source_state\": self.node.state_dict(),\n        }\n\n    def next(self) -&gt; Any:\n        \"\"\"Yield the next complete batch of tokens, filling from source as needed.\"\"\"\n        # Fill buffers with pre-shifted segments\n        while len(self.input_buffer) &lt; self.target_size:\n            item = next(self.node)\n            tokens = item[self.cfg.field]\n            if len(tokens) &lt;= 1:\n                continue\n\n            self.input_buffer.extend(tokens[:-1])\n            self.label_buffer.extend(tokens[1:])\n\n            if self.cfg.mask_documents:\n                self.position_ids_buffer.extend(range(len(tokens) - 1))\n                self.document_ids_buffer.extend(\n                    [self._current_doc_id] * (len(tokens) - 1)\n                )\n                self._current_doc_id += 1\n\n        # Extract segments\n        input_tokens = np.array(self.input_buffer[: self.target_size], dtype=np.int64)\n        target_tokens = np.array(self.label_buffer[: self.target_size], dtype=np.int64)\n\n        self.input_buffer = self.input_buffer[self.target_size :]\n        self.label_buffer = self.label_buffer[self.target_size :]\n\n        if self.cfg.flatten:\n            reshape_args = (1, -1)\n        else:\n            reshape_args = (self.cfg.batch_size, -1)\n\n        output = {\n            \"input_ids\": input_tokens.reshape(*reshape_args),\n            \"labels\": target_tokens.reshape(*reshape_args),\n        }\n\n        if self.cfg.mask_documents:\n            pos_in = np.array(\n                self.position_ids_buffer[: self.target_size], dtype=np.int64\n            )\n            doc_in = np.array(\n                self.document_ids_buffer[: self.target_size], dtype=np.int64\n            )\n            self.position_ids_buffer = self.position_ids_buffer[self.target_size :]\n            self.document_ids_buffer = self.document_ids_buffer[self.target_size :]\n\n            # Re-base document IDs\n            _, doc_ids = np.unique(doc_in, return_inverse=True)\n\n            output[\"position_ids\"] = pos_in.reshape(*reshape_args)\n            output[\"document_ids\"] = doc_ids.reshape(*reshape_args)\n\n            if self.cfg.flatten:\n                # Compute cu_seqlens for the flat batch\n                flat_doc_ids = doc_ids.reshape(-1)\n                diff = np.diff(flat_doc_ids, prepend=-1)\n                change_indices = np.where(diff != 0)[0]\n                cu_seqlens = np.append(change_indices, len(flat_doc_ids)).astype(\n                    np.int32\n                )\n                output[\"cu_seqlens\"] = cu_seqlens\n                output[\"max_seqlen\"] = int(np.max(np.diff(cu_seqlens)))\n\n        return output\n</code></pre>"},{"location":"reference/modules/data/transforms/flat_tokens_batcher/#optimus_dl.modules.data.transforms.flat_tokens_batcher.FlatTokensBatcherNode.target_size","title":"<code>target_size</code>  <code>property</code>","text":"<p>Calculate total number of tokens needed for one batch of inputs.</p>"},{"location":"reference/modules/data/transforms/flat_tokens_batcher/#optimus_dl.modules.data.transforms.flat_tokens_batcher.FlatTokensBatcherNode.get_state","title":"<code>get_state()</code>","text":"<p>Collect current buffer and source state for checkpointing.</p> Source code in <code>optimus_dl/modules/data/transforms/flat_tokens_batcher.py</code> <pre><code>def get_state(self) -&gt; dict[str, Any]:\n    \"\"\"Collect current buffer and source state for checkpointing.\"\"\"\n    return {\n        \"input_buffer\": self.input_buffer,\n        \"label_buffer\": self.label_buffer,\n        \"position_ids_buffer\": self.position_ids_buffer,\n        \"document_ids_buffer\": self.document_ids_buffer,\n        \"_current_doc_id\": self._current_doc_id,\n        \"cfg\": self.cfg,\n        \"source_state\": self.node.state_dict(),\n    }\n</code></pre>"},{"location":"reference/modules/data/transforms/flat_tokens_batcher/#optimus_dl.modules.data.transforms.flat_tokens_batcher.FlatTokensBatcherNode.next","title":"<code>next()</code>","text":"<p>Yield the next complete batch of tokens, filling from source as needed.</p> Source code in <code>optimus_dl/modules/data/transforms/flat_tokens_batcher.py</code> <pre><code>def next(self) -&gt; Any:\n    \"\"\"Yield the next complete batch of tokens, filling from source as needed.\"\"\"\n    # Fill buffers with pre-shifted segments\n    while len(self.input_buffer) &lt; self.target_size:\n        item = next(self.node)\n        tokens = item[self.cfg.field]\n        if len(tokens) &lt;= 1:\n            continue\n\n        self.input_buffer.extend(tokens[:-1])\n        self.label_buffer.extend(tokens[1:])\n\n        if self.cfg.mask_documents:\n            self.position_ids_buffer.extend(range(len(tokens) - 1))\n            self.document_ids_buffer.extend(\n                [self._current_doc_id] * (len(tokens) - 1)\n            )\n            self._current_doc_id += 1\n\n    # Extract segments\n    input_tokens = np.array(self.input_buffer[: self.target_size], dtype=np.int64)\n    target_tokens = np.array(self.label_buffer[: self.target_size], dtype=np.int64)\n\n    self.input_buffer = self.input_buffer[self.target_size :]\n    self.label_buffer = self.label_buffer[self.target_size :]\n\n    if self.cfg.flatten:\n        reshape_args = (1, -1)\n    else:\n        reshape_args = (self.cfg.batch_size, -1)\n\n    output = {\n        \"input_ids\": input_tokens.reshape(*reshape_args),\n        \"labels\": target_tokens.reshape(*reshape_args),\n    }\n\n    if self.cfg.mask_documents:\n        pos_in = np.array(\n            self.position_ids_buffer[: self.target_size], dtype=np.int64\n        )\n        doc_in = np.array(\n            self.document_ids_buffer[: self.target_size], dtype=np.int64\n        )\n        self.position_ids_buffer = self.position_ids_buffer[self.target_size :]\n        self.document_ids_buffer = self.document_ids_buffer[self.target_size :]\n\n        # Re-base document IDs\n        _, doc_ids = np.unique(doc_in, return_inverse=True)\n\n        output[\"position_ids\"] = pos_in.reshape(*reshape_args)\n        output[\"document_ids\"] = doc_ids.reshape(*reshape_args)\n\n        if self.cfg.flatten:\n            # Compute cu_seqlens for the flat batch\n            flat_doc_ids = doc_ids.reshape(-1)\n            diff = np.diff(flat_doc_ids, prepend=-1)\n            change_indices = np.where(diff != 0)[0]\n            cu_seqlens = np.append(change_indices, len(flat_doc_ids)).astype(\n                np.int32\n            )\n            output[\"cu_seqlens\"] = cu_seqlens\n            output[\"max_seqlen\"] = int(np.max(np.diff(cu_seqlens)))\n\n    return output\n</code></pre>"},{"location":"reference/modules/data/transforms/flat_tokens_batcher/#optimus_dl.modules.data.transforms.flat_tokens_batcher.FlatTokensBatcherNode.reset","title":"<code>reset(initial_state=None)</code>","text":"<p>Restore batcher buffer and source node state.</p> Source code in <code>optimus_dl/modules/data/transforms/flat_tokens_batcher.py</code> <pre><code>def reset(self, initial_state: dict | None = None):\n    \"\"\"Restore batcher buffer and source node state.\"\"\"\n    super().reset(initial_state)\n    self.input_buffer = []\n    self.label_buffer = []\n    self.position_ids_buffer = []\n    self.document_ids_buffer = []\n    self._current_doc_id = 0\n    if initial_state:\n        self.input_buffer = initial_state.get(\"input_buffer\", [])\n        self.label_buffer = initial_state.get(\"label_buffer\", [])\n        self.position_ids_buffer = initial_state.get(\"position_ids_buffer\", [])\n        self.document_ids_buffer = initial_state.get(\"document_ids_buffer\", [])\n        self._current_doc_id = initial_state.get(\"_current_doc_id\", 0)\n        self.cfg = initial_state[\"cfg\"]\n        self.node.reset(initial_state[\"source_state\"])\n    else:\n        self.node.reset()\n</code></pre>"},{"location":"reference/modules/data/transforms/prefetch/","title":"prefetch","text":""},{"location":"reference/modules/data/transforms/prefetch/#optimus_dl.modules.data.transforms.prefetch","title":"<code>optimus_dl.modules.data.transforms.prefetch</code>","text":""},{"location":"reference/modules/data/transforms/prefetch/#optimus_dl.modules.data.transforms.prefetch.PrefetchTransform","title":"<code>PrefetchTransform</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Transform that pre-fetches data items in a background thread.</p> <p>This helps hide data loading and transformation latency by keeping a buffer of items ready for the training loop.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>PrefetchTransformConfig</code> <p>Prefetching configuration.</p> required Source code in <code>optimus_dl/modules/data/transforms/prefetch.py</code> <pre><code>@register_transform(\"prefetch\", PrefetchTransformConfig)\nclass PrefetchTransform(BaseTransform):\n    \"\"\"Transform that pre-fetches data items in a background thread.\n\n    This helps hide data loading and transformation latency by keeping a buffer\n    of items ready for the training loop.\n\n    Args:\n        cfg: Prefetching configuration.\n    \"\"\"\n\n    def __init__(self, cfg: PrefetchTransformConfig, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.cfg = cfg\n\n    def build(self, source: BaseNode) -&gt; BaseNode:\n        \"\"\"Wrap the source node with a Prefetcher.\"\"\"\n        return torchdata.nodes.Prefetcher(\n            source, prefetch_factor=self.cfg.prefetch_factor\n        )\n</code></pre>"},{"location":"reference/modules/data/transforms/prefetch/#optimus_dl.modules.data.transforms.prefetch.PrefetchTransform.build","title":"<code>build(source)</code>","text":"<p>Wrap the source node with a Prefetcher.</p> Source code in <code>optimus_dl/modules/data/transforms/prefetch.py</code> <pre><code>def build(self, source: BaseNode) -&gt; BaseNode:\n    \"\"\"Wrap the source node with a Prefetcher.\"\"\"\n    return torchdata.nodes.Prefetcher(\n        source, prefetch_factor=self.cfg.prefetch_factor\n    )\n</code></pre>"},{"location":"reference/modules/data/transforms/prefetch/#optimus_dl.modules.data.transforms.prefetch.PrefetchTransformConfig","title":"<code>PrefetchTransformConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for prefetching.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>prefetch_factor</code> <code>int</code> <code>8</code> Source code in <code>optimus_dl/modules/data/transforms/prefetch.py</code> <pre><code>@dataclass\nclass PrefetchTransformConfig(RegistryConfigStrict):\n    \"\"\"Configuration for prefetching.\n\n    Attributes:\n        prefetch_factor: Number of items to fetch ahead of request.\n    \"\"\"\n\n    prefetch_factor: int = 8\n</code></pre>"},{"location":"reference/modules/data/transforms/shuffle/","title":"shuffle","text":""},{"location":"reference/modules/data/transforms/shuffle/#optimus_dl.modules.data.transforms.shuffle","title":"<code>optimus_dl.modules.data.transforms.shuffle</code>","text":""},{"location":"reference/modules/data/transforms/shuffle/#optimus_dl.modules.data.transforms.shuffle.ShuffleTransform","title":"<code>ShuffleTransform</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Transform that shuffles data items using an internal buffer.</p> <p>Ensures that items are yielded in a randomized order within a sliding window of <code>buffer_size</code>. Seed is automatically adjusted per rank to ensure variety in distributed training.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>ShuffleTransformConfig</code> <p>Shuffling configuration.</p> required <code>rank</code> <code>int</code> <p>Distributed rank.</p> required Source code in <code>optimus_dl/modules/data/transforms/shuffle.py</code> <pre><code>@register_transform(\"shuffle\", ShuffleTransformConfig)\nclass ShuffleTransform(BaseTransform):\n    \"\"\"Transform that shuffles data items using an internal buffer.\n\n    Ensures that items are yielded in a randomized order within a sliding window\n    of `buffer_size`. Seed is automatically adjusted per rank to ensure variety\n    in distributed training.\n\n    Args:\n        cfg: Shuffling configuration.\n        rank: Distributed rank.\n    \"\"\"\n\n    def __init__(self, cfg: ShuffleTransformConfig, rank: int, seed: int, **kwargs):\n        super().__init__(**kwargs)\n        self.cfg = cfg\n        self.rank = rank\n        self.seed = seed\n\n    def build(self, source: BaseNode) -&gt; BaseNode:\n        \"\"\"Apply the shuffling transformation to a source node.\"\"\"\n        return ShuffleTransformNode(source, self.cfg, rank=self.rank, seed=self.seed)\n</code></pre>"},{"location":"reference/modules/data/transforms/shuffle/#optimus_dl.modules.data.transforms.shuffle.ShuffleTransform.build","title":"<code>build(source)</code>","text":"<p>Apply the shuffling transformation to a source node.</p> Source code in <code>optimus_dl/modules/data/transforms/shuffle.py</code> <pre><code>def build(self, source: BaseNode) -&gt; BaseNode:\n    \"\"\"Apply the shuffling transformation to a source node.\"\"\"\n    return ShuffleTransformNode(source, self.cfg, rank=self.rank, seed=self.seed)\n</code></pre>"},{"location":"reference/modules/data/transforms/shuffle/#optimus_dl.modules.data.transforms.shuffle.ShuffleTransformConfig","title":"<code>ShuffleTransformConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for data shuffling.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>buffer_size</code> <code>int</code> <code>1024</code> Source code in <code>optimus_dl/modules/data/transforms/shuffle.py</code> <pre><code>@dataclass\nclass ShuffleTransformConfig(RegistryConfigStrict):\n    \"\"\"Configuration for data shuffling.\n\n    Attributes:\n        buffer_size: Number of items to hold in the shuffling buffer. Larger\n            buffers provide better shuffling but use more memory.\n    \"\"\"\n\n    buffer_size: int = 1024\n</code></pre>"},{"location":"reference/modules/data/transforms/shuffle/#optimus_dl.modules.data.transforms.shuffle.ShuffleTransformNode","title":"<code>ShuffleTransformNode</code>","text":"<p>               Bases: <code>BaseNode</code></p> <p>Internal node for performing buffer-based shuffling.</p> <p>Fills an internal buffer from the source node and yields items selected randomly from that buffer.</p> Source code in <code>optimus_dl/modules/data/transforms/shuffle.py</code> <pre><code>class ShuffleTransformNode(BaseNode):\n    \"\"\"Internal node for performing buffer-based shuffling.\n\n    Fills an internal buffer from the source node and yields items selected\n    randomly from that buffer.\n    \"\"\"\n\n    def __init__(\n        self,\n        node: BaseNode,\n        cfg: ShuffleTransformConfig,\n        rank: int,\n        seed: int,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.cfg = cfg\n        self.node = node\n        self.buffer = []\n        self.terminated = False\n        self.rank = rank\n\n        self.rng = np.random.default_rng(seed + rank * 41)\n\n    def reset(self, initial_state: dict | None = None):\n        \"\"\"Restore the shuffle buffer and RNG state.\"\"\"\n        super().reset(initial_state)\n        self.buffer = []\n        self.terminated = False\n        if initial_state:\n            self.buffer = initial_state[\"buffer\"]\n            self.cfg = initial_state[\"cfg\"]\n            self.rng.bit_generator.state = initial_state[\"rng_state\"]\n            self.terminated = initial_state[\"terminated\"]\n\n            assert self.rank == initial_state[\"rank\"]\n\n            self.node.reset(initial_state[\"source_state\"])\n        else:\n            self.node.reset()\n\n    def get_state(self):\n        \"\"\"Collect current buffer, terminated flag, and RNG state for checkpointing.\"\"\"\n        return {\n            \"buffer\": self.buffer,\n            \"cfg\": self.cfg,\n            \"source_state\": self.node.state_dict(),\n            \"rng_state\": self.rng.bit_generator.state,\n            \"terminated\": self.terminated,\n            \"rank\": self.rank,\n        }\n\n    def next(self):\n        \"\"\"Yield a randomly selected item from the shuffle buffer.\"\"\"\n        while len(self.buffer) &lt; self.cfg.buffer_size and not self.terminated:\n            try:\n                self.buffer.append(next(self.node))\n            except StopIteration:\n                self.terminated = True\n                break\n        if len(self.buffer) == 0:\n            raise StopIteration\n        return self.buffer.pop(self.rng.integers(0, len(self.buffer)))\n</code></pre>"},{"location":"reference/modules/data/transforms/shuffle/#optimus_dl.modules.data.transforms.shuffle.ShuffleTransformNode.get_state","title":"<code>get_state()</code>","text":"<p>Collect current buffer, terminated flag, and RNG state for checkpointing.</p> Source code in <code>optimus_dl/modules/data/transforms/shuffle.py</code> <pre><code>def get_state(self):\n    \"\"\"Collect current buffer, terminated flag, and RNG state for checkpointing.\"\"\"\n    return {\n        \"buffer\": self.buffer,\n        \"cfg\": self.cfg,\n        \"source_state\": self.node.state_dict(),\n        \"rng_state\": self.rng.bit_generator.state,\n        \"terminated\": self.terminated,\n        \"rank\": self.rank,\n    }\n</code></pre>"},{"location":"reference/modules/data/transforms/shuffle/#optimus_dl.modules.data.transforms.shuffle.ShuffleTransformNode.next","title":"<code>next()</code>","text":"<p>Yield a randomly selected item from the shuffle buffer.</p> Source code in <code>optimus_dl/modules/data/transforms/shuffle.py</code> <pre><code>def next(self):\n    \"\"\"Yield a randomly selected item from the shuffle buffer.\"\"\"\n    while len(self.buffer) &lt; self.cfg.buffer_size and not self.terminated:\n        try:\n            self.buffer.append(next(self.node))\n        except StopIteration:\n            self.terminated = True\n            break\n    if len(self.buffer) == 0:\n        raise StopIteration\n    return self.buffer.pop(self.rng.integers(0, len(self.buffer)))\n</code></pre>"},{"location":"reference/modules/data/transforms/shuffle/#optimus_dl.modules.data.transforms.shuffle.ShuffleTransformNode.reset","title":"<code>reset(initial_state=None)</code>","text":"<p>Restore the shuffle buffer and RNG state.</p> Source code in <code>optimus_dl/modules/data/transforms/shuffle.py</code> <pre><code>def reset(self, initial_state: dict | None = None):\n    \"\"\"Restore the shuffle buffer and RNG state.\"\"\"\n    super().reset(initial_state)\n    self.buffer = []\n    self.terminated = False\n    if initial_state:\n        self.buffer = initial_state[\"buffer\"]\n        self.cfg = initial_state[\"cfg\"]\n        self.rng.bit_generator.state = initial_state[\"rng_state\"]\n        self.terminated = initial_state[\"terminated\"]\n\n        assert self.rank == initial_state[\"rank\"]\n\n        self.node.reset(initial_state[\"source_state\"])\n    else:\n        self.node.reset()\n</code></pre>"},{"location":"reference/modules/data/transforms/to_device/","title":"to_device","text":""},{"location":"reference/modules/data/transforms/to_device/#optimus_dl.modules.data.transforms.to_device","title":"<code>optimus_dl.modules.data.transforms.to_device</code>","text":""},{"location":"reference/modules/data/transforms/to_device/#optimus_dl.modules.data.transforms.to_device.ToDeviceTransform","title":"<code>ToDeviceTransform</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Transform that moves data tensors to the target compute device.</p> <p>This transform ensures that input data is on the correct device (e.g., CUDA) before it enters the model. It includes performance optimizations for GPUs:</p> <ul> <li>Memory Pinning: Automatically uses <code>PinMemory</code> to speed up CPU-to-GPU transfers.</li> <li>Asynchronous Transfers: Uses <code>non_blocking=True</code> for CUDA devices.</li> <li>Prefetching: Adds an additional prefetch layer after pinning.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>ToDeviceTransformConfig</code> <p>Device transfer configuration.</p> required <code>device</code> <p>The target PyTorch device.</p> required Source code in <code>optimus_dl/modules/data/transforms/to_device.py</code> <pre><code>@register_transform(\"to_device\", ToDeviceTransformConfig)\nclass ToDeviceTransform(BaseTransform):\n    \"\"\"Transform that moves data tensors to the target compute device.\n\n    This transform ensures that input data is on the correct device (e.g., CUDA)\n    before it enters the model. It includes performance optimizations for GPUs:\n\n    - **Memory Pinning**: Automatically uses `PinMemory` to speed up CPU-to-GPU transfers.\n    - **Asynchronous Transfers**: Uses `non_blocking=True` for CUDA devices.\n    - **Prefetching**: Adds an additional prefetch layer after pinning.\n\n    Args:\n        cfg: Device transfer configuration.\n        device: The target PyTorch device.\n    \"\"\"\n\n    def __init__(self, cfg: ToDeviceTransformConfig, device, **kwargs):\n        super().__init__(**kwargs)\n        self.properties = cfg.properties\n        self.device = device\n\n        assert isinstance(device, torch.device)\n\n    def _map(self, sample: dict):\n        \"\"\"Map function to move specific dictionary entries to the device.\"\"\"\n        if self.properties is None:\n            properties = sample.keys()\n        else:\n            properties = self.properties\n\n        for property in properties:\n            if self.device.type != \"cuda\":\n                value = torch.as_tensor(sample[property], device=self.device)\n            else:\n                value = torch.as_tensor(sample[property])\n                value = value.to(self.device, non_blocking=True)\n            sample[property] = value\n        return sample\n\n    def build(self, source: BaseNode) -&gt; BaseNode:\n        \"\"\"Wrap the source node with pinning, prefetching, and the device map.\"\"\"\n        if self.device.type == \"cuda\":\n            source = torchdata.nodes.PinMemory(\n                source=source,\n                pin_memory_device=\"cuda\",\n            )\n            source = torchdata.nodes.Prefetcher(\n                source=source,\n                prefetch_factor=2,\n            )\n        return torchdata.nodes.Mapper(\n            source=source,\n            map_fn=self._map,\n        )\n</code></pre>"},{"location":"reference/modules/data/transforms/to_device/#optimus_dl.modules.data.transforms.to_device.ToDeviceTransform.build","title":"<code>build(source)</code>","text":"<p>Wrap the source node with pinning, prefetching, and the device map.</p> Source code in <code>optimus_dl/modules/data/transforms/to_device.py</code> <pre><code>def build(self, source: BaseNode) -&gt; BaseNode:\n    \"\"\"Wrap the source node with pinning, prefetching, and the device map.\"\"\"\n    if self.device.type == \"cuda\":\n        source = torchdata.nodes.PinMemory(\n            source=source,\n            pin_memory_device=\"cuda\",\n        )\n        source = torchdata.nodes.Prefetcher(\n            source=source,\n            prefetch_factor=2,\n        )\n    return torchdata.nodes.Mapper(\n        source=source,\n        map_fn=self._map,\n    )\n</code></pre>"},{"location":"reference/modules/data/transforms/to_device/#optimus_dl.modules.data.transforms.to_device.ToDeviceTransformConfig","title":"<code>ToDeviceTransformConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for device transfers.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>properties</code> <code>list[str] | None</code> <code>None</code> Source code in <code>optimus_dl/modules/data/transforms/to_device.py</code> <pre><code>@dataclass\nclass ToDeviceTransformConfig(RegistryConfigStrict):\n    \"\"\"Configuration for device transfers.\n\n    Attributes:\n        properties: List of dictionary keys to move to the device. If None,\n            moves all values in the dictionary.\n    \"\"\"\n\n    properties: list[str] | None = field(default_factory=lambda: None)\n</code></pre>"},{"location":"reference/modules/data/transforms/tokenize/","title":"tokenize","text":""},{"location":"reference/modules/data/transforms/tokenize/#optimus_dl.modules.data.transforms.tokenize","title":"<code>optimus_dl.modules.data.transforms.tokenize</code>","text":""},{"location":"reference/modules/data/transforms/tokenize/#optimus_dl.modules.data.transforms.tokenize.TokenizeTransform","title":"<code>TokenizeTransform</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Transform that converts raw text strings into sequences of token IDs.</p> <p>Uses the registry system to instantiate a tokenizer and applies it to the input data. Supports parallel mapping via <code>torchdata.nodes.ParallelMapper</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>TokenizeTransformConfig</code> <p>Tokenization configuration.</p> required Source code in <code>optimus_dl/modules/data/transforms/tokenize.py</code> <pre><code>@register_transform(\"tokenize\", TokenizeTransformConfig)\nclass TokenizeTransform(BaseTransform):\n    \"\"\"Transform that converts raw text strings into sequences of token IDs.\n\n    Uses the registry system to instantiate a tokenizer and applies it to the\n    input data. Supports parallel mapping via `torchdata.nodes.ParallelMapper`.\n\n    Args:\n        cfg: Tokenization configuration.\n    \"\"\"\n\n    def __init__(self, cfg: TokenizeTransformConfig, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.tokenizer = build_tokenizer(cfg.tokenizer_config)\n\n        # Convert MapperConfig dataclass to dict for torchdata\n        from dataclasses import asdict\n\n        if hasattr(cfg.worker_cfg, \"__dataclass_fields__\"):\n            # It's a dataclass\n            self.mapper_cfg = asdict(cfg.worker_cfg)\n        else:\n            # Try OmegaConf conversion for backwards compatibility\n            self.mapper_cfg = OmegaConf.to_container(cfg.worker_cfg)\n\n        self.debug_counter = 0\n        self.debug_samples = cfg.debug_samples\n\n    def _map(self, sample):\n        \"\"\"Map raw text sample to input_ids using the internal tokenizer.\"\"\"\n        text = sample[\"text\"]\n        ids = self.tokenizer.encode(text)\n        if self.debug_counter &lt; self.debug_samples:\n            self.debug_counter += 1\n            tokens_debug = []\n            for (\n                token_id\n            ) in (\n                ids\n            ):  # Renamed 'token' to 'token_id' to avoid confusion with token strings\n                token_decoded = self.tokenizer.decode([token_id])\n                tokens_debug.append(f\"{token_id}({token_decoded!r})\")\n\n            tokens_debug = \", \".join(tokens_debug)\n            logger.info(f\"Debugging tokenizer sample: \\n{tokens_debug}\\n=======\")\n\n        return {\n            \"input_ids\": ids,\n        }\n\n    def build(self, source: BaseNode) -&gt; BaseNode:\n        \"\"\"Wrap the source node with a parallel mapper using the tokenizer function.\"\"\"\n        return torchdata.nodes.ParallelMapper(\n            source=source,\n            map_fn=self._map,\n            **self.mapper_cfg,\n        )\n</code></pre>"},{"location":"reference/modules/data/transforms/tokenize/#optimus_dl.modules.data.transforms.tokenize.TokenizeTransform.build","title":"<code>build(source)</code>","text":"<p>Wrap the source node with a parallel mapper using the tokenizer function.</p> Source code in <code>optimus_dl/modules/data/transforms/tokenize.py</code> <pre><code>def build(self, source: BaseNode) -&gt; BaseNode:\n    \"\"\"Wrap the source node with a parallel mapper using the tokenizer function.\"\"\"\n    return torchdata.nodes.ParallelMapper(\n        source=source,\n        map_fn=self._map,\n        **self.mapper_cfg,\n    )\n</code></pre>"},{"location":"reference/modules/data/transforms/tokenize/#optimus_dl.modules.data.transforms.tokenize.TokenizeTransformConfig","title":"<code>TokenizeTransformConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for text tokenization.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>tokenizer_config</code> <code>Any</code> <code>'???'</code> <code>debug_samples</code> <code>int</code> <code>0</code> <code>worker_cfg</code> <code>MapperConfig</code> <p>Configuration for map operations in data transforms.</p> <p>This configuration is used by transforms that apply map operations to data. It controls parallelism, ordering, and batching behavior.</p> <p>Attributes:     num_workers: Number of worker processes/threads for parallel processing.     in_order: If True, preserve the order of items. If False, allow out-of-order         processing for better performance.     method: Parallelization method: \"thread\" (threading) or \"process\" (multiprocessing).     snapshot_frequency: How often to snapshot the iterator state for checkpointing.     prebatch: Number of items to batch together before processing (for efficiency).</p> <code>&lt;dynamic&gt;</code> Source code in <code>optimus_dl/modules/data/transforms/tokenize.py</code> <pre><code>@dataclass\nclass TokenizeTransformConfig(RegistryConfigStrict):\n    \"\"\"Configuration for text tokenization.\n\n    Attributes:\n        tokenizer_config: Registry configuration for the tokenizer to use.\n        debug_samples: Number of samples to print to the log for debugging.\n        worker_cfg: Configuration for parallel processing workers.\n    \"\"\"\n\n    tokenizer_config: Any = MISSING\n    debug_samples: int = 0\n    worker_cfg: MapperConfig = field(\n        default_factory=MapperConfig,\n    )\n</code></pre>"},{"location":"reference/modules/distributed/","title":"Index","text":""},{"location":"reference/modules/distributed/#optimus_dl.modules.distributed","title":"<code>optimus_dl.modules.distributed</code>","text":""},{"location":"reference/modules/distributed/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>base</code>: Abstract base class for distributed communication.</li> <li><code>config</code>: Configuration for distributed training topologies.</li> <li><code>fake</code>: A non-distributed implementation of the Collective interface.</li> <li><code>mesh</code>: Container for parallel and physical device meshes.</li> </ul>"},{"location":"reference/modules/distributed/base/","title":"base","text":""},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base","title":"<code>optimus_dl.modules.distributed.base</code>","text":""},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective","title":"<code>Collective</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for distributed communication.</p> <p>This class defines the interface for all collective operations and distributed topology information. It allows the framework to switch between real distributed training (MeshCollective) and single-device/CPU execution (FakeCollective) without changing the training logic.</p> <p>Attributes:</p> Name Type Description <code>rank</code> <code>int</code> <p>Global rank of the current process.</p> <code>world_size</code> <code>int</code> <p>Total number of processes in the global gang.</p> Source code in <code>optimus_dl/modules/distributed/base.py</code> <pre><code>class Collective(ABC):\n    \"\"\"Abstract base class for distributed communication.\n\n    This class defines the interface for all collective operations and distributed\n    topology information. It allows the framework to switch between real\n    distributed training (MeshCollective) and single-device/CPU execution\n    (FakeCollective) without changing the training logic.\n\n    Attributes:\n        rank: Global rank of the current process.\n        world_size: Total number of processes in the global gang.\n    \"\"\"\n\n    rank: int\n    world_size: int\n\n    def __init__(self, rank, world_size) -&gt; None:\n        super().__init__()\n        self.rank = rank\n        self.world_size = world_size\n\n        assert rank &lt; world_size\n\n    @property\n    @abstractmethod\n    def local(self) -&gt; \"Collective\":\n        \"\"\"Get a collective limited to the current node (local ranks).\"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def tp_world(self) -&gt; \"Collective\":\n        \"\"\"Get a collective for the current Tensor Parallelism group.\"\"\"\n        ...\n\n    @property\n    def is_master(self) -&gt; bool:\n        \"\"\"True if the current process is the master (rank 0).\"\"\"\n        return self.rank == 0\n\n    @property\n    def is_local_master(self) -&gt; bool:\n        \"\"\"True if the current process is the master of its node (local rank 0).\"\"\"\n        return self.local_rank == 0\n\n    @property\n    @abstractmethod\n    def local_rank(self) -&gt; int:\n        \"\"\"Rank within the current node.\"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def dp_rank(self) -&gt; int:\n        \"\"\"Rank within the Data Parallelism group.\"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def dp_world_size(self) -&gt; int:\n        \"\"\"Size of the Data Parallelism group.\"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def tp_rank(self) -&gt; int:\n        \"\"\"Rank within the Tensor Parallelism group.\"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def tp_world_size(self) -&gt; int:\n        \"\"\"Size of the Tensor Parallelism group.\"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def default_device(self) -&gt; torch.device:\n        \"\"\"Get the default PyTorch device for this process.\"\"\"\n        ...\n\n    @abstractmethod\n    def close(self) -&gt; None:\n        \"\"\"Clean up distributed resources.\"\"\"\n        ...\n\n    @abstractmethod\n    def barrier(self) -&gt; None:\n        \"\"\"Synchronize all processes in the collective.\"\"\"\n        ...\n\n    @abstractmethod\n    def all_reduce(self, tensor: Tensor, op: ReduceOp.RedOpType) -&gt; None:\n        \"\"\"Perform an all-reduce operation.\"\"\"\n        ...\n\n    @abstractmethod\n    def all_gather(self, output_tensor: Tensor, input_tensor: Tensor) -&gt; None:\n        \"\"\"Perform an all-gather operation into a single tensor.\"\"\"\n        ...\n\n    @abstractmethod\n    def all_gather_to_list(\n        self, output_tensors: list[Tensor], input_tensor: Tensor\n    ) -&gt; None:\n        \"\"\"Perform an all-gather operation into a list of tensors.\"\"\"\n        ...\n\n    @abstractmethod\n    def all_gather_objects(\n        self,\n        object: object,\n    ) -&gt; list[object]:\n        \"\"\"Perform an all-gather operation for picklable Python objects.\"\"\"\n        ...\n\n    @abstractmethod\n    def broadcast(self, tensor: Tensor, source_rank: int = 0) -&gt; None:\n        \"\"\"Broadcast a tensor from the source rank to all other ranks.\"\"\"\n        ...\n\n    @abstractmethod\n    def broadcast_objects(self, objects: list[object], source_rank: int = 0) -&gt; None:\n        \"\"\"Broadcast picklable Python objects from the source rank.\"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def process_group(self) -&gt; ProcessGroup | None:\n        \"\"\"The underlying PyTorch ProcessGroup, if available.\"\"\"\n        ...\n</code></pre>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.default_device","title":"<code>default_device</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the default PyTorch device for this process.</p>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.dp_rank","title":"<code>dp_rank</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Rank within the Data Parallelism group.</p>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.dp_world_size","title":"<code>dp_world_size</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Size of the Data Parallelism group.</p>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.is_local_master","title":"<code>is_local_master</code>  <code>property</code>","text":"<p>True if the current process is the master of its node (local rank 0).</p>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.is_master","title":"<code>is_master</code>  <code>property</code>","text":"<p>True if the current process is the master (rank 0).</p>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.local","title":"<code>local</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get a collective limited to the current node (local ranks).</p>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.local_rank","title":"<code>local_rank</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Rank within the current node.</p>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.process_group","title":"<code>process_group</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>The underlying PyTorch ProcessGroup, if available.</p>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.tp_rank","title":"<code>tp_rank</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Rank within the Tensor Parallelism group.</p>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.tp_world","title":"<code>tp_world</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get a collective for the current Tensor Parallelism group.</p>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.tp_world_size","title":"<code>tp_world_size</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Size of the Tensor Parallelism group.</p>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.all_gather","title":"<code>all_gather(output_tensor, input_tensor)</code>  <code>abstractmethod</code>","text":"<p>Perform an all-gather operation into a single tensor.</p> Source code in <code>optimus_dl/modules/distributed/base.py</code> <pre><code>@abstractmethod\ndef all_gather(self, output_tensor: Tensor, input_tensor: Tensor) -&gt; None:\n    \"\"\"Perform an all-gather operation into a single tensor.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.all_gather_objects","title":"<code>all_gather_objects(object)</code>  <code>abstractmethod</code>","text":"<p>Perform an all-gather operation for picklable Python objects.</p> Source code in <code>optimus_dl/modules/distributed/base.py</code> <pre><code>@abstractmethod\ndef all_gather_objects(\n    self,\n    object: object,\n) -&gt; list[object]:\n    \"\"\"Perform an all-gather operation for picklable Python objects.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.all_gather_to_list","title":"<code>all_gather_to_list(output_tensors, input_tensor)</code>  <code>abstractmethod</code>","text":"<p>Perform an all-gather operation into a list of tensors.</p> Source code in <code>optimus_dl/modules/distributed/base.py</code> <pre><code>@abstractmethod\ndef all_gather_to_list(\n    self, output_tensors: list[Tensor], input_tensor: Tensor\n) -&gt; None:\n    \"\"\"Perform an all-gather operation into a list of tensors.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.all_reduce","title":"<code>all_reduce(tensor, op)</code>  <code>abstractmethod</code>","text":"<p>Perform an all-reduce operation.</p> Source code in <code>optimus_dl/modules/distributed/base.py</code> <pre><code>@abstractmethod\ndef all_reduce(self, tensor: Tensor, op: ReduceOp.RedOpType) -&gt; None:\n    \"\"\"Perform an all-reduce operation.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.barrier","title":"<code>barrier()</code>  <code>abstractmethod</code>","text":"<p>Synchronize all processes in the collective.</p> Source code in <code>optimus_dl/modules/distributed/base.py</code> <pre><code>@abstractmethod\ndef barrier(self) -&gt; None:\n    \"\"\"Synchronize all processes in the collective.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.broadcast","title":"<code>broadcast(tensor, source_rank=0)</code>  <code>abstractmethod</code>","text":"<p>Broadcast a tensor from the source rank to all other ranks.</p> Source code in <code>optimus_dl/modules/distributed/base.py</code> <pre><code>@abstractmethod\ndef broadcast(self, tensor: Tensor, source_rank: int = 0) -&gt; None:\n    \"\"\"Broadcast a tensor from the source rank to all other ranks.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.broadcast_objects","title":"<code>broadcast_objects(objects, source_rank=0)</code>  <code>abstractmethod</code>","text":"<p>Broadcast picklable Python objects from the source rank.</p> Source code in <code>optimus_dl/modules/distributed/base.py</code> <pre><code>@abstractmethod\ndef broadcast_objects(self, objects: list[object], source_rank: int = 0) -&gt; None:\n    \"\"\"Broadcast picklable Python objects from the source rank.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/modules/distributed/base/#optimus_dl.modules.distributed.base.Collective.close","title":"<code>close()</code>  <code>abstractmethod</code>","text":"<p>Clean up distributed resources.</p> Source code in <code>optimus_dl/modules/distributed/base.py</code> <pre><code>@abstractmethod\ndef close(self) -&gt; None:\n    \"\"\"Clean up distributed resources.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/modules/distributed/config/","title":"config","text":""},{"location":"reference/modules/distributed/config/#optimus_dl.modules.distributed.config","title":"<code>optimus_dl.modules.distributed.config</code>","text":""},{"location":"reference/modules/distributed/config/#optimus_dl.modules.distributed.config.DistributedConfig","title":"<code>DistributedConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for distributed training topologies.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>tp_size</code> <code>int</code> <code>1</code> <code>sharding_world_size</code> <code>int | None</code> <code>None</code> Source code in <code>optimus_dl/modules/distributed/config.py</code> <pre><code>@dataclass\nclass DistributedConfig(RegistryConfigStrict):\n    \"\"\"Configuration for distributed training topologies.\n\n    Attributes:\n        tp_size: Degree of Tensor Parallelism (number of GPUs to shard each layer across).\n        sharding_world_size: Size of FSDP sharding groups. If None, defaults to\n            the number of GPUs per node (intra-node sharding).\n    \"\"\"\n\n    tp_size: int = 1\n    sharding_world_size: int | None = None\n</code></pre>"},{"location":"reference/modules/distributed/fake/","title":"fake","text":""},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake","title":"<code>optimus_dl.modules.distributed.fake</code>","text":""},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective","title":"<code>FakeCollective</code>","text":"<p>               Bases: <code>Collective</code></p> <p>A non-distributed implementation of the Collective interface.</p> <p>Used when running on a single rank (e.g., local debugging or single-GPU/CPU training). Most operations are identity mappings or simple copies, as there are no other processes to communicate with.</p> Source code in <code>optimus_dl/modules/distributed/fake.py</code> <pre><code>class FakeCollective(Collective):\n    \"\"\"A non-distributed implementation of the Collective interface.\n\n    Used when running on a single rank (e.g., local debugging or single-GPU/CPU\n    training). Most operations are identity mappings or simple copies, as\n    there are no other processes to communicate with.\n    \"\"\"\n\n    def __init__(self, rank, world_size, device_type: str = \"cpu\") -&gt; None:\n        super().__init__(rank, world_size)\n        assert world_size == 1, \"Fake collective is fake\"\n        self._device_type = device_type\n\n    @property\n    @override\n    def local(self) -&gt; \"FakeCollective\":\n        \"\"\"Return self, as there is only one rank.\"\"\"\n        return self\n\n    @property\n    @override\n    def tp_world(self) -&gt; \"FakeCollective\":\n        \"\"\"Return self, simulating TP size 1.\"\"\"\n        return self\n\n    @property\n    @override\n    def local_rank(self):\n        \"\"\"Always returns current rank.\"\"\"\n        return self.rank\n\n    @property\n    @override\n    def dp_rank(self):\n        \"\"\"Always returns current rank.\"\"\"\n        return self.rank\n\n    @property\n    @override\n    def dp_world_size(self):\n        \"\"\"Always returns world size (1).\"\"\"\n        return self.world_size\n\n    @property\n    @override\n    def tp_rank(self):\n        \"\"\"Always returns 0.\"\"\"\n        return 0\n\n    @property\n    @override\n    def tp_world_size(self):\n        \"\"\"Always returns 1.\"\"\"\n        return 1\n\n    @property\n    @override\n    def default_device(self) -&gt; torch.device:\n        \"\"\"Get the default device based on device_type.\"\"\"\n        if self._device_type == \"cuda\":\n            return torch.device(\"cuda:0\")  # Single device for fake collective\n        elif self._device_type == \"mps\":\n            return torch.device(\"mps\")\n        else:\n            return torch.device(\"cpu\")\n\n    @override\n    def close(self) -&gt; None:\n        \"\"\"No cleanup needed.\"\"\"\n        pass\n\n    @override\n    def barrier(self) -&gt; None:\n        \"\"\"No synchronization needed.\"\"\"\n        pass\n\n    @override\n    def all_reduce(self, tensor: Tensor, op: ReduceOp.RedOpType) -&gt; None:\n        \"\"\"No-op, as there is only one rank.\"\"\"\n        pass\n\n    @override\n    def all_gather(self, output_tensor: Tensor, input_tensor: Tensor) -&gt; None:\n        \"\"\"Copies input to the first slot of output.\"\"\"\n        if not output_tensor.is_contiguous():\n            raise ValueError(\"`output_tensor` must be contiguous.\")\n\n        if output_tensor.ndim != input_tensor.ndim + 1:\n            raise ValueError(\n                \"`output_tensor` must have a shape that is compatible with all-gather.\"\n            )\n\n        if output_tensor.size(0) != self.world_size:\n            raise ValueError(\n                f\"The size of the first dimension of `output_tensor` must match the number of processes in the gang ({self._size}), but is {output_tensor.size(0)} instead.\"\n            )\n\n        for i in range(self.world_size):\n            output_tensor[i].copy_(input_tensor)\n\n    @override\n    def all_gather_to_list(\n        self, output_tensors: list[Tensor], input_tensor: Tensor\n    ) -&gt; None:\n        \"\"\"Copies input to each element of the output list.\"\"\"\n        if len(output_tensors) != self.world_size:\n            raise ValueError(\n                f\"The length of `output_tensors` must match the number of processes in the gang ({self._size}), but is {len(output_tensors)} instead.\"\n            )\n\n        for i in range(self.world_size):\n            output_tensors[i].copy_(input_tensor)\n\n    @override\n    def all_gather_objects(\n        self,\n        object: object,\n    ):\n        \"\"\"Returns the object in a single-element list.\"\"\"\n        return [object]\n\n    @override\n    def broadcast(self, tensor: Tensor, source_rank: int = 0) -&gt; None:\n        \"\"\"No-op if source_rank matches.\"\"\"\n        if source_rank != self.rank:\n            raise ValueError(\n                f\"`source_rank` must be {self.rank}, but is {source_rank} instead.\"\n            )\n\n    @override\n    def broadcast_objects(self, objects: list[object], source_rank: int = 0) -&gt; None:\n        \"\"\"No-op if source_rank matches.\"\"\"\n        if source_rank != self.rank:\n            raise ValueError(\n                f\"`source_rank` must be {self.rank}, but is {source_rank} instead.\"\n            )\n\n    @property\n    @override\n    def process_group(self) -&gt; ProcessGroup | None:\n        \"\"\"No ProcessGroup for fake collective.\"\"\"\n        return None\n</code></pre>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.default_device","title":"<code>default_device</code>  <code>property</code>","text":"<p>Get the default device based on device_type.</p>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.dp_rank","title":"<code>dp_rank</code>  <code>property</code>","text":"<p>Always returns current rank.</p>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.dp_world_size","title":"<code>dp_world_size</code>  <code>property</code>","text":"<p>Always returns world size (1).</p>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.local","title":"<code>local</code>  <code>property</code>","text":"<p>Return self, as there is only one rank.</p>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.local_rank","title":"<code>local_rank</code>  <code>property</code>","text":"<p>Always returns current rank.</p>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.process_group","title":"<code>process_group</code>  <code>property</code>","text":"<p>No ProcessGroup for fake collective.</p>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.tp_rank","title":"<code>tp_rank</code>  <code>property</code>","text":"<p>Always returns 0.</p>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.tp_world","title":"<code>tp_world</code>  <code>property</code>","text":"<p>Return self, simulating TP size 1.</p>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.tp_world_size","title":"<code>tp_world_size</code>  <code>property</code>","text":"<p>Always returns 1.</p>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.all_gather","title":"<code>all_gather(output_tensor, input_tensor)</code>","text":"<p>Copies input to the first slot of output.</p> Source code in <code>optimus_dl/modules/distributed/fake.py</code> <pre><code>@override\ndef all_gather(self, output_tensor: Tensor, input_tensor: Tensor) -&gt; None:\n    \"\"\"Copies input to the first slot of output.\"\"\"\n    if not output_tensor.is_contiguous():\n        raise ValueError(\"`output_tensor` must be contiguous.\")\n\n    if output_tensor.ndim != input_tensor.ndim + 1:\n        raise ValueError(\n            \"`output_tensor` must have a shape that is compatible with all-gather.\"\n        )\n\n    if output_tensor.size(0) != self.world_size:\n        raise ValueError(\n            f\"The size of the first dimension of `output_tensor` must match the number of processes in the gang ({self._size}), but is {output_tensor.size(0)} instead.\"\n        )\n\n    for i in range(self.world_size):\n        output_tensor[i].copy_(input_tensor)\n</code></pre>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.all_gather_objects","title":"<code>all_gather_objects(object)</code>","text":"<p>Returns the object in a single-element list.</p> Source code in <code>optimus_dl/modules/distributed/fake.py</code> <pre><code>@override\ndef all_gather_objects(\n    self,\n    object: object,\n):\n    \"\"\"Returns the object in a single-element list.\"\"\"\n    return [object]\n</code></pre>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.all_gather_to_list","title":"<code>all_gather_to_list(output_tensors, input_tensor)</code>","text":"<p>Copies input to each element of the output list.</p> Source code in <code>optimus_dl/modules/distributed/fake.py</code> <pre><code>@override\ndef all_gather_to_list(\n    self, output_tensors: list[Tensor], input_tensor: Tensor\n) -&gt; None:\n    \"\"\"Copies input to each element of the output list.\"\"\"\n    if len(output_tensors) != self.world_size:\n        raise ValueError(\n            f\"The length of `output_tensors` must match the number of processes in the gang ({self._size}), but is {len(output_tensors)} instead.\"\n        )\n\n    for i in range(self.world_size):\n        output_tensors[i].copy_(input_tensor)\n</code></pre>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.all_reduce","title":"<code>all_reduce(tensor, op)</code>","text":"<p>No-op, as there is only one rank.</p> Source code in <code>optimus_dl/modules/distributed/fake.py</code> <pre><code>@override\ndef all_reduce(self, tensor: Tensor, op: ReduceOp.RedOpType) -&gt; None:\n    \"\"\"No-op, as there is only one rank.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.barrier","title":"<code>barrier()</code>","text":"<p>No synchronization needed.</p> Source code in <code>optimus_dl/modules/distributed/fake.py</code> <pre><code>@override\ndef barrier(self) -&gt; None:\n    \"\"\"No synchronization needed.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.broadcast","title":"<code>broadcast(tensor, source_rank=0)</code>","text":"<p>No-op if source_rank matches.</p> Source code in <code>optimus_dl/modules/distributed/fake.py</code> <pre><code>@override\ndef broadcast(self, tensor: Tensor, source_rank: int = 0) -&gt; None:\n    \"\"\"No-op if source_rank matches.\"\"\"\n    if source_rank != self.rank:\n        raise ValueError(\n            f\"`source_rank` must be {self.rank}, but is {source_rank} instead.\"\n        )\n</code></pre>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.broadcast_objects","title":"<code>broadcast_objects(objects, source_rank=0)</code>","text":"<p>No-op if source_rank matches.</p> Source code in <code>optimus_dl/modules/distributed/fake.py</code> <pre><code>@override\ndef broadcast_objects(self, objects: list[object], source_rank: int = 0) -&gt; None:\n    \"\"\"No-op if source_rank matches.\"\"\"\n    if source_rank != self.rank:\n        raise ValueError(\n            f\"`source_rank` must be {self.rank}, but is {source_rank} instead.\"\n        )\n</code></pre>"},{"location":"reference/modules/distributed/fake/#optimus_dl.modules.distributed.fake.FakeCollective.close","title":"<code>close()</code>","text":"<p>No cleanup needed.</p> Source code in <code>optimus_dl/modules/distributed/fake.py</code> <pre><code>@override\ndef close(self) -&gt; None:\n    \"\"\"No cleanup needed.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/modules/distributed/mesh/","title":"mesh","text":""},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh","title":"<code>optimus_dl.modules.distributed.mesh</code>","text":""},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective","title":"<code>MeshCollective</code>","text":"<p>               Bases: <code>Collective</code></p> <p>Distributed implementation of Collective using PyTorch DeviceMesh.</p> <p>This class orchestrates complex parallel topologies by nesting process groups. It supports a 3D parallelism strategy:</p> <ul> <li>dp_replicate: Inter-node data parallelism (replication).</li> <li>dp_shard: Intra-node data parallelism (FSDP-style sharding).</li> <li>tp: Tensor Parallelism (typically intra-node).</li> </ul> <p>It automatically initializes the default process group (NCCL for CUDA, Gloo otherwise) and builds the sub-meshes based on environment variables and config.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <p>Global process rank.</p> required <code>world_size</code> <p>Global world size.</p> required <code>local_world_size</code> <p>Number of processes per node.</p> required <code>local_rank</code> <p>Rank within the node.</p> required <code>device_type</code> <p>Target device (cuda, cpu, mps).</p> required <code>mesh</code> <code>Meshes | None</code> <p>Optional pre-initialized Meshes object.</p> <code>None</code> <code>process_group</code> <p>The ProcessGroup for collective operations (defaults to WORLD).</p> <code>None</code> <code>tp_size</code> <code>int</code> <p>Degree of Tensor Parallelism.</p> <code>1</code> <code>sharding_world_size</code> <code>int | None</code> <p>Size of FSDP sharding groups.</p> <code>None</code> Source code in <code>optimus_dl/modules/distributed/mesh.py</code> <pre><code>class MeshCollective(Collective):\n    \"\"\"Distributed implementation of Collective using PyTorch DeviceMesh.\n\n    This class orchestrates complex parallel topologies by nesting process groups.\n    It supports a 3D parallelism strategy:\n\n    - **dp_replicate**: Inter-node data parallelism (replication).\n    - **dp_shard**: Intra-node data parallelism (FSDP-style sharding).\n    - **tp**: Tensor Parallelism (typically intra-node).\n\n    It automatically initializes the default process group (NCCL for CUDA, Gloo\n    otherwise) and builds the sub-meshes based on environment variables and config.\n\n    Args:\n        rank: Global process rank.\n        world_size: Global world size.\n        local_world_size: Number of processes per node.\n        local_rank: Rank within the node.\n        device_type: Target device (cuda, cpu, mps).\n        mesh: Optional pre-initialized Meshes object.\n        process_group: The ProcessGroup for collective operations (defaults to WORLD).\n        tp_size: Degree of Tensor Parallelism.\n        sharding_world_size: Size of FSDP sharding groups.\n    \"\"\"\n\n    def __init__(\n        self,\n        rank,\n        world_size,\n        local_world_size,\n        local_rank,\n        device_type,\n        mesh: Meshes | None = None,\n        process_group=None,\n        tp_size: int = 1,\n        sharding_world_size: int | None = None,\n    ) -&gt; None:\n        super().__init__(rank, world_size)\n        assert world_size % local_world_size == 0\n\n        self._device_type = device_type\n        self._local_rank = local_rank\n        self._local_world_size = local_world_size\n        self._tp_size = tp_size\n\n        if mesh is None:\n            if world_size % tp_size != 0:\n                raise ValueError(\n                    f\"World size ({world_size}) must be divisible by tp_size ({tp_size})\"\n                )\n\n            # Determine mesh structure\n            # Goal: (dp_replicate, dp_shard, tp)\n            # tp_size: innermost dim\n            # dp_shard: intra-node data parallelism\n            # dp_replicate: inter-node data parallelism\n\n            if local_world_size % tp_size != 0:\n                raise ValueError(\n                    f\"Local world size ({local_world_size}) must be divisible by tp_size ({tp_size}).\"\n                )\n\n            sharding_world_size = sharding_world_size or (local_world_size // tp_size)\n            if world_size % (sharding_world_size * tp_size) != 0:\n                raise ValueError(\n                    f\"World size ({world_size}) must be divisible by ({sharding_world_size * tp_size = })\"\n                )\n\n            # Standard Case: TP is within node.\n            # Calculate dimension sizes\n            replicate_size = world_size // (sharding_world_size * tp_size)\n            shard_size = sharding_world_size\n\n            mesh_dims = (replicate_size, shard_size, tp_size)\n            mesh_names = (\"dp_replicate\", \"dp_shard\", \"tp\")\n\n            assert (\n                replicate_size * shard_size * tp_size == world_size\n            ), \"Invalid mesh dimensions\"\n\n            mesh_device_type = \"cpu\"\n            if device_type == \"cuda\":\n                mesh_device_type = \"cuda\"\n            if device_type == \"mps\":\n                logger.warning(\"MPS distributed training uses cpu collective\")\n                mesh_device_type = \"cpu\"\n\n            if not dist.is_initialized():\n                backend = \"nccl\" if mesh_device_type == \"cuda\" else \"gloo\"\n                logger.info(f\"Initializing default PG with {backend = }\")\n                device_id = None\n                if mesh_device_type == \"cuda\":\n                    torch.cuda.set_device(local_rank)\n                    device_id = torch.device(f\"cuda:{local_rank}\")\n                init_process_group(\n                    backend=backend,\n                    rank=rank,\n                    world_size=world_size,\n                    device_id=device_id,\n                )\n\n            logger.info(\n                f\"Initializing mesh with {mesh_device_type=}, shape={mesh_dims}, names={mesh_names}\"\n            )\n            parallel_mesh = init_device_mesh(\n                device_type=mesh_device_type,\n                mesh_shape=mesh_dims,\n                mesh_dim_names=mesh_names,\n            )\n            physical_mesh = init_device_mesh(\n                device_type=mesh_device_type,\n                mesh_shape=(world_size // local_world_size, local_world_size),\n                mesh_dim_names=(\"nodes\", \"local_ranks\"),\n            )\n            mesh = Meshes(parallel_mesh, physical_mesh)\n\n        self._mesh: Meshes = mesh\n        self._process_group = (\n            process_group if process_group is not None else dist.group.WORLD\n        )\n\n    def __repr__(self) -&gt; str:\n        group_size = (\n            dist.get_world_size(group=self._process_group)\n            if self._process_group\n            else \"unknown\"\n        )\n        group_rank = (\n            dist.get_rank(group=self._process_group)\n            if self._process_group\n            else \"unknown\"\n        )\n        is_local = self._process_group != dist.group.WORLD\n        group_type = \"local\" if is_local else \"global\"\n\n        # Get list of ranks in this group\n        ranks = dist.get_process_group_ranks(self._process_group)\n\n        return f\"MeshCollective(rank={self.rank}/{self.world_size}, {group_type}_group={group_rank}/{group_size}, tp_size={self._tp_size}, mesh={self._mesh}, ranks={ranks})\"\n\n    @property\n    def tp_mesh(self):\n        \"\"\"Returns the sub-mesh for Tensor Parallelism if it exists.\"\"\"\n        assert self._mesh.parallel_mesh.mesh_dim_names is not None\n        if \"tp\" in self._mesh.parallel_mesh.mesh_dim_names:\n            return self._mesh.parallel_mesh[\"tp\"]\n        return None\n\n    @property\n    def dp_mesh(self):\n        \"\"\"Returns the sub-mesh for Data Parallelism (Replicate + Shard).\"\"\"\n        assert self._mesh.physical_mesh.mesh_dim_names is not None\n        return self._mesh.parallel_mesh[\"dp_replicate\", \"dp_shard\"]\n\n    @property\n    @override\n    def local(self) -&gt; \"MeshCollective\":\n        \"\"\"Return a MeshCollective limited to ranks on the current node.\"\"\"\n        assert self._mesh.physical_mesh.mesh_dim_names is not None\n        if len(self._mesh.physical_mesh.mesh_dim_names) == 1:\n            return self\n        process_group = self._mesh.physical_mesh.get_group(\"local_ranks\")\n        return MeshCollective(\n            rank=self._local_rank,\n            world_size=self._local_world_size,\n            local_world_size=self._local_world_size,\n            local_rank=self._local_rank,\n            device_type=self._device_type,\n            mesh=self._mesh,\n            tp_size=self._tp_size,\n            process_group=process_group,\n        )\n\n    @property\n    @override\n    def tp_world(self) -&gt; \"MeshCollective\":\n        \"\"\"Return a MeshCollective limited to the current Tensor Parallel group.\"\"\"\n        assert self._mesh.parallel_mesh.mesh_dim_names is not None\n\n        process_group = self._mesh.parallel_mesh.get_group(\"tp\")\n        return MeshCollective(\n            rank=self.tp_rank,\n            world_size=self.tp_world_size,\n            local_world_size=self.tp_world_size,\n            local_rank=self.tp_rank,\n            device_type=self._device_type,\n            mesh=self._mesh,\n            tp_size=self._tp_size,\n            process_group=process_group,\n        )\n\n    @property\n    @override\n    def local_rank(self):\n        \"\"\"Rank within the current compute node.\"\"\"\n        return self._local_rank\n\n    @property\n    @override\n    def dp_rank(self):\n        \"\"\"Rank within the Data Parallel group (shared across nodes).\"\"\"\n        return self.rank // self._tp_size\n\n    @property\n    @override\n    def dp_world_size(self):\n        \"\"\"Total size of the Data Parallel gang.\"\"\"\n        return self.world_size // self._tp_size\n\n    @property\n    @override\n    def tp_rank(self):\n        \"\"\"Rank within the Tensor Parallel group.\"\"\"\n        return self.rank % self._tp_size\n\n    @property\n    @override\n    def tp_world_size(self):\n        \"\"\"Size of the Tensor Parallel group.\"\"\"\n        return self._tp_size\n\n    @property\n    @override\n    def default_device(self) -&gt; torch.device:\n        \"\"\"Default device (CUDA:local_rank or CPU).\"\"\"\n        if self._device_type == \"cuda\":\n            return torch.device(f\"cuda:{self._local_rank}\")\n        elif self._device_type == \"mps\":\n            return torch.device(\"mps\")\n        else:\n            return torch.device(\"cpu\")\n\n    @override\n    def close(self) -&gt; None:\n        \"\"\"Clean up process groups.\"\"\"\n        pass\n\n    @override\n    def barrier(self) -&gt; None:\n        \"\"\"Synchronize across the current process group.\"\"\"\n        dist.barrier(group=self._process_group)\n\n    @override\n    def all_reduce(self, tensor: Tensor, op: ReduceOp.RedOpType):\n        \"\"\"Perform all-reduce on the current process group.\"\"\"\n        dist.all_reduce(\n            tensor,\n            op,\n            group=self._process_group,\n        )\n\n    @override\n    def all_gather(self, output_tensor: Tensor, input_tensor: Tensor):\n        \"\"\"Perform all-gather onto a single tensor.\"\"\"\n        dist.all_gather_into_tensor(\n            output_tensor,\n            input_tensor,\n            group=self._process_group,\n        )\n\n    @override\n    def all_gather_to_list(\n        self, output_tensors: list[Tensor], input_tensor: Tensor\n    ) -&gt; None:\n        \"\"\"Perform all-gather into a list of tensors.\"\"\"\n        dist.all_gather(output_tensors, input_tensor, group=self._process_group)\n\n    def all_gather_objects(\n        self,\n        object: object,\n    ) -&gt; list[object]:\n        \"\"\"Collect Python objects from all ranks in the group.\"\"\"\n        object_list = [None] * dist.get_world_size(group=self._process_group)\n        dist.all_gather_object(\n            object_list=object_list, obj=object, group=self._process_group\n        )\n        return object_list\n\n    @override\n    def broadcast(self, tensor: Tensor, source_rank: int = 0) -&gt; None:\n        \"\"\"Broadcast a tensor from the source rank.\"\"\"\n        dist.broadcast(tensor, source_rank, group=self._process_group)\n\n    @override\n    def broadcast_objects(self, objects: list[object], source_rank: int = 0) -&gt; None:\n        \"\"\"Broadcast Python objects from the source rank.\"\"\"\n        dist.broadcast_object_list(objects, source_rank, group=self._process_group)\n\n    @property\n    @override\n    def process_group(self) -&gt; ProcessGroup | None:\n        \"\"\"Underlying PyTorch ProcessGroup.\"\"\"\n        return self._process_group\n</code></pre>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.default_device","title":"<code>default_device</code>  <code>property</code>","text":"<p>Default device (CUDA:local_rank or CPU).</p>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.dp_mesh","title":"<code>dp_mesh</code>  <code>property</code>","text":"<p>Returns the sub-mesh for Data Parallelism (Replicate + Shard).</p>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.dp_rank","title":"<code>dp_rank</code>  <code>property</code>","text":"<p>Rank within the Data Parallel group (shared across nodes).</p>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.dp_world_size","title":"<code>dp_world_size</code>  <code>property</code>","text":"<p>Total size of the Data Parallel gang.</p>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.local","title":"<code>local</code>  <code>property</code>","text":"<p>Return a MeshCollective limited to ranks on the current node.</p>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.local_rank","title":"<code>local_rank</code>  <code>property</code>","text":"<p>Rank within the current compute node.</p>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.process_group","title":"<code>process_group</code>  <code>property</code>","text":"<p>Underlying PyTorch ProcessGroup.</p>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.tp_mesh","title":"<code>tp_mesh</code>  <code>property</code>","text":"<p>Returns the sub-mesh for Tensor Parallelism if it exists.</p>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.tp_rank","title":"<code>tp_rank</code>  <code>property</code>","text":"<p>Rank within the Tensor Parallel group.</p>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.tp_world","title":"<code>tp_world</code>  <code>property</code>","text":"<p>Return a MeshCollective limited to the current Tensor Parallel group.</p>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.tp_world_size","title":"<code>tp_world_size</code>  <code>property</code>","text":"<p>Size of the Tensor Parallel group.</p>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.all_gather","title":"<code>all_gather(output_tensor, input_tensor)</code>","text":"<p>Perform all-gather onto a single tensor.</p> Source code in <code>optimus_dl/modules/distributed/mesh.py</code> <pre><code>@override\ndef all_gather(self, output_tensor: Tensor, input_tensor: Tensor):\n    \"\"\"Perform all-gather onto a single tensor.\"\"\"\n    dist.all_gather_into_tensor(\n        output_tensor,\n        input_tensor,\n        group=self._process_group,\n    )\n</code></pre>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.all_gather_objects","title":"<code>all_gather_objects(object)</code>","text":"<p>Collect Python objects from all ranks in the group.</p> Source code in <code>optimus_dl/modules/distributed/mesh.py</code> <pre><code>def all_gather_objects(\n    self,\n    object: object,\n) -&gt; list[object]:\n    \"\"\"Collect Python objects from all ranks in the group.\"\"\"\n    object_list = [None] * dist.get_world_size(group=self._process_group)\n    dist.all_gather_object(\n        object_list=object_list, obj=object, group=self._process_group\n    )\n    return object_list\n</code></pre>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.all_gather_to_list","title":"<code>all_gather_to_list(output_tensors, input_tensor)</code>","text":"<p>Perform all-gather into a list of tensors.</p> Source code in <code>optimus_dl/modules/distributed/mesh.py</code> <pre><code>@override\ndef all_gather_to_list(\n    self, output_tensors: list[Tensor], input_tensor: Tensor\n) -&gt; None:\n    \"\"\"Perform all-gather into a list of tensors.\"\"\"\n    dist.all_gather(output_tensors, input_tensor, group=self._process_group)\n</code></pre>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.all_reduce","title":"<code>all_reduce(tensor, op)</code>","text":"<p>Perform all-reduce on the current process group.</p> Source code in <code>optimus_dl/modules/distributed/mesh.py</code> <pre><code>@override\ndef all_reduce(self, tensor: Tensor, op: ReduceOp.RedOpType):\n    \"\"\"Perform all-reduce on the current process group.\"\"\"\n    dist.all_reduce(\n        tensor,\n        op,\n        group=self._process_group,\n    )\n</code></pre>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.barrier","title":"<code>barrier()</code>","text":"<p>Synchronize across the current process group.</p> Source code in <code>optimus_dl/modules/distributed/mesh.py</code> <pre><code>@override\ndef barrier(self) -&gt; None:\n    \"\"\"Synchronize across the current process group.\"\"\"\n    dist.barrier(group=self._process_group)\n</code></pre>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.broadcast","title":"<code>broadcast(tensor, source_rank=0)</code>","text":"<p>Broadcast a tensor from the source rank.</p> Source code in <code>optimus_dl/modules/distributed/mesh.py</code> <pre><code>@override\ndef broadcast(self, tensor: Tensor, source_rank: int = 0) -&gt; None:\n    \"\"\"Broadcast a tensor from the source rank.\"\"\"\n    dist.broadcast(tensor, source_rank, group=self._process_group)\n</code></pre>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.broadcast_objects","title":"<code>broadcast_objects(objects, source_rank=0)</code>","text":"<p>Broadcast Python objects from the source rank.</p> Source code in <code>optimus_dl/modules/distributed/mesh.py</code> <pre><code>@override\ndef broadcast_objects(self, objects: list[object], source_rank: int = 0) -&gt; None:\n    \"\"\"Broadcast Python objects from the source rank.\"\"\"\n    dist.broadcast_object_list(objects, source_rank, group=self._process_group)\n</code></pre>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.MeshCollective.close","title":"<code>close()</code>","text":"<p>Clean up process groups.</p> Source code in <code>optimus_dl/modules/distributed/mesh.py</code> <pre><code>@override\ndef close(self) -&gt; None:\n    \"\"\"Clean up process groups.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/modules/distributed/mesh/#optimus_dl.modules.distributed.mesh.Meshes","title":"<code>Meshes</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Container for parallel and physical device meshes.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>parallel_mesh</code> <code>DeviceMesh</code> <code>None</code> <code>physical_mesh</code> <code>DeviceMesh</code> <code>None</code> Source code in <code>optimus_dl/modules/distributed/mesh.py</code> <pre><code>class Meshes(NamedTuple):\n    \"\"\"Container for parallel and physical device meshes.\n\n    Attributes:\n        parallel_mesh: 3D mesh with dims (dp_replicate, dp_shard, tp).\n        physical_mesh: 2D mesh with dims (nodes, local_ranks).\n    \"\"\"\n\n    parallel_mesh: DeviceMesh\n    physical_mesh: DeviceMesh\n</code></pre>"},{"location":"reference/modules/eval/","title":"Index","text":""},{"location":"reference/modules/eval/#optimus_dl.modules.eval","title":"<code>optimus_dl.modules.eval</code>","text":""},{"location":"reference/modules/eval/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>model</code>: LLM Baselines evaluation model for llm_harness.</li> </ul>"},{"location":"reference/modules/eval/model/","title":"model","text":""},{"location":"reference/modules/eval/model/#optimus_dl.modules.eval.model","title":"<code>optimus_dl.modules.eval.model</code>","text":""},{"location":"reference/modules/eval/model/#optimus_dl.modules.eval.model.LLMBaselinesModel","title":"<code>LLMBaselinesModel</code>","text":"<p>               Bases: <code>LM</code></p> <p>LLM Baselines evaluation model for llm_harness.</p> <p>This class implements the lm_eval interface using a pre-loaded model and tokenizer. All checkpoint loading logic is handled by the EvalRecipe.</p> Source code in <code>optimus_dl/modules/eval/model.py</code> <pre><code>class LLMBaselinesModel(LM):\n    \"\"\"LLM Baselines evaluation model for llm_harness.\n\n    This class implements the lm_eval interface using a pre-loaded model and tokenizer.\n    All checkpoint loading logic is handled by the EvalRecipe.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: BaseModel,\n        tokenizer,\n        tokenizer_config,\n        device: str | torch.device,\n    ):\n        \"\"\"Initialize the model with pre-loaded components.\n\n        Args:\n            model: Pre-loaded BaseModel instance\n            tokenizer: Pre-loaded tokenizer instance\n            tokenizer_config: Tokenizer configuration for type detection\n            device: Device the model is running on\n        \"\"\"\n        super().__init__()\n\n        self.model = model\n        self.tokenizer = tokenizer\n        self.tokenizer_config = tokenizer_config\n        self.device = device\n\n        logger.info(f\"LLMBaselinesModel initialized on {self.device}\")\n\n    def _tokenize(self, text: str) -&gt; torch.Tensor:\n        encoded = self.tokenizer.encode(text)\n        return torch.as_tensor(encoded, dtype=torch.long)\n\n    def _decode_tokens(self, tokens: list[int]) -&gt; str:\n        \"\"\"Decode tokens to text using the model's tokenizer.\"\"\"\n        return self.tokenizer.decode(tokens)\n\n    def _compute_logprobs(self, input_ids: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute log probabilities for input tokens.\"\"\"\n        input_ids = input_ids.to(self.device)\n\n        with torch.no_grad():\n            if input_ids.dim() == 1:\n                input_ids = input_ids.unsqueeze(0)\n\n            outputs = self.model(input_ids)\n            logits = outputs[\"logits\"]  # Shape: [batch, seq_len, vocab_size]\n\n            # Convert to log probabilities\n            log_probs = F.log_softmax(logits, dim=-1)\n\n        return log_probs\n\n    def loglikelihood(self, requests: list[Instance]) -&gt; list[tuple[float, bool]]:\n        \"\"\"Compute log-likelihood of generating continuations from contexts.\n\n        Args:\n            requests: List of Instance objects with (context, continuation) pairs\n\n        Returns:\n            List of (logprob, is_greedy) tuples\n        \"\"\"\n        results = []\n\n        for request in tqdm(requests, desc=\"Computing loglikelihood\", leave=False):\n            args = request.args\n            if not isinstance(args, tuple) or len(args) != 2:\n                raise ValueError(f\"Expected 2 arguments for loglikelihood, got {args}\")\n            context, continuation = args\n\n            # Tokenize context and continuation\n            context_tokens = self._tokenize(context)\n            continuation_tokens = self._tokenize(continuation)\n\n            # Combine for full sequence\n            full_tokens = torch.cat([context_tokens, continuation_tokens])\n\n            # Get log probabilities\n            log_probs = self._compute_logprobs(full_tokens)\n\n            # Calculate log probability of continuation\n            context_len = len(context_tokens)\n            continuation_len = len(continuation_tokens)\n\n            if continuation_len == 0:\n                logprob = 0.0\n                is_greedy = True\n            else:\n                # Get logprobs for continuation tokens\n                relevant_logprobs = log_probs[\n                    0, context_len - 1 : context_len + continuation_len - 1\n                ]\n                token_logprobs = relevant_logprobs[\n                    torch.arange(continuation_len), continuation_tokens\n                ]\n                logprob = token_logprobs.sum().item()\n\n                # Check if this would be the greedy choice\n                greedy_tokens = log_probs[\n                    0, context_len - 1 : context_len + continuation_len - 1\n                ].argmax(dim=-1)\n                is_greedy = torch.equal(\n                    greedy_tokens, continuation_tokens.to(greedy_tokens.device)\n                )\n\n            results.append((logprob, is_greedy))\n\n        return results\n\n    def loglikelihood_rolling(self, requests: list[Instance]) -&gt; list[float]:\n        \"\"\"Compute rolling log-likelihood for perplexity calculation.\n\n        Args:\n            requests: List of Instance objects with (text,) tuples\n\n        Returns:\n            List of log probabilities\n        \"\"\"\n        results = []\n\n        for request in requests:\n            args = request.args\n            if not isinstance(args, tuple) or len(args) != 1:\n                raise ValueError(\n                    f\"Expected 1 argument for loglikelihood_rolling, got {args}\"\n                )\n            text = args[0]\n\n            # Tokenize text\n            tokens = self._tokenize(text)\n\n            if len(tokens) &lt;= 1:\n                results.append(0.0)\n                continue\n\n            # Get log probabilities\n            log_probs = self._compute_logprobs(tokens)\n\n            # Calculate total log probability (excluding first token)\n            token_indices = tokens[1:]  # Target tokens\n            context_logprobs = log_probs[0, :-1]  # Logprobs from context positions\n\n            selected_logprobs = context_logprobs[\n                torch.arange(len(token_indices)), token_indices\n            ]\n            total_logprob = selected_logprobs.sum().item()\n\n            results.append(total_logprob)\n\n        return results\n\n    def generate_until(self, requests: list[Instance]) -&gt; list[str]:\n        \"\"\"Generate text until stopping criteria are met.\n\n        Args:\n            requests: List of Instance objects with (context, gen_kwargs) pairs\n\n        Returns:\n            List of generated text continuations\n        \"\"\"\n        results = []\n\n        for request in requests:\n            args = request.args\n            if not isinstance(args, tuple) or len(args) != 2:\n                raise ValueError(f\"Expected 2 arguments for generate_until, got {args}\")\n            context, gen_kwargs = args\n\n            # Parse generation arguments\n            max_gen_toks = gen_kwargs.get(\"max_gen_toks\", 256)\n            until = gen_kwargs.get(\"until\", [])\n            temperature = gen_kwargs.get(\"temperature\", 0.0)  # Greedy by default\n\n            # Tokenize context\n            context_tokens = self._tokenize(context)\n            input_ids = context_tokens.unsqueeze(0).to(self.device)\n\n            # Generate tokens\n            generated_tokens = []\n\n            with torch.no_grad():\n                for _ in range(max_gen_toks):\n                    # Get next token logits\n                    outputs = self.model(input_ids)\n                    logits = outputs[\"logits\"][:, -1, :]  # Last position\n\n                    # Sample next token\n                    if temperature == 0.0:\n                        next_token = logits.argmax(dim=-1)\n                    else:\n                        probs = F.softmax(logits / temperature, dim=-1)\n                        next_token = torch.multinomial(probs, 1).squeeze(-1)\n\n                    generated_tokens.append(next_token.item())\n\n                    # Update input for next iteration\n                    input_ids = torch.cat(\n                        [input_ids, next_token.unsqueeze(0).unsqueeze(0)], dim=1\n                    )\n\n                    # Check stopping criteria\n                    current_text = self._decode_tokens(generated_tokens)\n                    if any(stop_seq in current_text for stop_seq in until):\n                        # Truncate at first stopping sequence\n                        for stop_seq in until:\n                            if stop_seq in current_text:\n                                current_text = current_text.split(stop_seq)[0]\n                                break\n                        break\n\n            generated_text = (\n                self._decode_tokens(generated_tokens) if generated_tokens else \"\"\n            )\n\n            # Remove stopping sequences from the end\n            for stop_seq in until:\n                if generated_text.endswith(stop_seq):\n                    generated_text = generated_text[: -len(stop_seq)]\n\n            results.append(generated_text)\n\n        return results\n\n    @property\n    def tokenizer_name(self) -&gt; str:\n        \"\"\"Return tokenizer name for caching.\"\"\"\n        return f\"{self.tokenizer_config.type}:{self.tokenizer_config.name}\"\n</code></pre>"},{"location":"reference/modules/eval/model/#optimus_dl.modules.eval.model.LLMBaselinesModel.tokenizer_name","title":"<code>tokenizer_name</code>  <code>property</code>","text":"<p>Return tokenizer name for caching.</p>"},{"location":"reference/modules/eval/model/#optimus_dl.modules.eval.model.LLMBaselinesModel.__init__","title":"<code>__init__(model, tokenizer, tokenizer_config, device)</code>","text":"<p>Initialize the model with pre-loaded components.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel</code> <p>Pre-loaded BaseModel instance</p> required <code>tokenizer</code> <p>Pre-loaded tokenizer instance</p> required <code>tokenizer_config</code> <p>Tokenizer configuration for type detection</p> required <code>device</code> <code>str | device</code> <p>Device the model is running on</p> required Source code in <code>optimus_dl/modules/eval/model.py</code> <pre><code>def __init__(\n    self,\n    model: BaseModel,\n    tokenizer,\n    tokenizer_config,\n    device: str | torch.device,\n):\n    \"\"\"Initialize the model with pre-loaded components.\n\n    Args:\n        model: Pre-loaded BaseModel instance\n        tokenizer: Pre-loaded tokenizer instance\n        tokenizer_config: Tokenizer configuration for type detection\n        device: Device the model is running on\n    \"\"\"\n    super().__init__()\n\n    self.model = model\n    self.tokenizer = tokenizer\n    self.tokenizer_config = tokenizer_config\n    self.device = device\n\n    logger.info(f\"LLMBaselinesModel initialized on {self.device}\")\n</code></pre>"},{"location":"reference/modules/eval/model/#optimus_dl.modules.eval.model.LLMBaselinesModel.generate_until","title":"<code>generate_until(requests)</code>","text":"<p>Generate text until stopping criteria are met.</p> <p>Parameters:</p> Name Type Description Default <code>requests</code> <code>list[Instance]</code> <p>List of Instance objects with (context, gen_kwargs) pairs</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of generated text continuations</p> Source code in <code>optimus_dl/modules/eval/model.py</code> <pre><code>def generate_until(self, requests: list[Instance]) -&gt; list[str]:\n    \"\"\"Generate text until stopping criteria are met.\n\n    Args:\n        requests: List of Instance objects with (context, gen_kwargs) pairs\n\n    Returns:\n        List of generated text continuations\n    \"\"\"\n    results = []\n\n    for request in requests:\n        args = request.args\n        if not isinstance(args, tuple) or len(args) != 2:\n            raise ValueError(f\"Expected 2 arguments for generate_until, got {args}\")\n        context, gen_kwargs = args\n\n        # Parse generation arguments\n        max_gen_toks = gen_kwargs.get(\"max_gen_toks\", 256)\n        until = gen_kwargs.get(\"until\", [])\n        temperature = gen_kwargs.get(\"temperature\", 0.0)  # Greedy by default\n\n        # Tokenize context\n        context_tokens = self._tokenize(context)\n        input_ids = context_tokens.unsqueeze(0).to(self.device)\n\n        # Generate tokens\n        generated_tokens = []\n\n        with torch.no_grad():\n            for _ in range(max_gen_toks):\n                # Get next token logits\n                outputs = self.model(input_ids)\n                logits = outputs[\"logits\"][:, -1, :]  # Last position\n\n                # Sample next token\n                if temperature == 0.0:\n                    next_token = logits.argmax(dim=-1)\n                else:\n                    probs = F.softmax(logits / temperature, dim=-1)\n                    next_token = torch.multinomial(probs, 1).squeeze(-1)\n\n                generated_tokens.append(next_token.item())\n\n                # Update input for next iteration\n                input_ids = torch.cat(\n                    [input_ids, next_token.unsqueeze(0).unsqueeze(0)], dim=1\n                )\n\n                # Check stopping criteria\n                current_text = self._decode_tokens(generated_tokens)\n                if any(stop_seq in current_text for stop_seq in until):\n                    # Truncate at first stopping sequence\n                    for stop_seq in until:\n                        if stop_seq in current_text:\n                            current_text = current_text.split(stop_seq)[0]\n                            break\n                    break\n\n        generated_text = (\n            self._decode_tokens(generated_tokens) if generated_tokens else \"\"\n        )\n\n        # Remove stopping sequences from the end\n        for stop_seq in until:\n            if generated_text.endswith(stop_seq):\n                generated_text = generated_text[: -len(stop_seq)]\n\n        results.append(generated_text)\n\n    return results\n</code></pre>"},{"location":"reference/modules/eval/model/#optimus_dl.modules.eval.model.LLMBaselinesModel.loglikelihood","title":"<code>loglikelihood(requests)</code>","text":"<p>Compute log-likelihood of generating continuations from contexts.</p> <p>Parameters:</p> Name Type Description Default <code>requests</code> <code>list[Instance]</code> <p>List of Instance objects with (context, continuation) pairs</p> required <p>Returns:</p> Type Description <code>list[tuple[float, bool]]</code> <p>List of (logprob, is_greedy) tuples</p> Source code in <code>optimus_dl/modules/eval/model.py</code> <pre><code>def loglikelihood(self, requests: list[Instance]) -&gt; list[tuple[float, bool]]:\n    \"\"\"Compute log-likelihood of generating continuations from contexts.\n\n    Args:\n        requests: List of Instance objects with (context, continuation) pairs\n\n    Returns:\n        List of (logprob, is_greedy) tuples\n    \"\"\"\n    results = []\n\n    for request in tqdm(requests, desc=\"Computing loglikelihood\", leave=False):\n        args = request.args\n        if not isinstance(args, tuple) or len(args) != 2:\n            raise ValueError(f\"Expected 2 arguments for loglikelihood, got {args}\")\n        context, continuation = args\n\n        # Tokenize context and continuation\n        context_tokens = self._tokenize(context)\n        continuation_tokens = self._tokenize(continuation)\n\n        # Combine for full sequence\n        full_tokens = torch.cat([context_tokens, continuation_tokens])\n\n        # Get log probabilities\n        log_probs = self._compute_logprobs(full_tokens)\n\n        # Calculate log probability of continuation\n        context_len = len(context_tokens)\n        continuation_len = len(continuation_tokens)\n\n        if continuation_len == 0:\n            logprob = 0.0\n            is_greedy = True\n        else:\n            # Get logprobs for continuation tokens\n            relevant_logprobs = log_probs[\n                0, context_len - 1 : context_len + continuation_len - 1\n            ]\n            token_logprobs = relevant_logprobs[\n                torch.arange(continuation_len), continuation_tokens\n            ]\n            logprob = token_logprobs.sum().item()\n\n            # Check if this would be the greedy choice\n            greedy_tokens = log_probs[\n                0, context_len - 1 : context_len + continuation_len - 1\n            ].argmax(dim=-1)\n            is_greedy = torch.equal(\n                greedy_tokens, continuation_tokens.to(greedy_tokens.device)\n            )\n\n        results.append((logprob, is_greedy))\n\n    return results\n</code></pre>"},{"location":"reference/modules/eval/model/#optimus_dl.modules.eval.model.LLMBaselinesModel.loglikelihood_rolling","title":"<code>loglikelihood_rolling(requests)</code>","text":"<p>Compute rolling log-likelihood for perplexity calculation.</p> <p>Parameters:</p> Name Type Description Default <code>requests</code> <code>list[Instance]</code> <p>List of Instance objects with (text,) tuples</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>List of log probabilities</p> Source code in <code>optimus_dl/modules/eval/model.py</code> <pre><code>def loglikelihood_rolling(self, requests: list[Instance]) -&gt; list[float]:\n    \"\"\"Compute rolling log-likelihood for perplexity calculation.\n\n    Args:\n        requests: List of Instance objects with (text,) tuples\n\n    Returns:\n        List of log probabilities\n    \"\"\"\n    results = []\n\n    for request in requests:\n        args = request.args\n        if not isinstance(args, tuple) or len(args) != 1:\n            raise ValueError(\n                f\"Expected 1 argument for loglikelihood_rolling, got {args}\"\n            )\n        text = args[0]\n\n        # Tokenize text\n        tokens = self._tokenize(text)\n\n        if len(tokens) &lt;= 1:\n            results.append(0.0)\n            continue\n\n        # Get log probabilities\n        log_probs = self._compute_logprobs(tokens)\n\n        # Calculate total log probability (excluding first token)\n        token_indices = tokens[1:]  # Target tokens\n        context_logprobs = log_probs[0, :-1]  # Logprobs from context positions\n\n        selected_logprobs = context_logprobs[\n            torch.arange(len(token_indices)), token_indices\n        ]\n        total_logprob = selected_logprobs.sum().item()\n\n        results.append(total_logprob)\n\n    return results\n</code></pre>"},{"location":"reference/modules/loggers/","title":"Index","text":""},{"location":"reference/modules/loggers/#optimus_dl.modules.loggers","title":"<code>optimus_dl.modules.loggers</code>","text":""},{"location":"reference/modules/loggers/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>base</code>: Base class for metrics loggers in the Optimus-DL framework.</li> <li><code>config</code>: Base configuration for metrics loggers.</li> <li><code>jsonl</code>: JSONL (JSON Lines) file metrics logger implementation.</li> <li><code>wandb</code>: Weights &amp; Biases (wandb) metrics logger implementation.</li> </ul>"},{"location":"reference/modules/loggers/base/","title":"base","text":""},{"location":"reference/modules/loggers/base/#optimus_dl.modules.loggers.base","title":"<code>optimus_dl.modules.loggers.base</code>","text":"<p>Base class for metrics loggers in the Optimus-DL framework.</p> <p>This module provides the abstract interface that all metrics logging backends must implement to integrate with the metrics system.</p>"},{"location":"reference/modules/loggers/base/#optimus_dl.modules.loggers.base.BaseMetricsLogger","title":"<code>BaseMetricsLogger</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for metrics logging backends.</p> <p>All metrics loggers in the framework should inherit from this class. The logger receives computed metrics from various training phases (train, eval, etc.) and is responsible for persisting them (e.g., to a file, a database, or a cloud service).</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <p>Configuration object for the logger.</p> <code>enabled</code> <p>Whether the logger is active.</p> Source code in <code>optimus_dl/modules/loggers/base.py</code> <pre><code>class BaseMetricsLogger(ABC):\n    \"\"\"Abstract base class for metrics logging backends.\n\n    All metrics loggers in the framework should inherit from this class.\n    The logger receives computed metrics from various training phases (train,\n    eval, etc.) and is responsible for persisting them (e.g., to a file,\n    a database, or a cloud service).\n\n    Attributes:\n        cfg: Configuration object for the logger.\n        enabled: Whether the logger is active.\n    \"\"\"\n\n    def __init__(self, cfg, state_dict=None, **kwargs):\n        \"\"\"Initialize the metrics logger.\n\n        Args:\n            cfg: Logger configuration (subclass of MetricsLoggerConfig).\n            state_dict: Optional state for resuming.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        self.cfg = cfg\n        self.enabled = cfg.enabled if hasattr(cfg, \"enabled\") else True\n\n        if not self.enabled:\n            logger.info(f\"{self.__class__.__name__} disabled via configuration\")\n\n    @abstractmethod\n    def setup(self, experiment_name: str, config: dict[str, Any]) -&gt; None:\n        \"\"\"Setup the logger with experiment metadata and config.\n\n        This is typically called once at the start of a training run.\n\n        Args:\n            experiment_name: A unique name for the experiment.\n            config: The full training configuration (as a dictionary).\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def log_metrics(\n        self, metrics: dict[str, Any], step: int, group: str = \"train\"\n    ) -&gt; None:\n        \"\"\"Record a set of metrics for a specific training step.\n\n        Args:\n            metrics: Dictionary mapping metric names to values.\n            step: The current training iteration or step.\n            group: The metrics group (e.g., 'train', 'eval').\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def close(self) -&gt; None:\n        \"\"\"Perform any necessary cleanup and flush remaining logs.\n\n        Called at the end of the training or evaluation process.\n        \"\"\"\n        pass\n\n    def state_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return the logger's internal state for checkpointing.\n\n        Returns:\n            A dictionary containing any state needed to resume the logger\n            (e.g., a run ID for WandB).\n        \"\"\"\n        return {}\n</code></pre>"},{"location":"reference/modules/loggers/base/#optimus_dl.modules.loggers.base.BaseMetricsLogger.__init__","title":"<code>__init__(cfg, state_dict=None, **kwargs)</code>","text":"<p>Initialize the metrics logger.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Logger configuration (subclass of MetricsLoggerConfig).</p> required <code>state_dict</code> <p>Optional state for resuming.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>optimus_dl/modules/loggers/base.py</code> <pre><code>def __init__(self, cfg, state_dict=None, **kwargs):\n    \"\"\"Initialize the metrics logger.\n\n    Args:\n        cfg: Logger configuration (subclass of MetricsLoggerConfig).\n        state_dict: Optional state for resuming.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    self.cfg = cfg\n    self.enabled = cfg.enabled if hasattr(cfg, \"enabled\") else True\n\n    if not self.enabled:\n        logger.info(f\"{self.__class__.__name__} disabled via configuration\")\n</code></pre>"},{"location":"reference/modules/loggers/base/#optimus_dl.modules.loggers.base.BaseMetricsLogger.close","title":"<code>close()</code>  <code>abstractmethod</code>","text":"<p>Perform any necessary cleanup and flush remaining logs.</p> <p>Called at the end of the training or evaluation process.</p> Source code in <code>optimus_dl/modules/loggers/base.py</code> <pre><code>@abstractmethod\ndef close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup and flush remaining logs.\n\n    Called at the end of the training or evaluation process.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/modules/loggers/base/#optimus_dl.modules.loggers.base.BaseMetricsLogger.log_metrics","title":"<code>log_metrics(metrics, step, group='train')</code>  <code>abstractmethod</code>","text":"<p>Record a set of metrics for a specific training step.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>dict[str, Any]</code> <p>Dictionary mapping metric names to values.</p> required <code>step</code> <code>int</code> <p>The current training iteration or step.</p> required <code>group</code> <code>str</code> <p>The metrics group (e.g., 'train', 'eval').</p> <code>'train'</code> Source code in <code>optimus_dl/modules/loggers/base.py</code> <pre><code>@abstractmethod\ndef log_metrics(\n    self, metrics: dict[str, Any], step: int, group: str = \"train\"\n) -&gt; None:\n    \"\"\"Record a set of metrics for a specific training step.\n\n    Args:\n        metrics: Dictionary mapping metric names to values.\n        step: The current training iteration or step.\n        group: The metrics group (e.g., 'train', 'eval').\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/modules/loggers/base/#optimus_dl.modules.loggers.base.BaseMetricsLogger.setup","title":"<code>setup(experiment_name, config)</code>  <code>abstractmethod</code>","text":"<p>Setup the logger with experiment metadata and config.</p> <p>This is typically called once at the start of a training run.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>A unique name for the experiment.</p> required <code>config</code> <code>dict[str, Any]</code> <p>The full training configuration (as a dictionary).</p> required Source code in <code>optimus_dl/modules/loggers/base.py</code> <pre><code>@abstractmethod\ndef setup(self, experiment_name: str, config: dict[str, Any]) -&gt; None:\n    \"\"\"Setup the logger with experiment metadata and config.\n\n    This is typically called once at the start of a training run.\n\n    Args:\n        experiment_name: A unique name for the experiment.\n        config: The full training configuration (as a dictionary).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/modules/loggers/base/#optimus_dl.modules.loggers.base.BaseMetricsLogger.state_dict","title":"<code>state_dict()</code>","text":"<p>Return the logger's internal state for checkpointing.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing any state needed to resume the logger</p> <code>dict[str, Any]</code> <p>(e.g., a run ID for WandB).</p> Source code in <code>optimus_dl/modules/loggers/base.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return the logger's internal state for checkpointing.\n\n    Returns:\n        A dictionary containing any state needed to resume the logger\n        (e.g., a run ID for WandB).\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"reference/modules/loggers/config/","title":"config","text":""},{"location":"reference/modules/loggers/config/#optimus_dl.modules.loggers.config","title":"<code>optimus_dl.modules.loggers.config</code>","text":""},{"location":"reference/modules/loggers/config/#optimus_dl.modules.loggers.config.MetricsLoggerConfig","title":"<code>MetricsLoggerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfig</code></p> <p>Base configuration for metrics loggers.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <code>True</code> <code>id</code> <code>str</code> <code>'${._name}'</code> <code>tags</code> <code>list[str] | None</code> <code>None</code> <code>notes</code> <code>str | None</code> <code>None</code> Source code in <code>optimus_dl/modules/loggers/config.py</code> <pre><code>@dataclass\nclass MetricsLoggerConfig(RegistryConfig):\n    \"\"\"Base configuration for metrics loggers.\"\"\"\n\n    # Common fields that all loggers might use\n    enabled: bool = True\n    id: str = II(\"._name\")\n\n    # Optional experiment metadata\n    tags: list[str] | None = None\n    notes: str | None = None\n</code></pre>"},{"location":"reference/modules/loggers/jsonl/","title":"jsonl","text":""},{"location":"reference/modules/loggers/jsonl/#optimus_dl.modules.loggers.jsonl","title":"<code>optimus_dl.modules.loggers.jsonl</code>","text":"<p>JSONL (JSON Lines) file metrics logger implementation.</p> <p>This logger writes metrics to JSONL files, with one JSON object per line, making it easy to process logs with standard tools or custom scripts.</p>"},{"location":"reference/modules/loggers/jsonl/#optimus_dl.modules.loggers.jsonl.JsonlLogger","title":"<code>JsonlLogger</code>","text":"<p>               Bases: <code>BaseMetricsLogger</code></p> <p>JSONL file metrics logger.</p> <p>Writes metrics to JSON Lines files, with one JSON object per line. Each line contains the step, group, timestamp (optional), and all metrics. Features automatic file rotation and separate file support for different metric groups.</p> <p>Example output: <pre><code>{\"step\": 1, \"group\": \"train\", \"timestamp\": \"2024-01-01T12:00:00\", \"loss\": 2.5, \"lr\": 0.001}\n{\"step\": 2, \"group\": \"train\", \"timestamp\": \"2024-01-01T12:00:01\", \"loss\": 2.3, \"lr\": 0.001}\n</code></pre></p> Source code in <code>optimus_dl/modules/loggers/jsonl.py</code> <pre><code>@register_metrics_logger(\"jsonl\", JsonlLoggerConfig)\nclass JsonlLogger(BaseMetricsLogger):\n    \"\"\"JSONL file metrics logger.\n\n    Writes metrics to JSON Lines files, with one JSON object per line.\n    Each line contains the step, group, timestamp (optional), and all metrics.\n    Features automatic file rotation and separate file support for different\n    metric groups.\n\n    Example output:\n    ```json\n    {\"step\": 1, \"group\": \"train\", \"timestamp\": \"2024-01-01T12:00:00\", \"loss\": 2.5, \"lr\": 0.001}\n    {\"step\": 2, \"group\": \"train\", \"timestamp\": \"2024-01-01T12:00:01\", \"loss\": 2.3, \"lr\": 0.001}\n    ```\n    \"\"\"\n\n    def __init__(self, cfg: JsonlLoggerConfig, **kwargs):\n        \"\"\"Initialize JSONL logger.\n\n        Args:\n            cfg: JSONL logger configuration\n            **kwargs: Additional keyword arguments\n        \"\"\"\n        super().__init__(cfg, **kwargs)\n\n        self.output_dir = Path(cfg.output_dir)\n        self.base_filename = cfg.filename\n        self.file_handles: dict[str, TextIO] = {}\n        self.file_paths: dict[str, Path] = {}  # Track current file paths\n        self.write_count = 0\n        self.max_file_size_bytes = (\n            cfg.max_file_size_mb * 1024 * 1024\n        )  # Convert MB to bytes\n\n        if self.enabled:\n            # Create output directory\n            self.output_dir.mkdir(parents=True, exist_ok=True)\n            logger.info(f\"JSONL logger will write to: {self.output_dir}\")\n\n    def setup(self, experiment_name: str, config: dict[str, Any]) -&gt; None:\n        \"\"\"Setup JSONL logger and export the experiment configuration.\n\n        Writes the provided configuration to both `.json` and `.yaml` files in the\n        output directory for reproducibility.\n        \"\"\"\n        if not self.enabled:\n            return\n\n        try:\n            # Write experiment config to separate file\n            config_file = self.output_dir / f\"{experiment_name}_config.json\"\n            with open(config_file, \"w\") as f:\n                if OmegaConf.is_config(config):\n                    config = OmegaConf.to_container(config, resolve=True)\n                json.dump(config, f, indent=2, default=str)\n\n            config_file = self.output_dir / f\"{experiment_name}_config.yaml\"\n            with open(config_file, \"w\") as f:\n                if OmegaConf.is_config(config):\n                    config = OmegaConf.to_container(config, resolve=True)\n                yaml.dump(config, f, indent=2)\n\n            logger.info(f\"Experiment config saved to: {config_file}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to write config file: {e}\")\n\n    def _rotate_file(self, group: str) -&gt; None:\n        \"\"\"Rotate log file when it gets too large.\n\n        Renames the current file (e.g., `metrics.jsonl` -&gt; `metrics.1.jsonl`) and\n        opens a new one.\n        \"\"\"\n        if group not in self.file_handles:\n            return\n\n        try:\n            # Close current handle\n            current_handle = self.file_handles[group]\n            current_handle.flush()\n            current_handle.close()\n\n            current_path = self.file_paths[group]\n\n            # Rotate existing files: file.log -&gt; file.1.log -&gt; file.2.log -&gt; ...\n            base_name = current_path.stem\n            extension = current_path.suffix\n\n            # Handle unlimited files case (-1)\n            if self.cfg.max_rotated_files &gt; 0:\n                # Remove oldest rotated file if max limit reached\n                oldest_file = (\n                    current_path.parent\n                    / f\"{base_name}.{self.cfg.max_rotated_files}{extension}\"\n                )\n                if oldest_file.exists():\n                    oldest_file.unlink()\n\n                # Rotate existing files (backwards to avoid overwrites)\n                for i in range(self.cfg.max_rotated_files - 1, 0, -1):\n                    old_file = current_path.parent / f\"{base_name}.{i}{extension}\"\n                    new_file = current_path.parent / f\"{base_name}.{i + 1}{extension}\"\n                    if old_file.exists():\n                        old_file.rename(new_file)\n            else:\n                # Unlimited files (-1): find highest numbered file and increment\n                max_index = 0\n                for existing_file in current_path.parent.glob(\n                    f\"{base_name}.*{extension}\"\n                ):\n                    try:\n                        # Extract number from filename like \"metrics.5.jsonl\"\n                        name_parts = existing_file.stem.split(\".\")\n                        if len(name_parts) &gt;= 2 and name_parts[-1].isdigit():\n                            index = int(name_parts[-1])\n                            max_index = max(max_index, index)\n                    except (ValueError, IndexError):\n                        continue\n\n                # Rotate existing files starting from highest index\n                for i in range(max_index, 0, -1):\n                    old_file = current_path.parent / f\"{base_name}.{i}{extension}\"\n                    new_file = current_path.parent / f\"{base_name}.{i + 1}{extension}\"\n                    if old_file.exists():\n                        old_file.rename(new_file)\n\n            # Move current file to .1 position\n            rotated_file = current_path.parent / f\"{base_name}.1{extension}\"\n            current_path.rename(rotated_file)\n\n            # Create new file handle\n            new_handle = open(current_path, \"a\", encoding=\"utf-8\")\n            self.file_handles[group] = new_handle\n\n            logger.info(f\"Rotated JSONL file for group '{group}': {current_path}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to rotate file for group '{group}': {e}\")\n            # Try to create a new handle anyway\n            try:\n                new_handle = open(self.file_paths[group], \"a\", encoding=\"utf-8\")\n                self.file_handles[group] = new_handle\n            except Exception as create_error:\n                logger.error(f\"Failed to create new file handle: {create_error}\")\n                # Remove the group from file_handles to force recreation on next write\n                self.file_handles.pop(group, None)\n\n    def _should_rotate_file(self, group: str) -&gt; bool:\n        \"\"\"Check if file should be rotated based on size.\"\"\"\n        if group not in self.file_paths:\n            return False\n\n        try:\n            file_path = self.file_paths[group]\n            if file_path.exists():\n                file_size = file_path.stat().st_size\n                return file_size &gt;= self.max_file_size_bytes\n        except Exception as e:\n            logger.error(f\"Error checking file size for group '{group}': {e}\")\n\n        return False\n\n    def _get_file_handle(self, group: str) -&gt; TextIO:\n        \"\"\"Get or create the file handle for a specific metrics group.\"\"\"\n        if group in self.file_handles:\n            return self.file_handles[group]\n\n        # Determine filename\n        if self.cfg.include_group_in_filename and group != \"train\":\n            # Use separate files for different groups\n            base_name = self.base_filename.rsplit(\".\", 1)[0]\n            extension = (\n                self.base_filename.rsplit(\".\", 1)[1]\n                if \".\" in self.base_filename\n                else \"jsonl\"\n            )\n            group_safe = group.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n            filename = f\"{base_name}_{group_safe}.{extension}\"\n        else:\n            filename = self.base_filename\n\n        filepath = self.output_dir / filename\n\n        # Store file path for rotation tracking\n        self.file_paths[group] = filepath\n\n        # Open file handle\n        mode = \"a\" if self.cfg.append_mode else \"w\"\n        handle = open(filepath, mode, encoding=\"utf-8\")\n\n        self.file_handles[group] = handle\n        logger.info(f\"Opened JSONL file for group '{group}': {filepath}\")\n\n        return handle\n\n    def log_metrics(\n        self, metrics: dict[str, Any], step: int, group: str = \"train\"\n    ) -&gt; None:\n        \"\"\"Flatten and write metrics to the appropriate JSONL file.\"\"\"\n        if not self.enabled:\n            return\n\n        try:\n            # Check if file needs rotation before writing\n            if self._should_rotate_file(group):\n                self._rotate_file(group)\n\n            # Get file handle for this group\n            file_handle = self._get_file_handle(group)\n\n            # Prepare log entry\n            log_entry = {\n                \"step\": step,\n                \"group\": group,\n            }\n\n            # Add timestamp if configured\n            if self.cfg.include_timestamp:\n                from datetime import (\n                    datetime,\n                    timezone,\n                )\n\n                log_entry[\"timestamp\"] = datetime.now(timezone.utc).isoformat()\n\n            # Flatten and add metrics\n            for key, value in metrics.items():\n                if isinstance(value, dict):\n                    # Handle nested metrics by flattening\n                    for nested_key, nested_value in value.items():\n                        full_key = f\"{key}_{nested_key}\"\n                        log_entry[full_key] = nested_value\n                else:\n                    log_entry[key] = value\n\n            # Write JSON line\n            json_line = json.dumps(\n                log_entry,\n                indent=self.cfg.indent,\n                sort_keys=self.cfg.sort_keys,\n                default=str,  # Convert non-serializable objects to string\n            )\n            file_handle.write(json_line + \"\\n\")\n\n            # Flush periodically\n            self.write_count += 1\n            if self.write_count % self.cfg.flush_every == 0:\n                file_handle.flush()\n                os.fsync(file_handle.fileno())\n\n        except Exception as e:\n            logger.error(f\"Failed to write metrics to JSONL: {e}\")\n\n    def close(self) -&gt; None:\n        \"\"\"Close all open file handles and sync to disk.\"\"\"\n        for group, handle in self.file_handles.items():\n            try:\n                handle.flush()\n                handle.close()\n                logger.info(f\"Closed JSONL file for group '{group}'\")\n            except Exception as e:\n                logger.error(f\"Error closing JSONL file for group '{group}': {e}\")\n\n        self.file_handles.clear()\n</code></pre>"},{"location":"reference/modules/loggers/jsonl/#optimus_dl.modules.loggers.jsonl.JsonlLogger.__init__","title":"<code>__init__(cfg, **kwargs)</code>","text":"<p>Initialize JSONL logger.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>JsonlLoggerConfig</code> <p>JSONL logger configuration</p> required <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> Source code in <code>optimus_dl/modules/loggers/jsonl.py</code> <pre><code>def __init__(self, cfg: JsonlLoggerConfig, **kwargs):\n    \"\"\"Initialize JSONL logger.\n\n    Args:\n        cfg: JSONL logger configuration\n        **kwargs: Additional keyword arguments\n    \"\"\"\n    super().__init__(cfg, **kwargs)\n\n    self.output_dir = Path(cfg.output_dir)\n    self.base_filename = cfg.filename\n    self.file_handles: dict[str, TextIO] = {}\n    self.file_paths: dict[str, Path] = {}  # Track current file paths\n    self.write_count = 0\n    self.max_file_size_bytes = (\n        cfg.max_file_size_mb * 1024 * 1024\n    )  # Convert MB to bytes\n\n    if self.enabled:\n        # Create output directory\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        logger.info(f\"JSONL logger will write to: {self.output_dir}\")\n</code></pre>"},{"location":"reference/modules/loggers/jsonl/#optimus_dl.modules.loggers.jsonl.JsonlLogger.close","title":"<code>close()</code>","text":"<p>Close all open file handles and sync to disk.</p> Source code in <code>optimus_dl/modules/loggers/jsonl.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close all open file handles and sync to disk.\"\"\"\n    for group, handle in self.file_handles.items():\n        try:\n            handle.flush()\n            handle.close()\n            logger.info(f\"Closed JSONL file for group '{group}'\")\n        except Exception as e:\n            logger.error(f\"Error closing JSONL file for group '{group}': {e}\")\n\n    self.file_handles.clear()\n</code></pre>"},{"location":"reference/modules/loggers/jsonl/#optimus_dl.modules.loggers.jsonl.JsonlLogger.log_metrics","title":"<code>log_metrics(metrics, step, group='train')</code>","text":"<p>Flatten and write metrics to the appropriate JSONL file.</p> Source code in <code>optimus_dl/modules/loggers/jsonl.py</code> <pre><code>def log_metrics(\n    self, metrics: dict[str, Any], step: int, group: str = \"train\"\n) -&gt; None:\n    \"\"\"Flatten and write metrics to the appropriate JSONL file.\"\"\"\n    if not self.enabled:\n        return\n\n    try:\n        # Check if file needs rotation before writing\n        if self._should_rotate_file(group):\n            self._rotate_file(group)\n\n        # Get file handle for this group\n        file_handle = self._get_file_handle(group)\n\n        # Prepare log entry\n        log_entry = {\n            \"step\": step,\n            \"group\": group,\n        }\n\n        # Add timestamp if configured\n        if self.cfg.include_timestamp:\n            from datetime import (\n                datetime,\n                timezone,\n            )\n\n            log_entry[\"timestamp\"] = datetime.now(timezone.utc).isoformat()\n\n        # Flatten and add metrics\n        for key, value in metrics.items():\n            if isinstance(value, dict):\n                # Handle nested metrics by flattening\n                for nested_key, nested_value in value.items():\n                    full_key = f\"{key}_{nested_key}\"\n                    log_entry[full_key] = nested_value\n            else:\n                log_entry[key] = value\n\n        # Write JSON line\n        json_line = json.dumps(\n            log_entry,\n            indent=self.cfg.indent,\n            sort_keys=self.cfg.sort_keys,\n            default=str,  # Convert non-serializable objects to string\n        )\n        file_handle.write(json_line + \"\\n\")\n\n        # Flush periodically\n        self.write_count += 1\n        if self.write_count % self.cfg.flush_every == 0:\n            file_handle.flush()\n            os.fsync(file_handle.fileno())\n\n    except Exception as e:\n        logger.error(f\"Failed to write metrics to JSONL: {e}\")\n</code></pre>"},{"location":"reference/modules/loggers/jsonl/#optimus_dl.modules.loggers.jsonl.JsonlLogger.setup","title":"<code>setup(experiment_name, config)</code>","text":"<p>Setup JSONL logger and export the experiment configuration.</p> <p>Writes the provided configuration to both <code>.json</code> and <code>.yaml</code> files in the output directory for reproducibility.</p> Source code in <code>optimus_dl/modules/loggers/jsonl.py</code> <pre><code>def setup(self, experiment_name: str, config: dict[str, Any]) -&gt; None:\n    \"\"\"Setup JSONL logger and export the experiment configuration.\n\n    Writes the provided configuration to both `.json` and `.yaml` files in the\n    output directory for reproducibility.\n    \"\"\"\n    if not self.enabled:\n        return\n\n    try:\n        # Write experiment config to separate file\n        config_file = self.output_dir / f\"{experiment_name}_config.json\"\n        with open(config_file, \"w\") as f:\n            if OmegaConf.is_config(config):\n                config = OmegaConf.to_container(config, resolve=True)\n            json.dump(config, f, indent=2, default=str)\n\n        config_file = self.output_dir / f\"{experiment_name}_config.yaml\"\n        with open(config_file, \"w\") as f:\n            if OmegaConf.is_config(config):\n                config = OmegaConf.to_container(config, resolve=True)\n            yaml.dump(config, f, indent=2)\n\n        logger.info(f\"Experiment config saved to: {config_file}\")\n\n    except Exception as e:\n        logger.error(f\"Failed to write config file: {e}\")\n</code></pre>"},{"location":"reference/modules/loggers/jsonl/#optimus_dl.modules.loggers.jsonl.JsonlLoggerConfig","title":"<code>JsonlLoggerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricsLoggerConfig</code></p> <p>Configuration for JSONL file logger.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <code>'logs'</code> <code>filename</code> <code>str</code> <code>'metrics.jsonl'</code> <code>append_mode</code> <code>bool</code> <code>True</code> <code>flush_every</code> <code>int</code> <code>10</code> <code>include_timestamp</code> <code>bool</code> <code>True</code> <code>include_group_in_filename</code> <code>bool</code> <code>True</code> <code>max_file_size_mb</code> <code>int</code> <code>200</code> <code>max_rotated_files</code> <code>int</code> <code>-1</code> <code>indent</code> <code>int | None</code> <code>None</code> <code>sort_keys</code> <code>bool</code> <code>False</code> Source code in <code>optimus_dl/modules/loggers/jsonl.py</code> <pre><code>@dataclass\nclass JsonlLoggerConfig(MetricsLoggerConfig):\n    \"\"\"Configuration for JSONL file logger.\n\n    Attributes:\n        output_dir: Directory where log files will be saved.\n        filename: Base name for the metrics file.\n        append_mode: If True, appends to existing files. If False, overwrites.\n        flush_every: Number of writes before forcing a disk sync.\n        include_timestamp: Whether to add an ISO timestamp to each entry.\n        include_group_in_filename: If True, creates separate files for each\n            metric group (e.g., 'metrics_eval.jsonl').\n        max_file_size_mb: Maximum size of a log file before it is rotated.\n        max_rotated_files: Number of old log files to keep (-1 for unlimited).\n    \"\"\"\n\n    # File settings\n    output_dir: str = \"logs\"\n    filename: str = \"metrics.jsonl\"\n\n    # Logging behavior\n    append_mode: bool = True  # Append to existing file vs overwrite\n    flush_every: int = 10  # Flush to disk every N writes\n    include_timestamp: bool = True\n    include_group_in_filename: bool = True  # Separate files per group\n\n    # File rotation settings\n    max_file_size_mb: int = 200  # Maximum file size in MB before rotation\n    max_rotated_files: int = (\n        -1\n    )  # Maximum number of rotated files to keep (-1 for unlimited)\n\n    # Data formatting\n    indent: int | None = None  # JSON indentation (None for compact)\n    sort_keys: bool = False\n</code></pre>"},{"location":"reference/modules/loggers/wandb/","title":"wandb","text":""},{"location":"reference/modules/loggers/wandb/#optimus_dl.modules.loggers.wandb","title":"<code>optimus_dl.modules.loggers.wandb</code>","text":"<p>Weights &amp; Biases (wandb) metrics logger implementation.</p> <p>This logger integrates with Weights &amp; Biases for experiment tracking, supporting both online and offline modes.</p>"},{"location":"reference/modules/loggers/wandb/#optimus_dl.modules.loggers.wandb.WandbLogger","title":"<code>WandbLogger</code>","text":"<p>               Bases: <code>BaseMetricsLogger</code></p> <p>Weights &amp; Biases metrics logger.</p> <p>Logs training metrics, configuration, and optionally model artifacts to Weights &amp; Biases for experiment tracking and visualization.</p> <p>Supports resuming runs by storing and reloading the WandB <code>run_id</code> from the training state dict.</p> Source code in <code>optimus_dl/modules/loggers/wandb.py</code> <pre><code>@register_metrics_logger(\"wandb\", WandbLoggerConfig)\nclass WandbLogger(BaseMetricsLogger):\n    \"\"\"Weights &amp; Biases metrics logger.\n\n    Logs training metrics, configuration, and optionally model artifacts\n    to Weights &amp; Biases for experiment tracking and visualization.\n\n    Supports resuming runs by storing and reloading the WandB `run_id` from\n    the training state dict.\n    \"\"\"\n\n    def __init__(self, cfg: WandbLoggerConfig, state_dict=None, **kwargs):\n        \"\"\"Initialize WandB logger.\n\n        Args:\n            cfg: WandB logger configuration.\n            state_dict: Optional state containing 'run_id' for resuming.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(cfg, **kwargs)\n\n        if not WANDB_AVAILABLE:\n            self.enabled = False\n            logger.error(\"WandB logger disabled - wandb package not available\")\n            return\n\n        if cfg.mode == \"disabled\":\n            self.enabled = False\n            logger.info(\"WandB logger disabled via mode setting\")\n            return\n\n        self.run_id = (state_dict or {}).get(\"run_id\")\n        self.run = None\n\n    def setup(self, experiment_name: str, config: dict[str, Any]) -&gt; None:\n        \"\"\"Initialize a WandB run with experiment metadata and configuration.\n\n        If `self.run_id` is present, attempts to resume the existing run.\n        \"\"\"\n        if not self.enabled:\n            return\n\n        try:\n            # Initialize wandb run\n            if OmegaConf.is_config(config):\n                config = OmegaConf.to_container(config, resolve=True)\n            self.run = wandb.init(\n                project=self.cfg.project,\n                entity=self.cfg.entity,\n                mode=self.cfg.mode,\n                name=self.cfg.name or experiment_name,\n                group=self.cfg.group,\n                job_type=self.cfg.job_type,\n                tags=list(self.cfg.tags) if self.cfg.tags else None,\n                notes=self.cfg.notes,\n                save_code=self.cfg.save_code,\n                config=config,\n                id=self.run_id,  # Resume from preemption etc\n                resume=\"allow\",  # Allow resuming if run exists\n            )\n\n            logger.info(f\"WandB run initialized: {self.run.name} ({self.run.id})\")\n\n        except Exception as e:\n            logger.error(f\"Failed to initialize WandB: {e}\", exc_info=True)\n            self.enabled = False\n\n    def log_metrics(\n        self, metrics: dict[str, Any], step: int, group: str = \"train\"\n    ) -&gt; None:\n        \"\"\"Flatten and log metrics to WandB.\n\n        Args:\n            metrics: Dictionary of metric names to values.\n            step: Training step/iteration number.\n            group: Metrics group (e.g., 'train', 'eval').\n        \"\"\"\n        if not self.enabled:\n            return\n\n        if self.run is None:\n            logger.warning(\"WandB run not initialized, skipping metrics logging\")\n            return\n\n        try:\n            # Flatten nested metrics and add group prefix\n            flattened_metrics = {}\n\n            for key, value in metrics.items():\n                if isinstance(value, dict):\n                    # Handle nested metrics\n                    for nested_key, nested_value in value.items():\n                        full_key = f\"{group}/{key}/{nested_key}\"\n                        flattened_metrics[full_key] = nested_value\n                else:\n                    # Simple metric\n                    full_key = f\"{group}/{key}\"\n                    flattened_metrics[full_key] = value\n\n            # Log to wandb\n            self.run.log(flattened_metrics, step=step)\n\n        except Exception as e:\n            logger.error(f\"Failed to log metrics to WandB: {e}\")\n\n    def close(self) -&gt; None:\n        \"\"\"Finalize and close the WandB run.\"\"\"\n        if self.run is not None:\n            try:\n                self.run.finish()\n                logger.info(\"WandB run finished successfully\")\n            except Exception as e:\n                logger.error(f\"Error finishing WandB run: {e}\")\n            finally:\n                self.run = None\n\n    def state_dict(self):\n        \"\"\"Return the current run ID for resuming later.\"\"\"\n        return {\n            \"run_id\": self.run.id if self.run is not None else None,\n        }\n</code></pre>"},{"location":"reference/modules/loggers/wandb/#optimus_dl.modules.loggers.wandb.WandbLogger.__init__","title":"<code>__init__(cfg, state_dict=None, **kwargs)</code>","text":"<p>Initialize WandB logger.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>WandbLoggerConfig</code> <p>WandB logger configuration.</p> required <code>state_dict</code> <p>Optional state containing 'run_id' for resuming.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>optimus_dl/modules/loggers/wandb.py</code> <pre><code>def __init__(self, cfg: WandbLoggerConfig, state_dict=None, **kwargs):\n    \"\"\"Initialize WandB logger.\n\n    Args:\n        cfg: WandB logger configuration.\n        state_dict: Optional state containing 'run_id' for resuming.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(cfg, **kwargs)\n\n    if not WANDB_AVAILABLE:\n        self.enabled = False\n        logger.error(\"WandB logger disabled - wandb package not available\")\n        return\n\n    if cfg.mode == \"disabled\":\n        self.enabled = False\n        logger.info(\"WandB logger disabled via mode setting\")\n        return\n\n    self.run_id = (state_dict or {}).get(\"run_id\")\n    self.run = None\n</code></pre>"},{"location":"reference/modules/loggers/wandb/#optimus_dl.modules.loggers.wandb.WandbLogger.close","title":"<code>close()</code>","text":"<p>Finalize and close the WandB run.</p> Source code in <code>optimus_dl/modules/loggers/wandb.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Finalize and close the WandB run.\"\"\"\n    if self.run is not None:\n        try:\n            self.run.finish()\n            logger.info(\"WandB run finished successfully\")\n        except Exception as e:\n            logger.error(f\"Error finishing WandB run: {e}\")\n        finally:\n            self.run = None\n</code></pre>"},{"location":"reference/modules/loggers/wandb/#optimus_dl.modules.loggers.wandb.WandbLogger.log_metrics","title":"<code>log_metrics(metrics, step, group='train')</code>","text":"<p>Flatten and log metrics to WandB.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>dict[str, Any]</code> <p>Dictionary of metric names to values.</p> required <code>step</code> <code>int</code> <p>Training step/iteration number.</p> required <code>group</code> <code>str</code> <p>Metrics group (e.g., 'train', 'eval').</p> <code>'train'</code> Source code in <code>optimus_dl/modules/loggers/wandb.py</code> <pre><code>def log_metrics(\n    self, metrics: dict[str, Any], step: int, group: str = \"train\"\n) -&gt; None:\n    \"\"\"Flatten and log metrics to WandB.\n\n    Args:\n        metrics: Dictionary of metric names to values.\n        step: Training step/iteration number.\n        group: Metrics group (e.g., 'train', 'eval').\n    \"\"\"\n    if not self.enabled:\n        return\n\n    if self.run is None:\n        logger.warning(\"WandB run not initialized, skipping metrics logging\")\n        return\n\n    try:\n        # Flatten nested metrics and add group prefix\n        flattened_metrics = {}\n\n        for key, value in metrics.items():\n            if isinstance(value, dict):\n                # Handle nested metrics\n                for nested_key, nested_value in value.items():\n                    full_key = f\"{group}/{key}/{nested_key}\"\n                    flattened_metrics[full_key] = nested_value\n            else:\n                # Simple metric\n                full_key = f\"{group}/{key}\"\n                flattened_metrics[full_key] = value\n\n        # Log to wandb\n        self.run.log(flattened_metrics, step=step)\n\n    except Exception as e:\n        logger.error(f\"Failed to log metrics to WandB: {e}\")\n</code></pre>"},{"location":"reference/modules/loggers/wandb/#optimus_dl.modules.loggers.wandb.WandbLogger.setup","title":"<code>setup(experiment_name, config)</code>","text":"<p>Initialize a WandB run with experiment metadata and configuration.</p> <p>If <code>self.run_id</code> is present, attempts to resume the existing run.</p> Source code in <code>optimus_dl/modules/loggers/wandb.py</code> <pre><code>def setup(self, experiment_name: str, config: dict[str, Any]) -&gt; None:\n    \"\"\"Initialize a WandB run with experiment metadata and configuration.\n\n    If `self.run_id` is present, attempts to resume the existing run.\n    \"\"\"\n    if not self.enabled:\n        return\n\n    try:\n        # Initialize wandb run\n        if OmegaConf.is_config(config):\n            config = OmegaConf.to_container(config, resolve=True)\n        self.run = wandb.init(\n            project=self.cfg.project,\n            entity=self.cfg.entity,\n            mode=self.cfg.mode,\n            name=self.cfg.name or experiment_name,\n            group=self.cfg.group,\n            job_type=self.cfg.job_type,\n            tags=list(self.cfg.tags) if self.cfg.tags else None,\n            notes=self.cfg.notes,\n            save_code=self.cfg.save_code,\n            config=config,\n            id=self.run_id,  # Resume from preemption etc\n            resume=\"allow\",  # Allow resuming if run exists\n        )\n\n        logger.info(f\"WandB run initialized: {self.run.name} ({self.run.id})\")\n\n    except Exception as e:\n        logger.error(f\"Failed to initialize WandB: {e}\", exc_info=True)\n        self.enabled = False\n</code></pre>"},{"location":"reference/modules/loggers/wandb/#optimus_dl.modules.loggers.wandb.WandbLogger.state_dict","title":"<code>state_dict()</code>","text":"<p>Return the current run ID for resuming later.</p> Source code in <code>optimus_dl/modules/loggers/wandb.py</code> <pre><code>def state_dict(self):\n    \"\"\"Return the current run ID for resuming later.\"\"\"\n    return {\n        \"run_id\": self.run.id if self.run is not None else None,\n    }\n</code></pre>"},{"location":"reference/modules/loggers/wandb/#optimus_dl.modules.loggers.wandb.WandbLoggerConfig","title":"<code>WandbLoggerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricsLoggerConfig</code></p> <p>Configuration for Weights &amp; Biases logger.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>project</code> <code>str | None</code> <code>None</code> <code>entity</code> <code>str | None</code> <code>None</code> <code>mode</code> <code>str</code> <code>'online'</code> <code>save_code</code> <code>bool</code> <code>True</code> <code>group</code> <code>str | None</code> <code>None</code> <code>job_type</code> <code>str | None</code> <code>'train'</code> <code>name</code> <code>str | None</code> <code>None</code> <code>log_model</code> <code>bool</code> <code>False</code> <code>log_gradients</code> <code>bool</code> <code>False</code> Source code in <code>optimus_dl/modules/loggers/wandb.py</code> <pre><code>@dataclass\nclass WandbLoggerConfig(MetricsLoggerConfig):\n    \"\"\"Configuration for Weights &amp; Biases logger.\n\n    Attributes:\n        project: Name of the WandB project.\n        entity: WandB entity (user or team) to log to.\n        mode: WandB run mode: 'online', 'offline', or 'disabled'.\n        save_code: If True, saves the main script and its dependencies.\n        group: Name for grouping related runs.\n        job_type: Label for the type of run (e.g., 'train', 'eval').\n        name: Display name for the run. If None, uses experiment name.\n        log_model: If True, automatically logs model checkpoints as artifacts.\n        log_gradients: If True, logs gradient distributions (requires model reference).\n    \"\"\"\n\n    # WandB specific settings\n    project: str | None = None\n    entity: str | None = None\n    mode: str = \"online\"  # \"online\", \"offline\", or \"disabled\"\n    save_code: bool = True\n\n    # Run configuration\n    group: str | None = None\n    job_type: str | None = \"train\"\n    name: str | None = None\n\n    # Logging settings\n    log_model: bool = False\n    log_gradients: bool = False\n</code></pre>"},{"location":"reference/modules/lr_scheduler/","title":"Index","text":""},{"location":"reference/modules/lr_scheduler/#optimus_dl.modules.lr_scheduler","title":"<code>optimus_dl.modules.lr_scheduler</code>","text":""},{"location":"reference/modules/lr_scheduler/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>base</code>: Base configuration for learning rate schedulers.</li> <li><code>cosine_annealing</code>: Configuration for cosine annealing learning rate scheduler.</li> <li><code>linear_warmup</code>: Configuration for linear warmup learning rate scheduler.</li> <li><code>wsd_scheduler</code>: Configuration for WSD (Warmup, Sustain, Decay) learning rate scheduler.</li> </ul>"},{"location":"reference/modules/lr_scheduler/base/","title":"base","text":""},{"location":"reference/modules/lr_scheduler/base/#optimus_dl.modules.lr_scheduler.base","title":"<code>optimus_dl.modules.lr_scheduler.base</code>","text":""},{"location":"reference/modules/lr_scheduler/base/#optimus_dl.modules.lr_scheduler.base.BaseLRScheduler","title":"<code>BaseLRScheduler</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for learning rate schedulers.</p> <p>This class provides a uniform interface for learning rate scheduling that is decoupled from specific optimizer implementations. It manages the stepping of learning rates across multiple parameter groups and handles state serialization for checkpointing.</p> <p>Attributes:</p> Name Type Description <code>optimizer</code> <p>The PyTorch optimizer whose learning rates are managed.</p> <code>base_lrs</code> <p>Initial learning rates for each parameter group.</p> Source code in <code>optimus_dl/modules/lr_scheduler/base.py</code> <pre><code>class BaseLRScheduler(ABC):\n    \"\"\"Abstract base class for learning rate schedulers.\n\n    This class provides a uniform interface for learning rate scheduling that\n    is decoupled from specific optimizer implementations. It manages the\n    stepping of learning rates across multiple parameter groups and handles\n    state serialization for checkpointing.\n\n    Attributes:\n        optimizer: The PyTorch optimizer whose learning rates are managed.\n        base_lrs: Initial learning rates for each parameter group.\n    \"\"\"\n\n    def __init__(self, optimizer: Optimizer, **kwargs):\n        \"\"\"Initialize the scheduler.\n\n        Args:\n            optimizer: The optimizer to manage.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        self.optimizer = optimizer\n        self._step_count = 0\n        self.base_lrs = [group[\"lr\"] for group in optimizer.param_groups]\n\n    @abstractmethod\n    def get_lr(self) -&gt; list[float]:\n        \"\"\"Calculate the target learning rates for the current step.\n\n        Returns:\n            List of floats representing the new learning rates for each\n            parameter group in the optimizer.\n        \"\"\"\n        pass\n\n    def step(self) -&gt; None:\n        \"\"\"Update the optimizer's learning rates based on the current step count.\n\n        This should be called at the end of each training iteration.\n        \"\"\"\n        self._step_count += 1\n        self.set()\n\n    def set(self) -&gt; None:\n        \"\"\"Set the learning rates of the optimizer to the current values.\"\"\"\n        values = self.get_lr()\n        for param_group, lr in zip(self.optimizer.param_groups, values, strict=True):\n            param_group[\"lr\"] = lr\n\n    def get_last_lr(self) -&gt; list[float]:\n        \"\"\"Return the most recently computed learning rates.\"\"\"\n        return [group[\"lr\"] for group in self.optimizer.param_groups]\n\n    def state_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return the scheduler's state for checkpointing.\"\"\"\n        return {\n            \"step_count\": self._step_count,\n            \"base_lrs\": self.base_lrs,\n        }\n\n    def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n        \"\"\"Restore the scheduler's state from a checkpoint.\"\"\"\n        self._step_count = state_dict[\"step_count\"]\n        self.base_lrs = state_dict[\"base_lrs\"]\n\n    @property\n    def last_epoch(self) -&gt; int:\n        \"\"\"The current step count (for compatibility with PyTorch schedulers).\"\"\"\n        return self._step_count\n</code></pre>"},{"location":"reference/modules/lr_scheduler/base/#optimus_dl.modules.lr_scheduler.base.BaseLRScheduler.last_epoch","title":"<code>last_epoch</code>  <code>property</code>","text":"<p>The current step count (for compatibility with PyTorch schedulers).</p>"},{"location":"reference/modules/lr_scheduler/base/#optimus_dl.modules.lr_scheduler.base.BaseLRScheduler.__init__","title":"<code>__init__(optimizer, **kwargs)</code>","text":"<p>Initialize the scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to manage.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>optimus_dl/modules/lr_scheduler/base.py</code> <pre><code>def __init__(self, optimizer: Optimizer, **kwargs):\n    \"\"\"Initialize the scheduler.\n\n    Args:\n        optimizer: The optimizer to manage.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    self.optimizer = optimizer\n    self._step_count = 0\n    self.base_lrs = [group[\"lr\"] for group in optimizer.param_groups]\n</code></pre>"},{"location":"reference/modules/lr_scheduler/base/#optimus_dl.modules.lr_scheduler.base.BaseLRScheduler.get_last_lr","title":"<code>get_last_lr()</code>","text":"<p>Return the most recently computed learning rates.</p> Source code in <code>optimus_dl/modules/lr_scheduler/base.py</code> <pre><code>def get_last_lr(self) -&gt; list[float]:\n    \"\"\"Return the most recently computed learning rates.\"\"\"\n    return [group[\"lr\"] for group in self.optimizer.param_groups]\n</code></pre>"},{"location":"reference/modules/lr_scheduler/base/#optimus_dl.modules.lr_scheduler.base.BaseLRScheduler.get_lr","title":"<code>get_lr()</code>  <code>abstractmethod</code>","text":"<p>Calculate the target learning rates for the current step.</p> <p>Returns:</p> Type Description <code>list[float]</code> <p>List of floats representing the new learning rates for each</p> <code>list[float]</code> <p>parameter group in the optimizer.</p> Source code in <code>optimus_dl/modules/lr_scheduler/base.py</code> <pre><code>@abstractmethod\ndef get_lr(self) -&gt; list[float]:\n    \"\"\"Calculate the target learning rates for the current step.\n\n    Returns:\n        List of floats representing the new learning rates for each\n        parameter group in the optimizer.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/modules/lr_scheduler/base/#optimus_dl.modules.lr_scheduler.base.BaseLRScheduler.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Restore the scheduler's state from a checkpoint.</p> Source code in <code>optimus_dl/modules/lr_scheduler/base.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Restore the scheduler's state from a checkpoint.\"\"\"\n    self._step_count = state_dict[\"step_count\"]\n    self.base_lrs = state_dict[\"base_lrs\"]\n</code></pre>"},{"location":"reference/modules/lr_scheduler/base/#optimus_dl.modules.lr_scheduler.base.BaseLRScheduler.set","title":"<code>set()</code>","text":"<p>Set the learning rates of the optimizer to the current values.</p> Source code in <code>optimus_dl/modules/lr_scheduler/base.py</code> <pre><code>def set(self) -&gt; None:\n    \"\"\"Set the learning rates of the optimizer to the current values.\"\"\"\n    values = self.get_lr()\n    for param_group, lr in zip(self.optimizer.param_groups, values, strict=True):\n        param_group[\"lr\"] = lr\n</code></pre>"},{"location":"reference/modules/lr_scheduler/base/#optimus_dl.modules.lr_scheduler.base.BaseLRScheduler.state_dict","title":"<code>state_dict()</code>","text":"<p>Return the scheduler's state for checkpointing.</p> Source code in <code>optimus_dl/modules/lr_scheduler/base.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return the scheduler's state for checkpointing.\"\"\"\n    return {\n        \"step_count\": self._step_count,\n        \"base_lrs\": self.base_lrs,\n    }\n</code></pre>"},{"location":"reference/modules/lr_scheduler/base/#optimus_dl.modules.lr_scheduler.base.BaseLRScheduler.step","title":"<code>step()</code>","text":"<p>Update the optimizer's learning rates based on the current step count.</p> <p>This should be called at the end of each training iteration.</p> Source code in <code>optimus_dl/modules/lr_scheduler/base.py</code> <pre><code>def step(self) -&gt; None:\n    \"\"\"Update the optimizer's learning rates based on the current step count.\n\n    This should be called at the end of each training iteration.\n    \"\"\"\n    self._step_count += 1\n    self.set()\n</code></pre>"},{"location":"reference/modules/lr_scheduler/base/#optimus_dl.modules.lr_scheduler.base.BaseLRSchedulerConfig","title":"<code>BaseLRSchedulerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfig</code></p> <p>Base configuration for learning rate schedulers.</p> Source code in <code>optimus_dl/modules/lr_scheduler/base.py</code> <pre><code>@dataclass\nclass BaseLRSchedulerConfig(RegistryConfig):\n    \"\"\"Base configuration for learning rate schedulers.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/modules/lr_scheduler/cosine_annealing/","title":"cosine_annealing","text":""},{"location":"reference/modules/lr_scheduler/cosine_annealing/#optimus_dl.modules.lr_scheduler.cosine_annealing","title":"<code>optimus_dl.modules.lr_scheduler.cosine_annealing</code>","text":""},{"location":"reference/modules/lr_scheduler/cosine_annealing/#optimus_dl.modules.lr_scheduler.cosine_annealing.CosineAnnealingLR","title":"<code>CosineAnnealingLR</code>","text":"<p>               Bases: <code>BaseLRScheduler</code></p> <p>Cosine annealing learning rate scheduler.</p> <p>Decays the learning rate using a cosine curve following: <pre><code>lr = eta_min + (base_lr - eta_min) * (1 + cos(pi * step / T_max)) / 2\n</code></pre> This implementation allows the learning rate to smoothly transition from its initial value down to <code>eta_min</code> over <code>T_max</code> steps.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>CosineAnnealingLRConfig</code> <p>Scheduler configuration.</p> required <code>optimizer</code> <code>Optimizer</code> <p>Managed optimizer.</p> required <code>iterations</code> <code>int</code> <p>Total training iterations (used if T_max is not specified).</p> required Source code in <code>optimus_dl/modules/lr_scheduler/cosine_annealing.py</code> <pre><code>@register_lr_scheduler(\"cosine_annealing\", CosineAnnealingLRConfig)\nclass CosineAnnealingLR(BaseLRScheduler):\n    \"\"\"Cosine annealing learning rate scheduler.\n\n    Decays the learning rate using a cosine curve following:\n    ```\n    lr = eta_min + (base_lr - eta_min) * (1 + cos(pi * step / T_max)) / 2\n    ```\n    This implementation allows the learning rate to smoothly transition from\n    its initial value down to `eta_min` over `T_max` steps.\n\n    Args:\n        cfg: Scheduler configuration.\n        optimizer: Managed optimizer.\n        iterations: Total training iterations (used if T_max is not specified).\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: CosineAnnealingLRConfig,\n        optimizer: Optimizer,\n        iterations: int,\n        **kwargs: Any,\n    ):\n        super().__init__(optimizer)\n        self.T_max = iterations\n        self.eta_min = cfg.eta_min\n        self._step_count = cfg.last_epoch + 1\n        self.set()\n\n    def get_lr(self) -&gt; list[float]:\n        \"\"\"Calculate learning rates using the cosine annealing formula.\"\"\"\n        return [\n            self.eta_min\n            + (base_lr - self.eta_min)\n            * (1 + math.cos(math.pi * self._step_count / self.T_max))\n            / 2\n            for base_lr in self.base_lrs\n        ]\n\n    def state_dict(self) -&gt; dict[str, any]:\n        \"\"\"Return the scheduler's state, including cosine-specific parameters.\"\"\"\n        state = super().state_dict()\n        state.update(\n            {\n                \"T_max\": self.T_max,\n                \"eta_min\": self.eta_min,\n            }\n        )\n        return state\n\n    def load_state_dict(self, state_dict: dict[str, any]) -&gt; None:\n        \"\"\"Restore the scheduler's state.\"\"\"\n        super().load_state_dict(state_dict)\n        self.T_max = state_dict[\"T_max\"]\n        self.eta_min = state_dict[\"eta_min\"]\n        self.set()\n</code></pre>"},{"location":"reference/modules/lr_scheduler/cosine_annealing/#optimus_dl.modules.lr_scheduler.cosine_annealing.CosineAnnealingLR.get_lr","title":"<code>get_lr()</code>","text":"<p>Calculate learning rates using the cosine annealing formula.</p> Source code in <code>optimus_dl/modules/lr_scheduler/cosine_annealing.py</code> <pre><code>def get_lr(self) -&gt; list[float]:\n    \"\"\"Calculate learning rates using the cosine annealing formula.\"\"\"\n    return [\n        self.eta_min\n        + (base_lr - self.eta_min)\n        * (1 + math.cos(math.pi * self._step_count / self.T_max))\n        / 2\n        for base_lr in self.base_lrs\n    ]\n</code></pre>"},{"location":"reference/modules/lr_scheduler/cosine_annealing/#optimus_dl.modules.lr_scheduler.cosine_annealing.CosineAnnealingLR.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Restore the scheduler's state.</p> Source code in <code>optimus_dl/modules/lr_scheduler/cosine_annealing.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, any]) -&gt; None:\n    \"\"\"Restore the scheduler's state.\"\"\"\n    super().load_state_dict(state_dict)\n    self.T_max = state_dict[\"T_max\"]\n    self.eta_min = state_dict[\"eta_min\"]\n    self.set()\n</code></pre>"},{"location":"reference/modules/lr_scheduler/cosine_annealing/#optimus_dl.modules.lr_scheduler.cosine_annealing.CosineAnnealingLR.state_dict","title":"<code>state_dict()</code>","text":"<p>Return the scheduler's state, including cosine-specific parameters.</p> Source code in <code>optimus_dl/modules/lr_scheduler/cosine_annealing.py</code> <pre><code>def state_dict(self) -&gt; dict[str, any]:\n    \"\"\"Return the scheduler's state, including cosine-specific parameters.\"\"\"\n    state = super().state_dict()\n    state.update(\n        {\n            \"T_max\": self.T_max,\n            \"eta_min\": self.eta_min,\n        }\n    )\n    return state\n</code></pre>"},{"location":"reference/modules/lr_scheduler/cosine_annealing/#optimus_dl.modules.lr_scheduler.cosine_annealing.CosineAnnealingLRConfig","title":"<code>CosineAnnealingLRConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseLRSchedulerConfig</code></p> <p>Configuration for cosine annealing learning rate scheduler.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>T_max</code> <code>int</code> <code>1000</code> <code>eta_min</code> <code>float</code> <code>0.0</code> <code>last_epoch</code> <code>int</code> <code>-1</code> Source code in <code>optimus_dl/modules/lr_scheduler/cosine_annealing.py</code> <pre><code>@dataclass\nclass CosineAnnealingLRConfig(BaseLRSchedulerConfig):\n    \"\"\"Configuration for cosine annealing learning rate scheduler.\n\n    Attributes:\n        T_max: Maximum number of iterations for the decay.\n        eta_min: Minimum learning rate (bottom of the cosine curve).\n        last_epoch: Initial step count for resuming.\n    \"\"\"\n\n    T_max: int = 1000  # Maximum number of iterations\n    eta_min: float = 0.0  # Minimum learning rate\n    last_epoch: int = -1  # Last epoch (for resuming)\n</code></pre>"},{"location":"reference/modules/lr_scheduler/linear_warmup/","title":"linear_warmup","text":""},{"location":"reference/modules/lr_scheduler/linear_warmup/#optimus_dl.modules.lr_scheduler.linear_warmup","title":"<code>optimus_dl.modules.lr_scheduler.linear_warmup</code>","text":""},{"location":"reference/modules/lr_scheduler/linear_warmup/#optimus_dl.modules.lr_scheduler.linear_warmup.LinearWarmupLR","title":"<code>LinearWarmupLR</code>","text":"<p>               Bases: <code>BaseLRScheduler</code></p> <p>Linear warmup learning rate scheduler.</p> <p>Linearly increases the learning rate from <code>start_lr</code> to <code>target_lr</code> over a specified number of steps. Once the warmup phase is complete, the learning rate is held constant at <code>target_lr</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>LinearWarmupLRConfig</code> <p>Scheduler configuration.</p> required <code>optimizer</code> <code>Optimizer</code> <p>Managed optimizer.</p> required <code>iterations</code> <code>int</code> <p>Total training iterations (used to calculate warmup steps if configured by percentage).</p> required Source code in <code>optimus_dl/modules/lr_scheduler/linear_warmup.py</code> <pre><code>@register_lr_scheduler(\"linear_warmup\", LinearWarmupLRConfig)\nclass LinearWarmupLR(BaseLRScheduler):\n    \"\"\"Linear warmup learning rate scheduler.\n\n    Linearly increases the learning rate from `start_lr` to `target_lr` over a\n    specified number of steps. Once the warmup phase is complete, the learning\n    rate is held constant at `target_lr`.\n\n    Args:\n        cfg: Scheduler configuration.\n        optimizer: Managed optimizer.\n        iterations: Total training iterations (used to calculate warmup steps\n            if configured by percentage).\n    \"\"\"\n\n    def __init__(\n        self, cfg: LinearWarmupLRConfig, optimizer: Optimizer, iterations: int, **kwargs\n    ):\n        super().__init__(optimizer)\n        if cfg.warmup_steps is None:\n            if cfg.warmup_percent is not None:\n                cfg.warmup_steps = int(iterations * cfg.warmup_percent)\n            else:\n                raise ValueError(\"Either warmup_steps or warmup_percent must be set\")\n        self.warmup_steps = cfg.warmup_steps\n        self.start_lr = cfg.start_lr\n        self.target_lrs = [cfg.target_lr or base_lr for base_lr in self.base_lrs]\n        self.set()\n\n    def get_lr(self) -&gt; list[float]:\n        \"\"\"Calculate learning rates using the linear warmup formula.\"\"\"\n        if self.warmup_steps == 0 or self._step_count &gt; self.warmup_steps:\n            # No warmup or post-warmup: maintain target learning rate\n            return self.target_lrs.copy()\n        else:\n            # Linear warmup phase\n            warmup_factor = self._step_count / self.warmup_steps\n            return [\n                self.start_lr + (target_lr - self.start_lr) * warmup_factor\n                for target_lr in self.target_lrs\n            ]\n\n    def state_dict(self) -&gt; dict[str, any]:\n        \"\"\"Return the scheduler's state, including warmup-specific parameters.\"\"\"\n        state = super().state_dict()\n        state.update(\n            {\n                \"warmup_steps\": self.warmup_steps,\n                \"start_lr\": self.start_lr,\n                \"target_lrs\": self.target_lrs,\n            }\n        )\n        return state\n\n    def load_state_dict(self, state_dict: dict[str, any]) -&gt; None:\n        \"\"\"Restore the scheduler's state.\"\"\"\n        super().load_state_dict(state_dict)\n        self.warmup_steps = state_dict[\"warmup_steps\"]\n        self.start_lr = state_dict[\"start_lr\"]\n        self.target_lrs = state_dict[\"target_lrs\"]\n        self.set()\n</code></pre>"},{"location":"reference/modules/lr_scheduler/linear_warmup/#optimus_dl.modules.lr_scheduler.linear_warmup.LinearWarmupLR.get_lr","title":"<code>get_lr()</code>","text":"<p>Calculate learning rates using the linear warmup formula.</p> Source code in <code>optimus_dl/modules/lr_scheduler/linear_warmup.py</code> <pre><code>def get_lr(self) -&gt; list[float]:\n    \"\"\"Calculate learning rates using the linear warmup formula.\"\"\"\n    if self.warmup_steps == 0 or self._step_count &gt; self.warmup_steps:\n        # No warmup or post-warmup: maintain target learning rate\n        return self.target_lrs.copy()\n    else:\n        # Linear warmup phase\n        warmup_factor = self._step_count / self.warmup_steps\n        return [\n            self.start_lr + (target_lr - self.start_lr) * warmup_factor\n            for target_lr in self.target_lrs\n        ]\n</code></pre>"},{"location":"reference/modules/lr_scheduler/linear_warmup/#optimus_dl.modules.lr_scheduler.linear_warmup.LinearWarmupLR.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Restore the scheduler's state.</p> Source code in <code>optimus_dl/modules/lr_scheduler/linear_warmup.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, any]) -&gt; None:\n    \"\"\"Restore the scheduler's state.\"\"\"\n    super().load_state_dict(state_dict)\n    self.warmup_steps = state_dict[\"warmup_steps\"]\n    self.start_lr = state_dict[\"start_lr\"]\n    self.target_lrs = state_dict[\"target_lrs\"]\n    self.set()\n</code></pre>"},{"location":"reference/modules/lr_scheduler/linear_warmup/#optimus_dl.modules.lr_scheduler.linear_warmup.LinearWarmupLR.state_dict","title":"<code>state_dict()</code>","text":"<p>Return the scheduler's state, including warmup-specific parameters.</p> Source code in <code>optimus_dl/modules/lr_scheduler/linear_warmup.py</code> <pre><code>def state_dict(self) -&gt; dict[str, any]:\n    \"\"\"Return the scheduler's state, including warmup-specific parameters.\"\"\"\n    state = super().state_dict()\n    state.update(\n        {\n            \"warmup_steps\": self.warmup_steps,\n            \"start_lr\": self.start_lr,\n            \"target_lrs\": self.target_lrs,\n        }\n    )\n    return state\n</code></pre>"},{"location":"reference/modules/lr_scheduler/linear_warmup/#optimus_dl.modules.lr_scheduler.linear_warmup.LinearWarmupLRConfig","title":"<code>LinearWarmupLRConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseLRSchedulerConfig</code></p> <p>Configuration for linear warmup learning rate scheduler.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>warmup_steps</code> <code>int | None</code> <code>None</code> <code>warmup_percent</code> <code>float | None</code> <code>0.05</code> <code>target_lr</code> <code>float | None</code> <code>None</code> <code>start_lr</code> <code>float</code> <code>0.0</code> Source code in <code>optimus_dl/modules/lr_scheduler/linear_warmup.py</code> <pre><code>@dataclass\nclass LinearWarmupLRConfig(BaseLRSchedulerConfig):\n    \"\"\"Configuration for linear warmup learning rate scheduler.\n\n    Attributes:\n        warmup_steps: Number of iterations for the linear warmup.\n        warmup_percent: Fraction of total iterations for warmup (used if\n            warmup_steps is None).\n        target_lr: Final learning rate after warmup (defaults to base_lr).\n        start_lr: Initial learning rate at step 0.\n    \"\"\"\n\n    warmup_steps: int | None = None\n    warmup_percent: float | None = 0.05  # Percentage of total steps for warmup\n    target_lr: float | None = None  # Target learning rate (defaults to base_lr)\n    start_lr: float = 0.0  # Starting learning rate\n</code></pre>"},{"location":"reference/modules/lr_scheduler/wsd_scheduler/","title":"wsd_scheduler","text":""},{"location":"reference/modules/lr_scheduler/wsd_scheduler/#optimus_dl.modules.lr_scheduler.wsd_scheduler","title":"<code>optimus_dl.modules.lr_scheduler.wsd_scheduler</code>","text":""},{"location":"reference/modules/lr_scheduler/wsd_scheduler/#optimus_dl.modules.lr_scheduler.wsd_scheduler.WSDScheduler","title":"<code>WSDScheduler</code>","text":"<p>               Bases: <code>BaseLRScheduler</code></p> <p>WSD (Warmup, Sustain, Decay) learning rate scheduler.</p> <p>This scheduler is designed for pre-training large models and consists of: 1. Warmup: Linear increase from <code>base_lr / init_div_factor</code> to <code>base_lr</code>. 2. Sustain: Constant learning rate at <code>base_lr</code>. 3. Decay (Cooldown): Decrease from <code>base_lr</code> to <code>base_lr * final_lr_factor</code>.</p> <p>The decay phase supports multiple shapes, including linear, cosine, and piecewise linear.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>WSDSchedulerConfig</code> <p>Scheduler configuration.</p> required <code>optimizer</code> <code>Optimizer</code> <p>Managed optimizer.</p> required <code>iterations</code> <code>int</code> <p>Total training iterations.</p> required Source code in <code>optimus_dl/modules/lr_scheduler/wsd_scheduler.py</code> <pre><code>@register_lr_scheduler(\"wsd\", WSDSchedulerConfig)\nclass WSDScheduler(BaseLRScheduler):\n    \"\"\"WSD (Warmup, Sustain, Decay) learning rate scheduler.\n\n    This scheduler is designed for pre-training large models and consists of:\n    1. **Warmup**: Linear increase from `base_lr / init_div_factor` to `base_lr`.\n    2. **Sustain**: Constant learning rate at `base_lr`.\n    3. **Decay (Cooldown)**: Decrease from `base_lr` to `base_lr * final_lr_factor`.\n\n    The decay phase supports multiple shapes, including linear, cosine, and\n    piecewise linear.\n\n    Args:\n        cfg: Scheduler configuration.\n        optimizer: Managed optimizer.\n        iterations: Total training iterations.\n    \"\"\"\n\n    def __init__(\n        self, cfg: WSDSchedulerConfig, optimizer: Optimizer, iterations: int, **kwargs\n    ):\n        super().__init__(optimizer)\n\n        assert (\n            cfg.warmup_steps is not None or cfg.warmup_steps_fraction is not None\n        ), \"Either warmup_steps or warmup_steps_fraction must be specified\"\n        if cfg.warmup_steps is None:\n            assert cfg.warmup_steps_fraction is not None\n            cfg.warmup_steps = int(cfg.warmup_steps_fraction * iterations)\n\n        self.iterations = iterations\n        self.final_lr_factor = cfg.final_lr_factor\n        self.warmup_steps = cfg.warmup_steps\n        self.init_div_factor = cfg.init_div_factor\n        self.fract_decay = cfg.fract_decay\n        self.decay_type = cfg.decay_type\n        self.sqrt_power = cfg.sqrt_power\n        self.linear_pw_subdivisions = cfg.linear_pw_subdivisions or []\n        self.cooldown_start_lr_factor = cfg.cooldown_start_lr_factor\n\n        # Calculate phase boundaries\n        self.n_anneal_steps = int(self.fract_decay * iterations)\n        self.n_hold = iterations - self.n_anneal_steps\n\n        # Validate decay type\n        valid_decay_types = [\n            \"linear\",\n            \"linear_pw\",\n            \"exp\",\n            \"cosine\",\n            \"miror_cosine\",\n            \"square\",\n            \"sqrt\",\n        ]\n        if self.decay_type not in valid_decay_types:\n            raise ValueError(\n                f\"decay_type {self.decay_type} is not in {valid_decay_types}\"\n            )\n\n        self.set()\n\n    def get_lr(self) -&gt; list[float]:\n        \"\"\"Calculate learning rates using the WSD formula for the current step.\"\"\"\n        step = self._step_count\n        lr_factor = self._get_lr_factor(step)\n        return [base_lr * lr_factor for base_lr in self.base_lrs]\n\n    def _get_lr_factor(self, step: int) -&gt; float:\n        \"\"\"Identify current phase and compute the corresponding LR factor.\"\"\"\n        if step &lt; self.warmup_steps:\n            # Warmup phase: linear interpolation from 1/init_div_factor to 1.0\n            return (step / self.warmup_steps) + (\n                1 - step / self.warmup_steps\n            ) / self.init_div_factor\n        elif step &lt; self.n_hold:\n            # Hold phase: constant at 1.0\n            return 1.0\n        elif step &lt; self.iterations:\n            # Decay phase: various decay strategies\n            return self._get_decay_factor(step)\n        else:\n            # Past end: final learning rate factor\n            return self.final_lr_factor\n\n    def _get_decay_factor(self, step: int) -&gt; float:\n        \"\"\"Compute decay factor shape based on configuration.\"\"\"\n        if self.decay_type == \"linear\":\n            progress = (step - self.n_hold) / self.n_anneal_steps\n            return self.final_lr_factor + (\n                self.cooldown_start_lr_factor - self.final_lr_factor\n            ) * (1 - progress)\n\n        elif self.decay_type == \"linear_pw\":\n            subdivisions = (\n                [self.cooldown_start_lr_factor]\n                + self.linear_pw_subdivisions\n                + [self.final_lr_factor]\n            )\n            division_step = 1 / (len(subdivisions) - 1)\n\n            cooldown_fraction = (step - self.n_hold) / self.n_anneal_steps\n            now_subdivision = math.floor(cooldown_fraction / division_step)\n            now_subdivision = min(\n                now_subdivision, len(subdivisions) - 2\n            )  # Ensure we don't go out of bounds\n\n            left_frac, right_frac = (\n                subdivisions[now_subdivision],\n                subdivisions[now_subdivision + 1],\n            )\n            local_fraction = (\n                cooldown_fraction - division_step * now_subdivision\n            ) / division_step\n            return left_frac + (right_frac - left_frac) * local_fraction\n\n        elif self.decay_type == \"exp\":\n            progress = (step - self.n_hold) / self.n_anneal_steps\n            return self.final_lr_factor**progress\n\n        elif self.decay_type == \"cosine\":\n            progress = (step - self.n_hold) / self.n_anneal_steps\n            return (\n                self.final_lr_factor\n                + (self.cooldown_start_lr_factor - self.final_lr_factor)\n                * (1 + math.cos(math.pi * progress))\n                * 0.5\n            )\n\n        elif self.decay_type == \"miror_cosine\":\n            progress = (step - self.n_hold) / self.n_anneal_steps\n            cosine_value = (\n                self.final_lr_factor\n                + (self.cooldown_start_lr_factor - self.final_lr_factor)\n                * (1 + math.cos(math.pi * progress))\n                * 0.5\n            )\n            linear_value = self.final_lr_factor + (\n                self.cooldown_start_lr_factor - self.final_lr_factor\n            ) * (1 - progress)\n            return linear_value * 2 - cosine_value\n\n        elif self.decay_type == \"square\":\n            progress = (step - self.n_hold) / self.n_anneal_steps\n            return self.final_lr_factor + (\n                self.cooldown_start_lr_factor - self.final_lr_factor\n            ) * (1 - progress**2)\n\n        elif self.decay_type == \"sqrt\":\n            progress = (step - self.n_hold) / self.n_anneal_steps\n            return self.final_lr_factor + (\n                self.cooldown_start_lr_factor - self.final_lr_factor\n            ) * (1 - progress**self.sqrt_power)\n        else:\n            raise ValueError(f\"Unknown decay_type: {self.decay_type}\")\n\n    def state_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return the scheduler's state, including WSD-specific parameters.\"\"\"\n        state = super().state_dict()\n        state.update(\n            {\n                \"iterations\": self.iterations,\n                \"final_lr_factor\": self.final_lr_factor,\n                \"warmup_steps\": self.warmup_steps,\n                \"init_div_factor\": self.init_div_factor,\n                \"fract_decay\": self.fract_decay,\n                \"decay_type\": self.decay_type,\n                \"sqrt_power\": self.sqrt_power,\n                \"linear_pw_subdivisions\": self.linear_pw_subdivisions,\n                \"cooldown_start_lr_factor\": self.cooldown_start_lr_factor,\n                \"n_anneal_steps\": self.n_anneal_steps,\n                \"n_hold\": self.n_hold,\n            }\n        )\n        return state\n\n    def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n        \"\"\"Restore the scheduler's state.\"\"\"\n        super().load_state_dict(state_dict)\n        self.iterations = state_dict[\"iterations\"]\n        self.final_lr_factor = state_dict[\"final_lr_factor\"]\n        self.warmup_steps = state_dict[\"warmup_steps\"]\n        self.init_div_factor = state_dict[\"init_div_factor\"]\n        self.fract_decay = state_dict[\"fract_decay\"]\n        self.decay_type = state_dict[\"decay_type\"]\n        self.sqrt_power = state_dict[\"sqrt_power\"]\n        self.linear_pw_subdivisions = state_dict[\"linear_pw_subdivisions\"]\n        self.cooldown_start_lr_factor = state_dict[\"cooldown_start_lr_factor\"]\n        self.n_anneal_steps = state_dict[\"n_anneal_steps\"]\n        self.n_hold = state_dict[\"n_hold\"]\n        self.set()\n</code></pre>"},{"location":"reference/modules/lr_scheduler/wsd_scheduler/#optimus_dl.modules.lr_scheduler.wsd_scheduler.WSDScheduler.get_lr","title":"<code>get_lr()</code>","text":"<p>Calculate learning rates using the WSD formula for the current step.</p> Source code in <code>optimus_dl/modules/lr_scheduler/wsd_scheduler.py</code> <pre><code>def get_lr(self) -&gt; list[float]:\n    \"\"\"Calculate learning rates using the WSD formula for the current step.\"\"\"\n    step = self._step_count\n    lr_factor = self._get_lr_factor(step)\n    return [base_lr * lr_factor for base_lr in self.base_lrs]\n</code></pre>"},{"location":"reference/modules/lr_scheduler/wsd_scheduler/#optimus_dl.modules.lr_scheduler.wsd_scheduler.WSDScheduler.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Restore the scheduler's state.</p> Source code in <code>optimus_dl/modules/lr_scheduler/wsd_scheduler.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Restore the scheduler's state.\"\"\"\n    super().load_state_dict(state_dict)\n    self.iterations = state_dict[\"iterations\"]\n    self.final_lr_factor = state_dict[\"final_lr_factor\"]\n    self.warmup_steps = state_dict[\"warmup_steps\"]\n    self.init_div_factor = state_dict[\"init_div_factor\"]\n    self.fract_decay = state_dict[\"fract_decay\"]\n    self.decay_type = state_dict[\"decay_type\"]\n    self.sqrt_power = state_dict[\"sqrt_power\"]\n    self.linear_pw_subdivisions = state_dict[\"linear_pw_subdivisions\"]\n    self.cooldown_start_lr_factor = state_dict[\"cooldown_start_lr_factor\"]\n    self.n_anneal_steps = state_dict[\"n_anneal_steps\"]\n    self.n_hold = state_dict[\"n_hold\"]\n    self.set()\n</code></pre>"},{"location":"reference/modules/lr_scheduler/wsd_scheduler/#optimus_dl.modules.lr_scheduler.wsd_scheduler.WSDScheduler.state_dict","title":"<code>state_dict()</code>","text":"<p>Return the scheduler's state, including WSD-specific parameters.</p> Source code in <code>optimus_dl/modules/lr_scheduler/wsd_scheduler.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return the scheduler's state, including WSD-specific parameters.\"\"\"\n    state = super().state_dict()\n    state.update(\n        {\n            \"iterations\": self.iterations,\n            \"final_lr_factor\": self.final_lr_factor,\n            \"warmup_steps\": self.warmup_steps,\n            \"init_div_factor\": self.init_div_factor,\n            \"fract_decay\": self.fract_decay,\n            \"decay_type\": self.decay_type,\n            \"sqrt_power\": self.sqrt_power,\n            \"linear_pw_subdivisions\": self.linear_pw_subdivisions,\n            \"cooldown_start_lr_factor\": self.cooldown_start_lr_factor,\n            \"n_anneal_steps\": self.n_anneal_steps,\n            \"n_hold\": self.n_hold,\n        }\n    )\n    return state\n</code></pre>"},{"location":"reference/modules/lr_scheduler/wsd_scheduler/#optimus_dl.modules.lr_scheduler.wsd_scheduler.WSDSchedulerConfig","title":"<code>WSDSchedulerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseLRSchedulerConfig</code></p> <p>Configuration for WSD (Warmup, Sustain, Decay) learning rate scheduler.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>final_lr_factor</code> <code>float</code> <code>0.0</code> <code>warmup_steps</code> <code>int | None</code> <code>300</code> <code>warmup_steps_fraction</code> <code>float | None</code> <code>None</code> <code>init_div_factor</code> <code>int</code> <code>100</code> <code>fract_decay</code> <code>float</code> <code>0.1</code> <code>decay_type</code> <code>str</code> <code>'linear'</code> <code>sqrt_power</code> <code>float</code> <code>0.5</code> <code>linear_pw_subdivisions</code> <code>list[float] | None</code> <code>None</code> <code>cooldown_start_lr_factor</code> <code>float</code> <code>1.0</code> Source code in <code>optimus_dl/modules/lr_scheduler/wsd_scheduler.py</code> <pre><code>@dataclass\nclass WSDSchedulerConfig(BaseLRSchedulerConfig):\n    \"\"\"Configuration for WSD (Warmup, Sustain, Decay) learning rate scheduler.\n\n    Attributes:\n        final_lr_factor: Factor of base_lr for the final learning rate.\n        warmup_steps: Number of iterations for linear warmup.\n        warmup_steps_fraction: Fraction of total iterations for warmup.\n        init_div_factor: Initial division factor for start of warmup (1/X).\n        fract_decay: Fraction of total iterations dedicated to decay phase.\n        decay_type: Strategy for decay ('linear', 'cosine', 'exp', etc.).\n        sqrt_power: Power for 'sqrt' decay strategy.\n        linear_pw_subdivisions: Intermediate factors for piecewise linear decay.\n        cooldown_start_lr_factor: LR factor at the start of decay phase.\n    \"\"\"\n\n    final_lr_factor: float = 0.0  # factor by which to reduce max_lr at the end\n    warmup_steps: int | None = 300  # number of warmup iterations\n    warmup_steps_fraction: float | None = None  # fraction of iterations used for warmup\n    init_div_factor: int = 100  # initial division factor for warmup\n    fract_decay: float = 0.1  # fraction of iterations used for decay\n    decay_type: str = (\n        \"linear\"  # type of decay: linear, linear_pw, exp, cosine, miror_cosine, square, sqrt\n    )\n    sqrt_power: float = 0.5  # power for sqrt decay type\n    linear_pw_subdivisions: list[float] | None = (\n        None  # subdivisions for linear_pw decay\n    )\n    cooldown_start_lr_factor: float = 1.0  # starting factor for cooldown phase\n</code></pre>"},{"location":"reference/modules/metrics/","title":"Index","text":""},{"location":"reference/modules/metrics/#optimus_dl.modules.metrics","title":"<code>optimus_dl.modules.metrics</code>","text":""},{"location":"reference/modules/metrics/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>base</code>: Container for a meter and its logging metadata.</li> <li><code>common</code>: Common meter implementations and logging utilities.</li> <li><code>engine</code>: Internal representation of a validated metric group.</li> <li><code>metrics</code>: Stateless definition for computing metrics from model/source data.</li> <li><code>source</code>: Standardized string constants for common metric data protocols.</li> <li><code>sources</code>: </li> </ul>"},{"location":"reference/modules/metrics/base/","title":"base","text":""},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base","title":"<code>optimus_dl.modules.metrics.base</code>","text":""},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.BaseMeter","title":"<code>BaseMeter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all individual stateful meter implementations.</p> <p>Meters are responsible for accumulating raw data (via the <code>log</code> method) and processing it into a final, reportable value (via the <code>compute</code> method). A key feature of <code>BaseMeter</code> is its support for merging states from other meter instances, which is crucial for distributed aggregation across multiple workers or processes.</p> <p>Subclasses must implement: - <code>compute()</code>: To return the current aggregated value(s). - <code>log(**kwargs)</code>: To accumulate new data points. - <code>merge(other_state)</code>: To combine its state with that of another meter.</p> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>class BaseMeter(ABC):\n    \"\"\"Abstract base class for all individual stateful meter implementations.\n\n    Meters are responsible for accumulating raw data (via the `log` method)\n    and processing it into a final, reportable value (via the `compute` method).\n    A key feature of `BaseMeter` is its support for merging states from other\n    meter instances, which is crucial for distributed aggregation across multiple\n    workers or processes.\n\n    Subclasses must implement:\n    - `compute()`: To return the current aggregated value(s).\n    - `log(**kwargs)`: To accumulate new data points.\n    - `merge(other_state)`: To combine its state with that of another meter.\n    \"\"\"\n\n    @abstractmethod\n    def compute(self) -&gt; float | int | dict[str, float | int]:\n        \"\"\"Compute the final meter value from accumulated data.\n\n        This method should perform any necessary calculations on the internally\n        accumulated data and return the result. It should not modify the meter's\n        internal state.\n\n        Returns:\n            The computed value, which can be a float, integer, or a dictionary\n            of sub-values (e.g., {'precision': 0.8, 'recall': 0.9}).\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def log(self, **kwargs):\n        \"\"\"Accumulate new raw data points into the meter's internal state.\n\n        This method is called for each data point or batch that needs to be\n        processed by the meter. The specific arguments in `**kwargs` depend\n        on the concrete meter implementation.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments representing the data to be logged.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def merge(self, other_state: dict[str, Any]):\n        \"\"\"Merge state from another instance of the same meter type.\n\n        This method is critical for distributed training, allowing the states\n        of meters from different processes/ranks to be combined into a single,\n        globally consistent state. The `other_state` should be a dictionary\n        representing the internal state of another meter.\n\n        Args:\n            other_state: A dictionary containing the internal state of another\n                         `BaseMeter` instance, typically obtained via `state_dict()`.\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def from_state_dict(cls, state_dict: dict[str, Any]) -&gt; BaseMeter:\n        \"\"\"Create a new meter instance and restore its state from a dictionary.\n\n        This factory method constructs an instance of the meter class (`cls`)\n        and then calls its `load_state_dict` method to populate its internal state.\n\n        Args:\n            state_dict: A dictionary containing the saved internal state of a meter.\n\n        Returns:\n            A new instance of the `BaseMeter` subclass with its state restored.\n        \"\"\"\n        instance = cls()\n        instance.load_state_dict(state_dict)\n        return instance\n\n    def state_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return the internal meter state as a dictionary for checkpointing.\n\n        By default, this returns a shallow copy of `self.__dict__`. Subclasses\n        may override this method if they need custom serialization logic (e.g.,\n        to handle non-serializable attributes or specific data structures).\n\n        Returns:\n            A dictionary representing the internal, serializable state of the meter.\n        \"\"\"\n        return self.__dict__\n\n    def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n        \"\"\"Restore the internal meter state from a dictionary.\n\n        By default, this updates `self.__dict__` with the provided `state_dict`.\n        Subclasses may override this method for custom deserialization,\n        especially if `state_dict()` was also overridden.\n\n        Args:\n            state_dict: A dictionary containing the saved internal state of a meter.\n        \"\"\"\n        self.__dict__.update(state_dict)\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.BaseMeter.compute","title":"<code>compute()</code>  <code>abstractmethod</code>","text":"<p>Compute the final meter value from accumulated data.</p> <p>This method should perform any necessary calculations on the internally accumulated data and return the result. It should not modify the meter's internal state.</p> <p>Returns:</p> Type Description <code>float | int | dict[str, float | int]</code> <p>The computed value, which can be a float, integer, or a dictionary</p> <code>float | int | dict[str, float | int]</code> <p>of sub-values (e.g., {'precision': 0.8, 'recall': 0.9}).</p> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>@abstractmethod\ndef compute(self) -&gt; float | int | dict[str, float | int]:\n    \"\"\"Compute the final meter value from accumulated data.\n\n    This method should perform any necessary calculations on the internally\n    accumulated data and return the result. It should not modify the meter's\n    internal state.\n\n    Returns:\n        The computed value, which can be a float, integer, or a dictionary\n        of sub-values (e.g., {'precision': 0.8, 'recall': 0.9}).\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.BaseMeter.from_state_dict","title":"<code>from_state_dict(state_dict)</code>  <code>classmethod</code>","text":"<p>Create a new meter instance and restore its state from a dictionary.</p> <p>This factory method constructs an instance of the meter class (<code>cls</code>) and then calls its <code>load_state_dict</code> method to populate its internal state.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict[str, Any]</code> <p>A dictionary containing the saved internal state of a meter.</p> required <p>Returns:</p> Type Description <code>BaseMeter</code> <p>A new instance of the <code>BaseMeter</code> subclass with its state restored.</p> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>@classmethod\ndef from_state_dict(cls, state_dict: dict[str, Any]) -&gt; BaseMeter:\n    \"\"\"Create a new meter instance and restore its state from a dictionary.\n\n    This factory method constructs an instance of the meter class (`cls`)\n    and then calls its `load_state_dict` method to populate its internal state.\n\n    Args:\n        state_dict: A dictionary containing the saved internal state of a meter.\n\n    Returns:\n        A new instance of the `BaseMeter` subclass with its state restored.\n    \"\"\"\n    instance = cls()\n    instance.load_state_dict(state_dict)\n    return instance\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.BaseMeter.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Restore the internal meter state from a dictionary.</p> <p>By default, this updates <code>self.__dict__</code> with the provided <code>state_dict</code>. Subclasses may override this method for custom deserialization, especially if <code>state_dict()</code> was also overridden.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict[str, Any]</code> <p>A dictionary containing the saved internal state of a meter.</p> required Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Restore the internal meter state from a dictionary.\n\n    By default, this updates `self.__dict__` with the provided `state_dict`.\n    Subclasses may override this method for custom deserialization,\n    especially if `state_dict()` was also overridden.\n\n    Args:\n        state_dict: A dictionary containing the saved internal state of a meter.\n    \"\"\"\n    self.__dict__.update(state_dict)\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.BaseMeter.log","title":"<code>log(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Accumulate new raw data points into the meter's internal state.</p> <p>This method is called for each data point or batch that needs to be processed by the meter. The specific arguments in <code>**kwargs</code> depend on the concrete meter implementation.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments representing the data to be logged.</p> <code>{}</code> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>@abstractmethod\ndef log(self, **kwargs):\n    \"\"\"Accumulate new raw data points into the meter's internal state.\n\n    This method is called for each data point or batch that needs to be\n    processed by the meter. The specific arguments in `**kwargs` depend\n    on the concrete meter implementation.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments representing the data to be logged.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.BaseMeter.merge","title":"<code>merge(other_state)</code>  <code>abstractmethod</code>","text":"<p>Merge state from another instance of the same meter type.</p> <p>This method is critical for distributed training, allowing the states of meters from different processes/ranks to be combined into a single, globally consistent state. The <code>other_state</code> should be a dictionary representing the internal state of another meter.</p> <p>Parameters:</p> Name Type Description Default <code>other_state</code> <code>dict[str, Any]</code> <p>A dictionary containing the internal state of another          <code>BaseMeter</code> instance, typically obtained via <code>state_dict()</code>.</p> required Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>@abstractmethod\ndef merge(self, other_state: dict[str, Any]):\n    \"\"\"Merge state from another instance of the same meter type.\n\n    This method is critical for distributed training, allowing the states\n    of meters from different processes/ranks to be combined into a single,\n    globally consistent state. The `other_state` should be a dictionary\n    representing the internal state of another meter.\n\n    Args:\n        other_state: A dictionary containing the internal state of another\n                     `BaseMeter` instance, typically obtained via `state_dict()`.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.BaseMeter.state_dict","title":"<code>state_dict()</code>","text":"<p>Return the internal meter state as a dictionary for checkpointing.</p> <p>By default, this returns a shallow copy of <code>self.__dict__</code>. Subclasses may override this method if they need custom serialization logic (e.g., to handle non-serializable attributes or specific data structures).</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary representing the internal, serializable state of the meter.</p> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return the internal meter state as a dictionary for checkpointing.\n\n    By default, this returns a shallow copy of `self.__dict__`. Subclasses\n    may override this method if they need custom serialization logic (e.g.,\n    to handle non-serializable attributes or specific data structures).\n\n    Returns:\n        A dictionary representing the internal, serializable state of the meter.\n    \"\"\"\n    return self.__dict__\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.MeterEntry","title":"<code>MeterEntry</code>  <code>dataclass</code>","text":"<p>Container for a meter and its logging metadata.</p> <p>This dataclass holds an instance of a <code>BaseMeter</code> along with metadata that controls its behavior within a <code>MeterGroup</code> and during logging.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>meter</code> <code>BaseMeter</code> required <code>reset</code> <code>bool</code> <code>False</code> <code>priority</code> <code>int</code> <code>0</code> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>@dataclass\nclass MeterEntry:\n    \"\"\"Container for a meter and its logging metadata.\n\n    This dataclass holds an instance of a `BaseMeter` along with metadata\n    that controls its behavior within a `MeterGroup` and during logging.\n\n    Attributes:\n        meter: The actual `BaseMeter` instance responsible for accumulating data.\n        reset: If True, this meter will be reset (removed from the group)\n               after each logging step in its `MeterGroup`. This is typically\n               used for per-step or per-iteration meters. If False, the meter\n               accumulates its state across multiple steps/iterations.\n        priority: An integer representing the logging priority. Meters with\n                  lower priority values will appear earlier in the logs.\n    \"\"\"\n\n    meter: BaseMeter\n    reset: bool = False\n    priority: int = 0\n\n    def state_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return the current state of this `MeterEntry` for checkpointing.\n\n        This method serializes the internal state of the `meter` itself,\n        along with the `reset` and `priority` flags, and the class name\n        of the meter, to allow for reconstruction.\n\n        Returns:\n            A dictionary containing the serializable state of the `MeterEntry`.\n        \"\"\"\n        return {\n            \"meter\": self.meter.state_dict(),\n            \"reset\": self.reset,\n            \"priority\": self.priority,\n            \"meter_class\": self.meter.__class__.__name__,\n        }\n\n    def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n        \"\"\"Restore the `MeterEntry`'s state from a checkpoint.\n\n        This method reconstructs the `BaseMeter` instance and restores its\n        internal state from the provided `state_dict`. It also updates\n        the `reset` and `priority` flags.\n\n        Args:\n            state_dict: A dictionary containing the saved state of a `MeterEntry`.\n                        Supports legacy checkpoints by looking for \"metric_class\"\n                        and \"metric\" keys if \"meter_class\" and \"meter\" are not found.\n        \"\"\"\n        import optimus_dl.modules.metrics as metrics\n\n        # Fallback for legacy checkpoints where meters were named 'metrics'\n        class_name = state_dict.get(\"meter_class\") or state_dict.get(\"metric_class\")\n        if not class_name:\n            logger.warning(\n                \"Could not find 'meter_class' or 'metric_class' in MeterEntry state_dict. \"\n                \"Attempting to infer from available classes, this may lead to errors.\"\n            )\n            # Attempt to infer if class_name is missing for old checkpoints\n            # This might require more sophisticated logic if class names have changed drastically\n            raise NotImplementedError(\n                \"Dynamic class name inference not yet implemented for MeterEntry load_state_dict without 'class_name'.\"\n            )\n\n        meter_state = state_dict.get(\"meter\") or state_dict.get(\"metric\")\n        if not meter_state:\n            logger.warning(\n                \"Could not find 'meter' or 'metric' state in MeterEntry state_dict. \"\n                \"Attempting to load with empty state, this may lead to errors.\"\n            )\n            meter_state = (\n                {}\n            )  # Allow to proceed with empty state, constructor should handle it\n\n        # Use getattr to fetch the meter class, then reconstruct it\n        meter_cls = getattr(metrics, class_name)\n        legacy_class_name_map = {\n            \"AverageMetric\": \"AverageMeter\",\n            \"SummedMetric\": \"SummedMeter\",\n            \"FrequencyMetric\": \"FrequencyMeter\",\n            \"BaseMetric\": \"BaseMeter\",\n        }\n        mapped_class_name = legacy_class_name_map.get(class_name, class_name)\n        if mapped_class_name != class_name:\n            logger.info(\n                \"Mapping legacy metric class '%s' to meter class '%s' when loading MeterEntry.\",\n                class_name,\n                mapped_class_name,\n            )\n        # Use getattr to fetch the meter class, then reconstruct it\n        meter_cls = getattr(metrics, mapped_class_name)\n        self.meter = meter_cls.from_state_dict(meter_state)\n        self.reset = state_dict[\"reset\"]\n        self.priority = state_dict[\"priority\"]\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.MeterEntry.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Restore the <code>MeterEntry</code>'s state from a checkpoint.</p> <p>This method reconstructs the <code>BaseMeter</code> instance and restores its internal state from the provided <code>state_dict</code>. It also updates the <code>reset</code> and <code>priority</code> flags.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict[str, Any]</code> <p>A dictionary containing the saved state of a <code>MeterEntry</code>.         Supports legacy checkpoints by looking for \"metric_class\"         and \"metric\" keys if \"meter_class\" and \"meter\" are not found.</p> required Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Restore the `MeterEntry`'s state from a checkpoint.\n\n    This method reconstructs the `BaseMeter` instance and restores its\n    internal state from the provided `state_dict`. It also updates\n    the `reset` and `priority` flags.\n\n    Args:\n        state_dict: A dictionary containing the saved state of a `MeterEntry`.\n                    Supports legacy checkpoints by looking for \"metric_class\"\n                    and \"metric\" keys if \"meter_class\" and \"meter\" are not found.\n    \"\"\"\n    import optimus_dl.modules.metrics as metrics\n\n    # Fallback for legacy checkpoints where meters were named 'metrics'\n    class_name = state_dict.get(\"meter_class\") or state_dict.get(\"metric_class\")\n    if not class_name:\n        logger.warning(\n            \"Could not find 'meter_class' or 'metric_class' in MeterEntry state_dict. \"\n            \"Attempting to infer from available classes, this may lead to errors.\"\n        )\n        # Attempt to infer if class_name is missing for old checkpoints\n        # This might require more sophisticated logic if class names have changed drastically\n        raise NotImplementedError(\n            \"Dynamic class name inference not yet implemented for MeterEntry load_state_dict without 'class_name'.\"\n        )\n\n    meter_state = state_dict.get(\"meter\") or state_dict.get(\"metric\")\n    if not meter_state:\n        logger.warning(\n            \"Could not find 'meter' or 'metric' state in MeterEntry state_dict. \"\n            \"Attempting to load with empty state, this may lead to errors.\"\n        )\n        meter_state = (\n            {}\n        )  # Allow to proceed with empty state, constructor should handle it\n\n    # Use getattr to fetch the meter class, then reconstruct it\n    meter_cls = getattr(metrics, class_name)\n    legacy_class_name_map = {\n        \"AverageMetric\": \"AverageMeter\",\n        \"SummedMetric\": \"SummedMeter\",\n        \"FrequencyMetric\": \"FrequencyMeter\",\n        \"BaseMetric\": \"BaseMeter\",\n    }\n    mapped_class_name = legacy_class_name_map.get(class_name, class_name)\n    if mapped_class_name != class_name:\n        logger.info(\n            \"Mapping legacy metric class '%s' to meter class '%s' when loading MeterEntry.\",\n            class_name,\n            mapped_class_name,\n        )\n    # Use getattr to fetch the meter class, then reconstruct it\n    meter_cls = getattr(metrics, mapped_class_name)\n    self.meter = meter_cls.from_state_dict(meter_state)\n    self.reset = state_dict[\"reset\"]\n    self.priority = state_dict[\"priority\"]\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.MeterEntry.state_dict","title":"<code>state_dict()</code>","text":"<p>Return the current state of this <code>MeterEntry</code> for checkpointing.</p> <p>This method serializes the internal state of the <code>meter</code> itself, along with the <code>reset</code> and <code>priority</code> flags, and the class name of the meter, to allow for reconstruction.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the serializable state of the <code>MeterEntry</code>.</p> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return the current state of this `MeterEntry` for checkpointing.\n\n    This method serializes the internal state of the `meter` itself,\n    along with the `reset` and `priority` flags, and the class name\n    of the meter, to allow for reconstruction.\n\n    Returns:\n        A dictionary containing the serializable state of the `MeterEntry`.\n    \"\"\"\n    return {\n        \"meter\": self.meter.state_dict(),\n        \"reset\": self.reset,\n        \"priority\": self.priority,\n        \"meter_class\": self.meter.__class__.__name__,\n    }\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.MeterGroup","title":"<code>MeterGroup</code>","text":"<p>A named collection of meters that are logged together.</p> <p>This class manages a group of related meters (e.g., 'train' or 'eval'). It handles:</p> <ul> <li>Sampling Frequency: Only triggers logging every <code>log_freq</code> steps.</li> <li>Priority Sorting: Ensures consistent ordering of meters in output.</li> <li>State Management: Can reset meters after logging and serialize the   entire group state for checkpointing.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique name for the group.</p> required <code>log_freq</code> <code>int | None</code> <p>Frequency (in iterations) at which to trigger logging.       If None, defaults to 1 (log every iteration).</p> <code>None</code> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>class MeterGroup:\n    \"\"\"A named collection of meters that are logged together.\n\n    This class manages a group of related meters (e.g., 'train' or 'eval'). It\n    handles:\n\n    - **Sampling Frequency**: Only triggers logging every `log_freq` steps.\n    - **Priority Sorting**: Ensures consistent ordering of meters in output.\n    - **State Management**: Can reset meters after logging and serialize the\n      entire group state for checkpointing.\n\n    Args:\n        name: Unique name for the group.\n        log_freq: Frequency (in iterations) at which to trigger logging.\n                  If None, defaults to 1 (log every iteration).\n    \"\"\"\n\n    def __init__(self, name: str, log_freq: int | None = None):\n        self.name = name\n        self.log_freq = log_freq or 1\n        self._meters: OrderedDict[str, MeterEntry] = OrderedDict()\n        self._keys_sorted: list[str] = []\n        self._iteration_counter: int = 0\n\n    def compute(self) -&gt; dict[str, float | int | dict[str, float | int]]:\n        \"\"\"Compute the current values for all meters in the group.\n\n        Iterates through all meters currently in the group (sorted by priority)\n        and calls their `compute()` method to get their current value.\n\n        Returns:\n            An `OrderedDict` mapping meter names to their computed values.\n            The values can be floats, integers, or nested dictionaries\n            (for meters emitting multiple sub-values).\n        \"\"\"\n        return OrderedDict(\n            (name, self._meters[name].meter.compute()) for name in self._keys_sorted\n        )\n\n    @property\n    def meters(self) -&gt; OrderedDict[str, MeterEntry]:\n        \"\"\"Returns the internal `OrderedDict` of `MeterEntry` objects.\n\n        Note: The meters are returned in their natural insertion order,\n              not sorted by priority. Use `_keys_sorted` for ordered iteration.\n        \"\"\"\n        return self._meters\n\n    def step(self) -&gt; bool:\n        \"\"\"Increment the internal iteration counter for the group.\n\n        This method should be called once per relevant step (e.g., per batch)\n        to track progress and determine when logging should occur based on `log_freq`.\n\n        Returns:\n            True if the current step is a logging step (i.e., `_iteration_counter`\n            is a multiple of `log_freq`), False otherwise.\n        \"\"\"\n        self._iteration_counter += 1\n        return (self._iteration_counter % self.log_freq) == 0\n\n    def should_log(self) -&gt; bool:\n        \"\"\"Check if the current iteration should trigger logging.\n\n        This is a passive check that does not increment the iteration counter.\n\n        Returns:\n            True if logging should occur at the current iteration, False otherwise.\n        \"\"\"\n        return (self._iteration_counter % self.log_freq) == 0\n\n    def add_meter(self, name: str, meter_entry: MeterEntry):\n        \"\"\"Add a new meter entry to the group.\n\n        If a meter with the same `name` already exists, it will be overwritten.\n        After adding, the sorted list of keys is updated to reflect any priority changes.\n\n        Args:\n            name: The unique identifier for the meter within this group.\n            meter_entry: The `MeterEntry` object containing the `BaseMeter` instance\n                         and its metadata.\n        \"\"\"\n        self._meters[name] = meter_entry\n        self._update_keys_sorted()\n\n    def _update_keys_sorted(self):\n        \"\"\"Update the sorted list of meter keys based on priorities.\n\n        This internal helper method re-sorts `self._keys_sorted` whenever a meter\n        is added or removed, ensuring that `compute()` and other operations\n        process meters in the correct priority order.\n        \"\"\"\n        self._keys_sorted = sorted(\n            self._meters.keys(),\n            key=lambda k: self._meters[k].priority,\n        )\n\n    def get_meter(self, name: str) -&gt; MeterEntry | None:\n        \"\"\"Retrieve a specific `MeterEntry` by its name.\n\n        Args:\n            name: The name of the meter to retrieve.\n\n        Returns:\n            The `MeterEntry` if found, otherwise None.\n        \"\"\"\n        return self._meters.get(name)\n\n    def reset(self):\n        \"\"\"Reset all meters marked for reset after logging.\n\n        This method iterates through all `MeterEntry` objects in the group.\n        If an entry's `reset` flag is True, the corresponding meter is removed\n        from the group. This is typically called after a logging event to\n        prepare for the next accumulation cycle for per-step meters.\n        \"\"\"\n        # Create a copy of keys to iterate over as we might modify _meters\n        for key in list(self._meters.keys()):\n            entry = self._meters[key]\n            if entry.reset:\n                self._meters.pop(key)\n        self._update_keys_sorted()\n\n    def state_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return the entire `MeterGroup` state for checkpointing.\n\n        Serializes the group's name, logging frequency, and the state of all\n        contained `MeterEntry` objects.\n\n        Returns:\n            A dictionary containing the serializable state of the `MeterGroup`.\n        \"\"\"\n        return {\n            \"name\": self.name,\n            \"log_freq\": self.log_freq,\n            \"meters\": {\n                name: entry.state_dict() for name, entry in self._meters.items()\n            },\n            \"iteration_counter\": self._iteration_counter,\n        }\n\n    def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n        \"\"\"Restore the `MeterGroup` state from a checkpoint.\n\n        Reconstructs the group's internal state, including all its meters\n        and their individual states, from the provided `state_dict`.\n\n        Args:\n            state_dict: A dictionary containing the saved state of a `MeterGroup`.\n                        Supports legacy checkpoints by looking for a \"metrics\"\n                        key if \"meters\" is not found for the collection of meters.\n\n        Raises:\n            AssertionError: If the name in the `state_dict` does not match the\n                            current group's name, indicating a mismatch.\n        \"\"\"\n        assert (\n            self.name == state_dict[\"name\"]\n        ), f\"Name mismatch: expected {self.name}, got {state_dict['name']}\"\n        self.log_freq = state_dict[\"log_freq\"]\n        self._iteration_counter = state_dict.get(\n            \"iteration_counter\", 0\n        )  # Backward compatibility\n        self._meters = OrderedDict()\n\n        # Backward compatibility for 'metrics' key in state_dict\n        meters_data = state_dict.get(\"meters\") or state_dict.get(\"metrics\", {})\n        if not meters_data:\n            logger.warning(\n                f\"No 'meters' or 'metrics' found in state_dict for MeterGroup '{self.name}'. Initializing with empty meters.\"\n            )\n\n        for name, entry_state in meters_data.items():\n            entry = MeterEntry(meter=None)  # type: ignore # Meter will be set by load_state_dict\n            try:\n                entry.load_state_dict(entry_state)\n                self._meters[name] = entry\n            except Exception as e:\n                logger.error(\n                    f\"Failed to load MeterEntry '{name}' for MeterGroup '{self.name}': {e}\"\n                )\n                # Decide how to handle this error: skip, raise, or re-initialize\n                # For now, we will skip the problematic meter but log the error.\n        self._update_keys_sorted()\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.MeterGroup.meters","title":"<code>meters</code>  <code>property</code>","text":"<p>Returns the internal <code>OrderedDict</code> of <code>MeterEntry</code> objects.</p> The meters are returned in their natural insertion order, <p>not sorted by priority. Use <code>_keys_sorted</code> for ordered iteration.</p>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.MeterGroup.add_meter","title":"<code>add_meter(name, meter_entry)</code>","text":"<p>Add a new meter entry to the group.</p> <p>If a meter with the same <code>name</code> already exists, it will be overwritten. After adding, the sorted list of keys is updated to reflect any priority changes.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique identifier for the meter within this group.</p> required <code>meter_entry</code> <code>MeterEntry</code> <p>The <code>MeterEntry</code> object containing the <code>BaseMeter</code> instance          and its metadata.</p> required Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def add_meter(self, name: str, meter_entry: MeterEntry):\n    \"\"\"Add a new meter entry to the group.\n\n    If a meter with the same `name` already exists, it will be overwritten.\n    After adding, the sorted list of keys is updated to reflect any priority changes.\n\n    Args:\n        name: The unique identifier for the meter within this group.\n        meter_entry: The `MeterEntry` object containing the `BaseMeter` instance\n                     and its metadata.\n    \"\"\"\n    self._meters[name] = meter_entry\n    self._update_keys_sorted()\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.MeterGroup.compute","title":"<code>compute()</code>","text":"<p>Compute the current values for all meters in the group.</p> <p>Iterates through all meters currently in the group (sorted by priority) and calls their <code>compute()</code> method to get their current value.</p> <p>Returns:</p> Type Description <code>dict[str, float | int | dict[str, float | int]]</code> <p>An <code>OrderedDict</code> mapping meter names to their computed values.</p> <code>dict[str, float | int | dict[str, float | int]]</code> <p>The values can be floats, integers, or nested dictionaries</p> <code>dict[str, float | int | dict[str, float | int]]</code> <p>(for meters emitting multiple sub-values).</p> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def compute(self) -&gt; dict[str, float | int | dict[str, float | int]]:\n    \"\"\"Compute the current values for all meters in the group.\n\n    Iterates through all meters currently in the group (sorted by priority)\n    and calls their `compute()` method to get their current value.\n\n    Returns:\n        An `OrderedDict` mapping meter names to their computed values.\n        The values can be floats, integers, or nested dictionaries\n        (for meters emitting multiple sub-values).\n    \"\"\"\n    return OrderedDict(\n        (name, self._meters[name].meter.compute()) for name in self._keys_sorted\n    )\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.MeterGroup.get_meter","title":"<code>get_meter(name)</code>","text":"<p>Retrieve a specific <code>MeterEntry</code> by its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the meter to retrieve.</p> required <p>Returns:</p> Type Description <code>MeterEntry | None</code> <p>The <code>MeterEntry</code> if found, otherwise None.</p> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def get_meter(self, name: str) -&gt; MeterEntry | None:\n    \"\"\"Retrieve a specific `MeterEntry` by its name.\n\n    Args:\n        name: The name of the meter to retrieve.\n\n    Returns:\n        The `MeterEntry` if found, otherwise None.\n    \"\"\"\n    return self._meters.get(name)\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.MeterGroup.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Restore the <code>MeterGroup</code> state from a checkpoint.</p> <p>Reconstructs the group's internal state, including all its meters and their individual states, from the provided <code>state_dict</code>.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict[str, Any]</code> <p>A dictionary containing the saved state of a <code>MeterGroup</code>.         Supports legacy checkpoints by looking for a \"metrics\"         key if \"meters\" is not found for the collection of meters.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the name in the <code>state_dict</code> does not match the             current group's name, indicating a mismatch.</p> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Restore the `MeterGroup` state from a checkpoint.\n\n    Reconstructs the group's internal state, including all its meters\n    and their individual states, from the provided `state_dict`.\n\n    Args:\n        state_dict: A dictionary containing the saved state of a `MeterGroup`.\n                    Supports legacy checkpoints by looking for a \"metrics\"\n                    key if \"meters\" is not found for the collection of meters.\n\n    Raises:\n        AssertionError: If the name in the `state_dict` does not match the\n                        current group's name, indicating a mismatch.\n    \"\"\"\n    assert (\n        self.name == state_dict[\"name\"]\n    ), f\"Name mismatch: expected {self.name}, got {state_dict['name']}\"\n    self.log_freq = state_dict[\"log_freq\"]\n    self._iteration_counter = state_dict.get(\n        \"iteration_counter\", 0\n    )  # Backward compatibility\n    self._meters = OrderedDict()\n\n    # Backward compatibility for 'metrics' key in state_dict\n    meters_data = state_dict.get(\"meters\") or state_dict.get(\"metrics\", {})\n    if not meters_data:\n        logger.warning(\n            f\"No 'meters' or 'metrics' found in state_dict for MeterGroup '{self.name}'. Initializing with empty meters.\"\n        )\n\n    for name, entry_state in meters_data.items():\n        entry = MeterEntry(meter=None)  # type: ignore # Meter will be set by load_state_dict\n        try:\n            entry.load_state_dict(entry_state)\n            self._meters[name] = entry\n        except Exception as e:\n            logger.error(\n                f\"Failed to load MeterEntry '{name}' for MeterGroup '{self.name}': {e}\"\n            )\n            # Decide how to handle this error: skip, raise, or re-initialize\n            # For now, we will skip the problematic meter but log the error.\n    self._update_keys_sorted()\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.MeterGroup.reset","title":"<code>reset()</code>","text":"<p>Reset all meters marked for reset after logging.</p> <p>This method iterates through all <code>MeterEntry</code> objects in the group. If an entry's <code>reset</code> flag is True, the corresponding meter is removed from the group. This is typically called after a logging event to prepare for the next accumulation cycle for per-step meters.</p> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def reset(self):\n    \"\"\"Reset all meters marked for reset after logging.\n\n    This method iterates through all `MeterEntry` objects in the group.\n    If an entry's `reset` flag is True, the corresponding meter is removed\n    from the group. This is typically called after a logging event to\n    prepare for the next accumulation cycle for per-step meters.\n    \"\"\"\n    # Create a copy of keys to iterate over as we might modify _meters\n    for key in list(self._meters.keys()):\n        entry = self._meters[key]\n        if entry.reset:\n            self._meters.pop(key)\n    self._update_keys_sorted()\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.MeterGroup.should_log","title":"<code>should_log()</code>","text":"<p>Check if the current iteration should trigger logging.</p> <p>This is a passive check that does not increment the iteration counter.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if logging should occur at the current iteration, False otherwise.</p> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def should_log(self) -&gt; bool:\n    \"\"\"Check if the current iteration should trigger logging.\n\n    This is a passive check that does not increment the iteration counter.\n\n    Returns:\n        True if logging should occur at the current iteration, False otherwise.\n    \"\"\"\n    return (self._iteration_counter % self.log_freq) == 0\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.MeterGroup.state_dict","title":"<code>state_dict()</code>","text":"<p>Return the entire <code>MeterGroup</code> state for checkpointing.</p> <p>Serializes the group's name, logging frequency, and the state of all contained <code>MeterEntry</code> objects.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the serializable state of the <code>MeterGroup</code>.</p> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return the entire `MeterGroup` state for checkpointing.\n\n    Serializes the group's name, logging frequency, and the state of all\n    contained `MeterEntry` objects.\n\n    Returns:\n        A dictionary containing the serializable state of the `MeterGroup`.\n    \"\"\"\n    return {\n        \"name\": self.name,\n        \"log_freq\": self.log_freq,\n        \"meters\": {\n            name: entry.state_dict() for name, entry in self._meters.items()\n        },\n        \"iteration_counter\": self._iteration_counter,\n    }\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.MeterGroup.step","title":"<code>step()</code>","text":"<p>Increment the internal iteration counter for the group.</p> <p>This method should be called once per relevant step (e.g., per batch) to track progress and determine when logging should occur based on <code>log_freq</code>.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the current step is a logging step (i.e., <code>_iteration_counter</code></p> <code>bool</code> <p>is a multiple of <code>log_freq</code>), False otherwise.</p> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def step(self) -&gt; bool:\n    \"\"\"Increment the internal iteration counter for the group.\n\n    This method should be called once per relevant step (e.g., per batch)\n    to track progress and determine when logging should occur based on `log_freq`.\n\n    Returns:\n        True if the current step is a logging step (i.e., `_iteration_counter`\n        is a multiple of `log_freq`), False otherwise.\n    \"\"\"\n    self._iteration_counter += 1\n    return (self._iteration_counter % self.log_freq) == 0\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.compute_meters","title":"<code>compute_meters(group_name, aggregate=False, collective=None)</code>","text":"<p>Compute final values for a named <code>MeterGroup</code>, with optional distributed aggregation.</p> <p>This function retrieves the specified <code>MeterGroup</code>, computes the current value for each of its meters, and optionally aggregates these values across distributed ranks.</p> <p>If <code>aggregate</code> is True, it performs an all-gather of meter states across all distributed ranks and merges them before computing final values. This ensures that metrics reflect a global view of the data.</p> <p>Parameters:</p> Name Type Description Default <code>group_name</code> <code>str</code> <p>Name of the <code>MeterGroup</code> to compute.</p> required <code>aggregate</code> <code>bool</code> <p>If True, meter states are aggregated from all ranks        using the provided <code>collective</code>. If False, only local        meter values are returned.</p> <code>False</code> <code>collective</code> <code>Collective | None</code> <p>A <code>Collective</code> instance (from <code>optimus_dl.modules.distributed</code>)         required for distributed aggregation if <code>aggregate</code> is True.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, float | int | dict[str, float | int]]</code> <p>A dictionary mapping meter names (or metric names, as exposed) to</p> <code>dict[str, float | int | dict[str, float | int]]</code> <p>their computed values. These values can be floats, integers, or nested</p> <code>dict[str, float | int | dict[str, float | int]]</code> <p>dictionaries. Returns an empty dictionary if the group name is not found.</p> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def compute_meters(\n    group_name: str, aggregate: bool = False, collective: Collective | None = None\n) -&gt; dict[str, float | int | dict[str, float | int]]:\n    \"\"\"Compute final values for a named `MeterGroup`, with optional distributed aggregation.\n\n    This function retrieves the specified `MeterGroup`, computes the current\n    value for each of its meters, and optionally aggregates these values\n    across distributed ranks.\n\n    If `aggregate` is True, it performs an all-gather of meter states across\n    all distributed ranks and merges them before computing final values. This\n    ensures that metrics reflect a global view of the data.\n\n    Args:\n        group_name: Name of the `MeterGroup` to compute.\n        aggregate: If True, meter states are aggregated from all ranks\n                   using the provided `collective`. If False, only local\n                   meter values are returned.\n        collective: A `Collective` instance (from `optimus_dl.modules.distributed`)\n                    required for distributed aggregation if `aggregate` is True.\n\n    Returns:\n        A dictionary mapping meter names (or metric names, as exposed) to\n        their computed values. These values can be floats, integers, or nested\n        dictionaries. Returns an empty dictionary if the group name is not found.\n    \"\"\"\n    if group_name not in _meter_groups:\n        logger.debug(f\"MeterGroup '{group_name}' not found for computing metrics.\")\n        return {}\n\n    group = _meter_groups[group_name]\n    local_metrics = group.compute()  # These are actually meter outputs\n\n    if not aggregate or collective is None:\n        return local_metrics\n\n    # Collect local meter states to send for aggregation\n    local_meter_states = {\n        name: entry.meter.state_dict()\n        for name, entry in group.meters.items()\n        if name in local_metrics  # Only consider meters that produced a local metric\n    }\n\n    # Gather all meter states from all ranks in one communication\n    all_rank_states: list[dict[str, dict[str, Any]]] = collective.all_gather_objects(\n        local_meter_states\n    )\n\n    # Aggregate meters across ranks using their merge functionality\n    aggregated_metrics: dict[str, float | int | dict[str, float | int]] = {}\n\n    for name in local_metrics.keys():  # Iterate over keys that were computed locally\n        if name not in group.meters:\n            continue\n\n        entry = group.meters[name]\n        # Create a fresh instance of the meter's class for aggregation\n        aggregated_meter = entry.meter.__class__()\n\n        # Merge states from all ranks\n        for rank_states in all_rank_states:\n            if name in rank_states:\n                try:\n                    aggregated_meter.merge(rank_states[name])\n                except Exception as e:\n                    logger.error(\n                        f\"Error merging state for meter '{name}' from rank states: {e}\"\n                    )\n                    # Depending on error, could skip this rank's state or handle differently\n                    continue\n\n        # Compute final aggregated value\n        try:\n            aggregated_metrics[name] = aggregated_meter.compute()\n        except Exception as e:\n            logger.error(\n                f\"Error computing aggregated metric for '{name}': {e}. Falling back to local value.\"\n            )\n            # Fall back to local value if aggregation computation fails\n            aggregated_metrics[name] = local_metrics[name]\n\n    return aggregated_metrics\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Restore the state for all managed <code>MeterGroup</code>s from a state dictionary.</p> <p>This function iterates through the provided <code>state_dict</code>, recreating <code>MeterGroup</code>s as needed and loading their saved states.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict[str, Any]</code> <p>A dictionary containing the saved state of all <code>MeterGroup</code>s,         typically obtained from a previous call to <code>state_dict()</code>.</p> required Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def load_state_dict(state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Restore the state for all managed `MeterGroup`s from a state dictionary.\n\n    This function iterates through the provided `state_dict`, recreating\n    `MeterGroup`s as needed and loading their saved states.\n\n    Args:\n        state_dict: A dictionary containing the saved state of all `MeterGroup`s,\n                    typically obtained from a previous call to `state_dict()`.\n    \"\"\"\n    for group_name, group_data in state_dict.items():\n        if group_name not in _meter_groups:\n            # Recreate MeterGroup if it doesn't exist, using saved log_freq\n            log_freq = group_data.get(\n                \"log_freq\", 1\n            )  # Default to 1 if not in state_dict\n            _meter_groups[group_name] = MeterGroup(name=group_name, log_freq=log_freq)\n        try:\n            _meter_groups[group_name].load_state_dict(group_data)\n        except Exception as e:\n            logger.error(\n                f\"Failed to load state_dict for MeterGroup '{group_name}': {e}\"\n            )\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.log_meter","title":"<code>log_meter(name, meter_factory, reset=True, priority=100, force_log=False, **kwargs)</code>","text":"<p>Log data point(s) to all currently active meter groups.</p> <p>This is the primary function for adding data to meters within active <code>MeterGroup</code>s. It ensures that data is only logged when the group's <code>should_log()</code> condition is met (unless <code>force_log</code> is True) and handles the creation of meters if they don't already exist in a group.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name under which this metric's data will be stored and reported.   This acts as the key for the <code>MeterEntry</code> within the <code>MeterGroup</code>.</p> required <code>meter_factory</code> <code>Callable[[], BaseMeter]</code> <p>A callable (e.g., a lambda function) that, when called            with no arguments, returns a new instance of a <code>BaseMeter</code>            subclass. This factory is used only if a meter with the            given <code>name</code> does not already exist in the group.</p> required <code>reset</code> <code>bool</code> <p>If True, the meter created or used for this log will be removed    from its <code>MeterGroup</code> after the group's <code>reset()</code> method is called.    Defaults to True, suitable for per-iteration metrics.</p> <code>True</code> <code>priority</code> <code>int</code> <p>An integer determining the order of this meter in logs.       Lower numbers mean higher priority (appear earlier). Defaults to 100.</p> <code>100</code> <code>force_log</code> <code>bool</code> <p>If True, the metric will be logged even if the current        <code>MeterGroup</code>'s <code>should_log()</code> method returns False. This is        useful for critical events or debugging that need to be logged        regardless of frequency settings. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments that will be passed directly       to the <code>log()</code> method of the <code>BaseMeter</code> instance. These       typically represent the actual data points (e.g., <code>value</code>, <code>weight</code>).</p> <code>{}</code> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def log_meter(\n    name: str,\n    meter_factory: Callable[[], BaseMeter],\n    reset: bool = True,\n    priority: int = 100,\n    force_log: bool = False,\n    **kwargs: Any,\n):\n    \"\"\"Log data point(s) to all currently active meter groups.\n\n    This is the primary function for adding data to meters within active\n    `MeterGroup`s. It ensures that data is only logged when the group's\n    `should_log()` condition is met (unless `force_log` is True) and\n    handles the creation of meters if they don't already exist in a group.\n\n    Args:\n        name: The name under which this metric's data will be stored and reported.\n              This acts as the key for the `MeterEntry` within the `MeterGroup`.\n        meter_factory: A callable (e.g., a lambda function) that, when called\n                       with no arguments, returns a new instance of a `BaseMeter`\n                       subclass. This factory is used only if a meter with the\n                       given `name` does not already exist in the group.\n        reset: If True, the meter created or used for this log will be removed\n               from its `MeterGroup` after the group's `reset()` method is called.\n               Defaults to True, suitable for per-iteration metrics.\n        priority: An integer determining the order of this meter in logs.\n                  Lower numbers mean higher priority (appear earlier). Defaults to 100.\n        force_log: If True, the metric will be logged even if the current\n                   `MeterGroup`'s `should_log()` method returns False. This is\n                   useful for critical events or debugging that need to be logged\n                   regardless of frequency settings. Defaults to False.\n        **kwargs: Arbitrary keyword arguments that will be passed directly\n                  to the `log()` method of the `BaseMeter` instance. These\n                  typically represent the actual data points (e.g., `value`, `weight`).\n    \"\"\"\n    for group_name in _active_meter_groups:\n        group = _meter_groups[group_name]\n\n        # Only evaluate expensive callables if we should log or are forcing a log\n        if group.should_log() or force_log:\n            # Evaluate any callable values in kwargs lazily, only if logging is active\n            evaluated_kwargs = {k: _evaluate_value(v) for k, v in kwargs.items()}\n\n            if name not in group.meters:\n                # If meter doesn't exist, create it using the factory and add to group\n                group.add_meter(\n                    name,\n                    MeterEntry(\n                        meter=meter_factory(),\n                        reset=reset,\n                        priority=priority,\n                    ),\n                )\n            # Log the evaluated data to the meter\n            group.meters[name].meter.log(**evaluated_kwargs)\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.meters_group","title":"<code>meters_group(name, log_freq=None, force_recreate=False)</code>","text":"<p>Context manager for activating a metrics group.</p> <p>While inside this context, any calls to <code>log_meter</code> will be directed to the <code>MeterGroup</code> identified by <code>name</code>. This allows for grouping related meters (e.g., \"train\" or \"eval\" metrics).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the <code>MeterGroup</code> to activate.</p> required <code>log_freq</code> <code>int | None</code> <p>Optional logging frequency (in iterations) to set or update       for this group. If the group already exists and <code>log_freq</code>       is provided, its frequency will be updated.</p> <code>None</code> <code>force_recreate</code> <code>bool</code> <p>If True, any existing <code>MeterGroup</code> with the given <code>name</code>             will be removed and a new one created, effectively clearing             its state.</p> <code>False</code> <p>Yields:</p> Name Type Description <code>bool</code> <p>True if the group should trigger logging at this step, based on   its <code>log_freq</code> and internal iteration counter. False otherwise.</p> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>@contextlib.contextmanager\ndef meters_group(name: str, log_freq: int | None = None, force_recreate: bool = False):\n    \"\"\"Context manager for activating a metrics group.\n\n    While inside this context, any calls to `log_meter` will be directed to\n    the `MeterGroup` identified by `name`. This allows for grouping related\n    meters (e.g., \"train\" or \"eval\" metrics).\n\n    Args:\n        name: Name of the `MeterGroup` to activate.\n        log_freq: Optional logging frequency (in iterations) to set or update\n                  for this group. If the group already exists and `log_freq`\n                  is provided, its frequency will be updated.\n        force_recreate: If True, any existing `MeterGroup` with the given `name`\n                        will be removed and a new one created, effectively clearing\n                        its state.\n\n    Yields:\n        bool: True if the group should trigger logging at this step, based on\n              its `log_freq` and internal iteration counter. False otherwise.\n    \"\"\"\n    if force_recreate:\n        _meter_groups.pop(name, None)\n    _meter_groups.setdefault(name, MeterGroup(name, log_freq=log_freq))\n    if log_freq is not None:\n        _meter_groups[name].log_freq = log_freq\n    _active_meter_groups[name] += 1\n\n    # Return whether we should log at current iteration\n    should_log = _meter_groups[name].should_log()\n\n    try:\n        yield should_log\n    finally:\n        _active_meter_groups[name] -= 1\n        if _active_meter_groups[name] == 0:\n            _active_meter_groups.pop(name)\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.reset_meters","title":"<code>reset_meters(name)</code>","text":"<p>Reset all resettable meters within a named <code>MeterGroup</code>.</p> <p>This function triggers the <code>reset()</code> method on the specified <code>MeterGroup</code>, which in turn removes all <code>MeterEntry</code> objects that have their <code>reset</code> flag set to True.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the <code>MeterGroup</code> to reset.</p> required Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def reset_meters(name: str) -&gt; None:\n    \"\"\"Reset all resettable meters within a named `MeterGroup`.\n\n    This function triggers the `reset()` method on the specified `MeterGroup`,\n    which in turn removes all `MeterEntry` objects that have their `reset` flag\n    set to True.\n\n    Args:\n        name: The name of the `MeterGroup` to reset.\n    \"\"\"\n    if name in _meter_groups:\n        _meter_groups[name].reset()\n    else:\n        logger.debug(f\"Attempted to reset non-existent MeterGroup '{name}'.\")\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.state_dict","title":"<code>state_dict()</code>","text":"<p>Return the combined state dictionary for all managed <code>MeterGroup</code>s.</p> <p>This function collects the <code>state_dict()</code> from each active <code>MeterGroup</code>, allowing the entire metrics system state to be checkpointed.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary where keys are <code>MeterGroup</code> names and values are their</p> <code>dict[str, Any]</code> <p>respective state dictionaries.</p> Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def state_dict() -&gt; dict[str, Any]:\n    \"\"\"Return the combined state dictionary for all managed `MeterGroup`s.\n\n    This function collects the `state_dict()` from each active `MeterGroup`,\n    allowing the entire metrics system state to be checkpointed.\n\n    Returns:\n        A dictionary where keys are `MeterGroup` names and values are their\n        respective state dictionaries.\n    \"\"\"\n    return {\n        group_name: group.state_dict() for group_name, group in _meter_groups.items()\n    }\n</code></pre>"},{"location":"reference/modules/metrics/base/#optimus_dl.modules.metrics.base.step_meters","title":"<code>step_meters(name)</code>","text":"<p>Explicitly step the iteration counter for a named <code>MeterGroup</code>.</p> <p>This function allows external components to manually advance the iteration counter of a specific <code>MeterGroup</code>, which can influence when <code>should_log</code> returns True.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the <code>MeterGroup</code> to step.</p> required Source code in <code>optimus_dl/modules/metrics/base.py</code> <pre><code>def step_meters(name: str) -&gt; None:\n    \"\"\"Explicitly step the iteration counter for a named `MeterGroup`.\n\n    This function allows external components to manually advance the iteration\n    counter of a specific `MeterGroup`, which can influence when `should_log`\n    returns True.\n\n    Args:\n        name: The name of the `MeterGroup` to step.\n    \"\"\"\n    if name in _meter_groups:\n        _meter_groups[name].step()\n    else:\n        logger.debug(f\"Attempted to step non-existent MeterGroup '{name}'.\")\n</code></pre>"},{"location":"reference/modules/metrics/common/","title":"common","text":""},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common","title":"<code>optimus_dl.modules.metrics.common</code>","text":"<p>Common meter implementations and logging utilities.</p> <p>This module provides standard meter types (averages, sums, frequencies, etc.) and convenience functions for logging meter values during training. All meters support distributed aggregation and checkpointing.</p>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.AverageMeter","title":"<code>AverageMeter</code>","text":"<p>               Bases: <code>BaseMeter</code></p> <p>Meter that computes a weighted average of logged values.</p> <p>This meter accumulates weighted values and computes the average when <code>compute()</code> is called. Useful for values like loss, accuracy, etc. that should be averaged over batches.</p> <p>Attributes:</p> Name Type Description <code>round</code> <p>Number of decimal places to round the result to (None = no rounding).</p> <code>sum</code> <p>Accumulated sum of (value * weight).</p> <code>count</code> <p>Accumulated sum of weights.</p> Example <pre><code>meter = AverageMeter(round=4)\nmeter.log(value=0.5, weight=32)  # Batch size 32\nmeter.log(value=0.6, weight=32)\nmeter.compute()  # (0.5*32 + 0.6*32) / (32+32) = 0.55\n</code></pre> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>class AverageMeter(BaseMeter):\n    \"\"\"Meter that computes a weighted average of logged values.\n\n    This meter accumulates weighted values and computes the average when\n    `compute()` is called. Useful for values like loss, accuracy, etc.\n    that should be averaged over batches.\n\n    Attributes:\n        round: Number of decimal places to round the result to (None = no rounding).\n        sum: Accumulated sum of (value * weight).\n        count: Accumulated sum of weights.\n\n    Example:\n        ```python\n        meter = AverageMeter(round=4)\n        meter.log(value=0.5, weight=32)  # Batch size 32\n        meter.log(value=0.6, weight=32)\n        meter.compute()  # (0.5*32 + 0.6*32) / (32+32) = 0.55\n\n        ```\"\"\"\n\n    def __init__(self, round: int | None = None):\n        \"\"\"Initialize the average meter.\n\n        Args:\n            round: Number of decimal places to round results to. If None,\n                results are not rounded.\n        \"\"\"\n        self.round = round\n        self.sum = 0\n        self.count = 0\n\n    def compute(self) -&gt; float | int:\n        \"\"\"Compute the weighted average.\n\n        Returns:\n            Weighted average of all logged values, optionally rounded.\n        \"\"\"\n        if self.count == 0:\n            return 0\n        return safe_round(self.sum / self.count, self.round)\n\n    def log(self, value: float | int, weight: float | int) -&gt; None:\n        \"\"\"Log a value with an associated weight.\n\n        Args:\n            value: The value to add to the average.\n            weight: The weight for this value (typically batch size).\n        \"\"\"\n        self.sum += value * weight\n        self.count += weight\n\n    def merge(self, other_state: dict[str, Any]) -&gt; None:\n        \"\"\"Merge state from another meter instance (for distributed aggregation).\n\n        Args:\n            other_state: State dictionary from another AverageMeter instance.\n        \"\"\"\n        self.sum += other_state[\"sum\"]\n        self.count += other_state[\"count\"]\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.AverageMeter.__init__","title":"<code>__init__(round=None)</code>","text":"<p>Initialize the average meter.</p> <p>Parameters:</p> Name Type Description Default <code>round</code> <code>int | None</code> <p>Number of decimal places to round results to. If None, results are not rounded.</p> <code>None</code> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def __init__(self, round: int | None = None):\n    \"\"\"Initialize the average meter.\n\n    Args:\n        round: Number of decimal places to round results to. If None,\n            results are not rounded.\n    \"\"\"\n    self.round = round\n    self.sum = 0\n    self.count = 0\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.AverageMeter.compute","title":"<code>compute()</code>","text":"<p>Compute the weighted average.</p> <p>Returns:</p> Type Description <code>float | int</code> <p>Weighted average of all logged values, optionally rounded.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def compute(self) -&gt; float | int:\n    \"\"\"Compute the weighted average.\n\n    Returns:\n        Weighted average of all logged values, optionally rounded.\n    \"\"\"\n    if self.count == 0:\n        return 0\n    return safe_round(self.sum / self.count, self.round)\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.AverageMeter.log","title":"<code>log(value, weight)</code>","text":"<p>Log a value with an associated weight.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float | int</code> <p>The value to add to the average.</p> required <code>weight</code> <code>float | int</code> <p>The weight for this value (typically batch size).</p> required Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def log(self, value: float | int, weight: float | int) -&gt; None:\n    \"\"\"Log a value with an associated weight.\n\n    Args:\n        value: The value to add to the average.\n        weight: The weight for this value (typically batch size).\n    \"\"\"\n    self.sum += value * weight\n    self.count += weight\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.AverageMeter.merge","title":"<code>merge(other_state)</code>","text":"<p>Merge state from another meter instance (for distributed aggregation).</p> <p>Parameters:</p> Name Type Description Default <code>other_state</code> <code>dict[str, Any]</code> <p>State dictionary from another AverageMeter instance.</p> required Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def merge(self, other_state: dict[str, Any]) -&gt; None:\n    \"\"\"Merge state from another meter instance (for distributed aggregation).\n\n    Args:\n        other_state: State dictionary from another AverageMeter instance.\n    \"\"\"\n    self.sum += other_state[\"sum\"]\n    self.count += other_state[\"count\"]\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.AveragedExponentMeter","title":"<code>AveragedExponentMeter</code>","text":"<p>               Bases: <code>BaseMeter</code></p> <p>Meter that computes the exponent of a weighted average.</p> <p>Commonly used for computing perplexity (exp(loss)).</p> <p>Attributes:</p> Name Type Description <code>_internal</code> <p>An <code>AverageMeter</code> instance used to compute the weighted average.</p> <code>round</code> <p>Number of decimal places to round the final result to.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>class AveragedExponentMeter(BaseMeter):  # Ensures it inherits from BaseMeter\n    \"\"\"Meter that computes the exponent of a weighted average.\n\n    Commonly used for computing perplexity (exp(loss)).\n\n    Attributes:\n        _internal: An `AverageMeter` instance used to compute the weighted average.\n        round: Number of decimal places to round the final result to.\n    \"\"\"\n\n    def __init__(self, round: int | None = None):\n        \"\"\"Initialize the averaged exponent meter.\n\n        Args:\n            round: Number of decimal places to round results to. If None,\n                results are not rounded.\n        \"\"\"\n        self._internal = AverageMeter()\n        self.round = round\n\n    def log(self, value: float | int, weight: float | int):\n        \"\"\"Log a log-scale value with its weight to the internal average meter.\n\n        Args:\n            value: The log-scale value to add.\n            weight: The weight for this value.\n        \"\"\"\n        self._internal.log(value, weight)\n\n    def compute(self) -&gt; float | int:\n        \"\"\"Return the exponent of the average.\n\n        Computes the weighted average of logged values using the internal\n        `AverageMeter` and then returns `exp()` of that average.\n\n        Returns:\n            The exponent of the weighted average, optionally rounded.\n        \"\"\"\n        return safe_round(np.exp(self._internal.compute()), self.round)\n\n    def merge(self, other_state: dict[str, Any]):\n        \"\"\"Merge state from another AveragedExponentMeter.\n\n        Args:\n            other_state: State dictionary from another AveragedExponentMeter instance.\n        \"\"\"\n        self._internal.merge(other_state[\"internal\"])\n\n    def load_state_dict(self, state_dict: dict[str, Any]):\n        \"\"\"Restore state.\n\n        Args:\n            state_dict: State dictionary to restore from.\n        \"\"\"\n        self._internal.load_state_dict(state_dict[\"internal\"])\n        self.round = state_dict[\"round\"]\n\n    def state_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Collect state for checkpointing.\n\n        Returns:\n            A dictionary containing the internal state of the `AverageMeter`\n            and the rounding precision.\n        \"\"\"\n        return {\n            \"internal\": self._internal.state_dict(),\n            \"round\": self.round,\n        }\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.AveragedExponentMeter.__init__","title":"<code>__init__(round=None)</code>","text":"<p>Initialize the averaged exponent meter.</p> <p>Parameters:</p> Name Type Description Default <code>round</code> <code>int | None</code> <p>Number of decimal places to round results to. If None, results are not rounded.</p> <code>None</code> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def __init__(self, round: int | None = None):\n    \"\"\"Initialize the averaged exponent meter.\n\n    Args:\n        round: Number of decimal places to round results to. If None,\n            results are not rounded.\n    \"\"\"\n    self._internal = AverageMeter()\n    self.round = round\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.AveragedExponentMeter.compute","title":"<code>compute()</code>","text":"<p>Return the exponent of the average.</p> <p>Computes the weighted average of logged values using the internal <code>AverageMeter</code> and then returns <code>exp()</code> of that average.</p> <p>Returns:</p> Type Description <code>float | int</code> <p>The exponent of the weighted average, optionally rounded.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def compute(self) -&gt; float | int:\n    \"\"\"Return the exponent of the average.\n\n    Computes the weighted average of logged values using the internal\n    `AverageMeter` and then returns `exp()` of that average.\n\n    Returns:\n        The exponent of the weighted average, optionally rounded.\n    \"\"\"\n    return safe_round(np.exp(self._internal.compute()), self.round)\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.AveragedExponentMeter.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Restore state.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict[str, Any]</code> <p>State dictionary to restore from.</p> required Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]):\n    \"\"\"Restore state.\n\n    Args:\n        state_dict: State dictionary to restore from.\n    \"\"\"\n    self._internal.load_state_dict(state_dict[\"internal\"])\n    self.round = state_dict[\"round\"]\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.AveragedExponentMeter.log","title":"<code>log(value, weight)</code>","text":"<p>Log a log-scale value with its weight to the internal average meter.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float | int</code> <p>The log-scale value to add.</p> required <code>weight</code> <code>float | int</code> <p>The weight for this value.</p> required Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def log(self, value: float | int, weight: float | int):\n    \"\"\"Log a log-scale value with its weight to the internal average meter.\n\n    Args:\n        value: The log-scale value to add.\n        weight: The weight for this value.\n    \"\"\"\n    self._internal.log(value, weight)\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.AveragedExponentMeter.merge","title":"<code>merge(other_state)</code>","text":"<p>Merge state from another AveragedExponentMeter.</p> <p>Parameters:</p> Name Type Description Default <code>other_state</code> <code>dict[str, Any]</code> <p>State dictionary from another AveragedExponentMeter instance.</p> required Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def merge(self, other_state: dict[str, Any]):\n    \"\"\"Merge state from another AveragedExponentMeter.\n\n    Args:\n        other_state: State dictionary from another AveragedExponentMeter instance.\n    \"\"\"\n    self._internal.merge(other_state[\"internal\"])\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.AveragedExponentMeter.state_dict","title":"<code>state_dict()</code>","text":"<p>Collect state for checkpointing.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the internal state of the <code>AverageMeter</code></p> <code>dict[str, Any]</code> <p>and the rounding precision.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Collect state for checkpointing.\n\n    Returns:\n        A dictionary containing the internal state of the `AverageMeter`\n        and the rounding precision.\n    \"\"\"\n    return {\n        \"internal\": self._internal.state_dict(),\n        \"round\": self.round,\n    }\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.CachedLambda","title":"<code>CachedLambda</code>","text":"<p>Wrapper that caches the result of a callable function.</p> <p>This is useful for expensive computations that are used multiple times in meter logging. The function is only called once, and subsequent calls return the cached result.</p> Example <pre><code># Expensive computation\ndef compute_expensive_value():\n    return complex_calculation()\n\ncached = CachedLambda(compute_expensive_value)\nvalue1 = cached()  # Computes and caches\nvalue2 = cached()  # Returns cached value\n</code></pre> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>class CachedLambda:\n    \"\"\"Wrapper that caches the result of a callable function.\n\n    This is useful for expensive computations that are used multiple times\n    in meter logging. The function is only called once, and subsequent calls\n    return the cached result.\n\n    Example:\n        ```python\n        # Expensive computation\n        def compute_expensive_value():\n            return complex_calculation()\n\n        cached = CachedLambda(compute_expensive_value)\n        value1 = cached()  # Computes and caches\n        value2 = cached()  # Returns cached value\n\n        ```\"\"\"\n\n    def __init__(self, func: Callable[[], Any]):\n        \"\"\"Initialize the cached lambda.\n\n        Args:\n            func: Callable function that takes no arguments and returns a value.\n        \"\"\"\n        self._func = func\n        self._cache = None\n        self._cached = False\n\n    def __call__(self) -&gt; Any:\n        \"\"\"Call the function, caching the result.\n\n        Returns:\n            The result of the function call. On first call, the function is\n            executed and the result is cached. On subsequent calls, the cached value\n            is returned.\n        \"\"\"\n        if not self._cached:\n            self._cache = self._func()\n            self._cached = True\n        return self._cache\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.CachedLambda.__call__","title":"<code>__call__()</code>","text":"<p>Call the function, caching the result.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the function call. On first call, the function is</p> <code>Any</code> <p>executed and the result is cached. On subsequent calls, the cached value</p> <code>Any</code> <p>is returned.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def __call__(self) -&gt; Any:\n    \"\"\"Call the function, caching the result.\n\n    Returns:\n        The result of the function call. On first call, the function is\n        executed and the result is cached. On subsequent calls, the cached value\n        is returned.\n    \"\"\"\n    if not self._cached:\n        self._cache = self._func()\n        self._cached = True\n    return self._cache\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.CachedLambda.__init__","title":"<code>__init__(func)</code>","text":"<p>Initialize the cached lambda.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[[], Any]</code> <p>Callable function that takes no arguments and returns a value.</p> required Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def __init__(self, func: Callable[[], Any]):\n    \"\"\"Initialize the cached lambda.\n\n    Args:\n        func: Callable function that takes no arguments and returns a value.\n    \"\"\"\n    self._func = func\n    self._cache = None\n    self._cached = False\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.FrequencyMeter","title":"<code>FrequencyMeter</code>","text":"<p>               Bases: <code>BaseMeter</code></p> <p>Meter that computes the frequency (duration per call) of an event.</p> <p>Measures time between successive calls to <code>log()</code>.</p> <p>Attributes:</p> Name Type Description <code>round</code> <p>Rounding precision for the result.</p> <code>start</code> <code>int | None</code> <p>Internal timestamp of the last call to <code>log()</code>.</p> <code>elapsed</code> <code>int</code> <p>Total elapsed time in nanoseconds.</p> <code>counter</code> <code>int</code> <p>Number of events recorded.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>class FrequencyMeter(BaseMeter):\n    \"\"\"Meter that computes the frequency (duration per call) of an event.\n\n    Measures time between successive calls to `log()`.\n\n    Attributes:\n        round: Rounding precision for the result.\n        start: Internal timestamp of the last call to `log()`.\n        elapsed: Total elapsed time in nanoseconds.\n        counter: Number of events recorded.\n    \"\"\"\n\n    def __init__(self, round: int | None = None):\n        \"\"\"Initialize the frequency meter.\n\n        Args:\n            round: Number of decimal places to round results to. If None,\n                results are not rounded.\n        \"\"\"\n        self.round = round\n        self.start: int | None = None\n        self.elapsed: int = 0\n        self.counter: int = 0\n\n    def log(self):\n        \"\"\"Record an occurrence and compute elapsed time since the last call.\n\n        If this is the first call, it initializes the start time.\n        Subsequent calls update the elapsed time and increment the counter.\n        \"\"\"\n        if self.start is None:\n            self.start = time.perf_counter_ns()\n            return\n        self.counter += 1\n        self.elapsed += time.perf_counter_ns() - self.start\n        self.start = time.perf_counter_ns()\n\n    def compute(self) -&gt; float | int | dict[str, float | int]:\n        \"\"\"Compute average time per occurrence in milliseconds.\n\n        Returns:\n            The average time per occurrence in milliseconds, optionally rounded.\n            Returns 0 if no events have been recorded.\n        \"\"\"\n        if self.counter == 0:\n            return 0\n        return safe_round(self.elapsed / self.counter / 1e6, self.round)\n\n    def merge(self, other_state: dict[str, Any]):\n        \"\"\"Merge state from another FrequencyMeter.\n\n        Args:\n            other_state: State dictionary from another FrequencyMeter instance.\n        \"\"\"\n        self.elapsed += other_state[\"elapsed\"]\n        self.counter += other_state[\"counter\"]\n\n    def load_state_dict(self, state_dict: dict[str, Any]):\n        \"\"\"Restore state and reset the start timer.\n\n        Args:\n            state_dict: State dictionary to restore from.\n        \"\"\"\n        super().load_state_dict(state_dict)\n        self.start = None  # Reset start timer on load to avoid inaccurate timing across checkpoints\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.FrequencyMeter.__init__","title":"<code>__init__(round=None)</code>","text":"<p>Initialize the frequency meter.</p> <p>Parameters:</p> Name Type Description Default <code>round</code> <code>int | None</code> <p>Number of decimal places to round results to. If None, results are not rounded.</p> <code>None</code> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def __init__(self, round: int | None = None):\n    \"\"\"Initialize the frequency meter.\n\n    Args:\n        round: Number of decimal places to round results to. If None,\n            results are not rounded.\n    \"\"\"\n    self.round = round\n    self.start: int | None = None\n    self.elapsed: int = 0\n    self.counter: int = 0\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.FrequencyMeter.compute","title":"<code>compute()</code>","text":"<p>Compute average time per occurrence in milliseconds.</p> <p>Returns:</p> Type Description <code>float | int | dict[str, float | int]</code> <p>The average time per occurrence in milliseconds, optionally rounded.</p> <code>float | int | dict[str, float | int]</code> <p>Returns 0 if no events have been recorded.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def compute(self) -&gt; float | int | dict[str, float | int]:\n    \"\"\"Compute average time per occurrence in milliseconds.\n\n    Returns:\n        The average time per occurrence in milliseconds, optionally rounded.\n        Returns 0 if no events have been recorded.\n    \"\"\"\n    if self.counter == 0:\n        return 0\n    return safe_round(self.elapsed / self.counter / 1e6, self.round)\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.FrequencyMeter.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Restore state and reset the start timer.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict[str, Any]</code> <p>State dictionary to restore from.</p> required Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]):\n    \"\"\"Restore state and reset the start timer.\n\n    Args:\n        state_dict: State dictionary to restore from.\n    \"\"\"\n    super().load_state_dict(state_dict)\n    self.start = None  # Reset start timer on load to avoid inaccurate timing across checkpoints\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.FrequencyMeter.log","title":"<code>log()</code>","text":"<p>Record an occurrence and compute elapsed time since the last call.</p> <p>If this is the first call, it initializes the start time. Subsequent calls update the elapsed time and increment the counter.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def log(self):\n    \"\"\"Record an occurrence and compute elapsed time since the last call.\n\n    If this is the first call, it initializes the start time.\n    Subsequent calls update the elapsed time and increment the counter.\n    \"\"\"\n    if self.start is None:\n        self.start = time.perf_counter_ns()\n        return\n    self.counter += 1\n    self.elapsed += time.perf_counter_ns() - self.start\n    self.start = time.perf_counter_ns()\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.FrequencyMeter.merge","title":"<code>merge(other_state)</code>","text":"<p>Merge state from another FrequencyMeter.</p> <p>Parameters:</p> Name Type Description Default <code>other_state</code> <code>dict[str, Any]</code> <p>State dictionary from another FrequencyMeter instance.</p> required Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def merge(self, other_state: dict[str, Any]):\n    \"\"\"Merge state from another FrequencyMeter.\n\n    Args:\n        other_state: State dictionary from another FrequencyMeter instance.\n    \"\"\"\n    self.elapsed += other_state[\"elapsed\"]\n    self.counter += other_state[\"counter\"]\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.GatherMeter","title":"<code>GatherMeter</code>","text":"<p>               Bases: <code>BaseMeter</code></p> <p>Accumulator that gathers all raw values across the entire dataset.</p> <p>Use this for meters that require full dataset context (e.g., BLEU, ROC-AUC).</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>class GatherMeter(BaseMeter):\n    \"\"\"Accumulator that gathers all raw values across the entire dataset.\n\n    Use this for meters that require full dataset context (e.g., BLEU, ROC-AUC).\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes the GatherMeter with an empty list to store values.\"\"\"\n        self.values: list[Any] = []\n\n    def log(self, value: Any):\n        \"\"\"Logs a single value to be gathered.\"\"\"\n        self.values.append(value)\n\n    def compute(self) -&gt; list[Any]:\n        \"\"\"Returns the list of all gathered values.\"\"\"\n        return list(self.values)\n\n    def merge(self, other_state: dict[str, Any]):\n        \"\"\"Merges the state from another GatherMeter instance.\"\"\"\n        self.values.extend(other_state[\"values\"])\n\n    def state_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Returns the state of the GatherMeter for checkpointing.\"\"\"\n        return {\"values\": self.values}\n\n    def load_state_dict(self, state_dict: dict[str, Any]):\n        \"\"\"Restores the state of the GatherMeter from a state dictionary.\"\"\"\n        self.values = state_dict[\"values\"]\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.GatherMeter.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the GatherMeter with an empty list to store values.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the GatherMeter with an empty list to store values.\"\"\"\n    self.values: list[Any] = []\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.GatherMeter.compute","title":"<code>compute()</code>","text":"<p>Returns the list of all gathered values.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def compute(self) -&gt; list[Any]:\n    \"\"\"Returns the list of all gathered values.\"\"\"\n    return list(self.values)\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.GatherMeter.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Restores the state of the GatherMeter from a state dictionary.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]):\n    \"\"\"Restores the state of the GatherMeter from a state dictionary.\"\"\"\n    self.values = state_dict[\"values\"]\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.GatherMeter.log","title":"<code>log(value)</code>","text":"<p>Logs a single value to be gathered.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def log(self, value: Any):\n    \"\"\"Logs a single value to be gathered.\"\"\"\n    self.values.append(value)\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.GatherMeter.merge","title":"<code>merge(other_state)</code>","text":"<p>Merges the state from another GatherMeter instance.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def merge(self, other_state: dict[str, Any]):\n    \"\"\"Merges the state from another GatherMeter instance.\"\"\"\n    self.values.extend(other_state[\"values\"])\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.GatherMeter.state_dict","title":"<code>state_dict()</code>","text":"<p>Returns the state of the GatherMeter for checkpointing.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Returns the state of the GatherMeter for checkpointing.\"\"\"\n    return {\"values\": self.values}\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.StopwatchMeter","title":"<code>StopwatchMeter</code>","text":"<p>               Bases: <code>BaseMeter</code></p> <p>Meter that acts as a manual stopwatch for measuring event durations.</p> <p>Expects pairs of <code>log(mode=\"start\")</code> and <code>log(mode=\"end\")</code> calls.</p> <p>Attributes:</p> Name Type Description <code>round</code> <p>Rounding precision for the result.</p> <code>_start</code> <code>float | None</code> <p>Internal timestamp of when the stopwatch was started.</p> <code>elapsed</code> <code>int</code> <p>Total elapsed time in nanoseconds across all recorded intervals.</p> <code>counter</code> <code>int</code> <p>Number of completed intervals.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>class StopwatchMeter(BaseMeter):  # Ensures it inherits from BaseMeter\n    \"\"\"Meter that acts as a manual stopwatch for measuring event durations.\n\n    Expects pairs of `log(mode=\"start\")` and `log(mode=\"end\")` calls.\n\n    Attributes:\n        round: Rounding precision for the result.\n        _start: Internal timestamp of when the stopwatch was started.\n        elapsed: Total elapsed time in nanoseconds across all recorded intervals.\n        counter: Number of completed intervals.\n    \"\"\"\n\n    def __init__(self, round: int | None = None):\n        \"\"\"Initialize the stopwatch meter.\n\n        Args:\n            round: Number of decimal places to round results to. If None,\n                results are not rounded.\n        \"\"\"\n        self.round = round\n        self._start: float | None = None\n        self.elapsed: int = 0\n        self.counter: int = 0\n\n    def log(self, mode: str):\n        \"\"\"Start or stop the timer.\n\n        Args:\n            mode: \"start\" to begin timing, \"end\" to stop and record duration.\n\n        Raises:\n            AssertionError: If an unknown mode is provided.\n        \"\"\"\n        if mode == \"start\":\n            self.start()\n        elif mode == \"end\":\n            self.end()\n        else:\n            raise AssertionError(\"Unknown mode\")\n\n    def start(self):\n        \"\"\"Start the timer.\"\"\"\n        self._start = time.perf_counter_ns()\n\n    def end(self):\n        \"\"\"Stop the timer and record the duration.\n\n        Raises:\n            AssertionError: If `start()` was not called before `end()`.\n        \"\"\"\n        assert self._start is not None, \"Stopwatch was never started\"\n        self.elapsed += time.perf_counter_ns() - self._start\n        self.counter += 1\n        self._start = None\n\n    def compute(self) -&gt; float | int | dict[str, float | int]:\n        \"\"\"Compute average duration in milliseconds.\n\n        Returns:\n            The average duration in milliseconds, optionally rounded.\n            Returns 0 if no intervals have been recorded.\n        \"\"\"\n        if self.counter == 0:\n            return 0\n        return safe_round(self.elapsed / self.counter / 1e6, self.round)\n\n    def merge(self, other_state: dict[str, Any]):\n        \"\"\"Merge state from another StopwatchMeter.\n\n        Args:\n            other_state: State dictionary from another StopwatchMeter instance.\n        \"\"\"\n        self.elapsed += other_state[\"elapsed\"]\n        self.counter += other_state[\"counter\"]\n\n    def load_state_dict(self, state_dict: dict[str, Any]):\n        \"\"\"Restore state and reset current timer.\n\n        Args:\n            state_dict: State dictionary to restore from.\n        \"\"\"\n        super().load_state_dict(state_dict)\n        self._start = None  # Reset current timer on load to avoid inaccurate timing across checkpoints\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.StopwatchMeter.__init__","title":"<code>__init__(round=None)</code>","text":"<p>Initialize the stopwatch meter.</p> <p>Parameters:</p> Name Type Description Default <code>round</code> <code>int | None</code> <p>Number of decimal places to round results to. If None, results are not rounded.</p> <code>None</code> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def __init__(self, round: int | None = None):\n    \"\"\"Initialize the stopwatch meter.\n\n    Args:\n        round: Number of decimal places to round results to. If None,\n            results are not rounded.\n    \"\"\"\n    self.round = round\n    self._start: float | None = None\n    self.elapsed: int = 0\n    self.counter: int = 0\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.StopwatchMeter.compute","title":"<code>compute()</code>","text":"<p>Compute average duration in milliseconds.</p> <p>Returns:</p> Type Description <code>float | int | dict[str, float | int]</code> <p>The average duration in milliseconds, optionally rounded.</p> <code>float | int | dict[str, float | int]</code> <p>Returns 0 if no intervals have been recorded.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def compute(self) -&gt; float | int | dict[str, float | int]:\n    \"\"\"Compute average duration in milliseconds.\n\n    Returns:\n        The average duration in milliseconds, optionally rounded.\n        Returns 0 if no intervals have been recorded.\n    \"\"\"\n    if self.counter == 0:\n        return 0\n    return safe_round(self.elapsed / self.counter / 1e6, self.round)\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.StopwatchMeter.end","title":"<code>end()</code>","text":"<p>Stop the timer and record the duration.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>start()</code> was not called before <code>end()</code>.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def end(self):\n    \"\"\"Stop the timer and record the duration.\n\n    Raises:\n        AssertionError: If `start()` was not called before `end()`.\n    \"\"\"\n    assert self._start is not None, \"Stopwatch was never started\"\n    self.elapsed += time.perf_counter_ns() - self._start\n    self.counter += 1\n    self._start = None\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.StopwatchMeter.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Restore state and reset current timer.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict[str, Any]</code> <p>State dictionary to restore from.</p> required Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]):\n    \"\"\"Restore state and reset current timer.\n\n    Args:\n        state_dict: State dictionary to restore from.\n    \"\"\"\n    super().load_state_dict(state_dict)\n    self._start = None  # Reset current timer on load to avoid inaccurate timing across checkpoints\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.StopwatchMeter.log","title":"<code>log(mode)</code>","text":"<p>Start or stop the timer.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>\"start\" to begin timing, \"end\" to stop and record duration.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If an unknown mode is provided.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def log(self, mode: str):\n    \"\"\"Start or stop the timer.\n\n    Args:\n        mode: \"start\" to begin timing, \"end\" to stop and record duration.\n\n    Raises:\n        AssertionError: If an unknown mode is provided.\n    \"\"\"\n    if mode == \"start\":\n        self.start()\n    elif mode == \"end\":\n        self.end()\n    else:\n        raise AssertionError(\"Unknown mode\")\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.StopwatchMeter.merge","title":"<code>merge(other_state)</code>","text":"<p>Merge state from another StopwatchMeter.</p> <p>Parameters:</p> Name Type Description Default <code>other_state</code> <code>dict[str, Any]</code> <p>State dictionary from another StopwatchMeter instance.</p> required Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def merge(self, other_state: dict[str, Any]):\n    \"\"\"Merge state from another StopwatchMeter.\n\n    Args:\n        other_state: State dictionary from another StopwatchMeter instance.\n    \"\"\"\n    self.elapsed += other_state[\"elapsed\"]\n    self.counter += other_state[\"counter\"]\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.StopwatchMeter.start","title":"<code>start()</code>","text":"<p>Start the timer.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def start(self):\n    \"\"\"Start the timer.\"\"\"\n    self._start = time.perf_counter_ns()\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.SummedMeter","title":"<code>SummedMeter</code>","text":"<p>               Bases: <code>BaseMeter</code></p> <p>Meter that sums logged values.</p> <p>This meter simply accumulates values without averaging. Useful for values like total tokens processed, total examples seen, etc.</p> <p>Attributes:</p> Name Type Description <code>round</code> <p>Number of decimal places to round results to (None = no rounding).</p> <code>sum</code> <p>Accumulated sum of all logged values.</p> Example <pre><code>meter = SummedMeter()\nmeter.log(100)\nmeter.log(200)\nmeter.compute()  # 300\n</code></pre> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>class SummedMeter(BaseMeter):\n    \"\"\"Meter that sums logged values.\n\n    This meter simply accumulates values without averaging. Useful for\n    values like total tokens processed, total examples seen, etc.\n\n    Attributes:\n        round: Number of decimal places to round results to (None = no rounding).\n        sum: Accumulated sum of all logged values.\n\n    Example:\n        ```python\n        meter = SummedMeter()\n        meter.log(100)\n        meter.log(200)\n        meter.compute()  # 300\n\n        ```\"\"\"\n\n    def __init__(self, round: int | None = None):\n        \"\"\"Initialize the summed meter.\n\n        Args:\n            round: Number of decimal places to round results to. If None,\n                results are not rounded.\n        \"\"\"\n        self.round = round\n        self.sum = 0\n\n    def compute(self) -&gt; float | int:\n        \"\"\"Compute the sum.\n\n        Returns:\n            Sum of all logged values, optionally rounded.\n        \"\"\"\n        return safe_round(self.sum, self.round)\n\n    def log(self, value: float | int) -&gt; None:\n        \"\"\"Log a value to add to the sum.\n\n        Args:\n            value: The value to add to the sum.\n        \"\"\"\n        self.sum += value\n\n    def merge(self, other_state: dict[str, Any]) -&gt; None:\n        \"\"\"Merge state from another meter instance (for distributed aggregation).\n\n        Args:\n            other_state: State dictionary from another SummedMeter instance.\n        \"\"\"\n        self.sum += other_state[\"sum\"]\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.SummedMeter.__init__","title":"<code>__init__(round=None)</code>","text":"<p>Initialize the summed meter.</p> <p>Parameters:</p> Name Type Description Default <code>round</code> <code>int | None</code> <p>Number of decimal places to round results to. If None, results are not rounded.</p> <code>None</code> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def __init__(self, round: int | None = None):\n    \"\"\"Initialize the summed meter.\n\n    Args:\n        round: Number of decimal places to round results to. If None,\n            results are not rounded.\n    \"\"\"\n    self.round = round\n    self.sum = 0\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.SummedMeter.compute","title":"<code>compute()</code>","text":"<p>Compute the sum.</p> <p>Returns:</p> Type Description <code>float | int</code> <p>Sum of all logged values, optionally rounded.</p> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def compute(self) -&gt; float | int:\n    \"\"\"Compute the sum.\n\n    Returns:\n        Sum of all logged values, optionally rounded.\n    \"\"\"\n    return safe_round(self.sum, self.round)\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.SummedMeter.log","title":"<code>log(value)</code>","text":"<p>Log a value to add to the sum.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float | int</code> <p>The value to add to the sum.</p> required Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def log(self, value: float | int) -&gt; None:\n    \"\"\"Log a value to add to the sum.\n\n    Args:\n        value: The value to add to the sum.\n    \"\"\"\n    self.sum += value\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.SummedMeter.merge","title":"<code>merge(other_state)</code>","text":"<p>Merge state from another meter instance (for distributed aggregation).</p> <p>Parameters:</p> Name Type Description Default <code>other_state</code> <code>dict[str, Any]</code> <p>State dictionary from another SummedMeter instance.</p> required Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def merge(self, other_state: dict[str, Any]) -&gt; None:\n    \"\"\"Merge state from another meter instance (for distributed aggregation).\n\n    Args:\n        other_state: State dictionary from another SummedMeter instance.\n    \"\"\"\n    self.sum += other_state[\"sum\"]\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.cached_lambda","title":"<code>cached_lambda(x)</code>","text":"<p>Create a cached lambda wrapper.</p> <p>Convenience function for creating a CachedLambda instance.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Callable[[], Any]</code> <p>Callable function to cache.</p> required <p>Returns:</p> Type Description <code>CachedLambda</code> <p>CachedLambda instance that caches the function's result.</p> Example <pre><code>expensive = cached_lambda(lambda: expensive_computation())\nresult = expensive()  # Computes once\nresult = expensive()  # Uses cache\n</code></pre> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def cached_lambda(x: Callable[[], Any]) -&gt; CachedLambda:\n    \"\"\"Create a cached lambda wrapper.\n\n    Convenience function for creating a CachedLambda instance.\n\n    Args:\n        x: Callable function to cache.\n\n    Returns:\n        CachedLambda instance that caches the function's result.\n\n    Example:\n        ```python\n        expensive = cached_lambda(lambda: expensive_computation())\n        result = expensive()  # Computes once\n        result = expensive()  # Uses cache\n\n        ```\"\"\"\n    return CachedLambda(x)\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.log_averaged","title":"<code>log_averaged(name, value, weight=1.0, round=None, reset=True, priority=100)</code>","text":"<p>Log a value to an averaged meter.</p> <p>This is a convenience function for logging values to an <code>AverageMeter</code>. The value and weight can be callables (lambdas) that are only evaluated when the meter is actually logged (lazy evaluation).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the meter (e.g., \"train/loss\").</p> required <code>value</code> <code>DelayedValue</code> <p>Value to log. Can be a number or a callable that returns a number.</p> required <code>weight</code> <code>DelayedValue</code> <p>Weight for this value (typically batch size). Can be a number or a callable. Defaults to 1.0.</p> <code>1.0</code> <code>round</code> <code>int | None</code> <p>Number of decimal places to round the result to.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>If True, the meter is reset after logging (for per-iteration meters). If False, the meter accumulates across iterations.</p> <code>True</code> <code>priority</code> <code>int</code> <p>Priority for meter ordering when logging. Higher priority meters appear first.</p> <code>100</code> Example <pre><code># Log a simple value\nlog_averaged(\"train/loss\", 0.5, weight=32)\n\n# Log with lazy evaluation (only computed if meter is logged)\nlog_averaged(\"train/loss\", lambda: compute_loss(), weight=lambda: batch_size)\n\n# Log with rounding\nlog_averaged(\"train/accuracy\", 0.95, round=4)\n</code></pre> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def log_averaged(\n    name: str,\n    value: DelayedValue,\n    weight: DelayedValue = 1.0,\n    round: int | None = None,\n    reset: bool = True,\n    priority: int = 100,\n) -&gt; None:\n    \"\"\"Log a value to an averaged meter.\n\n    This is a convenience function for logging values to an `AverageMeter`.\n    The value and weight can be callables (lambdas) that are only evaluated\n    when the meter is actually logged (lazy evaluation).\n\n    Args:\n        name: Name of the meter (e.g., \"train/loss\").\n        value: Value to log. Can be a number or a callable that returns a number.\n        weight: Weight for this value (typically batch size). Can be a number\n            or a callable. Defaults to 1.0.\n        round: Number of decimal places to round the result to.\n        reset: If True, the meter is reset after logging (for per-iteration meters).\n            If False, the meter accumulates across iterations.\n        priority: Priority for meter ordering when logging. Higher priority\n            meters appear first.\n\n    Example:\n        ```python\n        # Log a simple value\n        log_averaged(\"train/loss\", 0.5, weight=32)\n\n        # Log with lazy evaluation (only computed if meter is logged)\n        log_averaged(\"train/loss\", lambda: compute_loss(), weight=lambda: batch_size)\n\n        # Log with rounding\n        log_averaged(\"train/accuracy\", 0.95, round=4)\n\n        ```\"\"\"\n    log_meter(\n        name=name,\n        meter_factory=lambda: AverageMeter(round=round),\n        reset=reset,\n        priority=priority,\n        value=value,\n        weight=weight,\n    )\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.log_averaged_exponent","title":"<code>log_averaged_exponent(name, value, weight=1.0, round=None, reset=True, priority=100)</code>","text":"<p>Log a value to an averaged exponent meter.</p> <p>This is a convenience function for logging values to an <code>AveragedExponentMeter</code>. The value and weight can be callables (lambdas) for lazy evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the meter (e.g., \"train/perplexity\").</p> required <code>value</code> <code>DelayedValue</code> <p>Log-scale value to log. Can be a number or a callable.</p> required <code>weight</code> <code>DelayedValue</code> <p>Weight for this value. Can be a number or a callable. Defaults to 1.0.</p> <code>1.0</code> <code>round</code> <code>int | None</code> <p>Number of decimal places to round the result to.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>If True, the meter is reset after logging.</p> <code>True</code> <code>priority</code> <code>int</code> <p>Priority for meter ordering.</p> <code>100</code> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def log_averaged_exponent(\n    name: str,\n    value: DelayedValue,\n    weight: DelayedValue = 1.0,\n    round: int | None = None,\n    reset: bool = True,\n    priority: int = 100,\n):\n    \"\"\"Log a value to an averaged exponent meter.\n\n    This is a convenience function for logging values to an `AveragedExponentMeter`.\n    The value and weight can be callables (lambdas) for lazy evaluation.\n\n    Args:\n        name: Name of the meter (e.g., \"train/perplexity\").\n        value: Log-scale value to log. Can be a number or a callable.\n        weight: Weight for this value. Can be a number or a callable. Defaults to 1.0.\n        round: Number of decimal places to round the result to.\n        reset: If True, the meter is reset after logging.\n        priority: Priority for meter ordering.\n    \"\"\"\n    log_meter(\n        name=name,\n        meter_factory=lambda: AveragedExponentMeter(round=round),\n        reset=reset,\n        priority=priority,\n        value=value,\n        weight=weight,\n    )\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.log_event_end","title":"<code>log_event_end(name, round=None, reset=True, priority=100)</code>","text":"<p>End timing an event.</p> <p>This function stops a stopwatch that was started with <code>log_event_start()</code>. The duration between start and end is recorded and averaged across multiple occurrences.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the event (must match the name used in <code>log_event_start()</code>).</p> required <code>round</code> <code>int | None</code> <p>Number of decimal places to round the duration to (in milliseconds).</p> <code>None</code> <code>reset</code> <code>bool</code> <p>If True, the meter is reset after logging.</p> <code>True</code> <code>priority</code> <code>int</code> <p>Priority for meter ordering when logging.</p> <code>100</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>log_event_start()</code> was not called for this event name.</p> Example <pre><code>log_event_start(\"perf/backward_pass\")\n# ... do backward pass ...\nlog_event_end(\"perf/backward_pass\")\n</code></pre> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def log_event_end(\n    name: str,\n    round: int | None = None,\n    reset: bool = True,\n    priority: int = 100,\n) -&gt; None:\n    \"\"\"End timing an event.\n\n    This function stops a stopwatch that was started with `log_event_start()`.\n    The duration between start and end is recorded and averaged across\n    multiple occurrences.\n\n    Args:\n        name: Name of the event (must match the name used in `log_event_start()`).\n        round: Number of decimal places to round the duration to (in milliseconds).\n        reset: If True, the meter is reset after logging.\n        priority: Priority for meter ordering when logging.\n\n    Raises:\n        AssertionError: If `log_event_start()` was not called for this event name.\n\n    Example:\n        ```python\n        log_event_start(\"perf/backward_pass\")\n        # ... do backward pass ...\n        log_event_end(\"perf/backward_pass\")\n\n        ```\"\"\"\n    log_meter(\n        name=name,\n        meter_factory=lambda: StopwatchMeter(round=round),\n        reset=reset,\n        priority=priority,\n        mode=\"end\",\n        force_log=True,  # Always log event occurrences\n    )\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.log_event_occurence","title":"<code>log_event_occurence(name, round=None, reset=True, priority=100)</code>","text":"<p>Log an occurrence of an event and track its frequency.</p> <p>This function uses a <code>FrequencyMeter</code> to measure the time between successive calls to <code>log_event_occurrence()</code> for a given <code>name</code>. It can be used to track the rate at which certain events happen.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the event to track (e.g., \"perf/dataloader_ready\").</p> required <code>round</code> <code>int | None</code> <p>Number of decimal places to round the duration to (in milliseconds).</p> <code>None</code> <code>reset</code> <code>bool</code> <p>If True, the meter is reset after logging.</p> <code>True</code> <code>priority</code> <code>int</code> <p>Priority for meter ordering when logging.</p> <code>100</code> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def log_event_occurence(\n    name: str,\n    round: int | None = None,\n    reset: bool = True,\n    priority: int = 100,\n):\n    \"\"\"Log an occurrence of an event and track its frequency.\n\n    This function uses a `FrequencyMeter` to measure the time between\n    successive calls to `log_event_occurrence()` for a given `name`.\n    It can be used to track the rate at which certain events happen.\n\n    Args:\n        name: Name of the event to track (e.g., \"perf/dataloader_ready\").\n        round: Number of decimal places to round the duration to (in milliseconds).\n        reset: If True, the meter is reset after logging.\n        priority: Priority for meter ordering when logging.\n    \"\"\"\n    log_meter(\n        name=name,\n        meter_factory=lambda: FrequencyMeter(round=round),\n        reset=reset,\n        priority=priority,\n        force_log=True,  # Always log event occurrences\n    )\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.log_event_start","title":"<code>log_event_start(name, round=None, reset=True, priority=100)</code>","text":"<p>Start timing an event.</p> <p>This function starts a stopwatch for measuring the duration of an event. Call <code>log_event_end()</code> with the same name to stop timing and record the duration. The duration is automatically averaged across multiple occurrences.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the event to time (e.g., \"perf/forward_pass\").</p> required <code>round</code> <code>int | None</code> <p>Number of decimal places to round the duration to (in milliseconds).</p> <code>None</code> <code>reset</code> <code>bool</code> <p>If True, the meter is reset after logging.</p> <code>True</code> <code>priority</code> <code>int</code> <p>Priority for meter ordering when logging.</p> <code>100</code> Example <pre><code>log_event_start(\"perf/forward_pass\")\n# ... do work ...\nlog_event_end(\"perf/forward_pass\")\n# Meter will show average duration in milliseconds\n</code></pre> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def log_event_start(\n    name: str,\n    round: int | None = None,\n    reset: bool = True,\n    priority: int = 100,\n) -&gt; None:\n    \"\"\"Start timing an event.\n\n    This function starts a stopwatch for measuring the duration of an event.\n    Call `log_event_end()` with the same name to stop timing and record the\n    duration. The duration is automatically averaged across multiple occurrences.\n\n    Args:\n        name: Name of the event to time (e.g., \"perf/forward_pass\").\n        round: Number of decimal places to round the duration to (in milliseconds).\n        reset: If True, the meter is reset after logging.\n        priority: Priority for meter ordering when logging.\n\n    Example:\n        ```python\n        log_event_start(\"perf/forward_pass\")\n        # ... do work ...\n        log_event_end(\"perf/forward_pass\")\n        # Meter will show average duration in milliseconds\n\n        ```\"\"\"\n    log_meter(\n        name=name,\n        meter_factory=lambda: StopwatchMeter(round=round),\n        reset=reset,\n        priority=priority,\n        mode=\"start\",\n        force_log=True,  # Always log event occurrences\n    )\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.log_summed","title":"<code>log_summed(name, value, round=None, reset=True, priority=100)</code>","text":"<p>Log a value to a summed meter.</p> <p>This is a convenience function for logging values to a <code>SummedMeter</code>. The value can be a callable (lambda) that is only evaluated when the meter is actually logged (lazy evaluation).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the meter (e.g., \"train/tokens_processed\").</p> required <code>value</code> <code>DelayedValue</code> <p>Value to add to the sum. Can be a number or a callable that returns a number.</p> required <code>round</code> <code>int | None</code> <p>Number of decimal places to round the result to.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>If True, the meter is reset after logging. If False, the meter accumulates across iterations.</p> <code>True</code> <code>priority</code> <code>int</code> <p>Priority for meter ordering when logging.</p> <code>100</code> Example <pre><code># Log total tokens processed\nlog_summed(\"train/tokens\", batch_size * seq_len)\n\n# Log with lazy evaluation\nlog_summed(\"train/tokens\", lambda: get_token_count())\n</code></pre> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def log_summed(\n    name: str,\n    value: DelayedValue,\n    round: int | None = None,\n    reset: bool = True,\n    priority: int = 100,\n) -&gt; None:\n    \"\"\"Log a value to a summed meter.\n\n    This is a convenience function for logging values to a `SummedMeter`.\n    The value can be a callable (lambda) that is only evaluated when the\n    meter is actually logged (lazy evaluation).\n\n    Args:\n        name: Name of the meter (e.g., \"train/tokens_processed\").\n        value: Value to add to the sum. Can be a number or a callable that\n            returns a number.\n        round: Number of decimal places to round the result to.\n        reset: If True, the meter is reset after logging. If False, the\n            meter accumulates across iterations.\n        priority: Priority for meter ordering when logging.\n\n    Example:\n        ```python\n        # Log total tokens processed\n        log_summed(\"train/tokens\", batch_size * seq_len)\n\n        # Log with lazy evaluation\n        log_summed(\"train/tokens\", lambda: get_token_count())\n\n        ```\"\"\"\n    log_meter(\n        name=name,\n        meter_factory=lambda: SummedMeter(round=round),\n        reset=reset,\n        priority=priority,\n        value=value,\n    )\n</code></pre>"},{"location":"reference/modules/metrics/common/#optimus_dl.modules.metrics.common.safe_round","title":"<code>safe_round(number, ndigits)</code>","text":"<p>Safely round a number, handling various numeric types.</p> <p>This function handles rounding for Python numbers, PyTorch tensors, and NumPy arrays. It recursively handles nested types (e.g., single-element tensors) until it reaches a roundable Python number.</p> <p>Parameters:</p> Name Type Description Default <code>number</code> <code>float | int | Any</code> <p>The number to round. Can be a Python number, PyTorch tensor, or NumPy array.</p> required <code>ndigits</code> <code>int | None</code> <p>Number of decimal places to round to. If None, returns the number unchanged.</p> required <p>Returns:</p> Type Description <code>float | int</code> <p>Rounded number as float or int (depending on whether rounding occurred).</p> Example <pre><code>safe_round(3.14159, 2)  # 3.14\nsafe_round(torch.tensor(3.14159), 2)  # 3.14\nsafe_round(3.14159, None)  # 3.14159 (no rounding)\n</code></pre> Source code in <code>optimus_dl/modules/metrics/common.py</code> <pre><code>def safe_round(number: float | int | Any, ndigits: int | None) -&gt; float | int:\n    \"\"\"Safely round a number, handling various numeric types.\n\n    This function handles rounding for Python numbers, PyTorch tensors, and\n    NumPy arrays. It recursively handles nested types (e.g., single-element\n    tensors) until it reaches a roundable Python number.\n\n    Args:\n        number: The number to round. Can be a Python number, PyTorch tensor,\n            or NumPy array.\n        ndigits: Number of decimal places to round to. If None, returns the\n            number unchanged.\n\n    Returns:\n        Rounded number as float or int (depending on whether rounding occurred).\n\n    Example:\n        ```python\n        safe_round(3.14159, 2)  # 3.14\n        safe_round(torch.tensor(3.14159), 2)  # 3.14\n        safe_round(3.14159, None)  # 3.14159 (no rounding)\n\n        ```\"\"\"\n    if ndigits is None:\n        return number\n    if hasattr(number, \"__round__\"):\n        return round(number, ndigits)\n    elif torch is not None and torch.is_tensor(number) and number.numel() == 1:\n        return safe_round(number.item(), ndigits)\n    elif np is not None and np.ndim(number) == 0 and hasattr(number, \"item\"):\n        return safe_round(number.item(), ndigits)\n    else:\n        return number\n</code></pre>"},{"location":"reference/modules/metrics/engine/","title":"engine","text":""},{"location":"reference/modules/metrics/engine/#optimus_dl.modules.metrics.engine","title":"<code>optimus_dl.modules.metrics.engine</code>","text":""},{"location":"reference/modules/metrics/engine/#optimus_dl.modules.metrics.engine.MetricEngine","title":"<code>MetricEngine</code>","text":"<p>Orchestrator for complex metric calculation and aggregation.</p> <p>The <code>MetricEngine</code> coordinates <code>MetricSource</code>s (data producers) and <code>Metric</code>s (stateless compute logic). It handles: - Protocol Handshake: Ensuring sources provide the data metrics require. - Lazy Evaluation: Executing each source at most once per batch across all groups via hash caching. - Grouping: Allowing the same metric to run over different sources. - Source Dependencies: Allowing sources to depend on other sources in the group.</p> Source code in <code>optimus_dl/modules/metrics/engine.py</code> <pre><code>class MetricEngine:\n    \"\"\"Orchestrator for complex metric calculation and aggregation.\n\n    The `MetricEngine` coordinates `MetricSource`s (data producers) and\n    `Metric`s (stateless compute logic). It handles:\n    - Protocol Handshake: Ensuring sources provide the data metrics require.\n    - Lazy Evaluation: Executing each source at most once per batch across all groups via hash caching.\n    - Grouping: Allowing the same metric to run over different sources.\n    - Source Dependencies: Allowing sources to depend on other sources in the group.\n    \"\"\"\n\n    def __init__(\n        self,\n        group_name: str,\n        configs: list[dict[str, Any]],\n    ):\n        \"\"\"Initializes the MetricEngine.\n\n        Args:\n            group_name: The name of the `MeterGroup` (logging namespace).\n            configs: A list of configuration dicts.\n        \"\"\"\n        self.group_name = group_name\n        self.groups: list[ParsedGroup] = []\n\n        self._parse_and_validate_configs(configs)\n\n    def _parse_and_validate_configs(self, configs: list[dict[str, Any]]):\n        \"\"\"Parses configurations, builds sources/metrics, and performs handshakes.\"\"\"\n        for idx, cfg in enumerate(configs):\n            if cfg.get(\"_name\") == \"source_group\":\n                prefix = cfg.get(\"prefix\", f\"group_{idx}\")\n                sources_dict = cfg.get(\"sources\", {})\n                metrics_list = cfg.get(\"metrics\", [])\n            else:\n                prefix = \"\"\n                sources_dict = {}\n                metrics_list = [cfg]\n\n            group = ParsedGroup(prefix=prefix)\n\n            # 1. Build Sources\n            for role, s_cfg in sources_dict.items():\n                group.sources[role] = build_metric_source(s_cfg)\n\n            # 2. Validate Source Dependencies (Internal Handshake)\n            for source in group.sources.values():\n                self._validate_handshake(\n                    component_requires=source.requires,\n                    available_sources=group.sources,\n                )\n\n            # 3. Build Metrics and perform Protocol Handshake\n            for m_dict in metrics_list:\n                metric_impl = build_metric(m_dict)\n\n                self._validate_handshake(\n                    component_requires=metric_impl.requires,\n                    available_sources=group.sources,\n                )\n\n                group.metrics.append(metric_impl)\n\n            self.groups.append(group)\n\n    def _validate_handshake(\n        self,\n        component_requires: set[str],\n        available_sources: dict[str, MetricSource],\n    ):\n        \"\"\"Validates that all dependencies and protocols for a component are met.\"\"\"\n        available_protocols = set()\n        for source in available_sources.values():\n            available_protocols |= source.provides\n\n        missing = component_requires - available_protocols\n        if missing:\n            logger.debug(\n                f\"Handshake missing protocols: {missing}. These must be provided via 'computed_data' during update.\"\n            )\n\n    @property\n    def required_external_protocols(self) -&gt; set[str]:\n        \"\"\"Returns the set of protocols required by metrics but not provided by internal sources.\"\"\"\n        external = set()\n        for group in self.groups:\n            internal_provides = set()\n            for source in group.sources.values():\n                internal_provides |= source.provides\n\n            for metric in group.metrics:\n                external |= metric.requires - internal_provides\n        return external\n\n    def _eval_source(\n        self,\n        group: ParsedGroup,\n        source_name: str,\n        data: dict[str, Any],\n        global_cache: dict[str, dict[str, Any]],\n        _evaluating: set[str] | None = None,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Recursively evaluates a source and its dependencies, using a global cross-group cache.\n\n        Args:\n            group: The ParsedGroup containing the source.\n            source_name: The name of the source to evaluate within the group.\n            data: The input data to pass to the source.\n            global_cache: A dictionary caching results across all groups to prevent redundant computation.\n            _evaluating: Internal set used for cycle detection during recursive evaluation.\n\n        Returns:\n            A dictionary containing the source's provided protocols and their corresponding data.\n\n        Raises:\n            RuntimeError: If a cyclic dependency is detected between sources.\n            Exception: Re-raises any exception that occurs during source execution.\n        \"\"\"\n        source = group.sources[source_name]\n        h = source.config_hash\n\n        # Cross-group cache hit\n        if h in global_cache:\n            if isinstance(global_cache[h], Exception):\n                raise global_cache[h]\n            return global_cache[h]\n\n        if _evaluating is None:\n            _evaluating = set()\n\n        if source_name in _evaluating:\n            raise RuntimeError(\n                f\"Cyclic dependency detected for source '{source_name}' in group '{group.prefix}'. {_evaluating = }\"\n            )\n\n        _evaluating.add(source_name)\n\n        protocols_to_sources = group.protocols_to_sources\n\n        try:\n            # Evaluate dependencies first\n            deps_data: dict[str, dict[str, Any]] = {}\n\n            for req_protocol in source.requires:\n                try:\n                    providers = protocols_to_sources[req_protocol]\n                    if len(providers) == 0:\n                        raise ValueError(\n                            f\"No source provides the required protocol {req_protocol}\"\n                        )\n\n                    provider = providers[0]\n\n                    deps_data[req_protocol] = self._eval_source(\n                        group=group,\n                        source_name=provider,\n                        data=data,\n                        global_cache=global_cache,\n                        _evaluating=_evaluating,\n                    )[req_protocol]\n                except Exception as e:\n                    # If a dependency fails, mark this as failed too\n                    global_cache[h] = e\n                    raise e\n\n            # Evaluate this source\n            try:\n                result = source(deps_data, **data)\n                global_cache[h] = result\n                return result\n            except Exception as e:\n                global_cache[h] = e\n                raise e\n        finally:\n            _evaluating.remove(source_name)\n\n    def update(self, data: dict[str, Any], computed_data: dict[str, Any] | None = None):\n        \"\"\"Runs sources and metrics for a given batch.\n\n        Args:\n            data: Data dictionary containing inputs for sources (model, batch).\n            computed_data: Optional dictionary mapping protocol names to already computed data.\n                This allows reusing results (like logits) to avoid redundant forward passes.\n        \"\"\"\n        with meters_group(self.group_name, force_recreate=False) as should_log:\n            if not should_log:\n                return\n            # Global cache for the entire batch. Keys are source config hashes.\n            global_source_cache: dict[str, Any] = {}\n\n            # Seed cache with computed data if provided\n            computed_data = computed_data or {}\n\n            for group in self.groups:\n                protocols_to_sources = group.protocols_to_sources\n\n                for i, metric in enumerate(group.metrics):\n                    metric_name = metric.nested_name or (\n                        getattr(metric.cfg, \"_name\", f\"{i}\") + \"_metric\"\n                    )\n\n                    sources_data: dict[str, dict[str, Any]] = {}\n                    execution_failed = False\n\n                    for req_protocol in metric.requires:\n                        # 1. Try precomputed data first\n                        if req_protocol in computed_data:\n                            sources_data[req_protocol] = computed_data[req_protocol]\n                            continue\n\n                        # 2. Fallback to source evaluation\n                        try:\n                            providers = protocols_to_sources[req_protocol]\n                            if len(providers) == 0:\n                                raise ValueError(\n                                    f\"No source provides the required protocol {req_protocol}\"\n                                )\n\n                            provider = providers[0]\n                            sources_data[req_protocol] = self._eval_source(\n                                group=group,\n                                source_name=provider,\n                                data=data,\n                                global_cache=global_source_cache,\n                            )[req_protocol]\n                        except Exception as e:\n                            logger.exception(\n                                f\"Source execution failed for the metric {metric} in group '{group.prefix}': {e}\"\n                            )\n                            execution_failed = True\n                            break\n\n                    if execution_failed:\n                        continue\n\n                    try:\n                        batch_results = metric(sources_data)\n                    except Exception as e:\n                        logger.exception(\n                            f\"Metric computation failed for '{metric_name}' in group '{group.prefix}': {e}\"\n                        )\n                        continue\n\n                    for sub_name, log_kwargs in batch_results.items():\n                        is_internal = sub_name.startswith(\"_\")\n                        base_name = (\n                            f\"{metric_name}/{sub_name}\"\n                            if metric_name != sub_name\n                            else metric_name\n                        )\n                        full_name = (\n                            f\"{group.prefix}/{base_name}\" if group.prefix else base_name\n                        )\n\n                        if is_internal:\n                            full_name = f\"_internal/{full_name}\"\n\n                        acc_type = metric.accumulators.get(sub_name)\n                        if acc_type is None:\n                            logger.warning(\n                                f\"No accumulator defined for sub-metric '{sub_name}' in metric '{metric_name}'. Skipping.\"\n                            )\n                            continue\n\n                        factory = self._get_accumulator_factory(acc_type)\n\n                        log_meter(\n                            name=full_name,\n                            meter_factory=factory,\n                            **log_kwargs,\n                        )\n\n    def _get_accumulator_factory(self, acc_type: str) -&gt; Callable[[], BaseMeter]:\n        if acc_type == \"average\":\n            return lambda: AverageMeter()\n        if acc_type == \"sum\":\n            return lambda: SummedMeter()\n        if acc_type == \"gather\":\n            return lambda: GatherMeter()\n        if acc_type == \"perplexity\":\n            return lambda: AveragedExponentMeter()\n        raise ValueError(f\"Unknown accumulator type: {acc_type}\")\n\n    def compute(self, raw_results: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Runs finalization logic for each metric based on raw computed results.\"\"\"\n        final_report: dict[str, Any] = {}\n\n        for group in self.groups:\n            for i, metric in enumerate(group.metrics):\n                metric_name = metric.nested_name or (\n                    getattr(metric.cfg, \"_name\", f\"{i}\") + \"_metric\"\n                )\n\n                acc_data: dict[str, Any] = {}\n                # Use metric.accumulators if provided, otherwise fallback to metric's defaults\n                expected_keys = metric.accumulators.keys()\n\n                for sub_name in expected_keys:\n                    is_internal = sub_name.startswith(\"_\")\n                    base_name = (\n                        f\"{metric_name}/{sub_name}\"\n                        if metric_name != sub_name\n                        else metric_name\n                    )\n                    full_name = (\n                        f\"{group.prefix}/{base_name}\" if group.prefix else base_name\n                    )\n\n                    if is_internal:\n                        full_name = f\"_internal/{full_name}\"\n\n                    if full_name in raw_results:\n                        acc_data[sub_name] = raw_results.pop(full_name)\n\n                if not acc_data:\n                    continue\n\n                try:\n                    finalized = metric.finalize(acc_data)\n                except Exception as e:\n                    logger.exception(\n                        f\"Metric finalization failed for '{metric_name}' in group '{group.prefix}': {e}\"\n                    )\n                    continue\n\n                for k, v in finalized.items():\n                    if k.startswith(\"_\"):\n                        continue\n\n                    base_name = (\n                        f\"{metric_name}/{k}\" if k != metric_name else metric_name\n                    )\n                    full_name = (\n                        f\"{group.prefix}/{base_name}\" if group.prefix else base_name\n                    )\n                    final_report[full_name] = v\n\n        return raw_results | final_report\n</code></pre>"},{"location":"reference/modules/metrics/engine/#optimus_dl.modules.metrics.engine.MetricEngine.required_external_protocols","title":"<code>required_external_protocols</code>  <code>property</code>","text":"<p>Returns the set of protocols required by metrics but not provided by internal sources.</p>"},{"location":"reference/modules/metrics/engine/#optimus_dl.modules.metrics.engine.MetricEngine.__init__","title":"<code>__init__(group_name, configs)</code>","text":"<p>Initializes the MetricEngine.</p> <p>Parameters:</p> Name Type Description Default <code>group_name</code> <code>str</code> <p>The name of the <code>MeterGroup</code> (logging namespace).</p> required <code>configs</code> <code>list[dict[str, Any]]</code> <p>A list of configuration dicts.</p> required Source code in <code>optimus_dl/modules/metrics/engine.py</code> <pre><code>def __init__(\n    self,\n    group_name: str,\n    configs: list[dict[str, Any]],\n):\n    \"\"\"Initializes the MetricEngine.\n\n    Args:\n        group_name: The name of the `MeterGroup` (logging namespace).\n        configs: A list of configuration dicts.\n    \"\"\"\n    self.group_name = group_name\n    self.groups: list[ParsedGroup] = []\n\n    self._parse_and_validate_configs(configs)\n</code></pre>"},{"location":"reference/modules/metrics/engine/#optimus_dl.modules.metrics.engine.MetricEngine.compute","title":"<code>compute(raw_results)</code>","text":"<p>Runs finalization logic for each metric based on raw computed results.</p> Source code in <code>optimus_dl/modules/metrics/engine.py</code> <pre><code>def compute(self, raw_results: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Runs finalization logic for each metric based on raw computed results.\"\"\"\n    final_report: dict[str, Any] = {}\n\n    for group in self.groups:\n        for i, metric in enumerate(group.metrics):\n            metric_name = metric.nested_name or (\n                getattr(metric.cfg, \"_name\", f\"{i}\") + \"_metric\"\n            )\n\n            acc_data: dict[str, Any] = {}\n            # Use metric.accumulators if provided, otherwise fallback to metric's defaults\n            expected_keys = metric.accumulators.keys()\n\n            for sub_name in expected_keys:\n                is_internal = sub_name.startswith(\"_\")\n                base_name = (\n                    f\"{metric_name}/{sub_name}\"\n                    if metric_name != sub_name\n                    else metric_name\n                )\n                full_name = (\n                    f\"{group.prefix}/{base_name}\" if group.prefix else base_name\n                )\n\n                if is_internal:\n                    full_name = f\"_internal/{full_name}\"\n\n                if full_name in raw_results:\n                    acc_data[sub_name] = raw_results.pop(full_name)\n\n            if not acc_data:\n                continue\n\n            try:\n                finalized = metric.finalize(acc_data)\n            except Exception as e:\n                logger.exception(\n                    f\"Metric finalization failed for '{metric_name}' in group '{group.prefix}': {e}\"\n                )\n                continue\n\n            for k, v in finalized.items():\n                if k.startswith(\"_\"):\n                    continue\n\n                base_name = (\n                    f\"{metric_name}/{k}\" if k != metric_name else metric_name\n                )\n                full_name = (\n                    f\"{group.prefix}/{base_name}\" if group.prefix else base_name\n                )\n                final_report[full_name] = v\n\n    return raw_results | final_report\n</code></pre>"},{"location":"reference/modules/metrics/engine/#optimus_dl.modules.metrics.engine.MetricEngine.update","title":"<code>update(data, computed_data=None)</code>","text":"<p>Runs sources and metrics for a given batch.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Data dictionary containing inputs for sources (model, batch).</p> required <code>computed_data</code> <code>dict[str, Any] | None</code> <p>Optional dictionary mapping protocol names to already computed data. This allows reusing results (like logits) to avoid redundant forward passes.</p> <code>None</code> Source code in <code>optimus_dl/modules/metrics/engine.py</code> <pre><code>def update(self, data: dict[str, Any], computed_data: dict[str, Any] | None = None):\n    \"\"\"Runs sources and metrics for a given batch.\n\n    Args:\n        data: Data dictionary containing inputs for sources (model, batch).\n        computed_data: Optional dictionary mapping protocol names to already computed data.\n            This allows reusing results (like logits) to avoid redundant forward passes.\n    \"\"\"\n    with meters_group(self.group_name, force_recreate=False) as should_log:\n        if not should_log:\n            return\n        # Global cache for the entire batch. Keys are source config hashes.\n        global_source_cache: dict[str, Any] = {}\n\n        # Seed cache with computed data if provided\n        computed_data = computed_data or {}\n\n        for group in self.groups:\n            protocols_to_sources = group.protocols_to_sources\n\n            for i, metric in enumerate(group.metrics):\n                metric_name = metric.nested_name or (\n                    getattr(metric.cfg, \"_name\", f\"{i}\") + \"_metric\"\n                )\n\n                sources_data: dict[str, dict[str, Any]] = {}\n                execution_failed = False\n\n                for req_protocol in metric.requires:\n                    # 1. Try precomputed data first\n                    if req_protocol in computed_data:\n                        sources_data[req_protocol] = computed_data[req_protocol]\n                        continue\n\n                    # 2. Fallback to source evaluation\n                    try:\n                        providers = protocols_to_sources[req_protocol]\n                        if len(providers) == 0:\n                            raise ValueError(\n                                f\"No source provides the required protocol {req_protocol}\"\n                            )\n\n                        provider = providers[0]\n                        sources_data[req_protocol] = self._eval_source(\n                            group=group,\n                            source_name=provider,\n                            data=data,\n                            global_cache=global_source_cache,\n                        )[req_protocol]\n                    except Exception as e:\n                        logger.exception(\n                            f\"Source execution failed for the metric {metric} in group '{group.prefix}': {e}\"\n                        )\n                        execution_failed = True\n                        break\n\n                if execution_failed:\n                    continue\n\n                try:\n                    batch_results = metric(sources_data)\n                except Exception as e:\n                    logger.exception(\n                        f\"Metric computation failed for '{metric_name}' in group '{group.prefix}': {e}\"\n                    )\n                    continue\n\n                for sub_name, log_kwargs in batch_results.items():\n                    is_internal = sub_name.startswith(\"_\")\n                    base_name = (\n                        f\"{metric_name}/{sub_name}\"\n                        if metric_name != sub_name\n                        else metric_name\n                    )\n                    full_name = (\n                        f\"{group.prefix}/{base_name}\" if group.prefix else base_name\n                    )\n\n                    if is_internal:\n                        full_name = f\"_internal/{full_name}\"\n\n                    acc_type = metric.accumulators.get(sub_name)\n                    if acc_type is None:\n                        logger.warning(\n                            f\"No accumulator defined for sub-metric '{sub_name}' in metric '{metric_name}'. Skipping.\"\n                        )\n                        continue\n\n                    factory = self._get_accumulator_factory(acc_type)\n\n                    log_meter(\n                        name=full_name,\n                        meter_factory=factory,\n                        **log_kwargs,\n                    )\n</code></pre>"},{"location":"reference/modules/metrics/engine/#optimus_dl.modules.metrics.engine.ParsedGroup","title":"<code>ParsedGroup</code>","text":"<p>Internal representation of a validated metric group.</p> Source code in <code>optimus_dl/modules/metrics/engine.py</code> <pre><code>class ParsedGroup:\n    \"\"\"Internal representation of a validated metric group.\"\"\"\n\n    def __init__(self, prefix: str):\n        self.prefix = prefix\n        self.sources: dict[str, MetricSource] = {}\n        self.metrics: list[Metric] = []\n\n    @property\n    def protocols_to_sources(self):\n        protocols_to_sources = defaultdict(list)\n        for source_name, source in self.sources.items():\n            for protocol in source.provides:\n                protocols_to_sources[protocol].append(source_name)\n        return {\n            protocol: list(sources)\n            for protocol, sources in protocols_to_sources.items()\n        }\n</code></pre>"},{"location":"reference/modules/metrics/metrics/","title":"metrics","text":""},{"location":"reference/modules/metrics/metrics/#optimus_dl.modules.metrics.metrics","title":"<code>optimus_dl.modules.metrics.metrics</code>","text":""},{"location":"reference/modules/metrics/metrics/#optimus_dl.modules.metrics.metrics.AccuracyMetric","title":"<code>AccuracyMetric</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes Top-1 accuracy for classification tasks.</p> Source code in <code>optimus_dl/modules/metrics/metrics.py</code> <pre><code>@register_metric(\"accuracy\", AccuracyMetricConfig)\nclass AccuracyMetric(Metric):\n    \"\"\"Computes Top-1 accuracy for classification tasks.\"\"\"\n\n    @property\n    def requires(self) -&gt; set[str]:\n        return {StandardProtocols.CLASSIFICATION}\n\n    @property\n    def accumulators(self) -&gt; dict[str, str]:\n        return {\"accuracy\": \"average\", \"total\": \"sum\"}\n\n    def __call__(\n        self, sources_data: dict[str, dict[str, Any]]\n    ) -&gt; dict[str, dict[str, Any]]:\n        data = sources_data.get(StandardProtocols.CLASSIFICATION)\n        if not data:\n            return {}\n\n        preds = data[\"predictions\"]\n        targets = data[\"targets\"]\n        mask = data.get(\"mask\")\n\n        correct = preds == targets\n        if mask is not None:\n            correct = correct &amp; mask\n            total = mask.sum().item()\n        else:\n            total = targets.numel()\n\n        if total == 0:\n            return {}\n\n        return {\n            \"accuracy\": {\"value\": correct.sum().item() / total, \"weight\": total},\n            \"total\": {\"value\": total},\n        }\n</code></pre>"},{"location":"reference/modules/metrics/metrics/#optimus_dl.modules.metrics.metrics.AccuracyMetricConfig","title":"<code>AccuracyMetricConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricConfig</code></p> <p>AccuracyMetricConfig(_name: 'str' = 'accuracy', nested_name: 'str | None' = None)</p> Source code in <code>optimus_dl/modules/metrics/metrics.py</code> <pre><code>@dataclass\nclass AccuracyMetricConfig(MetricConfig):\n    _name: str = \"accuracy\"\n</code></pre>"},{"location":"reference/modules/metrics/metrics/#optimus_dl.modules.metrics.metrics.LossMetric","title":"<code>LossMetric</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Simple wrapper to report loss via MetricEngine.</p> Source code in <code>optimus_dl/modules/metrics/metrics.py</code> <pre><code>@register_metric(\"loss\", LossMetricConfig)\nclass LossMetric(Metric):\n    \"\"\"Simple wrapper to report loss via MetricEngine.\"\"\"\n\n    @property\n    def requires(self) -&gt; set[str]:\n        return {StandardProtocols.LOSS}\n\n    @property\n    def accumulators(self) -&gt; dict[str, str]:\n        return {\"loss\": \"average\"}\n\n    def __call__(\n        self, sources_data: dict[str, dict[str, Any]]\n    ) -&gt; dict[str, dict[str, Any]]:\n        loss = sources_data.get(StandardProtocols.LOSS)\n        if loss is None:\n            return {}\n\n        weight = 1.0\n        classif = sources_data.get(StandardProtocols.CLASSIFICATION)\n        if classif and \"mask\" in classif:\n            weight = classif[\"mask\"].sum().item()\n\n        return {\n            \"loss\": {\n                \"value\": loss.item() if hasattr(loss, \"item\") else loss,\n                \"weight\": weight,\n            }\n        }\n</code></pre>"},{"location":"reference/modules/metrics/metrics/#optimus_dl.modules.metrics.metrics.LossMetricConfig","title":"<code>LossMetricConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricConfig</code></p> <p>LossMetricConfig(_name: 'str' = 'loss', nested_name: 'str | None' = None)</p> Source code in <code>optimus_dl/modules/metrics/metrics.py</code> <pre><code>@dataclass\nclass LossMetricConfig(MetricConfig):\n    _name: str = \"loss\"\n</code></pre>"},{"location":"reference/modules/metrics/metrics/#optimus_dl.modules.metrics.metrics.Metric","title":"<code>Metric</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Stateless definition for computing metrics from model/source data.</p> <p>A Metric implementation defines: - What data it requires via <code>requires</code> mapping (Role -&gt; set of protocol strings). - How to calculate raw results for a batch, potentially emitting multiple sub-values. - How to finalize those values after they've been aggregated (e.g., F1 from counts).</p> Source code in <code>optimus_dl/modules/metrics/metrics.py</code> <pre><code>class Metric(ABC):\n    \"\"\"Stateless definition for computing metrics from model/source data.\n\n    A Metric implementation defines:\n    - What data it requires via `requires` mapping (Role -&gt; set of protocol strings).\n    - How to calculate raw results for a batch, potentially emitting multiple sub-values.\n    - How to finalize those values after they've been aggregated (e.g., F1 from counts).\n    \"\"\"\n\n    def __init__(self, cfg: MetricConfig):\n        self.cfg = cfg\n        self.nested_name = cfg.nested_name\n\n    @property\n    @abstractmethod\n    def requires(self) -&gt; set[str]:\n        \"\"\"Mapping from source role name to a set of required protocol strings.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def accumulators(self) -&gt; dict[str, str]:\n        \"\"\"Define how each sub-metric should be aggregated across batches.\n\n        Returns a mapping from sub-metric names to accumulator types\n        (e.g., 'average', 'sum', 'gather', 'perplexity').\n        \"\"\"\n        return {self.cfg._name: \"average\"}\n\n    @abstractmethod\n    def __call__(\n        self, sources_data: dict[str, dict[str, Any]]\n    ) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"Compute raw metric values for the batch.\n\n        Args:\n            batch: The original batch dictionary.\n            sources_data: Protocol string -&gt; data.\n\n        Returns:\n            Dict mapping sub-metric names to log kwargs (e.g., {'value': ..., 'weight': ...})\n            for accumulators.\n        \"\"\"\n        raise NotImplementedError\n\n    def finalize(self, aggregated_data: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Perform final calculations on aggregated data.\n\n        Args:\n            aggregated_data: Dict mapping sub-metric names to their\n                computed/aggregated values from accumulators.\n\n        Returns:\n            Dict of final metrics to be logged/reported.\n        \"\"\"\n        return {k: v for k, v in aggregated_data.items() if not k.startswith(\"_\")}\n</code></pre>"},{"location":"reference/modules/metrics/metrics/#optimus_dl.modules.metrics.metrics.Metric.accumulators","title":"<code>accumulators</code>  <code>property</code>","text":"<p>Define how each sub-metric should be aggregated across batches.</p> <p>Returns a mapping from sub-metric names to accumulator types (e.g., 'average', 'sum', 'gather', 'perplexity').</p>"},{"location":"reference/modules/metrics/metrics/#optimus_dl.modules.metrics.metrics.Metric.requires","title":"<code>requires</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Mapping from source role name to a set of required protocol strings.</p>"},{"location":"reference/modules/metrics/metrics/#optimus_dl.modules.metrics.metrics.Metric.__call__","title":"<code>__call__(sources_data)</code>  <code>abstractmethod</code>","text":"<p>Compute raw metric values for the batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <p>The original batch dictionary.</p> required <code>sources_data</code> <code>dict[str, dict[str, Any]]</code> <p>Protocol string -&gt; data.</p> required <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>Dict mapping sub-metric names to log kwargs (e.g., {'value': ..., 'weight': ...})</p> <code>dict[str, dict[str, Any]]</code> <p>for accumulators.</p> Source code in <code>optimus_dl/modules/metrics/metrics.py</code> <pre><code>@abstractmethod\ndef __call__(\n    self, sources_data: dict[str, dict[str, Any]]\n) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"Compute raw metric values for the batch.\n\n    Args:\n        batch: The original batch dictionary.\n        sources_data: Protocol string -&gt; data.\n\n    Returns:\n        Dict mapping sub-metric names to log kwargs (e.g., {'value': ..., 'weight': ...})\n        for accumulators.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/modules/metrics/metrics/#optimus_dl.modules.metrics.metrics.Metric.finalize","title":"<code>finalize(aggregated_data)</code>","text":"<p>Perform final calculations on aggregated data.</p> <p>Parameters:</p> Name Type Description Default <code>aggregated_data</code> <code>dict[str, Any]</code> <p>Dict mapping sub-metric names to their computed/aggregated values from accumulators.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict of final metrics to be logged/reported.</p> Source code in <code>optimus_dl/modules/metrics/metrics.py</code> <pre><code>def finalize(self, aggregated_data: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Perform final calculations on aggregated data.\n\n    Args:\n        aggregated_data: Dict mapping sub-metric names to their\n            computed/aggregated values from accumulators.\n\n    Returns:\n        Dict of final metrics to be logged/reported.\n    \"\"\"\n    return {k: v for k, v in aggregated_data.items() if not k.startswith(\"_\")}\n</code></pre>"},{"location":"reference/modules/metrics/metrics/#optimus_dl.modules.metrics.metrics.MetricConfig","title":"<code>MetricConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>MetricConfig(_name: str | None = None, nested_name: 'str | None' = None)</p> <p>Parameters:</p> Name Type Description Default <code>nested_name</code> <code>str | None</code> <p>Optional name to nest this metric under in the metrics tree.</p> <code>None</code> Source code in <code>optimus_dl/modules/metrics/metrics.py</code> <pre><code>@dataclass\nclass MetricConfig(RegistryConfigStrict):\n    nested_name: str | None = field(\n        default=None,\n        metadata={\n            \"description\": \"Optional name to nest this metric under in the metrics tree.\"\n        },\n    )\n</code></pre>"},{"location":"reference/modules/metrics/metrics/#optimus_dl.modules.metrics.metrics.PerplexityMetric","title":"<code>PerplexityMetric</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes perplexity (exp(loss)).</p> Source code in <code>optimus_dl/modules/metrics/metrics.py</code> <pre><code>@register_metric(\"perplexity\", PerplexityMetricConfig)\nclass PerplexityMetric(Metric):\n    \"\"\"Computes perplexity (exp(loss)).\"\"\"\n\n    @property\n    def requires(self) -&gt; set[str]:\n        return {StandardProtocols.LOSS}\n\n    @property\n    def accumulators(self) -&gt; dict[str, str]:\n        return {\"perplexity\": \"perplexity\"}\n\n    def __call__(\n        self, sources_data: dict[str, dict[str, Any]]\n    ) -&gt; dict[str, dict[str, Any]]:\n        loss = sources_data.get(StandardProtocols.LOSS)\n        if loss is None:\n            return {}\n\n        weight = 1.0\n        classif = sources_data.get(StandardProtocols.CLASSIFICATION)\n        if classif and \"mask\" in classif:\n            weight = classif[\"mask\"].sum().item()\n\n        return {\n            \"perplexity\": {\n                \"value\": loss.item() if hasattr(loss, \"item\") else loss,\n                \"weight\": weight,\n            }\n        }\n</code></pre>"},{"location":"reference/modules/metrics/metrics/#optimus_dl.modules.metrics.metrics.PerplexityMetricConfig","title":"<code>PerplexityMetricConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricConfig</code></p> <p>PerplexityMetricConfig(_name: 'str' = 'perplexity', nested_name: 'str | None' = None)</p> Source code in <code>optimus_dl/modules/metrics/metrics.py</code> <pre><code>@dataclass\nclass PerplexityMetricConfig(MetricConfig):\n    _name: str = \"perplexity\"\n</code></pre>"},{"location":"reference/modules/metrics/source/","title":"source","text":""},{"location":"reference/modules/metrics/source/#optimus_dl.modules.metrics.source","title":"<code>optimus_dl.modules.metrics.source</code>","text":""},{"location":"reference/modules/metrics/source/#optimus_dl.modules.metrics.source.MetricSource","title":"<code>MetricSource</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for data producers that extract information from the model.</p> Source code in <code>optimus_dl/modules/metrics/source.py</code> <pre><code>class MetricSource(ABC):\n    \"\"\"Base class for data producers that extract information from the model.\"\"\"\n\n    def __init__(self, cfg: MetricSourceConfig):\n        self.cfg = cfg\n        self._hash: str | None = None\n\n    @property\n    def config_hash(self) -&gt; str:\n        \"\"\"Returns a deterministic hash of the source's configuration for cross-group caching.\"\"\"\n        if self._hash is None:\n            import dataclasses\n\n            if dataclasses.is_dataclass(self.cfg):\n                cfg_dict = dataclasses.asdict(self.cfg)\n            else:\n                cfg_dict = (\n                    self.cfg.__dict__\n                    if hasattr(self.cfg, \"__dict__\")\n                    else str(self.cfg)\n                )\n\n            def make_hashable(obj: Any) -&gt; Any:\n                if isinstance(obj, (tuple, list)):\n                    return tuple(make_hashable(e) for e in obj)\n                if isinstance(obj, dict):\n                    return tuple(sorted((k, make_hashable(v)) for k, v in obj.items()))\n                return obj\n\n            stable_repr = str(make_hashable(cfg_dict))\n            # Include the class name so different source types with same config don't collide\n            stable_repr = f\"{self.__class__.__name__}:{stable_repr}\"\n            self._hash = hashlib.md5(stable_repr.encode()).hexdigest()\n        return self._hash\n\n    @property\n    @abstractmethod\n    def provides(self) -&gt; set[str]:\n        \"\"\"Returns the set of protocol strings this source provides.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def requires(self) -&gt; set[str]:\n        \"\"\"Mapping from internal dependency role name to required protocol strings.\n\n        Override this if your source depends on the output of other sources.\n        \"\"\"\n        return set()\n\n    @abstractmethod\n    def __call__(\n        self, dependencies: dict[str, dict[str, Any]], **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"Execute the source and return a dictionary mapping Protocol string to data.\n\n        Args:\n            dependencies: Data from required sources, mapped by protocol.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/modules/metrics/source/#optimus_dl.modules.metrics.source.MetricSource.config_hash","title":"<code>config_hash</code>  <code>property</code>","text":"<p>Returns a deterministic hash of the source's configuration for cross-group caching.</p>"},{"location":"reference/modules/metrics/source/#optimus_dl.modules.metrics.source.MetricSource.provides","title":"<code>provides</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the set of protocol strings this source provides.</p>"},{"location":"reference/modules/metrics/source/#optimus_dl.modules.metrics.source.MetricSource.requires","title":"<code>requires</code>  <code>property</code>","text":"<p>Mapping from internal dependency role name to required protocol strings.</p> <p>Override this if your source depends on the output of other sources.</p>"},{"location":"reference/modules/metrics/source/#optimus_dl.modules.metrics.source.MetricSource.__call__","title":"<code>__call__(dependencies, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Execute the source and return a dictionary mapping Protocol string to data.</p> <p>Parameters:</p> Name Type Description Default <code>dependencies</code> <code>dict[str, dict[str, Any]]</code> <p>Data from required sources, mapped by protocol.</p> required Source code in <code>optimus_dl/modules/metrics/source.py</code> <pre><code>@abstractmethod\ndef __call__(\n    self, dependencies: dict[str, dict[str, Any]], **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"Execute the source and return a dictionary mapping Protocol string to data.\n\n    Args:\n        dependencies: Data from required sources, mapped by protocol.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/modules/metrics/source/#optimus_dl.modules.metrics.source.MetricSourceConfig","title":"<code>MetricSourceConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Base configuration for metric sources.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>dependencies</code> <code>dict[str, str]</code> <p>dict() -&gt; new empty dictionary dict(mapping) -&gt; new dictionary initialized from a mapping object's     (key, value) pairs dict(iterable) -&gt; new dictionary initialized as if via:     d = {}     for k, v in iterable:         d[k] = v dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs     in the keyword argument list.  For example:  dict(one=1, two=2)</p> <code>&lt;class 'dict'&gt;</code> Source code in <code>optimus_dl/modules/metrics/source.py</code> <pre><code>@dataclass\nclass MetricSourceConfig(RegistryConfigStrict):\n    \"\"\"Base configuration for metric sources.\n\n    Attributes:\n        dependencies: Maps internal role requirements to source names within the group.\n    \"\"\"\n\n    dependencies: dict[str, str] = field(default_factory=dict)\n</code></pre>"},{"location":"reference/modules/metrics/source/#optimus_dl.modules.metrics.source.StandardProtocols","title":"<code>StandardProtocols</code>","text":"<p>Standardized string constants for common metric data protocols.</p> Source code in <code>optimus_dl/modules/metrics/source.py</code> <pre><code>class StandardProtocols:\n    \"\"\"Standardized string constants for common metric data protocols.\"\"\"\n\n    LOGITS = \"logits\"\n    LOSS = \"loss\"\n    GENERATED_TOKENS = \"generated_tokens\"\n    CLASSIFICATION = \"classification\"\n</code></pre>"},{"location":"reference/modules/metrics/sources/","title":"Index","text":""},{"location":"reference/modules/metrics/sources/#optimus_dl.modules.metrics.sources","title":"<code>optimus_dl.modules.metrics.sources</code>","text":""},{"location":"reference/modules/metrics/sources/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>causal_lm</code>: Configuration for CausalLMSource.</li> <li><code>generation</code>: Configuration for GenerationSource.</li> </ul>"},{"location":"reference/modules/metrics/sources/causal_lm/","title":"causal_lm","text":""},{"location":"reference/modules/metrics/sources/causal_lm/#optimus_dl.modules.metrics.sources.causal_lm","title":"<code>optimus_dl.modules.metrics.sources.causal_lm</code>","text":""},{"location":"reference/modules/metrics/sources/causal_lm/#optimus_dl.modules.metrics.sources.causal_lm.CausalLMSource","title":"<code>CausalLMSource</code>","text":"<p>               Bases: <code>MetricSource</code></p> <p>Source for Causal LM that extracts logits and labels from the model and batch.</p> Source code in <code>optimus_dl/modules/metrics/sources/causal_lm.py</code> <pre><code>@register_metric_source(\"causal_lm\", CausalLMSourceConfig)\nclass CausalLMSource(MetricSource):\n    \"\"\"Source for Causal LM that extracts logits and labels from the model and batch.\"\"\"\n\n    cfg: CausalLMSourceConfig\n\n    @property\n    def provides(self) -&gt; set[str]:\n        return {StandardProtocols.LOGITS, StandardProtocols.CLASSIFICATION}\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        dependencies: dict[str, dict[str, Any]],\n        model: Any,\n        batch: Any,\n        **kwargs: Any,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Execute the source.\n\n        Args:\n            dependencies: Data from required sources (none for this source).\n            model: The model to run forward pass on.\n            batch: The input batch, expected to contain 'input_ids'.\n            **kwargs: Additional arguments (like criterion if needed).\n        \"\"\"\n        batch = copy.copy(batch)\n        input_ids = batch.pop(\"input_ids\")\n        batch[\"input_ids\"] = input_ids[:, :-1]\n\n        output = model(**batch)\n\n        targets = input_ids[:, 1:]\n\n        # Handle different output types (dict, Namespace, or raw Tensor)\n        if isinstance(output, dict):\n            logits = output.get(\"logits\")\n        elif hasattr(output, \"logits\"):\n            logits = output.logits\n        else:\n            logits = output  # Assume it's the logits tensor\n\n        mask = targets != self.cfg.padding_token_id\n        if \"seq_lens\" in batch:\n            mask = mask &amp; (\n                torch.arange(mask.shape[1], device=mask.device)\n                &lt; batch[\"seq_lens\"][:, None]\n            )\n\n        classification = dict(\n            predictions=logits.argmax(dim=-1),\n            targets=targets,\n            mask=mask,\n        )\n\n        return {\n            StandardProtocols.LOGITS: logits,\n            StandardProtocols.CLASSIFICATION: classification,\n        }\n</code></pre>"},{"location":"reference/modules/metrics/sources/causal_lm/#optimus_dl.modules.metrics.sources.causal_lm.CausalLMSource.__call__","title":"<code>__call__(dependencies, model, batch, **kwargs)</code>","text":"<p>Execute the source.</p> <p>Parameters:</p> Name Type Description Default <code>dependencies</code> <code>dict[str, dict[str, Any]]</code> <p>Data from required sources (none for this source).</p> required <code>model</code> <code>Any</code> <p>The model to run forward pass on.</p> required <code>batch</code> <code>Any</code> <p>The input batch, expected to contain 'input_ids'.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments (like criterion if needed).</p> <code>{}</code> Source code in <code>optimus_dl/modules/metrics/sources/causal_lm.py</code> <pre><code>@torch.no_grad()\ndef __call__(\n    self,\n    dependencies: dict[str, dict[str, Any]],\n    model: Any,\n    batch: Any,\n    **kwargs: Any,\n) -&gt; dict[str, Any]:\n    \"\"\"Execute the source.\n\n    Args:\n        dependencies: Data from required sources (none for this source).\n        model: The model to run forward pass on.\n        batch: The input batch, expected to contain 'input_ids'.\n        **kwargs: Additional arguments (like criterion if needed).\n    \"\"\"\n    batch = copy.copy(batch)\n    input_ids = batch.pop(\"input_ids\")\n    batch[\"input_ids\"] = input_ids[:, :-1]\n\n    output = model(**batch)\n\n    targets = input_ids[:, 1:]\n\n    # Handle different output types (dict, Namespace, or raw Tensor)\n    if isinstance(output, dict):\n        logits = output.get(\"logits\")\n    elif hasattr(output, \"logits\"):\n        logits = output.logits\n    else:\n        logits = output  # Assume it's the logits tensor\n\n    mask = targets != self.cfg.padding_token_id\n    if \"seq_lens\" in batch:\n        mask = mask &amp; (\n            torch.arange(mask.shape[1], device=mask.device)\n            &lt; batch[\"seq_lens\"][:, None]\n        )\n\n    classification = dict(\n        predictions=logits.argmax(dim=-1),\n        targets=targets,\n        mask=mask,\n    )\n\n    return {\n        StandardProtocols.LOGITS: logits,\n        StandardProtocols.CLASSIFICATION: classification,\n    }\n</code></pre>"},{"location":"reference/modules/metrics/sources/causal_lm/#optimus_dl.modules.metrics.sources.causal_lm.CausalLMSourceConfig","title":"<code>CausalLMSourceConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricSourceConfig</code></p> <p>Configuration for CausalLMSource.</p> <p>Parameters:</p> Name Type Description Default <code>padding_token_id</code> <code>int</code> <code>-100</code> Source code in <code>optimus_dl/modules/metrics/sources/causal_lm.py</code> <pre><code>@dataclass\nclass CausalLMSourceConfig(MetricSourceConfig):\n    \"\"\"Configuration for CausalLMSource.\"\"\"\n\n    _name: str = \"causal_lm\"\n    padding_token_id: int = -100\n</code></pre>"},{"location":"reference/modules/metrics/sources/generation/","title":"generation","text":""},{"location":"reference/modules/metrics/sources/generation/#optimus_dl.modules.metrics.sources.generation","title":"<code>optimus_dl.modules.metrics.sources.generation</code>","text":""},{"location":"reference/modules/metrics/sources/generation/#optimus_dl.modules.metrics.sources.generation.GenerationSource","title":"<code>GenerationSource</code>","text":"<p>               Bases: <code>MetricSource</code></p> <p>Source that generates new tokens from a model given a prompt.</p> <p>Supports greedy search and various sampling techniques (temperature, top-k, top-p). The generated sequences are provided under the 'generated_tokens' protocol.</p> Source code in <code>optimus_dl/modules/metrics/sources/generation.py</code> <pre><code>@register_metric_source(\"generation\", GenerationSourceConfig)\nclass GenerationSource(MetricSource):\n    \"\"\"Source that generates new tokens from a model given a prompt.\n\n    Supports greedy search and various sampling techniques (temperature, top-k, top-p).\n    The generated sequences are provided under the 'generated_tokens' protocol.\n    \"\"\"\n\n    cfg: GenerationSourceConfig\n\n    def __init__(self, cfg: GenerationSourceConfig):\n        super().__init__(cfg)\n\n    @property\n    def provides(self) -&gt; set[str]:\n        return {StandardProtocols.GENERATED_TOKENS}\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        dependencies: dict[str, dict[str, Any]],\n        model: Any,\n        batch: Any,\n        **kwargs: Any,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Execute generation.\n\n        Args:\n            dependencies: Data from required sources (none).\n            model: The model to generate from.\n            batch: Input batch, expected to contain 'input_ids'.\n            **kwargs: Additional arguments.\n        \"\"\"\n        batch = copy.copy(batch)\n        input_ids = batch.get(\"input_ids\")\n        if input_ids is None and hasattr(batch, \"input_ids\"):\n            input_ids = batch.input_ids\n\n        if input_ids is None:\n            raise ValueError(\"GenerationSource requires 'input_ids' in the batch\")\n\n        # Start generation\n        batch_size = input_ids.size(0)\n        device = input_ids.device\n        curr_ids = input_ids\n\n        # Track finished sequences\n        finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n\n        # Use eos_token_id or a default (0) for padding finished sequences\n        pad_token_id = self.cfg.eos_token_id if self.cfg.eos_token_id is not None else 0\n\n        # Store only the new tokens\n        generated = []\n\n        for _ in range(self.cfg.max_new_tokens):\n            # If all sequences are finished, we can stop early\n            if self.cfg.eos_token_id is not None and finished.all():\n                break\n\n            # Forward pass to get logits for the last token\n            # Note: We update batch input_ids for models that might use other batch info\n            batch[\"input_ids\"] = curr_ids\n            outputs = model(**batch)\n\n            if isinstance(outputs, dict):\n                logits = outputs.get(\"logits\")\n            elif hasattr(outputs, \"logits\"):\n                logits = outputs.logits\n            else:\n                logits = outputs\n\n            # Take logits for the last position: [B, T, V] -&gt; [B, V]\n            next_token_logits = logits[:, -1, :]\n\n            if not self.cfg.do_sample:\n                # Greedy search\n                next_tokens = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n            else:\n                # Sampling logic\n                if self.cfg.temperature != 1.0:\n                    next_token_logits = next_token_logits / self.cfg.temperature\n\n                if self.cfg.top_k is not None:\n                    v, _ = torch.topk(\n                        next_token_logits,\n                        min(self.cfg.top_k, next_token_logits.size(-1)),\n                    )\n                    next_token_logits[next_token_logits &lt; v[:, [-1]]] = -float(\"Inf\")\n\n                if self.cfg.top_p is not None:\n                    sorted_logits, sorted_indices = torch.sort(\n                        next_token_logits, descending=True\n                    )\n                    cumulative_probs = torch.cumsum(\n                        F.softmax(sorted_logits, dim=-1), dim=-1\n                    )\n\n                    # Remove tokens with cumulative probability above the threshold\n                    sorted_indices_to_remove = cumulative_probs &gt; self.cfg.top_p\n                    # Shift the indices to the right to keep also the first token above the threshold\n                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n                        ..., :-1\n                    ].clone()\n                    sorted_indices_to_remove[..., 0] = 0\n\n                    # scatter sorted tensors to original indexing\n                    indices_to_remove = sorted_indices_to_remove.scatter(\n                        1, sorted_indices, sorted_indices_to_remove\n                    )\n                    next_token_logits[indices_to_remove] = -float(\"Inf\")\n\n                probs = F.softmax(next_token_logits, dim=-1)\n                next_tokens = torch.multinomial(probs, num_samples=1)\n\n            # Mask out tokens for finished sequences\n            if self.cfg.eos_token_id is not None:\n                next_tokens = torch.where(\n                    finished.unsqueeze(1),\n                    torch.tensor(pad_token_id, device=device),\n                    next_tokens,\n                )\n                # Update finished mask\n                finished |= next_tokens.squeeze(1) == self.cfg.eos_token_id\n\n            # Append generated tokens\n            generated.append(next_tokens)\n            curr_ids = torch.cat([curr_ids, next_tokens], dim=1)\n\n        return {\n            StandardProtocols.GENERATED_TOKENS: (\n                torch.cat(generated, dim=1)\n                if generated\n                else torch.empty((batch_size, 0), dtype=torch.long, device=device)\n            )\n        }\n</code></pre>"},{"location":"reference/modules/metrics/sources/generation/#optimus_dl.modules.metrics.sources.generation.GenerationSource.__call__","title":"<code>__call__(dependencies, model, batch, **kwargs)</code>","text":"<p>Execute generation.</p> <p>Parameters:</p> Name Type Description Default <code>dependencies</code> <code>dict[str, dict[str, Any]]</code> <p>Data from required sources (none).</p> required <code>model</code> <code>Any</code> <p>The model to generate from.</p> required <code>batch</code> <code>Any</code> <p>Input batch, expected to contain 'input_ids'.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>optimus_dl/modules/metrics/sources/generation.py</code> <pre><code>@torch.no_grad()\ndef __call__(\n    self,\n    dependencies: dict[str, dict[str, Any]],\n    model: Any,\n    batch: Any,\n    **kwargs: Any,\n) -&gt; dict[str, Any]:\n    \"\"\"Execute generation.\n\n    Args:\n        dependencies: Data from required sources (none).\n        model: The model to generate from.\n        batch: Input batch, expected to contain 'input_ids'.\n        **kwargs: Additional arguments.\n    \"\"\"\n    batch = copy.copy(batch)\n    input_ids = batch.get(\"input_ids\")\n    if input_ids is None and hasattr(batch, \"input_ids\"):\n        input_ids = batch.input_ids\n\n    if input_ids is None:\n        raise ValueError(\"GenerationSource requires 'input_ids' in the batch\")\n\n    # Start generation\n    batch_size = input_ids.size(0)\n    device = input_ids.device\n    curr_ids = input_ids\n\n    # Track finished sequences\n    finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n\n    # Use eos_token_id or a default (0) for padding finished sequences\n    pad_token_id = self.cfg.eos_token_id if self.cfg.eos_token_id is not None else 0\n\n    # Store only the new tokens\n    generated = []\n\n    for _ in range(self.cfg.max_new_tokens):\n        # If all sequences are finished, we can stop early\n        if self.cfg.eos_token_id is not None and finished.all():\n            break\n\n        # Forward pass to get logits for the last token\n        # Note: We update batch input_ids for models that might use other batch info\n        batch[\"input_ids\"] = curr_ids\n        outputs = model(**batch)\n\n        if isinstance(outputs, dict):\n            logits = outputs.get(\"logits\")\n        elif hasattr(outputs, \"logits\"):\n            logits = outputs.logits\n        else:\n            logits = outputs\n\n        # Take logits for the last position: [B, T, V] -&gt; [B, V]\n        next_token_logits = logits[:, -1, :]\n\n        if not self.cfg.do_sample:\n            # Greedy search\n            next_tokens = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n        else:\n            # Sampling logic\n            if self.cfg.temperature != 1.0:\n                next_token_logits = next_token_logits / self.cfg.temperature\n\n            if self.cfg.top_k is not None:\n                v, _ = torch.topk(\n                    next_token_logits,\n                    min(self.cfg.top_k, next_token_logits.size(-1)),\n                )\n                next_token_logits[next_token_logits &lt; v[:, [-1]]] = -float(\"Inf\")\n\n            if self.cfg.top_p is not None:\n                sorted_logits, sorted_indices = torch.sort(\n                    next_token_logits, descending=True\n                )\n                cumulative_probs = torch.cumsum(\n                    F.softmax(sorted_logits, dim=-1), dim=-1\n                )\n\n                # Remove tokens with cumulative probability above the threshold\n                sorted_indices_to_remove = cumulative_probs &gt; self.cfg.top_p\n                # Shift the indices to the right to keep also the first token above the threshold\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n                    ..., :-1\n                ].clone()\n                sorted_indices_to_remove[..., 0] = 0\n\n                # scatter sorted tensors to original indexing\n                indices_to_remove = sorted_indices_to_remove.scatter(\n                    1, sorted_indices, sorted_indices_to_remove\n                )\n                next_token_logits[indices_to_remove] = -float(\"Inf\")\n\n            probs = F.softmax(next_token_logits, dim=-1)\n            next_tokens = torch.multinomial(probs, num_samples=1)\n\n        # Mask out tokens for finished sequences\n        if self.cfg.eos_token_id is not None:\n            next_tokens = torch.where(\n                finished.unsqueeze(1),\n                torch.tensor(pad_token_id, device=device),\n                next_tokens,\n            )\n            # Update finished mask\n            finished |= next_tokens.squeeze(1) == self.cfg.eos_token_id\n\n        # Append generated tokens\n        generated.append(next_tokens)\n        curr_ids = torch.cat([curr_ids, next_tokens], dim=1)\n\n    return {\n        StandardProtocols.GENERATED_TOKENS: (\n            torch.cat(generated, dim=1)\n            if generated\n            else torch.empty((batch_size, 0), dtype=torch.long, device=device)\n        )\n    }\n</code></pre>"},{"location":"reference/modules/metrics/sources/generation/#optimus_dl.modules.metrics.sources.generation.GenerationSourceConfig","title":"<code>GenerationSourceConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricSourceConfig</code></p> <p>Configuration for GenerationSource.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>max_new_tokens</code> <code>int</code> <code>50</code> <code>temperature</code> <code>float</code> <code>1.0</code> <code>top_k</code> <code>int | None</code> <code>None</code> <code>top_p</code> <code>float | None</code> <code>None</code> <code>do_sample</code> <code>bool</code> <code>False</code> <code>eos_token_id</code> <code>int | None</code> <code>None</code> Source code in <code>optimus_dl/modules/metrics/sources/generation.py</code> <pre><code>@dataclass\nclass GenerationSourceConfig(MetricSourceConfig):\n    \"\"\"Configuration for GenerationSource.\n\n    Attributes:\n        max_new_tokens: Maximum number of tokens to generate.\n        temperature: Sampling temperature (1.0 = no change, &lt; 1.0 = sharper, &gt; 1.0 = smoother).\n        top_k: If set, only sample from the top k tokens.\n        top_p: If set, only sample from tokens with cumulative probability &gt;= p.\n        do_sample: Whether to use sampling; if False, uses greedy search.\n        eos_token_id: Token ID that signals the end of a sequence.\n    \"\"\"\n\n    _name: str = \"generation\"\n    max_new_tokens: int = 50\n    temperature: float = 1.0\n    top_k: int | None = None\n    top_p: float | None = None\n    do_sample: bool = False\n    eos_token_id: int | None = None\n</code></pre>"},{"location":"reference/modules/model/","title":"Index","text":""},{"location":"reference/modules/model/#optimus_dl.modules.model","title":"<code>optimus_dl.modules.model</code>","text":""},{"location":"reference/modules/model/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>base</code>: Base model class for all language models in Optimus-DL.</li> <li><code>blocks</code>: </li> <li><code>config</code>: </li> <li><code>gpt2</code>: Full definition of a GPT Language Model, all of it in this single file.</li> <li><code>llama2</code>: Llama style Language Model.</li> <li><code>olmo3</code>: Olmo3 Language Model implementation.</li> <li><code>presets</code>: </li> <li><code>qwen3</code>: Qwen3 Language Model implementation.</li> </ul>"},{"location":"reference/modules/model/base/","title":"base","text":""},{"location":"reference/modules/model/base/#optimus_dl.modules.model.base","title":"<code>optimus_dl.modules.model.base</code>","text":"<p>Base model class for all language models in Optimus-DL.</p> <p>This module defines the BaseModel class that all model architectures must inherit from. It provides common functionality for parameter grouping, distributed sharding, and tensor parallelism.</p>"},{"location":"reference/modules/model/base/#optimus_dl.modules.model.base.BaseModel","title":"<code>BaseModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for all language model architectures in the framework.</p> <p>All model implementations should inherit from this class. It provides a standardized interface for:</p> <ul> <li>Forward Pass: Standard PyTorch forward method.</li> <li>Optimizer Integration: Custom parameter grouping (e.g., weight decay   exclusion for norms/biases).</li> <li>FSDP2 Sharding: Support for fully sharded data parallelism via a custom   <code>fully_shard</code> method.</li> <li>Tensor Parallelism: Support for sharding parameters across multiple   devices via <code>apply_tp</code>.</li> </ul> <p>Subclasses must implement:</p> <ul> <li><code>forward()</code>: The main computation loop.</li> </ul> Example <pre><code>@register_model(\"my_model\", MyModelConfig)\nclass MyModel(BaseModel):\n    def __init__(self, cfg: MyModelConfig):\n        super().__init__()\n        self.embedding = nn.Embedding(cfg.vocab_size, cfg.n_embd)\n\n    def forward(self, input_ids):\n        return {\"logits\": self.embedding(input_ids)}\n</code></pre> Source code in <code>optimus_dl/modules/model/base.py</code> <pre><code>class BaseModel(torch.nn.Module):\n    \"\"\"Base class for all language model architectures in the framework.\n\n    All model implementations should inherit from this class. It provides a\n    standardized interface for:\n\n    - **Forward Pass**: Standard PyTorch forward method.\n    - **Optimizer Integration**: Custom parameter grouping (e.g., weight decay\n      exclusion for norms/biases).\n    - **FSDP2 Sharding**: Support for fully sharded data parallelism via a custom\n      `fully_shard` method.\n    - **Tensor Parallelism**: Support for sharding parameters across multiple\n      devices via `apply_tp`.\n\n    Subclasses must implement:\n\n    - `forward()`: The main computation loop.\n\n    Example:\n        ```python\n        @register_model(\"my_model\", MyModelConfig)\n        class MyModel(BaseModel):\n            def __init__(self, cfg: MyModelConfig):\n                super().__init__()\n                self.embedding = nn.Embedding(cfg.vocab_size, cfg.n_embd)\n\n            def forward(self, input_ids):\n                return {\"logits\": self.embedding(input_ids)}\n\n        ```\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the base model.\"\"\"\n        super().__init__()\n\n    @classmethod\n    def register_arch(cls, arch_name: str) -&gt; Callable[[Callable[[], Any]], Any]:\n        \"\"\"Decorator for registering an architecture variant of this model.\n\n        This method is dynamically populated on the class during registration\n        in the model registry. It allows registering variants like '7b', '13b', etc.\n\n        Args:\n            arch_name: Name of the architecture variant.\n\n        Returns:\n            A decorator function.\n        \"\"\"\n        raise NotImplementedError(\n            \"This is a placeholder for the register_arch decorator. Populated on model class registration\"\n        )\n\n    def make_parameter_groups(self) -&gt; dict[str, Any]:\n        \"\"\"Create parameter groups for optimizer configuration.\n\n        This method allows models to specify which parameters should have\n        weight decay applied, or to use different learning rates for different\n        sub-modules.\n\n        Returns:\n            Dictionary with a 'params' key, or a list of such dictionaries,\n            compatible with PyTorch optimizers.\n        \"\"\"\n        return {\"params\": self.named_parameters()}\n\n    def fully_shard(self, **fsdp_kwargs) -&gt; None:\n        \"\"\"Define the FSDP2 sharding strategy for this model.\n\n        This method should wrap sub-modules (e.g., transformer blocks) with\n        `fully_shard` to enable efficient distributed training.\n\n        Args:\n            **fsdp_kwargs: Arguments for the FSDP sharding process (e.g., mesh).\n        \"\"\"\n        logger.warning(\n            \"Model does not support fully sharding. Define this method or performance will be impacted.\"\n        )\n\n    def apply_tp(self, mesh, **kwargs):\n        \"\"\"Apply Tensor Parallelism (sharding) to the model's parameters.\n\n        This method should use `parallelize_module` or similar utilities to\n        shard specific linear or embedding layers across the provided mesh.\n\n        Args:\n            mesh: The DeviceMesh for tensor parallelism.\n            **kwargs: Additional model-specific TP flags (e.g., sequence_parallel).\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/modules/model/base/#optimus_dl.modules.model.base.BaseModel.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the base model.</p> Source code in <code>optimus_dl/modules/model/base.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the base model.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"reference/modules/model/base/#optimus_dl.modules.model.base.BaseModel.apply_tp","title":"<code>apply_tp(mesh, **kwargs)</code>","text":"<p>Apply Tensor Parallelism (sharding) to the model's parameters.</p> <p>This method should use <code>parallelize_module</code> or similar utilities to shard specific linear or embedding layers across the provided mesh.</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <p>The DeviceMesh for tensor parallelism.</p> required <code>**kwargs</code> <p>Additional model-specific TP flags (e.g., sequence_parallel).</p> <code>{}</code> Source code in <code>optimus_dl/modules/model/base.py</code> <pre><code>def apply_tp(self, mesh, **kwargs):\n    \"\"\"Apply Tensor Parallelism (sharding) to the model's parameters.\n\n    This method should use `parallelize_module` or similar utilities to\n    shard specific linear or embedding layers across the provided mesh.\n\n    Args:\n        mesh: The DeviceMesh for tensor parallelism.\n        **kwargs: Additional model-specific TP flags (e.g., sequence_parallel).\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/modules/model/base/#optimus_dl.modules.model.base.BaseModel.fully_shard","title":"<code>fully_shard(**fsdp_kwargs)</code>","text":"<p>Define the FSDP2 sharding strategy for this model.</p> <p>This method should wrap sub-modules (e.g., transformer blocks) with <code>fully_shard</code> to enable efficient distributed training.</p> <p>Parameters:</p> Name Type Description Default <code>**fsdp_kwargs</code> <p>Arguments for the FSDP sharding process (e.g., mesh).</p> <code>{}</code> Source code in <code>optimus_dl/modules/model/base.py</code> <pre><code>def fully_shard(self, **fsdp_kwargs) -&gt; None:\n    \"\"\"Define the FSDP2 sharding strategy for this model.\n\n    This method should wrap sub-modules (e.g., transformer blocks) with\n    `fully_shard` to enable efficient distributed training.\n\n    Args:\n        **fsdp_kwargs: Arguments for the FSDP sharding process (e.g., mesh).\n    \"\"\"\n    logger.warning(\n        \"Model does not support fully sharding. Define this method or performance will be impacted.\"\n    )\n</code></pre>"},{"location":"reference/modules/model/base/#optimus_dl.modules.model.base.BaseModel.make_parameter_groups","title":"<code>make_parameter_groups()</code>","text":"<p>Create parameter groups for optimizer configuration.</p> <p>This method allows models to specify which parameters should have weight decay applied, or to use different learning rates for different sub-modules.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with a 'params' key, or a list of such dictionaries,</p> <code>dict[str, Any]</code> <p>compatible with PyTorch optimizers.</p> Source code in <code>optimus_dl/modules/model/base.py</code> <pre><code>def make_parameter_groups(self) -&gt; dict[str, Any]:\n    \"\"\"Create parameter groups for optimizer configuration.\n\n    This method allows models to specify which parameters should have\n    weight decay applied, or to use different learning rates for different\n    sub-modules.\n\n    Returns:\n        Dictionary with a 'params' key, or a list of such dictionaries,\n        compatible with PyTorch optimizers.\n    \"\"\"\n    return {\"params\": self.named_parameters()}\n</code></pre>"},{"location":"reference/modules/model/base/#optimus_dl.modules.model.base.BaseModel.register_arch","title":"<code>register_arch(arch_name)</code>  <code>classmethod</code>","text":"<p>Decorator for registering an architecture variant of this model.</p> <p>This method is dynamically populated on the class during registration in the model registry. It allows registering variants like '7b', '13b', etc.</p> <p>Parameters:</p> Name Type Description Default <code>arch_name</code> <code>str</code> <p>Name of the architecture variant.</p> required <p>Returns:</p> Type Description <code>Callable[[Callable[[], Any]], Any]</code> <p>A decorator function.</p> Source code in <code>optimus_dl/modules/model/base.py</code> <pre><code>@classmethod\ndef register_arch(cls, arch_name: str) -&gt; Callable[[Callable[[], Any]], Any]:\n    \"\"\"Decorator for registering an architecture variant of this model.\n\n    This method is dynamically populated on the class during registration\n    in the model registry. It allows registering variants like '7b', '13b', etc.\n\n    Args:\n        arch_name: Name of the architecture variant.\n\n    Returns:\n        A decorator function.\n    \"\"\"\n    raise NotImplementedError(\n        \"This is a placeholder for the register_arch decorator. Populated on model class registration\"\n    )\n</code></pre>"},{"location":"reference/modules/model/config/","title":"config","text":""},{"location":"reference/modules/model/config/#optimus_dl.modules.model.config","title":"<code>optimus_dl.modules.model.config</code>","text":""},{"location":"reference/modules/model/config/#optimus_dl.modules.model.config.ModelConfig","title":"<code>ModelConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfig</code></p> <p>ModelConfig(_name: str | None = None)</p> Source code in <code>optimus_dl/modules/model/config.py</code> <pre><code>@dataclass\nclass ModelConfig(RegistryConfig):\n    pass\n</code></pre>"},{"location":"reference/modules/model/gpt2/","title":"gpt2","text":""},{"location":"reference/modules/model/gpt2/#optimus_dl.modules.model.gpt2","title":"<code>optimus_dl.modules.model.gpt2</code>","text":"<p>Full definition of a GPT Language Model, all of it in this single file. References: 1) the official GPT-2 TensorFlow implementation released by OpenAI: https://github.com/openai/gpt-2/blob/master/src/model.py 2) huggingface/transformers PyTorch implementation: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py</p>"},{"location":"reference/modules/model/gpt2/#optimus_dl.modules.model.gpt2.Block","title":"<code>Block</code>","text":"<p>               Bases: <code>Module</code></p> <p>A single Transformer block with self-attention and MLP.</p> Source code in <code>optimus_dl/modules/model/gpt2.py</code> <pre><code>class Block(nn.Module):\n    \"\"\"A single Transformer block with self-attention and MLP.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n</code></pre>"},{"location":"reference/modules/model/gpt2/#optimus_dl.modules.model.gpt2.GPT","title":"<code>GPT</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>GPT Language Model architecture.</p> <p>Implements a decoder-only transformer with causal self-attention, absolute position embeddings, and standard GPT-2 layer ordering (LayerNorm before attention/MLP).</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>GPT model configuration.</p> required Source code in <code>optimus_dl/modules/model/gpt2.py</code> <pre><code>@register_model(\"gpt2\", GPTConfig)\nclass GPT(BaseModel):\n    \"\"\"GPT Language Model architecture.\n\n    Implements a decoder-only transformer with causal self-attention, absolute\n    position embeddings, and standard GPT-2 layer ordering (LayerNorm before\n    attention/MLP).\n\n    Args:\n        config: GPT model configuration.\n    \"\"\"\n\n    def __init__(self, config, **kwargs):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(\n            {\n                \"wte\": nn.Embedding(\n                    config.vocab_size,\n                    config.n_embd,\n                    padding_idx=config.padding_token_id,\n                ),\n                \"wpe\": nn.Embedding(config.block_size, config.n_embd),\n                \"drop\": nn.Dropout(config.dropout),\n                \"h\": nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n                \"ln_f\": LayerNorm(config.n_embd, bias=config.bias),\n            }\n        )\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # Weight tying:\n        # When using torch.compile(), PyTorch may emit a UserWarning about multiple values\n        # for tied weights. This is a known behavior when tying weights for FSDP/compilation\n        # compatibility and is generally safe to ignore.\n        if config.tie_word_embeddings:\n            self.transformer.wte.weight = (\n                self.lm_head.weight\n            )  # https://paperswithcode.com/method/weight-tying\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith(\"c_proj.weight\"):\n                torch.nn.init.normal_(\n                    p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer)\n                )\n\n    def _init_weights(self, module):\n        \"\"\"Standard Gaussian initialization for weights.\"\"\"\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, input_ids, **kwargs):\n        \"\"\"Compute model output for the given input tokens.\"\"\"\n        idx = input_ids\n\n        device = idx.device\n        b, t = idx.size()\n        assert (\n            t &lt;= self.config.block_size\n        ), f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device)  # shape (t)\n\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x=x)\n        x = self.transformer.ln_f(x)\n\n        logits = self.lm_head(x)\n\n        return {\"logits\": logits}\n\n    def make_parameter_groups(self):\n        \"\"\"Divide parameters into decayed and non-decayed groups.\n\n        Excludes biases and 1D parameters (normalization weights, embeddings)\n        from weight decay. Handles weight tying correctly.\n\n        Returns:\n            List of dictionaries for PyTorch optimizer.\n        \"\"\"\n\n        # separate out all parameters to those that will and won't experience regularizing weight decay\n        decay = set()\n        no_decay = set()\n        whitelist_weight_modules = (torch.nn.Linear,)\n\n        for mn, m in self.named_modules():\n            for pn, _p in m.named_parameters():\n                fpn = f\"{mn}.{pn}\" if mn else pn  # full param name\n                # Note: because named_modules and named_parameters are recursive,\n                # we will see the same tensors multiple times. We use the parent module\n                # to determine the weight decay strategy.\n                if pn.endswith(\"weight_clip_val\"):\n                    # quant params are not decayed\n                    no_decay.add(fpn)\n                if pn.endswith(\"bias\"):\n                    # all biases will not be decayed\n                    no_decay.add(fpn)\n                elif pn.endswith(\"weight\") and isinstance(m, whitelist_weight_modules):\n                    # weights of whitelist modules will be weight decayed\n                    decay.add(fpn)\n                elif pn.endswith(\"weight\") and isinstance(m, BLACKLIST_WEIGHT_MODULES):\n                    # weights of blacklist modules will NOT be weight decayed\n                    no_decay.add(fpn)\n\n        # subtle: 'transformer.wte.weight' and 'lm_head.weight' are tied, so they\n        # will appear in the no_decay and decay sets respectively after the above.\n        # In addition, because named_parameters() doesn't return duplicates, it\n        # will only return the first occurence, key'd by 'transformer.wte.weight', below.\n        # so let's manually remove 'lm_head.weight' from decay set. This will include\n        # this tensor into optimization via transformer.wte.weight only, and not decayed.\n        decay.remove(\"lm_head.weight\")\n        if self.lm_head.weight is not self.transformer.wte.weight:\n            no_decay.add(\"lm_head.weight\")\n\n        # validate that we considered every parameter\n        param_dict = dict(self.named_parameters())\n        inter_params = decay &amp; no_decay\n        union_params = decay | no_decay\n        assert (\n            len(inter_params) == 0\n        ), f\"parameters {str(inter_params)} made it into both decay/no_decay sets!\"\n        assert (\n            len(param_dict.keys() - union_params) == 0\n        ), f\"parameters {str(param_dict.keys() - union_params)} were not separated into either decay/no_decay set!\"\n\n        # create the pytorch optimizer object\n        return [\n            {\"params\": [(n, p) for n, p in self.named_parameters() if n in decay]},\n            {\n                \"params\": [(n, p) for n, p in self.named_parameters() if n in no_decay],\n                \"weight_decay\": 0.0,\n            },\n        ]\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"Autoregressive generation of new tokens.\n\n        Args:\n            idx: Starting token sequence (LongTensor).\n            max_new_tokens: Number of tokens to generate.\n            temperature: Sampling temperature.\n            top_k: Optional top-k sampling threshold.\n\n        Returns:\n            LongTensor containing original and generated tokens.\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = (\n                idx\n                if idx.size(1) &lt;= self.config.block_size\n                else idx[:, -self.config.block_size :]\n            )\n            # forward the model to get the logits for the index in the sequence\n            output = self(idx_cond)\n            logits = output[\"logits\"]\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits &lt; v[:, [-1]]] = -float(\"Inf\")\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx\n\n    def fully_shard(self, **fsdp_kwargs):\n        \"\"\"Apply FSDP sharding to each transformer block.\"\"\"\n        for i, module in enumerate(self.transformer.h):\n            reshard_after_forward = fsdp_kwargs.get(\"reshard_after_forward\", False)\n            if i % self.config.shard_every_ith_layer == 0:\n                # Shard this layer\n                reshard_after_forward &amp;= True\n            else:\n                # Do not shard this layer\n                reshard_after_forward &amp;= False\n            fully_shard(\n                module,\n                **(fsdp_kwargs | {\"reshard_after_forward\": reshard_after_forward}),\n            )\n</code></pre>"},{"location":"reference/modules/model/gpt2/#optimus_dl.modules.model.gpt2.GPT.forward","title":"<code>forward(input_ids, **kwargs)</code>","text":"<p>Compute model output for the given input tokens.</p> Source code in <code>optimus_dl/modules/model/gpt2.py</code> <pre><code>def forward(self, input_ids, **kwargs):\n    \"\"\"Compute model output for the given input tokens.\"\"\"\n    idx = input_ids\n\n    device = idx.device\n    b, t = idx.size()\n    assert (\n        t &lt;= self.config.block_size\n    ), f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n    pos = torch.arange(0, t, dtype=torch.long, device=device)  # shape (t)\n\n    # forward the GPT model itself\n    tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n    pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (t, n_embd)\n    x = self.transformer.drop(tok_emb + pos_emb)\n    for block in self.transformer.h:\n        x = block(x=x)\n    x = self.transformer.ln_f(x)\n\n    logits = self.lm_head(x)\n\n    return {\"logits\": logits}\n</code></pre>"},{"location":"reference/modules/model/gpt2/#optimus_dl.modules.model.gpt2.GPT.fully_shard","title":"<code>fully_shard(**fsdp_kwargs)</code>","text":"<p>Apply FSDP sharding to each transformer block.</p> Source code in <code>optimus_dl/modules/model/gpt2.py</code> <pre><code>def fully_shard(self, **fsdp_kwargs):\n    \"\"\"Apply FSDP sharding to each transformer block.\"\"\"\n    for i, module in enumerate(self.transformer.h):\n        reshard_after_forward = fsdp_kwargs.get(\"reshard_after_forward\", False)\n        if i % self.config.shard_every_ith_layer == 0:\n            # Shard this layer\n            reshard_after_forward &amp;= True\n        else:\n            # Do not shard this layer\n            reshard_after_forward &amp;= False\n        fully_shard(\n            module,\n            **(fsdp_kwargs | {\"reshard_after_forward\": reshard_after_forward}),\n        )\n</code></pre>"},{"location":"reference/modules/model/gpt2/#optimus_dl.modules.model.gpt2.GPT.generate","title":"<code>generate(idx, max_new_tokens, temperature=1.0, top_k=None)</code>","text":"<p>Autoregressive generation of new tokens.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <p>Starting token sequence (LongTensor).</p> required <code>max_new_tokens</code> <p>Number of tokens to generate.</p> required <code>temperature</code> <p>Sampling temperature.</p> <code>1.0</code> <code>top_k</code> <p>Optional top-k sampling threshold.</p> <code>None</code> <p>Returns:</p> Type Description <p>LongTensor containing original and generated tokens.</p> Source code in <code>optimus_dl/modules/model/gpt2.py</code> <pre><code>@torch.no_grad()\ndef generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n    \"\"\"Autoregressive generation of new tokens.\n\n    Args:\n        idx: Starting token sequence (LongTensor).\n        max_new_tokens: Number of tokens to generate.\n        temperature: Sampling temperature.\n        top_k: Optional top-k sampling threshold.\n\n    Returns:\n        LongTensor containing original and generated tokens.\n    \"\"\"\n    for _ in range(max_new_tokens):\n        # if the sequence context is growing too long we must crop it at block_size\n        idx_cond = (\n            idx\n            if idx.size(1) &lt;= self.config.block_size\n            else idx[:, -self.config.block_size :]\n        )\n        # forward the model to get the logits for the index in the sequence\n        output = self(idx_cond)\n        logits = output[\"logits\"]\n        # pluck the logits at the final step and scale by desired temperature\n        logits = logits[:, -1, :] / temperature\n        # optionally crop the logits to only the top k options\n        if top_k is not None:\n            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n            logits[logits &lt; v[:, [-1]]] = -float(\"Inf\")\n        # apply softmax to convert logits to (normalized) probabilities\n        probs = F.softmax(logits, dim=-1)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)\n        # append sampled index to the running sequence and continue\n        idx = torch.cat((idx, idx_next), dim=1)\n\n    return idx\n</code></pre>"},{"location":"reference/modules/model/gpt2/#optimus_dl.modules.model.gpt2.GPT.make_parameter_groups","title":"<code>make_parameter_groups()</code>","text":"<p>Divide parameters into decayed and non-decayed groups.</p> <p>Excludes biases and 1D parameters (normalization weights, embeddings) from weight decay. Handles weight tying correctly.</p> <p>Returns:</p> Type Description <p>List of dictionaries for PyTorch optimizer.</p> Source code in <code>optimus_dl/modules/model/gpt2.py</code> <pre><code>def make_parameter_groups(self):\n    \"\"\"Divide parameters into decayed and non-decayed groups.\n\n    Excludes biases and 1D parameters (normalization weights, embeddings)\n    from weight decay. Handles weight tying correctly.\n\n    Returns:\n        List of dictionaries for PyTorch optimizer.\n    \"\"\"\n\n    # separate out all parameters to those that will and won't experience regularizing weight decay\n    decay = set()\n    no_decay = set()\n    whitelist_weight_modules = (torch.nn.Linear,)\n\n    for mn, m in self.named_modules():\n        for pn, _p in m.named_parameters():\n            fpn = f\"{mn}.{pn}\" if mn else pn  # full param name\n            # Note: because named_modules and named_parameters are recursive,\n            # we will see the same tensors multiple times. We use the parent module\n            # to determine the weight decay strategy.\n            if pn.endswith(\"weight_clip_val\"):\n                # quant params are not decayed\n                no_decay.add(fpn)\n            if pn.endswith(\"bias\"):\n                # all biases will not be decayed\n                no_decay.add(fpn)\n            elif pn.endswith(\"weight\") and isinstance(m, whitelist_weight_modules):\n                # weights of whitelist modules will be weight decayed\n                decay.add(fpn)\n            elif pn.endswith(\"weight\") and isinstance(m, BLACKLIST_WEIGHT_MODULES):\n                # weights of blacklist modules will NOT be weight decayed\n                no_decay.add(fpn)\n\n    # subtle: 'transformer.wte.weight' and 'lm_head.weight' are tied, so they\n    # will appear in the no_decay and decay sets respectively after the above.\n    # In addition, because named_parameters() doesn't return duplicates, it\n    # will only return the first occurence, key'd by 'transformer.wte.weight', below.\n    # so let's manually remove 'lm_head.weight' from decay set. This will include\n    # this tensor into optimization via transformer.wte.weight only, and not decayed.\n    decay.remove(\"lm_head.weight\")\n    if self.lm_head.weight is not self.transformer.wte.weight:\n        no_decay.add(\"lm_head.weight\")\n\n    # validate that we considered every parameter\n    param_dict = dict(self.named_parameters())\n    inter_params = decay &amp; no_decay\n    union_params = decay | no_decay\n    assert (\n        len(inter_params) == 0\n    ), f\"parameters {str(inter_params)} made it into both decay/no_decay sets!\"\n    assert (\n        len(param_dict.keys() - union_params) == 0\n    ), f\"parameters {str(param_dict.keys() - union_params)} were not separated into either decay/no_decay set!\"\n\n    # create the pytorch optimizer object\n    return [\n        {\"params\": [(n, p) for n, p in self.named_parameters() if n in decay]},\n        {\n            \"params\": [(n, p) for n, p in self.named_parameters() if n in no_decay],\n            \"weight_decay\": 0.0,\n        },\n    ]\n</code></pre>"},{"location":"reference/modules/model/gpt2/#optimus_dl.modules.model.gpt2.GPTConfig","title":"<code>GPTConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for GPT-style language models.</p> <p>Parameters:</p> Name Type Description Default <code>block_size</code> <code>int</code> <p>Maximum context length. Determines max pos embeddings</p> <code>1024</code> <code>vocab_size</code> <code>int</code> <p>Vocabulary size</p> <code>50304</code> <code>n_layer</code> <code>int</code> <p>Number of transformer blocks</p> <code>12</code> <code>n_head</code> <code>int</code> <p>Number of attention heads</p> <code>12</code> <code>n_embd</code> <code>int</code> <p>Embedding dimensionality</p> <code>768</code> <code>head_dim</code> <code>int | None</code> <p>Head dimension. If None, will be set to n_embd // n_head</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.0</code> <code>bias</code> <code>bool</code> <p>Whether to use bias in linear layers and norms</p> <code>True</code> <code>tie_word_embeddings</code> <code>bool</code> <p>Share weights between token embeddings and LM head</p> <code>True</code> <code>shard_every_ith_layer</code> <code>int</code> <p>Control FSDP sharding granularity. Shard every i-th layer, 1 means all layers are sharded (if global reshard_after_forward is True)</p> <code>1</code> <code>padding_token_id</code> <code>int | None</code> <p>Padding token id for the model embeddings</p> <code>None</code> Source code in <code>optimus_dl/modules/model/gpt2.py</code> <pre><code>@dataclass\nclass GPTConfig(RegistryConfigStrict):\n    \"\"\"Configuration for GPT-style language models.\"\"\"\n\n    block_size: int = field(\n        default=1024,\n        metadata={\n            \"description\": \"Maximum context length. Determines max pos embeddings\"\n        },\n    )\n    vocab_size: int = field(default=50304, metadata={\"description\": \"Vocabulary size\"})\n    n_layer: int = field(\n        default=12, metadata={\"description\": \"Number of transformer blocks\"}\n    )\n    n_head: int = field(\n        default=12, metadata={\"description\": \"Number of attention heads\"}\n    )\n    n_embd: int = field(\n        default=768, metadata={\"description\": \"Embedding dimensionality\"}\n    )\n    head_dim: int | None = field(\n        default=None,\n        metadata={\n            \"description\": \"Head dimension. If None, will be set to n_embd // n_head\"\n        },\n    )\n    dropout: float = field(default=0.0, metadata={\"description\": \"Dropout probability\"})\n    bias: bool = field(\n        default=True,\n        metadata={\"description\": \"Whether to use bias in linear layers and norms\"},\n    )\n    tie_word_embeddings: bool = field(\n        default=True,\n        metadata={\"description\": \"Share weights between token embeddings and LM head\"},\n    )\n    shard_every_ith_layer: int = field(\n        default=1,\n        metadata={\n            \"description\": \"Control FSDP sharding granularity. Shard every i-th layer, 1 means all layers are sharded (if global reshard_after_forward is True)\"\n        },\n    )\n    padding_token_id: int | None = field(\n        default=None,\n        metadata={\"description\": \"Padding token id for the model embeddings\"},\n    )\n</code></pre>"},{"location":"reference/modules/model/gpt2/#optimus_dl.modules.model.gpt2.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>Standard GPT-2 MLP with GELU activation.</p> Source code in <code>optimus_dl/modules/model/gpt2.py</code> <pre><code>class MLP(nn.Module):\n    \"\"\"Standard GPT-2 MLP with GELU activation.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu = nn.GELU()\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n</code></pre>"},{"location":"reference/modules/model/llama2/","title":"llama2","text":""},{"location":"reference/modules/model/llama2/#optimus_dl.modules.model.llama2","title":"<code>optimus_dl.modules.model.llama2</code>","text":"<p>Llama style Language Model. References:</p> <ul> <li>Llama inference code: https://github.com/facebookresearch/llama/blob/main/llama/model.py</li> <li>Mistral one file ref: https://github.com/mistralai/mistral-src/blob/main/one_file_ref.py</li> <li>Llama paper: https://arxiv.org/pdf/2302.13971.pdf</li> </ul> <p>Main differences from GPT2: - Uses RMSNorm instead of LayerNorm - Uses a slightly different MLP (SwiGLU) - rotary embeddings (RoPE)</p>"},{"location":"reference/modules/model/llama2/#optimus_dl.modules.model.llama2.Llama","title":"<code>Llama</code>","text":"<p>               Bases: <code>GPT</code></p> <p>Llama Language Model architecture.</p> <p>Based on the standard GPT class but incorporates modern architectural improvements:</p> <ul> <li>Rotary Embeddings (RoPE): Position encoding integrated into attention.</li> <li>RMSNorm: More efficient normalization layer.</li> <li>SwiGLU MLP: SiLU-gated MLP variant.</li> <li>Tensor Parallelism: Comprehensive sharding plan for distributed training.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LlamaConfig</code> <p>Llama model configuration.</p> required Source code in <code>optimus_dl/modules/model/llama2.py</code> <pre><code>@register_model(\"llama2\", LlamaConfig)\nclass Llama(GPT):\n    \"\"\"Llama Language Model architecture.\n\n    Based on the standard GPT class but incorporates modern architectural\n    improvements:\n\n    - **Rotary Embeddings (RoPE)**: Position encoding integrated into attention.\n    - **RMSNorm**: More efficient normalization layer.\n    - **SwiGLU MLP**: SiLU-gated MLP variant.\n    - **Tensor Parallelism**: Comprehensive sharding plan for distributed training.\n\n    Args:\n        config: Llama model configuration.\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig, **kwargs):\n        super().__init__(config)\n        assert config.vocab_size is not None\n        assert config.sequence_length is not None\n        self.config = config\n\n        # create the token and position embeddings\n        self.head_dim = config.n_embd // config.n_head\n        self.freqs_cis = precompute_freqs_cis(\n            self.head_dim,\n            config.sequence_length,\n            theta=config.rope_theta,\n            scaling_config=config.rope_scaling,\n        )\n\n        self.transformer = nn.ModuleDict(\n            {\n                \"wte\": nn.Embedding(\n                    config.vocab_size,\n                    config.n_embd,\n                    padding_idx=config.padding_token_id,\n                ),\n                \"drop\": nn.Dropout(config.dropout),\n                \"h\": nn.ModuleList([LlamaBlock(config) for _ in range(config.n_layer)]),\n                \"ln_f\": RMSNorm(\n                    config.n_embd,\n                    eps=config.rmsnorm_eps,\n                    use_liger=config.use_liger_rmsnorm,\n                ),\n            }\n        )\n        if config.tie_word_embeddings:\n            self.transformer.wte.weight = self.lm_head.weight\n\n        self.apply(self._init_weights)\n        for pn, p in self.named_parameters():\n            if pn.endswith(\"c_proj.weight\"):\n                torch.nn.init.normal_(\n                    p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer)\n                )\n\n    def apply_tp(\n        self, mesh, loss_parallel: bool = False, sequence_parallel: bool = False\n    ):\n        \"\"\"Apply a 1D Tensor Parallelism plan to the Llama model.\n\n        Shards attention (Q/K/V/O) and MLP (w1/w2/c_proj) layers across the\n        provided device mesh. Supports optional sequence parallelism for norms\n        and communication-efficient sharded loss.\n\n        Args:\n            mesh: DeviceMesh for sharding.\n            loss_parallel: If True, shards the LM head and uses loss_parallel.\n            sequence_parallel: If True, enables sequence sharding and sharded norms.\n        \"\"\"\n        tp_size = mesh.size(0)\n        assert (\n            self.config.n_head % tp_size == 0\n        ), f\"Number of heads ({self.config.n_head}) must be divisible by TP size ({tp_size})\"\n        n_kv_head = (\n            self.config.n_kv_head\n            if self.config.n_kv_head is not None\n            else self.config.n_head\n        )\n        assert (\n            n_kv_head % tp_size == 0\n        ), f\"Number of KV heads ({n_kv_head}) must be divisible by TP size ({tp_size})\"\n\n        from torch.distributed.tensor.parallel import (\n            ColwiseParallel,\n            PrepareModuleInput,\n            PrepareModuleOutput,\n            RowwiseParallel,\n            SequenceParallel,\n            parallelize_module,\n        )\n\n        layer_plan = {\n            \"transformer.wte\": RowwiseParallel(\n                input_layouts=Replicate(),\n            ),\n            \"transformer.h.*.attn.wq\": ColwiseParallel(use_local_output=False),\n            \"transformer.h.*.attn.wk\": ColwiseParallel(use_local_output=False),\n            \"transformer.h.*.attn.wv\": ColwiseParallel(use_local_output=False),\n            \"transformer.h.*.attn.wo\": RowwiseParallel(),\n            \"transformer.h.*.mlp.w1\": ColwiseParallel(use_local_output=False),\n            \"transformer.h.*.mlp.w2\": ColwiseParallel(use_local_output=False),\n            \"transformer.h.*.mlp.c_proj\": RowwiseParallel(),\n            \"lm_head\": ColwiseParallel(use_local_output=False),\n        }\n        if sequence_parallel:\n            layer_plan.update(\n                {\n                    \"transformer.wte\": RowwiseParallel(\n                        input_layouts=Replicate(),\n                        output_layouts=Shard(1),\n                        use_local_output=True,\n                    ),\n                    \"transformer.h.*.ln_1\": SequenceParallel(),\n                    \"transformer.h.*.ln_2\": SequenceParallel(),\n                    \"transformer.ln_f\": SequenceParallel(),\n                    \"transformer.h.*\": PrepareModuleInput(\n                        input_kwarg_layouts=dict(\n                            x=Shard(1),\n                            freqs_cis=Replicate(),\n                            seq_lens=Replicate(),\n                            document_ids=Replicate(),\n                            position_ids=Replicate(),\n                            cu_seqlens=Replicate(),\n                        ),\n                        desired_input_kwarg_layouts=dict(\n                            x=Shard(1),\n                            freqs_cis=Replicate(),\n                            seq_lens=Replicate(),\n                            document_ids=Replicate(),\n                            position_ids=Replicate(),\n                            cu_seqlens=Replicate(),\n                        ),\n                        use_local_output=False,\n                    ),\n                    \"transformer.h.*.attn.wo\": RowwiseParallel(\n                        output_layouts=Shard(1), use_local_output=False\n                    ),\n                    \"transformer.h.*.mlp.w1\": ColwiseParallel(\n                        input_layouts=Shard(1), use_local_output=False\n                    ),\n                    \"transformer.h.*.mlp.w2\": ColwiseParallel(\n                        input_layouts=Shard(1), use_local_output=False\n                    ),\n                    \"transformer.h.*.mlp.c_proj\": RowwiseParallel(\n                        output_layouts=Shard(1), use_local_output=False\n                    ),\n                    \"lm_head\": ColwiseParallel(\n                        input_layouts=Shard(1), use_local_output=False\n                    ),\n                }\n            )\n\n        parallelize_module(self, mesh, layer_plan)\n\n        if self.config.tie_word_embeddings:\n            # re-tie\n            self.transformer.wte.weight = self.lm_head.weight\n\n        if not loss_parallel:\n            parallelize_module(\n                self.lm_head,\n                mesh,\n                PrepareModuleOutput(\n                    output_layouts=Shard(2),\n                    desired_output_layouts=Replicate(),\n                    use_local_output=False,\n                ),\n            )\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        seq_lens: torch.Tensor | None = None,\n        document_ids: torch.Tensor | None = None,\n        position_ids: torch.Tensor | None = None,\n        cu_seqlens: torch.Tensor | None = None,\n        max_seqlen: int | None = None,\n        **kwargs,\n    ):\n        \"\"\"Perform the forward pass, handling rotary frequency lookup and optional masking.\n\n        Args:\n            input_ids: Tensor of shape (B, T).\n            seq_lens: Optional 1D tensor of sequence lengths (for padding).\n            document_ids: Optional 2D tensor of document IDs (for packed/flat batching).\n            position_ids: Optional 2D tensor of position IDs (for RoPE).\n            cu_seqlens: Optional 1D tensor of cumulative sequence lengths (for varlen attention).\n            max_seqlen: Optional maximum sequence length in the packed batch.\n            **kwargs: Extra arguments.\n\n        Returns:\n            Dictionary containing model logits.\n        \"\"\"\n        idx = input_ids\n        device = idx.device\n        _, t = idx.size()\n\n        if position_ids is None:\n            pos = torch.arange(0, t, dtype=torch.long, device=device)\n            # (T, hs/2, 2)\n            freqs_cis = self.freqs_cis.to(device)[pos]\n        else:\n            # position_ids: (B, T)\n            # self.freqs_cis: (max_T, hs/2, 2)\n            # Result: (B, T, hs/2, 2)\n            # However, RotaryTransformerBlock expects freqs_cis to be (T, hs/2, 2)\n            # OR (B, T, hs/2, 2) if it supports it.\n            # My updated apply_rotary_emb supports passing position_ids separately.\n            # So I will pass the FULL freqs_cis and the position_ids.\n            freqs_cis = self.freqs_cis.to(device)\n\n        tok_emb = self.transformer.wte(idx)\n        x = self.transformer.drop(tok_emb)\n\n        for block in self.transformer.h:\n            block_kwargs = {\n                \"x\": x,\n                \"freqs_cis\": freqs_cis,\n                \"seq_lens\": seq_lens,\n                \"document_ids\": document_ids,\n                \"position_ids\": position_ids,\n                \"cu_seqlens\": cu_seqlens,\n                \"max_seqlen\": max_seqlen,\n            }\n            # Filter out None values to avoid triggering TP input preparation on None inputs\n            block_kwargs = {k: v for k, v in block_kwargs.items() if v is not None}\n            x = block(**block_kwargs)\n        x = self.transformer.ln_f(x)\n\n        logits = self.lm_head(x)\n\n        return {\n            \"logits\": logits,\n        }\n</code></pre>"},{"location":"reference/modules/model/llama2/#optimus_dl.modules.model.llama2.Llama.apply_tp","title":"<code>apply_tp(mesh, loss_parallel=False, sequence_parallel=False)</code>","text":"<p>Apply a 1D Tensor Parallelism plan to the Llama model.</p> <p>Shards attention (Q/K/V/O) and MLP (w1/w2/c_proj) layers across the provided device mesh. Supports optional sequence parallelism for norms and communication-efficient sharded loss.</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <p>DeviceMesh for sharding.</p> required <code>loss_parallel</code> <code>bool</code> <p>If True, shards the LM head and uses loss_parallel.</p> <code>False</code> <code>sequence_parallel</code> <code>bool</code> <p>If True, enables sequence sharding and sharded norms.</p> <code>False</code> Source code in <code>optimus_dl/modules/model/llama2.py</code> <pre><code>def apply_tp(\n    self, mesh, loss_parallel: bool = False, sequence_parallel: bool = False\n):\n    \"\"\"Apply a 1D Tensor Parallelism plan to the Llama model.\n\n    Shards attention (Q/K/V/O) and MLP (w1/w2/c_proj) layers across the\n    provided device mesh. Supports optional sequence parallelism for norms\n    and communication-efficient sharded loss.\n\n    Args:\n        mesh: DeviceMesh for sharding.\n        loss_parallel: If True, shards the LM head and uses loss_parallel.\n        sequence_parallel: If True, enables sequence sharding and sharded norms.\n    \"\"\"\n    tp_size = mesh.size(0)\n    assert (\n        self.config.n_head % tp_size == 0\n    ), f\"Number of heads ({self.config.n_head}) must be divisible by TP size ({tp_size})\"\n    n_kv_head = (\n        self.config.n_kv_head\n        if self.config.n_kv_head is not None\n        else self.config.n_head\n    )\n    assert (\n        n_kv_head % tp_size == 0\n    ), f\"Number of KV heads ({n_kv_head}) must be divisible by TP size ({tp_size})\"\n\n    from torch.distributed.tensor.parallel import (\n        ColwiseParallel,\n        PrepareModuleInput,\n        PrepareModuleOutput,\n        RowwiseParallel,\n        SequenceParallel,\n        parallelize_module,\n    )\n\n    layer_plan = {\n        \"transformer.wte\": RowwiseParallel(\n            input_layouts=Replicate(),\n        ),\n        \"transformer.h.*.attn.wq\": ColwiseParallel(use_local_output=False),\n        \"transformer.h.*.attn.wk\": ColwiseParallel(use_local_output=False),\n        \"transformer.h.*.attn.wv\": ColwiseParallel(use_local_output=False),\n        \"transformer.h.*.attn.wo\": RowwiseParallel(),\n        \"transformer.h.*.mlp.w1\": ColwiseParallel(use_local_output=False),\n        \"transformer.h.*.mlp.w2\": ColwiseParallel(use_local_output=False),\n        \"transformer.h.*.mlp.c_proj\": RowwiseParallel(),\n        \"lm_head\": ColwiseParallel(use_local_output=False),\n    }\n    if sequence_parallel:\n        layer_plan.update(\n            {\n                \"transformer.wte\": RowwiseParallel(\n                    input_layouts=Replicate(),\n                    output_layouts=Shard(1),\n                    use_local_output=True,\n                ),\n                \"transformer.h.*.ln_1\": SequenceParallel(),\n                \"transformer.h.*.ln_2\": SequenceParallel(),\n                \"transformer.ln_f\": SequenceParallel(),\n                \"transformer.h.*\": PrepareModuleInput(\n                    input_kwarg_layouts=dict(\n                        x=Shard(1),\n                        freqs_cis=Replicate(),\n                        seq_lens=Replicate(),\n                        document_ids=Replicate(),\n                        position_ids=Replicate(),\n                        cu_seqlens=Replicate(),\n                    ),\n                    desired_input_kwarg_layouts=dict(\n                        x=Shard(1),\n                        freqs_cis=Replicate(),\n                        seq_lens=Replicate(),\n                        document_ids=Replicate(),\n                        position_ids=Replicate(),\n                        cu_seqlens=Replicate(),\n                    ),\n                    use_local_output=False,\n                ),\n                \"transformer.h.*.attn.wo\": RowwiseParallel(\n                    output_layouts=Shard(1), use_local_output=False\n                ),\n                \"transformer.h.*.mlp.w1\": ColwiseParallel(\n                    input_layouts=Shard(1), use_local_output=False\n                ),\n                \"transformer.h.*.mlp.w2\": ColwiseParallel(\n                    input_layouts=Shard(1), use_local_output=False\n                ),\n                \"transformer.h.*.mlp.c_proj\": RowwiseParallel(\n                    output_layouts=Shard(1), use_local_output=False\n                ),\n                \"lm_head\": ColwiseParallel(\n                    input_layouts=Shard(1), use_local_output=False\n                ),\n            }\n        )\n\n    parallelize_module(self, mesh, layer_plan)\n\n    if self.config.tie_word_embeddings:\n        # re-tie\n        self.transformer.wte.weight = self.lm_head.weight\n\n    if not loss_parallel:\n        parallelize_module(\n            self.lm_head,\n            mesh,\n            PrepareModuleOutput(\n                output_layouts=Shard(2),\n                desired_output_layouts=Replicate(),\n                use_local_output=False,\n            ),\n        )\n</code></pre>"},{"location":"reference/modules/model/llama2/#optimus_dl.modules.model.llama2.Llama.forward","title":"<code>forward(input_ids, seq_lens=None, document_ids=None, position_ids=None, cu_seqlens=None, max_seqlen=None, **kwargs)</code>","text":"<p>Perform the forward pass, handling rotary frequency lookup and optional masking.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Tensor of shape (B, T).</p> required <code>seq_lens</code> <code>Tensor | None</code> <p>Optional 1D tensor of sequence lengths (for padding).</p> <code>None</code> <code>document_ids</code> <code>Tensor | None</code> <p>Optional 2D tensor of document IDs (for packed/flat batching).</p> <code>None</code> <code>position_ids</code> <code>Tensor | None</code> <p>Optional 2D tensor of position IDs (for RoPE).</p> <code>None</code> <code>cu_seqlens</code> <code>Tensor | None</code> <p>Optional 1D tensor of cumulative sequence lengths (for varlen attention).</p> <code>None</code> <code>max_seqlen</code> <code>int | None</code> <p>Optional maximum sequence length in the packed batch.</p> <code>None</code> <code>**kwargs</code> <p>Extra arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Dictionary containing model logits.</p> Source code in <code>optimus_dl/modules/model/llama2.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    seq_lens: torch.Tensor | None = None,\n    document_ids: torch.Tensor | None = None,\n    position_ids: torch.Tensor | None = None,\n    cu_seqlens: torch.Tensor | None = None,\n    max_seqlen: int | None = None,\n    **kwargs,\n):\n    \"\"\"Perform the forward pass, handling rotary frequency lookup and optional masking.\n\n    Args:\n        input_ids: Tensor of shape (B, T).\n        seq_lens: Optional 1D tensor of sequence lengths (for padding).\n        document_ids: Optional 2D tensor of document IDs (for packed/flat batching).\n        position_ids: Optional 2D tensor of position IDs (for RoPE).\n        cu_seqlens: Optional 1D tensor of cumulative sequence lengths (for varlen attention).\n        max_seqlen: Optional maximum sequence length in the packed batch.\n        **kwargs: Extra arguments.\n\n    Returns:\n        Dictionary containing model logits.\n    \"\"\"\n    idx = input_ids\n    device = idx.device\n    _, t = idx.size()\n\n    if position_ids is None:\n        pos = torch.arange(0, t, dtype=torch.long, device=device)\n        # (T, hs/2, 2)\n        freqs_cis = self.freqs_cis.to(device)[pos]\n    else:\n        # position_ids: (B, T)\n        # self.freqs_cis: (max_T, hs/2, 2)\n        # Result: (B, T, hs/2, 2)\n        # However, RotaryTransformerBlock expects freqs_cis to be (T, hs/2, 2)\n        # OR (B, T, hs/2, 2) if it supports it.\n        # My updated apply_rotary_emb supports passing position_ids separately.\n        # So I will pass the FULL freqs_cis and the position_ids.\n        freqs_cis = self.freqs_cis.to(device)\n\n    tok_emb = self.transformer.wte(idx)\n    x = self.transformer.drop(tok_emb)\n\n    for block in self.transformer.h:\n        block_kwargs = {\n            \"x\": x,\n            \"freqs_cis\": freqs_cis,\n            \"seq_lens\": seq_lens,\n            \"document_ids\": document_ids,\n            \"position_ids\": position_ids,\n            \"cu_seqlens\": cu_seqlens,\n            \"max_seqlen\": max_seqlen,\n        }\n        # Filter out None values to avoid triggering TP input preparation on None inputs\n        block_kwargs = {k: v for k, v in block_kwargs.items() if v is not None}\n        x = block(**block_kwargs)\n    x = self.transformer.ln_f(x)\n\n    logits = self.lm_head(x)\n\n    return {\n        \"logits\": logits,\n    }\n</code></pre>"},{"location":"reference/modules/model/llama2/#optimus_dl.modules.model.llama2.LlamaBlock","title":"<code>LlamaBlock</code>","text":"<p>               Bases: <code>RotaryTransformerBlock</code></p> <p>Llama Transformer block with RMSNorm, Rotary Attention, and SwiGLU MLP.</p> Source code in <code>optimus_dl/modules/model/llama2.py</code> <pre><code>class LlamaBlock(RotaryTransformerBlock):\n    \"\"\"Llama Transformer block with RMSNorm, Rotary Attention, and SwiGLU MLP.\"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(\n            n_embd=config.n_embd,\n            n_head=config.n_head,\n            n_kv_head=config.n_kv_head,\n            dropout=config.dropout,\n            rmsnorm_eps=config.rmsnorm_eps,\n            bias=config.bias,\n            attention_bias=config.attention_bias,\n            use_qk_norm=False,\n            intermediate_size=config.intermediate_size,\n            multiple_of=config.multiple_of,\n            use_liger_rmsnorm=config.use_liger_rmsnorm,\n            use_liger_swiglu=config.use_liger_swiglu,\n        )\n</code></pre>"},{"location":"reference/modules/model/llama2/#optimus_dl.modules.model.llama2.LlamaConfig","title":"<code>LlamaConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GPTConfig</code></p> <p>Configuration for Llama-style models.</p> <p>Parameters:</p> Name Type Description Default <code>bias</code> <code>bool</code> <p>Whether to use bias (usually False for Llama).</p> <code>False</code> <code>tie_word_embeddings</code> <code>bool</code> <p>Whether to tie input and output embeddings.</p> <code>True</code> <code>sequence_length</code> <code>int</code> <p>Maximum context length.</p> <code>16000</code> <code>rmsnorm_eps</code> <code>float</code> <p>Epsilon for RMSNorm.</p> <code>1e-05</code> <code>attention_bias</code> <code>bool</code> <p>Specific bias flag for attention projections.</p> <code>False</code> <code>n_kv_head</code> <code>int | None</code> <p>Number of Key/Value heads (for GQA). If None, will be set to num_attention_heads.</p> <code>None</code> <code>intermediate_size</code> <code>int | None</code> <p>Dimension of SwiGLU hidden layer. If None, will be set based on multiple_of</p> <code>None</code> <code>multiple_of</code> <code>int</code> <p>Make SwiGLU hidden layer size multiple of large power of 2</p> <code>256</code> <code>rope_theta</code> <code>float</code> <p>Base frequency for rotary embeddings.</p> <code>10000.0</code> <code>rope_scaling</code> <code>dict | None</code> <p>RoPE scaling configuration.</p> <code>None</code> <code>use_liger_rmsnorm</code> <code>bool | None</code> <p>Enable Liger-kernel for RMSNorm. None = auto-enable if available.</p> <code>None</code> <code>use_liger_swiglu</code> <code>bool | None</code> <p>Enable Liger-kernel for SwiGLU. None = auto-enable if available.</p> <code>None</code> Source code in <code>optimus_dl/modules/model/llama2.py</code> <pre><code>@dataclass\nclass LlamaConfig(GPTConfig):\n    \"\"\"Configuration for Llama-style models.\"\"\"\n\n    sequence_length: int = field(\n        default=16000,\n        metadata={\"description\": \"Maximum context length.\"},\n    )\n    rmsnorm_eps: float = field(\n        default=1e-5,\n        metadata={\"description\": \"Epsilon for RMSNorm.\"},\n    )\n    bias: bool = field(\n        default=False,\n        metadata={\"description\": \"Whether to use bias (usually False for Llama).\"},\n    )\n    attention_bias: bool = field(\n        default=False,\n        metadata={\"description\": \"Specific bias flag for attention projections.\"},\n    )\n    tie_word_embeddings: bool = field(\n        default=True,\n        metadata={\"description\": \"Whether to tie input and output embeddings.\"},\n    )\n    n_kv_head: int | None = field(\n        default=None,\n        metadata={\n            \"description\": \"Number of Key/Value heads (for GQA). If None, will be set to num_attention_heads.\"\n        },\n    )\n    intermediate_size: int | None = field(\n        default=None,\n        metadata={\n            \"description\": \"Dimension of SwiGLU hidden layer. If None, will be set based on multiple_of\"\n        },\n    )\n    multiple_of: int = field(\n        default=256,\n        metadata={\n            \"description\": \"Make SwiGLU hidden layer size multiple of large power of 2\"\n        },\n    )\n    rope_theta: float = field(\n        default=10000.0,\n        metadata={\"description\": \"Base frequency for rotary embeddings.\"},\n    )\n    rope_scaling: dict | None = field(\n        default=None,\n        metadata={\"description\": \"RoPE scaling configuration.\"},\n    )\n    use_liger_rmsnorm: bool | None = field(\n        default=None,\n        metadata={\n            \"description\": \"Enable Liger-kernel for RMSNorm. None = auto-enable if available.\"\n        },\n    )\n    use_liger_swiglu: bool | None = field(\n        default=None,\n        metadata={\n            \"description\": \"Enable Liger-kernel for SwiGLU. None = auto-enable if available.\"\n        },\n    )\n</code></pre>"},{"location":"reference/modules/model/olmo3/","title":"olmo3","text":""},{"location":"reference/modules/model/olmo3/#optimus_dl.modules.model.olmo3","title":"<code>optimus_dl.modules.model.olmo3</code>","text":"<p>Olmo3 Language Model implementation. Features alternating sliding window and full attention, YaRN RoPE, and SwiGLU MLP.</p>"},{"location":"reference/modules/model/olmo3/#optimus_dl.modules.model.olmo3.Olmo3","title":"<code>Olmo3</code>","text":"<p>               Bases: <code>GPT</code></p> <p>Olmo3 Language Model architecture.</p> Source code in <code>optimus_dl/modules/model/olmo3.py</code> <pre><code>@register_model(\"olmo3\", Olmo3Config)\nclass Olmo3(GPT):\n    \"\"\"Olmo3 Language Model architecture.\"\"\"\n\n    def __init__(self, config: Olmo3Config, **kwargs):\n        super().__init__(config)\n        self.config = config\n\n        assert config.n_layer == len(\n            self.config.layer_types\n        ), \"Number of layers must match the length of layer_types\"\n\n        self.head_dim = (\n            config.head_dim\n            if config.head_dim is not None\n            else config.n_embd // config.n_head\n        )\n\n        # Olmo3 uses a single rotary embedding for the entire model\n        rope_params = config.rope_parameters.copy()\n        if \"rope_theta\" not in rope_params:\n            rope_params[\"rope_theta\"] = config.rope_theta\n\n        self.freqs_cis = precompute_freqs_cis(\n            self.head_dim,\n            config.sequence_length,\n            theta=rope_params[\"rope_theta\"],\n            scaling_config=rope_params,\n        )\n\n        self.transformer = nn.ModuleDict(\n            {\n                \"wte\": nn.Embedding(\n                    config.vocab_size,\n                    config.n_embd,\n                    padding_idx=config.padding_token_id,\n                ),\n                \"drop\": nn.Dropout(config.dropout),\n                \"h\": nn.ModuleList(\n                    [Olmo3Block(config, i) for i in range(config.n_layer)]\n                ),\n                \"ln_f\": RMSNorm(\n                    config.n_embd,\n                    eps=config.rmsnorm_eps,\n                    use_liger=config.use_liger_rmsnorm,\n                ),\n            }\n        )\n\n        self.apply(self._init_weights)\n        for pn, p in self.named_parameters():\n            if pn.endswith(\"c_proj.weight\"):\n                torch.nn.init.normal_(\n                    p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer)\n                )\n\n        if config.tie_word_embeddings:\n            self.transformer.wte.weight = self.lm_head.weight\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        seq_lens: torch.Tensor | None = None,\n        document_ids: torch.Tensor | None = None,\n        position_ids: torch.Tensor | None = None,\n        cu_seqlens: torch.Tensor | None = None,\n        max_seqlen: int | None = None,\n        **kwargs,\n    ):\n        idx = input_ids\n        device = idx.device\n        b, t = idx.size()\n\n        tok_emb = self.transformer.wte(idx)\n        x = self.transformer.drop(tok_emb)\n\n        self.freqs_cis = self.freqs_cis.to(x.device)\n        if position_ids is None:\n            pos = torch.arange(0, t, dtype=torch.long, device=device)\n            freqs_cis = self.freqs_cis[pos]\n        else:\n            freqs_cis = self.freqs_cis\n\n        for block in self.transformer.h:\n            block_kwargs = {\n                \"x\": x,\n                \"freqs_cis\": freqs_cis,\n                \"seq_lens\": seq_lens,\n                \"document_ids\": document_ids,\n                \"position_ids\": position_ids,\n                \"cu_seqlens\": cu_seqlens,\n                \"max_seqlen\": max_seqlen,\n            }\n            # Filter out None values to avoid triggering TP input preparation on None inputs\n            block_kwargs = {k: v for k, v in block_kwargs.items() if v is not None}\n            x = block(**block_kwargs)\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x)\n\n        return {\"logits\": logits}\n\n    def apply_tp(\n        self, mesh, loss_parallel: bool = False, sequence_parallel: bool = False\n    ):\n        \"\"\"Apply Tensor Parallelism plan to the Olmo3 model.\"\"\"\n        from torch.distributed.tensor.parallel import (\n            ColwiseParallel,\n            PrepareModuleInput,\n            PrepareModuleOutput,\n            RowwiseParallel,\n            SequenceParallel,\n            parallelize_module,\n        )\n\n        tp_size = mesh.size(0)\n        n_kv_head = (\n            self.config.n_kv_head\n            if self.config.n_kv_head is not None\n            else self.config.n_head\n        )\n\n        assert self.config.n_head % tp_size == 0\n        assert n_kv_head % tp_size == 0\n\n        layer_plan = {\n            \"transformer.wte\": RowwiseParallel(input_layouts=Replicate()),\n            \"transformer.h.*.attn.wq\": ColwiseParallel(use_local_output=False),\n            \"transformer.h.*.attn.wk\": ColwiseParallel(use_local_output=False),\n            \"transformer.h.*.attn.wv\": ColwiseParallel(use_local_output=False),\n            \"transformer.h.*.attn.wo\": RowwiseParallel(),\n            \"transformer.h.*.mlp.w1\": ColwiseParallel(use_local_output=False),\n            \"transformer.h.*.mlp.w2\": ColwiseParallel(use_local_output=False),\n            \"transformer.h.*.mlp.c_proj\": RowwiseParallel(),\n            \"lm_head\": ColwiseParallel(use_local_output=False),\n        }\n\n        if sequence_parallel:\n            layer_plan.update(\n                {\n                    \"transformer.wte\": RowwiseParallel(\n                        input_layouts=Replicate(),\n                        output_layouts=Shard(1),\n                        use_local_output=True,\n                    ),\n                    \"transformer.h.*.post_attention_layernorm\": SequenceParallel(),\n                    \"transformer.h.*.post_feedforward_layernorm\": SequenceParallel(),\n                    \"transformer.ln_f\": SequenceParallel(),\n                    \"transformer.h.*\": PrepareModuleInput(\n                        input_kwarg_layouts=dict(\n                            x=Shard(1),\n                            freqs_cis=Replicate(),\n                            seq_lens=Replicate(),\n                            document_ids=Replicate(),\n                            position_ids=Replicate(),\n                            cu_seqlens=Replicate(),\n                        ),\n                        desired_input_kwarg_layouts=dict(\n                            x=Shard(1),\n                            freqs_cis=Replicate(),\n                            seq_lens=Replicate(),\n                            document_ids=Replicate(),\n                            position_ids=Replicate(),\n                            cu_seqlens=Replicate(),\n                        ),\n                        use_local_output=False,\n                    ),\n                    \"transformer.h.*.attn.wo\": RowwiseParallel(\n                        output_layouts=Shard(1), use_local_output=False\n                    ),\n                    \"transformer.h.*.mlp\": PrepareModuleInput(\n                        input_layouts=(Shard(1),),\n                        desired_input_layouts=(Shard(1),),\n                        use_local_output=False,\n                    ),\n                    \"transformer.h.*.mlp.w1\": ColwiseParallel(\n                        input_layouts=Shard(1), use_local_output=False\n                    ),\n                    \"transformer.h.*.mlp.w2\": ColwiseParallel(\n                        input_layouts=Shard(1), use_local_output=False\n                    ),\n                    \"transformer.h.*.mlp.c_proj\": RowwiseParallel(\n                        output_layouts=Shard(1), use_local_output=False\n                    ),\n                    \"lm_head\": ColwiseParallel(\n                        input_layouts=Shard(1), use_local_output=False\n                    ),\n                }\n            )\n\n        parallelize_module(self, mesh, layer_plan)\n\n        if self.config.tie_word_embeddings:\n            self.transformer.wte.weight = self.lm_head.weight\n\n        if not loss_parallel:\n            parallelize_module(\n                self.lm_head,\n                mesh,\n                PrepareModuleOutput(\n                    output_layouts=Shard(2),\n                    desired_output_layouts=Replicate(),\n                    use_local_output=False,\n                ),\n            )\n</code></pre>"},{"location":"reference/modules/model/olmo3/#optimus_dl.modules.model.olmo3.Olmo3.apply_tp","title":"<code>apply_tp(mesh, loss_parallel=False, sequence_parallel=False)</code>","text":"<p>Apply Tensor Parallelism plan to the Olmo3 model.</p> Source code in <code>optimus_dl/modules/model/olmo3.py</code> <pre><code>def apply_tp(\n    self, mesh, loss_parallel: bool = False, sequence_parallel: bool = False\n):\n    \"\"\"Apply Tensor Parallelism plan to the Olmo3 model.\"\"\"\n    from torch.distributed.tensor.parallel import (\n        ColwiseParallel,\n        PrepareModuleInput,\n        PrepareModuleOutput,\n        RowwiseParallel,\n        SequenceParallel,\n        parallelize_module,\n    )\n\n    tp_size = mesh.size(0)\n    n_kv_head = (\n        self.config.n_kv_head\n        if self.config.n_kv_head is not None\n        else self.config.n_head\n    )\n\n    assert self.config.n_head % tp_size == 0\n    assert n_kv_head % tp_size == 0\n\n    layer_plan = {\n        \"transformer.wte\": RowwiseParallel(input_layouts=Replicate()),\n        \"transformer.h.*.attn.wq\": ColwiseParallel(use_local_output=False),\n        \"transformer.h.*.attn.wk\": ColwiseParallel(use_local_output=False),\n        \"transformer.h.*.attn.wv\": ColwiseParallel(use_local_output=False),\n        \"transformer.h.*.attn.wo\": RowwiseParallel(),\n        \"transformer.h.*.mlp.w1\": ColwiseParallel(use_local_output=False),\n        \"transformer.h.*.mlp.w2\": ColwiseParallel(use_local_output=False),\n        \"transformer.h.*.mlp.c_proj\": RowwiseParallel(),\n        \"lm_head\": ColwiseParallel(use_local_output=False),\n    }\n\n    if sequence_parallel:\n        layer_plan.update(\n            {\n                \"transformer.wte\": RowwiseParallel(\n                    input_layouts=Replicate(),\n                    output_layouts=Shard(1),\n                    use_local_output=True,\n                ),\n                \"transformer.h.*.post_attention_layernorm\": SequenceParallel(),\n                \"transformer.h.*.post_feedforward_layernorm\": SequenceParallel(),\n                \"transformer.ln_f\": SequenceParallel(),\n                \"transformer.h.*\": PrepareModuleInput(\n                    input_kwarg_layouts=dict(\n                        x=Shard(1),\n                        freqs_cis=Replicate(),\n                        seq_lens=Replicate(),\n                        document_ids=Replicate(),\n                        position_ids=Replicate(),\n                        cu_seqlens=Replicate(),\n                    ),\n                    desired_input_kwarg_layouts=dict(\n                        x=Shard(1),\n                        freqs_cis=Replicate(),\n                        seq_lens=Replicate(),\n                        document_ids=Replicate(),\n                        position_ids=Replicate(),\n                        cu_seqlens=Replicate(),\n                    ),\n                    use_local_output=False,\n                ),\n                \"transformer.h.*.attn.wo\": RowwiseParallel(\n                    output_layouts=Shard(1), use_local_output=False\n                ),\n                \"transformer.h.*.mlp\": PrepareModuleInput(\n                    input_layouts=(Shard(1),),\n                    desired_input_layouts=(Shard(1),),\n                    use_local_output=False,\n                ),\n                \"transformer.h.*.mlp.w1\": ColwiseParallel(\n                    input_layouts=Shard(1), use_local_output=False\n                ),\n                \"transformer.h.*.mlp.w2\": ColwiseParallel(\n                    input_layouts=Shard(1), use_local_output=False\n                ),\n                \"transformer.h.*.mlp.c_proj\": RowwiseParallel(\n                    output_layouts=Shard(1), use_local_output=False\n                ),\n                \"lm_head\": ColwiseParallel(\n                    input_layouts=Shard(1), use_local_output=False\n                ),\n            }\n        )\n\n    parallelize_module(self, mesh, layer_plan)\n\n    if self.config.tie_word_embeddings:\n        self.transformer.wte.weight = self.lm_head.weight\n\n    if not loss_parallel:\n        parallelize_module(\n            self.lm_head,\n            mesh,\n            PrepareModuleOutput(\n                output_layouts=Shard(2),\n                desired_output_layouts=Replicate(),\n                use_local_output=False,\n            ),\n        )\n</code></pre>"},{"location":"reference/modules/model/olmo3/#optimus_dl.modules.model.olmo3.Olmo3Attention","title":"<code>Olmo3Attention</code>","text":"<p>               Bases: <code>RotarySelfAttention</code></p> <p>Olmo3 Attention supporting sliding window and Q/K normalization.</p> Source code in <code>optimus_dl/modules/model/olmo3.py</code> <pre><code>class Olmo3Attention(RotarySelfAttention):\n    \"\"\"Olmo3 Attention supporting sliding window and Q/K normalization.\"\"\"\n\n    def __init__(self, config: Olmo3Config, layer_idx: int):\n        self.layer_type = config.layer_types[layer_idx]\n        self.layer_idx = layer_idx\n        assert self.layer_type in (\"sliding_attention\", \"full_attention\")\n        sliding_window = (\n            config.sliding_window if self.layer_type == \"sliding_attention\" else None\n        )\n        super().__init__(\n            n_embd=config.n_embd,\n            n_head=config.n_head,\n            n_kv_head=config.n_kv_head,\n            head_dim=config.head_dim,\n            dropout=config.dropout,\n            bias=config.attention_bias,\n            use_qk_norm=True,\n            qk_norm_per_head=False,  # Olmo3 uses across-heads norm\n            rmsnorm_eps=config.rmsnorm_eps,\n            sliding_window=sliding_window,\n        )\n</code></pre>"},{"location":"reference/modules/model/olmo3/#optimus_dl.modules.model.olmo3.Olmo3Block","title":"<code>Olmo3Block</code>","text":"<p>               Bases: <code>Module</code></p> <p>Olmo3 Transformer block.</p> <p>Architecture: x = x + Norm(Attn(x)) x = x + Norm(MLP(x))</p> Source code in <code>optimus_dl/modules/model/olmo3.py</code> <pre><code>class Olmo3Block(nn.Module):\n    \"\"\"Olmo3 Transformer block.\n\n    Architecture:\n    x = x + Norm(Attn(x))\n    x = x + Norm(MLP(x))\n    \"\"\"\n\n    def __init__(self, config: Olmo3Config, layer_idx: int):\n        super().__init__()\n        self.attn = Olmo3Attention(config, layer_idx)\n        self.post_attention_layernorm = RMSNorm(\n            config.n_embd, eps=config.rmsnorm_eps, use_liger=config.use_liger_rmsnorm\n        )\n        self.mlp = SwiGLUMLP(\n            n_embd=config.n_embd,\n            intermediate_size=config.intermediate_size,\n            multiple_of=config.multiple_of,\n            bias=False,\n            use_liger=config.use_liger_swiglu,\n        )\n        self.post_feedforward_layernorm = RMSNorm(\n            config.n_embd, eps=config.rmsnorm_eps, use_liger=config.use_liger_rmsnorm\n        )\n\n    def forward(\n        self,\n        x,\n        freqs_cis,\n        seq_lens: torch.Tensor | None = None,\n        document_ids: torch.Tensor | None = None,\n        position_ids: torch.Tensor | None = None,\n        cu_seqlens: torch.Tensor | None = None,\n        max_seqlen: int | None = None,\n    ):\n        # x = x + Norm(attn(x))\n        x = x + self.post_attention_layernorm(\n            self.attn(\n                x,\n                freqs_cis=freqs_cis,\n                seq_lens=seq_lens,\n                document_ids=document_ids,\n                position_ids=position_ids,\n                cu_seqlens=cu_seqlens,\n                max_seqlen=max_seqlen,\n            )\n        )\n        # x = x + Norm(mlp(x))\n        x = x + self.post_feedforward_layernorm(self.mlp(x))\n        return x\n</code></pre>"},{"location":"reference/modules/model/olmo3/#optimus_dl.modules.model.olmo3.Olmo3Config","title":"<code>Olmo3Config</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GPTConfig</code></p> <p>Configuration for Olmo3-style models.</p> <p>Parameters:</p> Name Type Description Default <code>n_layer</code> <code>int</code> <p>Number of transformer blocks</p> <code>16</code> <code>head_dim</code> <code>int | None</code> <p>Dimensionality of each attention head. If None, will be set to hidden_size // num_attention_heads.</p> <code>None</code> <code>bias</code> <code>bool</code> <p>Global bias flag for linear layers.</p> <code>False</code> <code>tie_word_embeddings</code> <code>bool</code> <p>Tie input and output embeddings.</p> <code>False</code> <code>sequence_length</code> <code>int</code> <p>Maximum context length.</p> <code>4096</code> <code>rmsnorm_eps</code> <code>float</code> <p>Epsilon for RMSNorm.</p> <code>1e-06</code> <code>rope_theta</code> <code>float</code> <p>Base frequency for rotary embeddings.</p> <code>500000.0</code> <code>rope_parameters</code> <code>dict</code> <p>Full RoPE configuration dictionary.</p> <code>{'rope_type': 'default'}</code> <code>attention_bias</code> <code>bool</code> <p>Specific bias flag for attention projections.</p> <code>False</code> <code>n_kv_head</code> <code>int | None</code> <p>Number of Key/Value heads. If None, will be set to num_attention_heads.</p> <code>4</code> <code>intermediate_size</code> <code>int | None</code> <p>Dimension of SwiGLU hidden layer.</p> <code>1024</code> <code>multiple_of</code> <code>int</code> <p>Make SwiGLU hidden layer size multiple of large power of 2</p> <code>256</code> <code>sliding_window</code> <code>int</code> <p>Window size for sliding window attention.</p> <code>4096</code> <code>layer_types</code> <code>list[str]</code> <p>List of attention types for each layer.</p> <code>['sliding_attention', 'sliding_attention', 'sliding_attention', 'full_attention', 'sliding_attention', 'sliding_attention', 'sliding_attention', 'full_attention', 'sliding_attention', 'sliding_attention', 'sliding_attention', 'full_attention', 'sliding_attention', 'sliding_attention', 'sliding_attention', 'full_attention']</code> <code>use_liger_rmsnorm</code> <code>bool | None</code> <p>Enable Liger-kernel for RMSNorm. None = auto-enable if available.</p> <code>None</code> <code>use_liger_swiglu</code> <code>bool | None</code> <p>Enable Liger-kernel for SwiGLU. None = auto-enable if available.</p> <code>None</code> Source code in <code>optimus_dl/modules/model/olmo3.py</code> <pre><code>@dataclass\nclass Olmo3Config(GPTConfig):\n    \"\"\"Configuration for Olmo3-style models.\"\"\"\n\n    sequence_length: int = field(\n        default=4096,\n        metadata={\"description\": \"Maximum context length.\"},\n    )\n    rmsnorm_eps: float = field(\n        default=1e-6,\n        metadata={\"description\": \"Epsilon for RMSNorm.\"},\n    )\n    rope_theta: float = field(\n        default=500000.0,\n        metadata={\"description\": \"Base frequency for rotary embeddings.\"},\n    )\n    rope_parameters: dict = field(\n        default_factory=lambda: {\n            \"rope_type\": \"default\",\n        },\n        metadata={\"description\": \"Full RoPE configuration dictionary.\"},\n    )\n    head_dim: int | None = field(\n        default=None,\n        metadata={\n            \"description\": \"Dimensionality of each attention head. If None, will be set to hidden_size // num_attention_heads.\"\n        },\n    )\n    bias: bool = field(\n        default=False,\n        metadata={\"description\": \"Global bias flag for linear layers.\"},\n    )\n    attention_bias: bool = field(\n        default=False,\n        metadata={\"description\": \"Specific bias flag for attention projections.\"},\n    )\n    tie_word_embeddings: bool = field(\n        default=False,\n        metadata={\"description\": \"Tie input and output embeddings.\"},\n    )\n    n_kv_head: int | None = field(\n        default=4,\n        metadata={\n            \"description\": \"Number of Key/Value heads. If None, will be set to num_attention_heads.\"\n        },\n    )\n    intermediate_size: int | None = field(\n        default=1024,\n        metadata={\"description\": \"Dimension of SwiGLU hidden layer.\"},\n    )\n    multiple_of: int = field(\n        default=256,\n        metadata={\n            \"description\": \"Make SwiGLU hidden layer size multiple of large power of 2\"\n        },\n    )\n    sliding_window: int = field(\n        default=4096,\n        metadata={\"description\": \"Window size for sliding window attention.\"},\n    )\n    n_layer: int = field(\n        default=16, metadata={\"description\": \"Number of transformer blocks\"}\n    )\n    layer_types: list[str] = field(\n        default_factory=lambda: [\n            \"sliding_attention\",\n            \"sliding_attention\",\n            \"sliding_attention\",\n            \"full_attention\",\n        ]\n        * 4,\n        metadata={\"description\": \"List of attention types for each layer.\"},\n    )\n    use_liger_rmsnorm: bool | None = field(\n        default=None,\n        metadata={\n            \"description\": \"Enable Liger-kernel for RMSNorm. None = auto-enable if available.\"\n        },\n    )\n    use_liger_swiglu: bool | None = field(\n        default=None,\n        metadata={\n            \"description\": \"Enable Liger-kernel for SwiGLU. None = auto-enable if available.\"\n        },\n    )\n</code></pre>"},{"location":"reference/modules/model/qwen3/","title":"qwen3","text":""},{"location":"reference/modules/model/qwen3/#optimus_dl.modules.model.qwen3","title":"<code>optimus_dl.modules.model.qwen3</code>","text":"<p>Qwen3 Language Model implementation. Features Q/K normalization in attention, optional biases, and SwiGLU MLP.</p>"},{"location":"reference/modules/model/qwen3/#optimus_dl.modules.model.qwen3.Qwen3","title":"<code>Qwen3</code>","text":"<p>               Bases: <code>GPT</code></p> <p>Qwen3 Language Model architecture.</p> <p>Extends the framework's GPT base with Qwen-specific features:</p> <ul> <li>Q/K Normalization: Applies RMSNorm to Query and Key tensors before   attention computation to improve training stability.</li> <li>Configurable Biases: Supports bias in attention and MLP layers.</li> <li>Large Context: Optimized for very long sequence lengths.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Qwen3Config</code> <p>Qwen3 model configuration.</p> required Source code in <code>optimus_dl/modules/model/qwen3.py</code> <pre><code>@register_model(\"qwen3\", Qwen3Config)\nclass Qwen3(GPT):\n    \"\"\"Qwen3 Language Model architecture.\n\n    Extends the framework's GPT base with Qwen-specific features:\n\n    - **Q/K Normalization**: Applies RMSNorm to Query and Key tensors before\n      attention computation to improve training stability.\n    - **Configurable Biases**: Supports bias in attention and MLP layers.\n    - **Large Context**: Optimized for very long sequence lengths.\n\n    Args:\n        config: Qwen3 model configuration.\n    \"\"\"\n\n    def __init__(self, config: Qwen3Config, **kwargs):\n        super().__init__(config)\n        assert config.vocab_size is not None\n        assert config.sequence_length is not None\n        self.config = config\n\n        # create the token and position embeddings\n        self.head_dim = (\n            config.head_dim\n            if config.head_dim is not None\n            else config.n_embd // config.n_head\n        )\n        self.freqs_cis = precompute_freqs_cis(\n            self.head_dim,\n            config.sequence_length,\n            theta=config.rope_theta,\n            scaling_config=config.rope_scaling,\n        )\n\n        self.transformer = nn.ModuleDict(\n            {\n                \"wte\": nn.Embedding(\n                    config.vocab_size,\n                    config.n_embd,\n                    padding_idx=config.padding_token_id,\n                ),\n                \"drop\": nn.Dropout(config.dropout),\n                \"h\": nn.ModuleList([Qwen3Block(config) for _ in range(config.n_layer)]),\n                \"ln_f\": RMSNorm(\n                    config.n_embd,\n                    eps=config.rmsnorm_eps,\n                    use_liger=config.use_liger_rmsnorm,\n                ),\n            }\n        )\n        # Weight tying\n        if config.tie_word_embeddings:\n            self.transformer.wte.weight = self.lm_head.weight\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith(\"c_proj.weight\"):\n                torch.nn.init.normal_(\n                    p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer)\n                )\n\n    def apply_tp(\n        self, mesh, loss_parallel: bool = False, sequence_parallel: bool = False\n    ):\n        \"\"\"Apply Tensor Parallelism plan to the Qwen3 model.\n\n        Similar to the Llama plan but handles Qwen3-specific parameter names\n        and bias configurations.\n\n        Args:\n            mesh: DeviceMesh for sharding.\n            loss_parallel: If True, shards the LM head.\n            sequence_parallel: If True, enables sequence sharding.\n        \"\"\"\n        tp_size = mesh.size(0)\n        assert (\n            self.config.n_head % tp_size == 0\n        ), f\"Number of heads ({self.config.n_head}) must be divisible by TP size ({tp_size})\"\n        n_kv_head = (\n            self.config.n_kv_head\n            if self.config.n_kv_head is not None\n            else self.config.n_head\n        )\n        assert (\n            n_kv_head % tp_size == 0\n        ), f\"Number of KV heads ({n_kv_head}) must be divisible by TP size ({tp_size})\"\n\n        from torch.distributed.tensor.parallel import (\n            ColwiseParallel,\n            PrepareModuleInput,\n            PrepareModuleOutput,\n            RowwiseParallel,\n            SequenceParallel,\n            parallelize_module,\n        )\n\n        layer_plan = {\n            \"transformer.wte\": RowwiseParallel(\n                input_layouts=Replicate(),\n            ),\n            \"transformer.h.*.attn.wq\": ColwiseParallel(use_local_output=False),\n            \"transformer.h.*.attn.wk\": ColwiseParallel(use_local_output=False),\n            \"transformer.h.*.attn.wv\": ColwiseParallel(use_local_output=False),\n            \"transformer.h.*.attn.wo\": RowwiseParallel(),\n            \"transformer.h.*.mlp.w1\": ColwiseParallel(use_local_output=False),\n            \"transformer.h.*.mlp.w2\": ColwiseParallel(use_local_output=False),\n            \"transformer.h.*.mlp.c_proj\": RowwiseParallel(),\n            \"lm_head\": ColwiseParallel(use_local_output=False),\n        }\n        if sequence_parallel:\n            layer_plan.update(\n                {\n                    \"transformer.wte\": RowwiseParallel(\n                        input_layouts=Replicate(),\n                        output_layouts=Shard(1),\n                        use_local_output=True,\n                    ),\n                    \"transformer.h.*.ln_1\": SequenceParallel(),\n                    \"transformer.h.*.ln_2\": SequenceParallel(),\n                    \"transformer.ln_f\": SequenceParallel(),\n                    \"transformer.h.*\": PrepareModuleInput(\n                        input_kwarg_layouts=dict(\n                            x=Shard(1),\n                            freqs_cis=Replicate(),\n                            seq_lens=Replicate(),\n                            document_ids=Replicate(),\n                            position_ids=Replicate(),\n                            cu_seqlens=Replicate(),\n                        ),\n                        desired_input_kwarg_layouts=dict(\n                            x=Shard(1),\n                            freqs_cis=Replicate(),\n                            seq_lens=Replicate(),\n                            document_ids=Replicate(),\n                            position_ids=Replicate(),\n                            cu_seqlens=Replicate(),\n                        ),\n                        use_local_output=False,\n                    ),\n                    \"transformer.h.*.attn.wo\": RowwiseParallel(\n                        output_layouts=Shard(1), use_local_output=False\n                    ),\n                    \"transformer.h.*.mlp.w1\": ColwiseParallel(\n                        input_layouts=Shard(1), use_local_output=False\n                    ),\n                    \"transformer.h.*.mlp.w2\": ColwiseParallel(\n                        input_layouts=Shard(1), use_local_output=False\n                    ),\n                    \"transformer.h.*.mlp.c_proj\": RowwiseParallel(\n                        output_layouts=Shard(1), use_local_output=False\n                    ),\n                    \"lm_head\": ColwiseParallel(\n                        input_layouts=Shard(1), use_local_output=False\n                    ),\n                }\n            )\n\n        parallelize_module(self, mesh, layer_plan)\n\n        if self.config.tie_word_embeddings:\n            # re-tie\n            self.transformer.wte.weight = self.lm_head.weight\n\n        if not loss_parallel:\n            parallelize_module(\n                self.lm_head,\n                mesh,\n                PrepareModuleOutput(\n                    output_layouts=Shard(2),\n                    desired_output_layouts=Replicate(),\n                    use_local_output=False,\n                ),\n            )\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        seq_lens: torch.Tensor | None = None,\n        document_ids: torch.Tensor | None = None,\n        position_ids: torch.Tensor | None = None,\n        cu_seqlens: torch.Tensor | None = None,\n        max_seqlen: int | None = None,\n        **kwargs,\n    ):\n        \"\"\"Forward pass with rotary frequency selection.\"\"\"\n        idx = input_ids\n        device = idx.device\n        _, t = idx.size()\n\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n\n        x = self.transformer.drop(tok_emb)\n        self.freqs_cis = self.freqs_cis.to(x.device)\n        if position_ids is None:\n            pos = torch.arange(0, t, dtype=torch.long, device=device)\n            freqs_cis = self.freqs_cis[pos]\n        else:\n            freqs_cis = self.freqs_cis\n\n        for _block_idx, block in enumerate(self.transformer.h):\n            block_kwargs = {\n                \"x\": x,\n                \"freqs_cis\": freqs_cis,\n                \"seq_lens\": seq_lens,\n                \"document_ids\": document_ids,\n                \"position_ids\": position_ids,\n                \"cu_seqlens\": cu_seqlens,\n                \"max_seqlen\": max_seqlen,\n            }\n            # Filter out None values to avoid triggering TP input preparation on None inputs\n            block_kwargs = {k: v for k, v in block_kwargs.items() if v is not None}\n            x = block(**block_kwargs)\n        x = self.transformer.ln_f(x)\n\n        logits = self.lm_head(x)\n\n        return {\n            \"logits\": logits,\n        }\n</code></pre>"},{"location":"reference/modules/model/qwen3/#optimus_dl.modules.model.qwen3.Qwen3.apply_tp","title":"<code>apply_tp(mesh, loss_parallel=False, sequence_parallel=False)</code>","text":"<p>Apply Tensor Parallelism plan to the Qwen3 model.</p> <p>Similar to the Llama plan but handles Qwen3-specific parameter names and bias configurations.</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <p>DeviceMesh for sharding.</p> required <code>loss_parallel</code> <code>bool</code> <p>If True, shards the LM head.</p> <code>False</code> <code>sequence_parallel</code> <code>bool</code> <p>If True, enables sequence sharding.</p> <code>False</code> Source code in <code>optimus_dl/modules/model/qwen3.py</code> <pre><code>def apply_tp(\n    self, mesh, loss_parallel: bool = False, sequence_parallel: bool = False\n):\n    \"\"\"Apply Tensor Parallelism plan to the Qwen3 model.\n\n    Similar to the Llama plan but handles Qwen3-specific parameter names\n    and bias configurations.\n\n    Args:\n        mesh: DeviceMesh for sharding.\n        loss_parallel: If True, shards the LM head.\n        sequence_parallel: If True, enables sequence sharding.\n    \"\"\"\n    tp_size = mesh.size(0)\n    assert (\n        self.config.n_head % tp_size == 0\n    ), f\"Number of heads ({self.config.n_head}) must be divisible by TP size ({tp_size})\"\n    n_kv_head = (\n        self.config.n_kv_head\n        if self.config.n_kv_head is not None\n        else self.config.n_head\n    )\n    assert (\n        n_kv_head % tp_size == 0\n    ), f\"Number of KV heads ({n_kv_head}) must be divisible by TP size ({tp_size})\"\n\n    from torch.distributed.tensor.parallel import (\n        ColwiseParallel,\n        PrepareModuleInput,\n        PrepareModuleOutput,\n        RowwiseParallel,\n        SequenceParallel,\n        parallelize_module,\n    )\n\n    layer_plan = {\n        \"transformer.wte\": RowwiseParallel(\n            input_layouts=Replicate(),\n        ),\n        \"transformer.h.*.attn.wq\": ColwiseParallel(use_local_output=False),\n        \"transformer.h.*.attn.wk\": ColwiseParallel(use_local_output=False),\n        \"transformer.h.*.attn.wv\": ColwiseParallel(use_local_output=False),\n        \"transformer.h.*.attn.wo\": RowwiseParallel(),\n        \"transformer.h.*.mlp.w1\": ColwiseParallel(use_local_output=False),\n        \"transformer.h.*.mlp.w2\": ColwiseParallel(use_local_output=False),\n        \"transformer.h.*.mlp.c_proj\": RowwiseParallel(),\n        \"lm_head\": ColwiseParallel(use_local_output=False),\n    }\n    if sequence_parallel:\n        layer_plan.update(\n            {\n                \"transformer.wte\": RowwiseParallel(\n                    input_layouts=Replicate(),\n                    output_layouts=Shard(1),\n                    use_local_output=True,\n                ),\n                \"transformer.h.*.ln_1\": SequenceParallel(),\n                \"transformer.h.*.ln_2\": SequenceParallel(),\n                \"transformer.ln_f\": SequenceParallel(),\n                \"transformer.h.*\": PrepareModuleInput(\n                    input_kwarg_layouts=dict(\n                        x=Shard(1),\n                        freqs_cis=Replicate(),\n                        seq_lens=Replicate(),\n                        document_ids=Replicate(),\n                        position_ids=Replicate(),\n                        cu_seqlens=Replicate(),\n                    ),\n                    desired_input_kwarg_layouts=dict(\n                        x=Shard(1),\n                        freqs_cis=Replicate(),\n                        seq_lens=Replicate(),\n                        document_ids=Replicate(),\n                        position_ids=Replicate(),\n                        cu_seqlens=Replicate(),\n                    ),\n                    use_local_output=False,\n                ),\n                \"transformer.h.*.attn.wo\": RowwiseParallel(\n                    output_layouts=Shard(1), use_local_output=False\n                ),\n                \"transformer.h.*.mlp.w1\": ColwiseParallel(\n                    input_layouts=Shard(1), use_local_output=False\n                ),\n                \"transformer.h.*.mlp.w2\": ColwiseParallel(\n                    input_layouts=Shard(1), use_local_output=False\n                ),\n                \"transformer.h.*.mlp.c_proj\": RowwiseParallel(\n                    output_layouts=Shard(1), use_local_output=False\n                ),\n                \"lm_head\": ColwiseParallel(\n                    input_layouts=Shard(1), use_local_output=False\n                ),\n            }\n        )\n\n    parallelize_module(self, mesh, layer_plan)\n\n    if self.config.tie_word_embeddings:\n        # re-tie\n        self.transformer.wte.weight = self.lm_head.weight\n\n    if not loss_parallel:\n        parallelize_module(\n            self.lm_head,\n            mesh,\n            PrepareModuleOutput(\n                output_layouts=Shard(2),\n                desired_output_layouts=Replicate(),\n                use_local_output=False,\n            ),\n        )\n</code></pre>"},{"location":"reference/modules/model/qwen3/#optimus_dl.modules.model.qwen3.Qwen3.forward","title":"<code>forward(input_ids, seq_lens=None, document_ids=None, position_ids=None, cu_seqlens=None, max_seqlen=None, **kwargs)</code>","text":"<p>Forward pass with rotary frequency selection.</p> Source code in <code>optimus_dl/modules/model/qwen3.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    seq_lens: torch.Tensor | None = None,\n    document_ids: torch.Tensor | None = None,\n    position_ids: torch.Tensor | None = None,\n    cu_seqlens: torch.Tensor | None = None,\n    max_seqlen: int | None = None,\n    **kwargs,\n):\n    \"\"\"Forward pass with rotary frequency selection.\"\"\"\n    idx = input_ids\n    device = idx.device\n    _, t = idx.size()\n\n    # forward the GPT model itself\n    tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n\n    x = self.transformer.drop(tok_emb)\n    self.freqs_cis = self.freqs_cis.to(x.device)\n    if position_ids is None:\n        pos = torch.arange(0, t, dtype=torch.long, device=device)\n        freqs_cis = self.freqs_cis[pos]\n    else:\n        freqs_cis = self.freqs_cis\n\n    for _block_idx, block in enumerate(self.transformer.h):\n        block_kwargs = {\n            \"x\": x,\n            \"freqs_cis\": freqs_cis,\n            \"seq_lens\": seq_lens,\n            \"document_ids\": document_ids,\n            \"position_ids\": position_ids,\n            \"cu_seqlens\": cu_seqlens,\n            \"max_seqlen\": max_seqlen,\n        }\n        # Filter out None values to avoid triggering TP input preparation on None inputs\n        block_kwargs = {k: v for k, v in block_kwargs.items() if v is not None}\n        x = block(**block_kwargs)\n    x = self.transformer.ln_f(x)\n\n    logits = self.lm_head(x)\n\n    return {\n        \"logits\": logits,\n    }\n</code></pre>"},{"location":"reference/modules/model/qwen3/#optimus_dl.modules.model.qwen3.Qwen3Block","title":"<code>Qwen3Block</code>","text":"<p>               Bases: <code>RotaryTransformerBlock</code></p> <p>Qwen3 Transformer block with Q/K normalization.</p> Source code in <code>optimus_dl/modules/model/qwen3.py</code> <pre><code>class Qwen3Block(RotaryTransformerBlock):\n    \"\"\"Qwen3 Transformer block with Q/K normalization.\"\"\"\n\n    def __init__(self, config: Qwen3Config):\n        super().__init__(\n            n_embd=config.n_embd,\n            n_head=config.n_head,\n            n_kv_head=config.n_kv_head,\n            head_dim=config.head_dim,\n            dropout=config.dropout,\n            rmsnorm_eps=config.rmsnorm_eps,\n            bias=config.bias,\n            attention_bias=config.attention_bias,\n            use_qk_norm=True,\n            qk_norm_per_head=True,\n            intermediate_size=config.intermediate_size,\n            multiple_of=config.multiple_of,\n            use_liger_rmsnorm=config.use_liger_rmsnorm,\n            use_liger_swiglu=config.use_liger_swiglu,\n        )\n</code></pre>"},{"location":"reference/modules/model/qwen3/#optimus_dl.modules.model.qwen3.Qwen3Config","title":"<code>Qwen3Config</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GPTConfig</code></p> <p>Configuration for Qwen3-style models.</p> <p>Parameters:</p> Name Type Description Default <code>head_dim</code> <code>int | None</code> <p>Dimensionality of each attention head. If None, will be set to hidden_size // num_attention_heads.</p> <code>None</code> <code>bias</code> <code>bool</code> <p>Global bias flag for linear layers.</p> <code>False</code> <code>tie_word_embeddings</code> <code>bool</code> <p>Tie input and output embeddings.</p> <code>True</code> <code>sequence_length</code> <code>int</code> <p>Maximum context length.</p> <code>32768</code> <code>rmsnorm_eps</code> <code>float</code> <p>Epsilon for RMSNorm.</p> <code>1e-06</code> <code>rope_theta</code> <code>float</code> <p>Base frequency for rotary embeddings.</p> <code>1000000.0</code> <code>rope_scaling</code> <code>dict | None</code> <p>RoPE scaling configuration.</p> <code>None</code> <code>attention_bias</code> <code>bool</code> <p>Specific bias flag for attention projections.</p> <code>True</code> <code>n_kv_head</code> <code>int | None</code> <p>Number of Key/Value heads. If None, will be set to num_attention_heads.</p> <code>None</code> <code>intermediate_size</code> <code>int | None</code> <p>Dimension of SwiGLU hidden layer. If None, will be set based on multiple_of</p> <code>None</code> <code>multiple_of</code> <code>int</code> <p>Make SwiGLU hidden layer size multiple of large power of 2</p> <code>256</code> <code>use_liger_rmsnorm</code> <code>bool | None</code> <p>Enable Liger-kernel for RMSNorm. None = auto-enable if available.</p> <code>None</code> <code>use_liger_swiglu</code> <code>bool | None</code> <p>Enable Liger-kernel for SwiGLU. None = auto-enable if available.</p> <code>None</code> Source code in <code>optimus_dl/modules/model/qwen3.py</code> <pre><code>@dataclass\nclass Qwen3Config(GPTConfig):\n    \"\"\"Configuration for Qwen3-style models.\"\"\"\n\n    sequence_length: int = field(\n        default=32768,\n        metadata={\"description\": \"Maximum context length.\"},\n    )\n    rmsnorm_eps: float = field(\n        default=1e-6,\n        metadata={\"description\": \"Epsilon for RMSNorm.\"},\n    )\n    rope_theta: float = field(\n        default=1000000.0,\n        metadata={\"description\": \"Base frequency for rotary embeddings.\"},\n    )\n    rope_scaling: dict | None = field(\n        default=None,\n        metadata={\"description\": \"RoPE scaling configuration.\"},\n    )\n    head_dim: int | None = field(\n        default=None,\n        metadata={\n            \"description\": \"Dimensionality of each attention head. If None, will be set to hidden_size // num_attention_heads.\"\n        },\n    )\n    bias: bool = field(\n        default=False,\n        metadata={\"description\": \"Global bias flag for linear layers.\"},\n    )\n    attention_bias: bool = field(\n        default=True,\n        metadata={\"description\": \"Specific bias flag for attention projections.\"},\n    )\n    tie_word_embeddings: bool = field(\n        default=True,\n        metadata={\"description\": \"Tie input and output embeddings.\"},\n    )\n    n_kv_head: int | None = field(\n        default=None,\n        metadata={\n            \"description\": \"Number of Key/Value heads. If None, will be set to num_attention_heads.\"\n        },\n    )\n    intermediate_size: int | None = field(\n        default=None,\n        metadata={\n            \"description\": \"Dimension of SwiGLU hidden layer. If None, will be set based on multiple_of\"\n        },\n    )\n    multiple_of: int = field(\n        default=256,\n        metadata={\n            \"description\": \"Make SwiGLU hidden layer size multiple of large power of 2\"\n        },\n    )\n    use_liger_rmsnorm: bool | None = field(\n        default=None,\n        metadata={\n            \"description\": \"Enable Liger-kernel for RMSNorm. None = auto-enable if available.\"\n        },\n    )\n    use_liger_swiglu: bool | None = field(\n        default=None,\n        metadata={\n            \"description\": \"Enable Liger-kernel for SwiGLU. None = auto-enable if available.\"\n        },\n    )\n</code></pre>"},{"location":"reference/modules/model/blocks/","title":"Index","text":""},{"location":"reference/modules/model/blocks/#optimus_dl.modules.model.blocks","title":"<code>optimus_dl.modules.model.blocks</code>","text":""},{"location":"reference/modules/model/blocks/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>attention</code>: Mask function for flex_attention supporting causal, sliding window, padding, and flat batching.</li> <li><code>layer_norms</code>: LayerNorm with optional bias.</li> <li><code>mlp</code>: SwiGLU MLP variant used in Llama, Qwen, and Mistral.</li> <li><code>rope</code>: Rotary Positional Embeddings (RoPE) implementation.</li> <li><code>transformer</code>: Unified Transformer block with RMSNorm, Rotary Attention, and SwiGLU MLP.</li> </ul>"},{"location":"reference/modules/model/blocks/attention/","title":"attention","text":""},{"location":"reference/modules/model/blocks/attention/#optimus_dl.modules.model.blocks.attention","title":"<code>optimus_dl.modules.model.blocks.attention</code>","text":""},{"location":"reference/modules/model/blocks/attention/#optimus_dl.modules.model.blocks.attention.CausalSelfAttention","title":"<code>CausalSelfAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Standard causal self-attention layer as used in GPT-2.</p> <p>Includes support for dropout and causal masking.</p> <p>Attributes:</p> Name Type Description <code>c_attn</code> <p>Combined Linear layer for query, key, and value projections.</p> <code>c_proj</code> <p>Linear layer for output projection.</p> <code>n_head</code> <p>Number of attention heads.</p> <code>n_embd</code> <p>Embedding dimensionality.</p> <code>dropout</code> <p>Dropout probability.</p> Source code in <code>optimus_dl/modules/model/blocks/attention.py</code> <pre><code>class CausalSelfAttention(nn.Module):\n    \"\"\"Standard causal self-attention layer as used in GPT-2.\n\n    Includes support for dropout and causal masking.\n\n    Attributes:\n        c_attn: Combined Linear layer for query, key, and value projections.\n        c_proj: Linear layer for output projection.\n        n_head: Number of attention heads.\n        n_embd: Embedding dimensionality.\n        dropout: Dropout probability.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Perform the forward pass of causal self-attention.\n\n        Args:\n            x: Input tensor of shape (batch, seq_len, embed_dim).\n\n        Returns:\n            Output tensor of shape (batch, seq_len, embed_dim).\n        \"\"\"\n        B, T, C = (\n            x.size()\n        )  # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n            1, 2\n        )  # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n            1, 2\n        )  # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n            1, 2\n        )  # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)\n        y = torch.nn.functional.scaled_dot_product_attention(\n            q,\n            k,\n            v,\n            attn_mask=None,\n            dropout_p=self.dropout if self.training else 0,\n            is_causal=True,\n        )\n        y = (\n            y.transpose(1, 2).contiguous().view(B, T, C)\n        )  # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n</code></pre>"},{"location":"reference/modules/model/blocks/attention/#optimus_dl.modules.model.blocks.attention.CausalSelfAttention.forward","title":"<code>forward(x)</code>","text":"<p>Perform the forward pass of causal self-attention.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch, seq_len, embed_dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch, seq_len, embed_dim).</p> Source code in <code>optimus_dl/modules/model/blocks/attention.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Perform the forward pass of causal self-attention.\n\n    Args:\n        x: Input tensor of shape (batch, seq_len, embed_dim).\n\n    Returns:\n        Output tensor of shape (batch, seq_len, embed_dim).\n    \"\"\"\n    B, T, C = (\n        x.size()\n    )  # batch size, sequence length, embedding dimensionality (n_embd)\n\n    # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n    q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n    k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n        1, 2\n    )  # (B, nh, T, hs)\n    q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n        1, 2\n    )  # (B, nh, T, hs)\n    v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n        1, 2\n    )  # (B, nh, T, hs)\n\n    # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)\n    y = torch.nn.functional.scaled_dot_product_attention(\n        q,\n        k,\n        v,\n        attn_mask=None,\n        dropout_p=self.dropout if self.training else 0,\n        is_causal=True,\n    )\n    y = (\n        y.transpose(1, 2).contiguous().view(B, T, C)\n    )  # re-assemble all head outputs side by side\n\n    # output projection\n    y = self.resid_dropout(self.c_proj(y))\n    return y\n</code></pre>"},{"location":"reference/modules/model/blocks/attention/#optimus_dl.modules.model.blocks.attention.RotarySelfAttention","title":"<code>RotarySelfAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Generalized Rotary Self-Attention.</p> <p>Supports several modern features:</p> <ul> <li>Grouped Query Attention (GQA): For improved inference efficiency.</li> <li>Rotary Positional Embeddings (RoPE): For better positional encoding.</li> <li>Q/K Normalization: Optional RMSNorm on Query/Key for training stability.</li> <li>Sliding Window Attention: Optional sliding window masking.</li> <li>Dynamic Sequence Padding: Support for <code>seq_lens</code> masking via flex_attention.</li> <li>Variable-length Attention: Support for optimized Flash Attention on packed batches via <code>cu_seqlens</code>.</li> </ul> <p>Attributes:</p> Name Type Description <code>wq</code> <p>Linear projection for Query.</p> <code>wk</code> <p>Linear projection for Key.</p> <code>wv</code> <p>Linear projection for Value.</p> <code>wo</code> <p>Linear projection for Output.</p> <code>q_norm</code> <p>Optional RMSNorm for Query.</p> <code>k_norm</code> <p>Optional RMSNorm for Key.</p> <code>n_head</code> <p>Number of Query heads.</p> <code>n_kv_head</code> <p>Number of Key/Value heads.</p> <code>head_dim</code> <p>Dimensionality of each head.</p> Source code in <code>optimus_dl/modules/model/blocks/attention.py</code> <pre><code>class RotarySelfAttention(nn.Module):\n    \"\"\"Generalized Rotary Self-Attention.\n\n    Supports several modern features:\n\n    - **Grouped Query Attention (GQA)**: For improved inference efficiency.\n    - **Rotary Positional Embeddings (RoPE)**: For better positional encoding.\n    - **Q/K Normalization**: Optional RMSNorm on Query/Key for training stability.\n    - **Sliding Window Attention**: Optional sliding window masking.\n    - **Dynamic Sequence Padding**: Support for `seq_lens` masking via flex_attention.\n    - **Variable-length Attention**: Support for optimized Flash Attention on packed batches via `cu_seqlens`.\n\n    Attributes:\n        wq: Linear projection for Query.\n        wk: Linear projection for Key.\n        wv: Linear projection for Value.\n        wo: Linear projection for Output.\n        q_norm: Optional RMSNorm for Query.\n        k_norm: Optional RMSNorm for Key.\n        n_head: Number of Query heads.\n        n_kv_head: Number of Key/Value heads.\n        head_dim: Dimensionality of each head.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_embd: int,\n        n_head: int,\n        n_kv_head: int | None = None,\n        head_dim: int | None = None,\n        dropout: float = 0.0,\n        bias: bool = False,\n        use_qk_norm: bool = False,\n        qk_norm_per_head: bool = True,\n        rmsnorm_eps: float = 1e-5,\n        sliding_window: int | None = None,\n    ):\n        super().__init__()\n        self.n_head = n_head\n        self.n_kv_head = n_kv_head if n_kv_head is not None else n_head\n        self.n_rep = self.n_head // self.n_kv_head\n        self.head_dim = head_dim or n_embd // n_head\n        self.dropout = dropout\n        self.use_qk_norm = use_qk_norm\n        self.qk_norm_per_head = qk_norm_per_head\n        self.sliding_window = sliding_window\n\n        assert (\n            self.n_head % self.n_kv_head == 0\n        ), \"n_head must be divisible by n_kv_head\"\n\n        self.wq = nn.Linear(n_embd, n_head * self.head_dim, bias=bias)\n        self.wk = nn.Linear(n_embd, self.n_kv_head * self.head_dim, bias=bias)\n        self.wv = nn.Linear(n_embd, self.n_kv_head * self.head_dim, bias=bias)\n        self.wo = nn.Linear(n_head * self.head_dim, n_embd, bias=bias)\n\n        if use_qk_norm:\n            q_norm_dim = self.head_dim if qk_norm_per_head else n_head * self.head_dim\n            k_norm_dim = (\n                self.head_dim if qk_norm_per_head else self.n_kv_head * self.head_dim\n            )\n            self.q_norm = RMSNorm(q_norm_dim, eps=rmsnorm_eps, use_liger=False)\n            self.k_norm = RMSNorm(k_norm_dim, eps=rmsnorm_eps, use_liger=False)\n\n        self.attn_dropout = nn.Dropout(dropout)\n        self.resid_dropout = nn.Dropout(dropout)\n\n        # Flex attention block mask\n        self._block_mask = None\n\n    def _varlen_attn_fallback(\n        self,\n        q: torch.Tensor,\n        k: torch.Tensor,\n        v: torch.Tensor,\n        cu_seqlens: torch.Tensor,\n        max_seqlen: int,\n    ) -&gt; torch.Tensor:\n        \"\"\"CPU fallback for varlen attention, used for testing and non-CUDA environments.\n\n        Args:\n            q: Flattened query (1, total_tokens, n_head, head_dim)\n            k: Flattened key (1, total_tokens, n_kv_head, head_dim)\n            v: Flattened value (1, total_tokens, n_kv_head, head_dim)\n            cu_seqlens: Cumulative sequence lengths\n            max_seqlen: Maximum sequence length\n\n        Returns:\n            Flattened attention output (1, total_tokens, n_head, head_dim)\n        \"\"\"\n        device = q.device\n        num_docs = len(cu_seqlens) - 1\n        n_head = q.shape[2]\n        n_kv_head = k.shape[2]\n        head_dim = q.shape[3]\n\n        # 1. Un-flatten into padded batch\n        q_padded = torch.zeros(\n            num_docs, max_seqlen, n_head, head_dim, device=device, dtype=q.dtype\n        )\n        k_padded = torch.zeros(\n            num_docs, max_seqlen, n_kv_head, head_dim, device=device, dtype=k.dtype\n        )\n        v_padded = torch.zeros(\n            num_docs, max_seqlen, n_kv_head, head_dim, device=device, dtype=v.dtype\n        )\n\n        for i in range(num_docs):\n            start, end = cu_seqlens[i].item(), cu_seqlens[i + 1].item()\n            length = end - start\n            q_padded[i, :length] = q[0, start:end]\n            k_padded[i, :length] = k[0, start:end]\n            v_padded[i, :length] = v[0, start:end]\n\n        # 2. Transpose for SDPA: (B, H, T, D)\n        q_padded = q_padded.transpose(1, 2)\n        k_padded = k_padded.transpose(1, 2)\n        v_padded = v_padded.transpose(1, 2)\n\n        # 3. Create mask\n        # We need a mask of shape (B, 1, T, T)\n        q_idx = torch.arange(max_seqlen, device=device).view(1, 1, -1, 1)\n        kv_idx = torch.arange(max_seqlen, device=device).view(1, 1, 1, -1)\n\n        doc_lens = (cu_seqlens[1:] - cu_seqlens[:-1]).view(-1, 1, 1, 1)\n        # Padding mask: doc attends only to valid tokens\n        mask = (q_idx &lt; doc_lens) &amp; (kv_idx &lt; doc_lens)\n\n        # Causal mask\n        mask &amp;= q_idx &gt;= kv_idx\n\n        # Sliding window mask\n        if self.sliding_window is not None:\n            mask &amp;= q_idx - kv_idx &lt; self.sliding_window\n\n        # 4. Compute attention\n        y = torch.nn.functional.scaled_dot_product_attention(\n            q_padded,\n            k_padded,\n            v_padded,\n            attn_mask=mask,\n            dropout_p=self.dropout if self.training else 0.0,\n            is_causal=False,  # Already handled in mask\n            enable_gqa=(self.n_rep &gt; 1),\n        )\n\n        # 5. Transpose back: (B, T, H, D)\n        y = y.transpose(1, 2)\n\n        # 6. Flatten back\n        y_flat = torch.zeros_like(q)\n        for i in range(num_docs):\n            start, end = cu_seqlens[i], cu_seqlens[i + 1]\n            length = end - start\n            y_flat[0, start:end] = y[i, :length]\n\n        return y_flat\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        freqs_cis: torch.Tensor,\n        seq_lens: torch.Tensor | None = None,\n        document_ids: torch.Tensor | None = None,\n        position_ids: torch.Tensor | None = None,\n        cu_seqlens: torch.Tensor | None = None,\n        max_seqlen: int | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Perform the forward pass with RoPE and GQA.\n\n        Args:\n            x: Input tensor of shape (B, T, C).\n            freqs_cis: Precomputed frequencies for RoPE.\n            seq_lens: Optional 1D tensor of sequence lengths to mask out padding.\n            document_ids: Optional 2D tensor of document IDs for packed/flat batching.\n            position_ids: Optional 2D tensor of position IDs for RoPE.\n            cu_seqlens: Optional 1D tensor of cumulative sequence lengths for Flash Attention varlen.\n            max_seqlen: Optional maximum sequence length in the packed batch.\n\n        Returns:\n            Output tensor after attention and projection.\n        \"\"\"\n        B, T, C = x.size()\n\n        # Input validation\n        if cu_seqlens is not None:\n            assert (\n                B == 1\n            ), f\"cu_seqlens is only supported for flat batches (B=1), but got B={B}\"\n            assert (\n                cu_seqlens.ndim == 1\n            ), f\"cu_seqlens must be a 1D tensor, got ndim={cu_seqlens.ndim}\"\n            assert (\n                cu_seqlens[0] == 0\n            ), f\"cu_seqlens must start with 0, got {cu_seqlens[0]}\"\n            assert (\n                cu_seqlens[-1] == T\n            ), f\"cu_seqlens[-1] ({cu_seqlens[-1]}) must match sequence length T ({T})\"\n\n        if document_ids is not None:\n            assert document_ids.shape == (\n                B,\n                T,\n            ), f\"document_ids shape must be (B, T) = ({B}, {T}), got {document_ids.shape}\"\n\n        if position_ids is not None:\n            assert position_ids.shape == (\n                B,\n                T,\n            ), f\"position_ids shape must be (B, T) = ({B}, {T}), got {position_ids.shape}\"\n\n        if seq_lens is not None:\n            assert seq_lens.shape == (\n                B,\n            ), f\"seq_lens shape must be (B,) = ({B},), got {seq_lens.shape}\"\n\n        # Infer if we are in SP mode. It is expected for input to be correct sequence-sharded DTensor\n        is_sp = isinstance(x, DTensor) and any(\n            isinstance(p, Shard) and p.dim == 1 for p in x.placements\n        )\n\n        if is_sp:\n            # If sequence parallel, attention is a global operation. We gather full sequence context before computing attention.\n            sp_placements = x.placements\n            sp_mesh = x.device_mesh\n            x = x.redistribute(placements=[Replicate()])\n\n        xq = self.wq(x)\n        xk = self.wk(x)\n        xv = self.wv(x)\n\n        # If TP, then Q K V are sharded across heads\n\n        if self.use_qk_norm and not self.qk_norm_per_head:\n            xq = self.q_norm(xq)\n            xk = self.k_norm(xk)\n\n        xq = xq.view(B, T, self.n_head, self.head_dim)\n        xk = xk.view(B, T, self.n_kv_head, self.head_dim)\n        xv = xv.view(B, T, self.n_kv_head, self.head_dim)\n\n        if self.use_qk_norm and self.qk_norm_per_head:\n            xq = self.q_norm(xq)\n            xk = self.k_norm(xk)\n\n        xq, xk = apply_rotary_emb(xq, xk, freqs_cis, position_ids=position_ids)\n\n        is_dtensor = isinstance(xq, DTensor)\n        if is_dtensor:\n            # heads are sharded, so we get local heads to compute attention\n            input_mesh = xq.device_mesh\n            input_placements = xq.placements\n            xq = xq.to_local()\n            xk = xk.to_local()\n            xv = xv.to_local()\n\n        enable_gqa = self.n_rep &gt; 1\n\n        y = None\n        if cu_seqlens is not None:\n            # Use optimized variable-length kernels on CUDA\n            if FLASH_ATTENTION_AVAILABLE and xq.is_cuda:\n                # Reshape to (total_tokens, n_heads, head_dim)\n                xq_varlen = xq.reshape(-1, self.n_head, self.head_dim)\n                xk_varlen = xk.reshape(-1, self.n_kv_head, self.head_dim)\n                xv_varlen = xv.reshape(-1, self.n_kv_head, self.head_dim)\n\n                # Use provided max_seqlen or compute if missing\n                if max_seqlen is not None:\n                    max_q = (\n                        int(max_seqlen.item())\n                        if isinstance(max_seqlen, torch.Tensor)\n                        else int(max_seqlen)\n                    )\n                else:\n                    max_q = int((cu_seqlens[1:] - cu_seqlens[:-1]).max().item())\n\n                # Tri Dao's Flash Attention natively supports GQA and sliding window\n                # Window size -1 means no window\n                window_size = (\n                    (self.sliding_window, self.sliding_window)\n                    if self.sliding_window is not None\n                    else (-1, -1)\n                )\n                y = flash_attn_varlen_func(\n                    xq_varlen,\n                    xk_varlen,\n                    xv_varlen,\n                    cu_seqlens_q=cu_seqlens,\n                    cu_seqlens_k=cu_seqlens,\n                    max_seqlen_q=max_q,\n                    max_seqlen_k=max_q,\n                    dropout_p=self.dropout if self.training else 0.0,\n                    causal=True,\n                    window_size=window_size,\n                )\n                # Reshape back to (B, T, n_heads, head_dim)\n                y = y.view(B, T, self.n_head, self.head_dim)\n\n            if y is None and xq.is_cuda:\n                # Fallback to Flex Attention path by converting cu_seqlens to document_ids\n                if document_ids is None:\n                    document_ids = torch.zeros(1, T, dtype=torch.long, device=x.device)\n                    for i in range(len(cu_seqlens) - 1):\n                        document_ids[\n                            0, cu_seqlens[i].item() : cu_seqlens[i + 1].item()\n                        ] = i\n                cu_seqlens = None\n            elif y is None:\n                # CPU path: use the un-flattening fallback\n                max_q = (\n                    max_seqlen\n                    if max_seqlen is not None\n                    else int((cu_seqlens[1:] - cu_seqlens[:-1]).max().item())\n                )\n                y = self._varlen_attn_fallback(\n                    xq, xk, xv, cu_seqlens=cu_seqlens, max_seqlen=max_q\n                )\n\n        if y is None:\n            # SDPA or Flex Attention path\n            xq = xq.transpose(1, 2)\n            xk = xk.transpose(1, 2)\n            xv = xv.transpose(1, 2)\n\n            # Decide if we can use flex_attention masks\n            use_flex = (\n                FLEX_ATTENTION_AVAILABLE\n                and (\n                    self.sliding_window is not None\n                    or seq_lens is not None\n                    or document_ids is not None\n                )\n                and (x.device.type in {\"cuda\", \"cpu\", \"xpu\", \"hpu\"})\n            )\n\n            if use_flex:\n                mask_fn = partial(\n                    attention_mask_fn,\n                    window_size=self.sliding_window,\n                    seq_lens=seq_lens,\n                    document_ids=document_ids,\n                )\n\n                # Since seq_lens or document_ids relies on dynamic per-batch metadata, it needs the true batch dimension `B`\n                if seq_lens is not None or document_ids is not None:\n                    block_mask = create_block_mask(\n                        mask_fn, B, None, T, T, device=x.device\n                    )\n                else:\n                    if self._block_mask is None or self._block_mask.shape[-1] != T:\n                        self._block_mask = create_block_mask(\n                            mask_fn, None, None, T, T, device=x.device\n                        )\n                    block_mask = self._block_mask\n\n                _flex_attention = flex_attention\n                if xq.device.type == \"cuda\":\n                    _flex_attention = torch.compile(flex_attention)\n\n                if self.dropout &gt; 1e-5:\n                    warn_once(\n                        logger=logger,\n                        message=\"Dropout is not supported in flex attention. Ignoring dropout.\",\n                    )\n\n                y = _flex_attention(\n                    xq, xk, xv, block_mask=block_mask, enable_gqa=enable_gqa\n                )\n            else:\n                mask = None\n                if (\n                    self.sliding_window is not None\n                    or seq_lens is not None\n                    or document_ids is not None\n                ):\n                    q_idx = torch.arange(T, device=x.device).view(1, 1, -1, 1)\n                    kv_idx = torch.arange(T, device=x.device).view(1, 1, 1, -1)\n                    mask = q_idx &gt;= kv_idx\n\n                    if self.sliding_window is not None:\n                        mask &amp;= q_idx - kv_idx &lt; self.sliding_window\n\n                    if seq_lens is not None:\n                        seq_lens_view = seq_lens.view(-1, 1, 1, 1)\n                        seq_lens_mask = (q_idx &lt; seq_lens_view) &amp; (\n                            kv_idx &lt; seq_lens_view\n                        )\n                        mask = torch.broadcast_to(mask, seq_lens_mask.shape)\n                        mask = mask &amp; seq_lens_mask\n\n                    if document_ids is not None:\n                        doc_ids_q = document_ids.view(B, 1, T, 1)\n                        doc_ids_kv = document_ids.view(B, 1, 1, T)\n                        doc_mask = doc_ids_q == doc_ids_kv\n                        if mask is None:\n                            mask = doc_mask\n                        else:\n                            mask = mask &amp; doc_mask\n\n                y = torch.nn.functional.scaled_dot_product_attention(\n                    xq,\n                    xk,\n                    xv,\n                    attn_mask=mask,\n                    dropout_p=self.dropout if self.training else 0.0,\n                    is_causal=(mask is None),\n                    enable_gqa=enable_gqa,\n                )\n            y = y.transpose(1, 2)\n\n        if is_dtensor:\n            # if it was dtensor, then attention output has the same sharding scheme as input (head-sharded)\n            y = DTensor.from_local(y, input_mesh, input_placements)\n\n        y = y.contiguous().view(B, -1, self.n_head * self.head_dim)\n        y = self.resid_dropout(self.wo(y))\n\n        # if it was SP, keep it SP\n        if is_sp:\n            y = y.redistribute(sp_mesh, sp_placements)\n\n        return y\n</code></pre>"},{"location":"reference/modules/model/blocks/attention/#optimus_dl.modules.model.blocks.attention.RotarySelfAttention.forward","title":"<code>forward(x, freqs_cis, seq_lens=None, document_ids=None, position_ids=None, cu_seqlens=None, max_seqlen=None)</code>","text":"<p>Perform the forward pass with RoPE and GQA.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (B, T, C).</p> required <code>freqs_cis</code> <code>Tensor</code> <p>Precomputed frequencies for RoPE.</p> required <code>seq_lens</code> <code>Tensor | None</code> <p>Optional 1D tensor of sequence lengths to mask out padding.</p> <code>None</code> <code>document_ids</code> <code>Tensor | None</code> <p>Optional 2D tensor of document IDs for packed/flat batching.</p> <code>None</code> <code>position_ids</code> <code>Tensor | None</code> <p>Optional 2D tensor of position IDs for RoPE.</p> <code>None</code> <code>cu_seqlens</code> <code>Tensor | None</code> <p>Optional 1D tensor of cumulative sequence lengths for Flash Attention varlen.</p> <code>None</code> <code>max_seqlen</code> <code>int | None</code> <p>Optional maximum sequence length in the packed batch.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after attention and projection.</p> Source code in <code>optimus_dl/modules/model/blocks/attention.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    freqs_cis: torch.Tensor,\n    seq_lens: torch.Tensor | None = None,\n    document_ids: torch.Tensor | None = None,\n    position_ids: torch.Tensor | None = None,\n    cu_seqlens: torch.Tensor | None = None,\n    max_seqlen: int | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Perform the forward pass with RoPE and GQA.\n\n    Args:\n        x: Input tensor of shape (B, T, C).\n        freqs_cis: Precomputed frequencies for RoPE.\n        seq_lens: Optional 1D tensor of sequence lengths to mask out padding.\n        document_ids: Optional 2D tensor of document IDs for packed/flat batching.\n        position_ids: Optional 2D tensor of position IDs for RoPE.\n        cu_seqlens: Optional 1D tensor of cumulative sequence lengths for Flash Attention varlen.\n        max_seqlen: Optional maximum sequence length in the packed batch.\n\n    Returns:\n        Output tensor after attention and projection.\n    \"\"\"\n    B, T, C = x.size()\n\n    # Input validation\n    if cu_seqlens is not None:\n        assert (\n            B == 1\n        ), f\"cu_seqlens is only supported for flat batches (B=1), but got B={B}\"\n        assert (\n            cu_seqlens.ndim == 1\n        ), f\"cu_seqlens must be a 1D tensor, got ndim={cu_seqlens.ndim}\"\n        assert (\n            cu_seqlens[0] == 0\n        ), f\"cu_seqlens must start with 0, got {cu_seqlens[0]}\"\n        assert (\n            cu_seqlens[-1] == T\n        ), f\"cu_seqlens[-1] ({cu_seqlens[-1]}) must match sequence length T ({T})\"\n\n    if document_ids is not None:\n        assert document_ids.shape == (\n            B,\n            T,\n        ), f\"document_ids shape must be (B, T) = ({B}, {T}), got {document_ids.shape}\"\n\n    if position_ids is not None:\n        assert position_ids.shape == (\n            B,\n            T,\n        ), f\"position_ids shape must be (B, T) = ({B}, {T}), got {position_ids.shape}\"\n\n    if seq_lens is not None:\n        assert seq_lens.shape == (\n            B,\n        ), f\"seq_lens shape must be (B,) = ({B},), got {seq_lens.shape}\"\n\n    # Infer if we are in SP mode. It is expected for input to be correct sequence-sharded DTensor\n    is_sp = isinstance(x, DTensor) and any(\n        isinstance(p, Shard) and p.dim == 1 for p in x.placements\n    )\n\n    if is_sp:\n        # If sequence parallel, attention is a global operation. We gather full sequence context before computing attention.\n        sp_placements = x.placements\n        sp_mesh = x.device_mesh\n        x = x.redistribute(placements=[Replicate()])\n\n    xq = self.wq(x)\n    xk = self.wk(x)\n    xv = self.wv(x)\n\n    # If TP, then Q K V are sharded across heads\n\n    if self.use_qk_norm and not self.qk_norm_per_head:\n        xq = self.q_norm(xq)\n        xk = self.k_norm(xk)\n\n    xq = xq.view(B, T, self.n_head, self.head_dim)\n    xk = xk.view(B, T, self.n_kv_head, self.head_dim)\n    xv = xv.view(B, T, self.n_kv_head, self.head_dim)\n\n    if self.use_qk_norm and self.qk_norm_per_head:\n        xq = self.q_norm(xq)\n        xk = self.k_norm(xk)\n\n    xq, xk = apply_rotary_emb(xq, xk, freqs_cis, position_ids=position_ids)\n\n    is_dtensor = isinstance(xq, DTensor)\n    if is_dtensor:\n        # heads are sharded, so we get local heads to compute attention\n        input_mesh = xq.device_mesh\n        input_placements = xq.placements\n        xq = xq.to_local()\n        xk = xk.to_local()\n        xv = xv.to_local()\n\n    enable_gqa = self.n_rep &gt; 1\n\n    y = None\n    if cu_seqlens is not None:\n        # Use optimized variable-length kernels on CUDA\n        if FLASH_ATTENTION_AVAILABLE and xq.is_cuda:\n            # Reshape to (total_tokens, n_heads, head_dim)\n            xq_varlen = xq.reshape(-1, self.n_head, self.head_dim)\n            xk_varlen = xk.reshape(-1, self.n_kv_head, self.head_dim)\n            xv_varlen = xv.reshape(-1, self.n_kv_head, self.head_dim)\n\n            # Use provided max_seqlen or compute if missing\n            if max_seqlen is not None:\n                max_q = (\n                    int(max_seqlen.item())\n                    if isinstance(max_seqlen, torch.Tensor)\n                    else int(max_seqlen)\n                )\n            else:\n                max_q = int((cu_seqlens[1:] - cu_seqlens[:-1]).max().item())\n\n            # Tri Dao's Flash Attention natively supports GQA and sliding window\n            # Window size -1 means no window\n            window_size = (\n                (self.sliding_window, self.sliding_window)\n                if self.sliding_window is not None\n                else (-1, -1)\n            )\n            y = flash_attn_varlen_func(\n                xq_varlen,\n                xk_varlen,\n                xv_varlen,\n                cu_seqlens_q=cu_seqlens,\n                cu_seqlens_k=cu_seqlens,\n                max_seqlen_q=max_q,\n                max_seqlen_k=max_q,\n                dropout_p=self.dropout if self.training else 0.0,\n                causal=True,\n                window_size=window_size,\n            )\n            # Reshape back to (B, T, n_heads, head_dim)\n            y = y.view(B, T, self.n_head, self.head_dim)\n\n        if y is None and xq.is_cuda:\n            # Fallback to Flex Attention path by converting cu_seqlens to document_ids\n            if document_ids is None:\n                document_ids = torch.zeros(1, T, dtype=torch.long, device=x.device)\n                for i in range(len(cu_seqlens) - 1):\n                    document_ids[\n                        0, cu_seqlens[i].item() : cu_seqlens[i + 1].item()\n                    ] = i\n            cu_seqlens = None\n        elif y is None:\n            # CPU path: use the un-flattening fallback\n            max_q = (\n                max_seqlen\n                if max_seqlen is not None\n                else int((cu_seqlens[1:] - cu_seqlens[:-1]).max().item())\n            )\n            y = self._varlen_attn_fallback(\n                xq, xk, xv, cu_seqlens=cu_seqlens, max_seqlen=max_q\n            )\n\n    if y is None:\n        # SDPA or Flex Attention path\n        xq = xq.transpose(1, 2)\n        xk = xk.transpose(1, 2)\n        xv = xv.transpose(1, 2)\n\n        # Decide if we can use flex_attention masks\n        use_flex = (\n            FLEX_ATTENTION_AVAILABLE\n            and (\n                self.sliding_window is not None\n                or seq_lens is not None\n                or document_ids is not None\n            )\n            and (x.device.type in {\"cuda\", \"cpu\", \"xpu\", \"hpu\"})\n        )\n\n        if use_flex:\n            mask_fn = partial(\n                attention_mask_fn,\n                window_size=self.sliding_window,\n                seq_lens=seq_lens,\n                document_ids=document_ids,\n            )\n\n            # Since seq_lens or document_ids relies on dynamic per-batch metadata, it needs the true batch dimension `B`\n            if seq_lens is not None or document_ids is not None:\n                block_mask = create_block_mask(\n                    mask_fn, B, None, T, T, device=x.device\n                )\n            else:\n                if self._block_mask is None or self._block_mask.shape[-1] != T:\n                    self._block_mask = create_block_mask(\n                        mask_fn, None, None, T, T, device=x.device\n                    )\n                block_mask = self._block_mask\n\n            _flex_attention = flex_attention\n            if xq.device.type == \"cuda\":\n                _flex_attention = torch.compile(flex_attention)\n\n            if self.dropout &gt; 1e-5:\n                warn_once(\n                    logger=logger,\n                    message=\"Dropout is not supported in flex attention. Ignoring dropout.\",\n                )\n\n            y = _flex_attention(\n                xq, xk, xv, block_mask=block_mask, enable_gqa=enable_gqa\n            )\n        else:\n            mask = None\n            if (\n                self.sliding_window is not None\n                or seq_lens is not None\n                or document_ids is not None\n            ):\n                q_idx = torch.arange(T, device=x.device).view(1, 1, -1, 1)\n                kv_idx = torch.arange(T, device=x.device).view(1, 1, 1, -1)\n                mask = q_idx &gt;= kv_idx\n\n                if self.sliding_window is not None:\n                    mask &amp;= q_idx - kv_idx &lt; self.sliding_window\n\n                if seq_lens is not None:\n                    seq_lens_view = seq_lens.view(-1, 1, 1, 1)\n                    seq_lens_mask = (q_idx &lt; seq_lens_view) &amp; (\n                        kv_idx &lt; seq_lens_view\n                    )\n                    mask = torch.broadcast_to(mask, seq_lens_mask.shape)\n                    mask = mask &amp; seq_lens_mask\n\n                if document_ids is not None:\n                    doc_ids_q = document_ids.view(B, 1, T, 1)\n                    doc_ids_kv = document_ids.view(B, 1, 1, T)\n                    doc_mask = doc_ids_q == doc_ids_kv\n                    if mask is None:\n                        mask = doc_mask\n                    else:\n                        mask = mask &amp; doc_mask\n\n            y = torch.nn.functional.scaled_dot_product_attention(\n                xq,\n                xk,\n                xv,\n                attn_mask=mask,\n                dropout_p=self.dropout if self.training else 0.0,\n                is_causal=(mask is None),\n                enable_gqa=enable_gqa,\n            )\n        y = y.transpose(1, 2)\n\n    if is_dtensor:\n        # if it was dtensor, then attention output has the same sharding scheme as input (head-sharded)\n        y = DTensor.from_local(y, input_mesh, input_placements)\n\n    y = y.contiguous().view(B, -1, self.n_head * self.head_dim)\n    y = self.resid_dropout(self.wo(y))\n\n    # if it was SP, keep it SP\n    if is_sp:\n        y = y.redistribute(sp_mesh, sp_placements)\n\n    return y\n</code></pre>"},{"location":"reference/modules/model/blocks/attention/#optimus_dl.modules.model.blocks.attention.attention_mask_fn","title":"<code>attention_mask_fn(b, _, q_idx, kv_idx, window_size=None, seq_lens=None, document_ids=None)</code>","text":"<p>Mask function for flex_attention supporting causal, sliding window, padding, and flat batching.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <p>Batch index.</p> required <code>_</code> <p>Head index (unused).</p> required <code>q_idx</code> <p>Query index.</p> required <code>kv_idx</code> <p>Key/Value index.</p> required <code>window_size</code> <p>Optional sliding window size.</p> <code>None</code> <code>seq_lens</code> <p>Optional 1D tensor of sequence lengths (for padding).</p> <code>None</code> <code>document_ids</code> <p>Optional 2D tensor of document IDs (for flat/packed batching).</p> <code>None</code> Source code in <code>optimus_dl/modules/model/blocks/attention.py</code> <pre><code>def attention_mask_fn(\n    b, _, q_idx, kv_idx, window_size=None, seq_lens=None, document_ids=None\n):\n    \"\"\"Mask function for flex_attention supporting causal, sliding window, padding, and flat batching.\n\n    Args:\n        b: Batch index.\n        _: Head index (unused).\n        q_idx: Query index.\n        kv_idx: Key/Value index.\n        window_size: Optional sliding window size.\n        seq_lens: Optional 1D tensor of sequence lengths (for padding).\n        document_ids: Optional 2D tensor of document IDs (for flat/packed batching).\n    \"\"\"\n    mask = q_idx &gt;= kv_idx  # causal\n    if window_size is not None:\n        mask = mask &amp; (q_idx - kv_idx &lt; window_size)\n    if seq_lens is not None:\n        mask = mask &amp; (q_idx &lt; seq_lens[b]) &amp; (kv_idx &lt; seq_lens[b])\n    if document_ids is not None:\n        mask = mask &amp; (document_ids[b, q_idx] == document_ids[b, kv_idx])\n    return mask\n</code></pre>"},{"location":"reference/modules/model/blocks/layer_norms/","title":"layer_norms","text":""},{"location":"reference/modules/model/blocks/layer_norms/#optimus_dl.modules.model.blocks.layer_norms","title":"<code>optimus_dl.modules.model.blocks.layer_norms</code>","text":""},{"location":"reference/modules/model/blocks/layer_norms/#optimus_dl.modules.model.blocks.layer_norms.LayerNorm","title":"<code>LayerNorm</code>","text":"<p>               Bases: <code>Module</code></p> <p>LayerNorm with optional bias.</p> <p>PyTorch's standard LayerNorm always expects a bias if elementwise_affine is True. This implementation allows for a more flexible bias=False option as seen in some LLM architectures.</p> <p>Attributes:</p> Name Type Description <code>weight</code> <p>Affine scale parameter.</p> <code>bias</code> <p>Optional affine bias parameter.</p> Source code in <code>optimus_dl/modules/model/blocks/layer_norms.py</code> <pre><code>class LayerNorm(nn.Module):\n    \"\"\"LayerNorm with optional bias.\n\n    PyTorch's standard LayerNorm always expects a bias if elementwise_affine\n    is True. This implementation allows for a more flexible bias=False option\n    as seen in some LLM architectures.\n\n    Attributes:\n        weight: Affine scale parameter.\n        bias: Optional affine bias parameter.\n    \"\"\"\n\n    def __init__(self, ndim: int, bias: bool):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply layer normalization.\n\n        Args:\n            input: Input tensor.\n\n        Returns:\n            Normalized tensor.\n        \"\"\"\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n</code></pre>"},{"location":"reference/modules/model/blocks/layer_norms/#optimus_dl.modules.model.blocks.layer_norms.LayerNorm.forward","title":"<code>forward(input)</code>","text":"<p>Apply layer normalization.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor.</p> Source code in <code>optimus_dl/modules/model/blocks/layer_norms.py</code> <pre><code>def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply layer normalization.\n\n    Args:\n        input: Input tensor.\n\n    Returns:\n        Normalized tensor.\n    \"\"\"\n    return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n</code></pre>"},{"location":"reference/modules/model/blocks/layer_norms/#optimus_dl.modules.model.blocks.layer_norms.RMSNorm","title":"<code>RMSNorm</code>","text":"<p>               Bases: <code>Module</code></p> <p>Root Mean Square Layer Normalization (RMSNorm).</p> <p>RMSNorm is a simplification of LayerNorm that only scales the input by the root mean square of the activations, omitting the mean subtraction and bias.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Input dimension.</p> required <code>eps</code> <code>float</code> <p>Small value for numerical stability.</p> <code>1e-06</code> <code>use_liger</code> <code>bool | None</code> <p>If True, uses the high-performance Liger kernel. If None, automatically enables if available.</p> <code>None</code> Source code in <code>optimus_dl/modules/model/blocks/layer_norms.py</code> <pre><code>class RMSNorm(nn.Module):\n    \"\"\"Root Mean Square Layer Normalization (RMSNorm).\n\n    RMSNorm is a simplification of LayerNorm that only scales the input by the\n    root mean square of the activations, omitting the mean subtraction and\n    bias.\n\n    Args:\n        dim: Input dimension.\n        eps: Small value for numerical stability.\n        use_liger: If True, uses the high-performance Liger kernel. If None,\n            automatically enables if available.\n    \"\"\"\n\n    def __init__(self, dim: int, eps: float = 1e-6, use_liger: bool | None = None):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n        if use_liger is None:\n            self.use_liger = LIGER_AVAILABLE\n            if self.use_liger:\n                logger.info(\"Using liger-kernel for RMSNorm.\")\n        else:\n            self.use_liger = use_liger\n\n        if self.use_liger and not LIGER_AVAILABLE:\n            logger.warning(\n                \"Liger Kernel requested for RMSNorm but not installed. Fallback to PyTorch.\"\n            )\n            self.use_liger = False\n\n    def _norm(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the RMS normalization of the input.\"\"\"\n        assert x.dtype == torch.float32, \"Accumulating in lower precision is dangerous\"\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Perform the forward pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            RMS normalized tensor.\n        \"\"\"\n        is_dtensor = isinstance(x, DTensor)\n\n        if self.use_liger and x.device.type != \"cpu\" and not is_dtensor:\n            return liger_rms_norm(x, self.weight, self.eps)\n\n        output = self._norm(x.float())\n\n        weight = self.weight\n        if is_dtensor and not isinstance(weight, DTensor):\n            from torch.distributed.tensor.placement_types import Replicate\n\n            # If x is DTensor, weight must be DTensor for multiplication.\n            # We assume weight is replicated (available on all ranks).\n            weight = DTensor.from_local(weight, x.device_mesh, (Replicate(),))\n\n        return (output * weight).type_as(x)\n</code></pre>"},{"location":"reference/modules/model/blocks/layer_norms/#optimus_dl.modules.model.blocks.layer_norms.RMSNorm.forward","title":"<code>forward(x)</code>","text":"<p>Perform the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>RMS normalized tensor.</p> Source code in <code>optimus_dl/modules/model/blocks/layer_norms.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Perform the forward pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        RMS normalized tensor.\n    \"\"\"\n    is_dtensor = isinstance(x, DTensor)\n\n    if self.use_liger and x.device.type != \"cpu\" and not is_dtensor:\n        return liger_rms_norm(x, self.weight, self.eps)\n\n    output = self._norm(x.float())\n\n    weight = self.weight\n    if is_dtensor and not isinstance(weight, DTensor):\n        from torch.distributed.tensor.placement_types import Replicate\n\n        # If x is DTensor, weight must be DTensor for multiplication.\n        # We assume weight is replicated (available on all ranks).\n        weight = DTensor.from_local(weight, x.device_mesh, (Replicate(),))\n\n    return (output * weight).type_as(x)\n</code></pre>"},{"location":"reference/modules/model/blocks/mlp/","title":"mlp","text":""},{"location":"reference/modules/model/blocks/mlp/#optimus_dl.modules.model.blocks.mlp","title":"<code>optimus_dl.modules.model.blocks.mlp</code>","text":""},{"location":"reference/modules/model/blocks/mlp/#optimus_dl.modules.model.blocks.mlp.GELUMLP","title":"<code>GELUMLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>Standard GPT-2 style MLP with GELU activation.</p> <p>Consists of an expansion layer, GELU activation, and a contraction layer.</p> <p>Attributes:</p> Name Type Description <code>c_fc</code> <p>Expansion projection layer.</p> <code>gelu</code> <p>GELU activation layer.</p> <code>c_proj</code> <p>Contraction projection layer.</p> <code>dropout</code> <p>Dropout layer.</p> Source code in <code>optimus_dl/modules/model/blocks/mlp.py</code> <pre><code>class GELUMLP(nn.Module):\n    \"\"\"Standard GPT-2 style MLP with GELU activation.\n\n    Consists of an expansion layer, GELU activation, and a contraction layer.\n\n    Attributes:\n        c_fc: Expansion projection layer.\n        gelu: GELU activation layer.\n        c_proj: Contraction projection layer.\n        dropout: Dropout layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_embd: int,\n        intermediate_size: int | None = None,\n        bias: bool = True,\n        dropout: float = 0.0,\n    ):\n        super().__init__()\n        # Default GPT-2 expansion is 4x\n        hidden_dim = intermediate_size if intermediate_size is not None else 4 * n_embd\n\n        self.c_fc = nn.Linear(n_embd, hidden_dim, bias=bias)\n        self.gelu = nn.GELU()\n        self.c_proj = nn.Linear(hidden_dim, n_embd, bias=bias)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Perform the forward pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output tensor.\n        \"\"\"\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n</code></pre>"},{"location":"reference/modules/model/blocks/mlp/#optimus_dl.modules.model.blocks.mlp.GELUMLP.forward","title":"<code>forward(x)</code>","text":"<p>Perform the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor.</p> Source code in <code>optimus_dl/modules/model/blocks/mlp.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Perform the forward pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output tensor.\n    \"\"\"\n    x = self.c_fc(x)\n    x = self.gelu(x)\n    x = self.c_proj(x)\n    x = self.dropout(x)\n    return x\n</code></pre>"},{"location":"reference/modules/model/blocks/mlp/#optimus_dl.modules.model.blocks.mlp.SwiGLUMLP","title":"<code>SwiGLUMLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>SwiGLU MLP variant used in Llama, Qwen, and Mistral.</p> <p>Consists of three linear layers (gate, up, down) and a SiLU (Swish) activation. Supports optional Liger kernel for performance.</p> <p>Attributes:</p> Name Type Description <code>w1</code> <p>Gate projection layer.</p> <code>w2</code> <p>Up projection layer.</p> <code>c_proj</code> <p>Down projection layer.</p> <code>use_liger</code> <p>Whether Liger kernel is enabled.</p> Source code in <code>optimus_dl/modules/model/blocks/mlp.py</code> <pre><code>class SwiGLUMLP(nn.Module):\n    \"\"\"SwiGLU MLP variant used in Llama, Qwen, and Mistral.\n\n    Consists of three linear layers (gate, up, down) and a SiLU (Swish)\n    activation. Supports optional Liger kernel for performance.\n\n    Attributes:\n        w1: Gate projection layer.\n        w2: Up projection layer.\n        c_proj: Down projection layer.\n        use_liger: Whether Liger kernel is enabled.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_embd: int,\n        intermediate_size: int | None = None,\n        multiple_of: int = 256,\n        bias: bool = False,\n        use_liger: bool | None = None,\n    ):\n        super().__init__()\n\n        if intermediate_size is not None:\n            hidden_dim = intermediate_size\n        else:\n            hidden_dim = n_embd * 4\n            hidden_dim = int(2 * hidden_dim / 3)\n            hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\n        self.w1 = nn.Linear(n_embd, hidden_dim, bias=bias)\n        self.w2 = nn.Linear(n_embd, hidden_dim, bias=bias)\n        self.c_proj = nn.Linear(hidden_dim, n_embd, bias=bias)\n\n        if use_liger is None:\n            self.use_liger = LIGER_AVAILABLE\n            if self.use_liger:\n                logger.info(\"Using liger-kernel for SwiGLU.\")\n        else:\n            self.use_liger = use_liger\n\n        if self.use_liger and not LIGER_AVAILABLE:\n            logger.warning(\n                \"Liger SwiGLU requested but not installed. Fallback to PyTorch.\"\n            )\n            self.use_liger = False\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Perform the forward pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output tensor.\n        \"\"\"\n        if self.use_liger and x.device.type != \"cpu\":\n            x_swiglu = liger_swiglu(self.w1(x), self.w2(x))\n        else:\n            x_swiglu = nn.functional.silu(self.w1(x)) * self.w2(x)\n\n        return self.c_proj(x_swiglu)\n</code></pre>"},{"location":"reference/modules/model/blocks/mlp/#optimus_dl.modules.model.blocks.mlp.SwiGLUMLP.forward","title":"<code>forward(x)</code>","text":"<p>Perform the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor.</p> Source code in <code>optimus_dl/modules/model/blocks/mlp.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Perform the forward pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output tensor.\n    \"\"\"\n    if self.use_liger and x.device.type != \"cpu\":\n        x_swiglu = liger_swiglu(self.w1(x), self.w2(x))\n    else:\n        x_swiglu = nn.functional.silu(self.w1(x)) * self.w2(x)\n\n    return self.c_proj(x_swiglu)\n</code></pre>"},{"location":"reference/modules/model/blocks/rope/","title":"rope","text":""},{"location":"reference/modules/model/blocks/rope/#optimus_dl.modules.model.blocks.rope","title":"<code>optimus_dl.modules.model.blocks.rope</code>","text":"<p>Rotary Positional Embeddings (RoPE) implementation.</p> <p>This module provides utilities for computing and applying Rotary Positional Embeddings, as used in models like Llama and Qwen.</p>"},{"location":"reference/modules/model/blocks/rope/#optimus_dl.modules.model.blocks.rope.apply_rotary_emb","title":"<code>apply_rotary_emb(q, k, freqs_cis, position_ids=None)</code>","text":"<p>Apply Rotary Positional Embeddings to Query and Key tensors.</p> <p>Handles both standard Tensors and distributed DTensors.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor of shape (B, T, nh, hs).</p> required <code>k</code> <code>Tensor</code> <p>Key tensor of shape (B, T, n_kv_h, hs).</p> required <code>freqs_cis</code> <code>Tensor</code> <p>Precomputed frequency tensor of shape (max_T, hs // 2, 2).</p> required <code>position_ids</code> <code>Tensor | None</code> <p>Optional tensor of shape (B, T) specifying the positions for each token.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Tuple of (q, k) with rotary embeddings applied.</p> Source code in <code>optimus_dl/modules/model/blocks/rope.py</code> <pre><code>def apply_rotary_emb(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    freqs_cis: torch.Tensor,\n    position_ids: torch.Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Apply Rotary Positional Embeddings to Query and Key tensors.\n\n    Handles both standard Tensors and distributed DTensors.\n\n    Args:\n        q: Query tensor of shape (B, T, nh, hs).\n        k: Key tensor of shape (B, T, n_kv_h, hs).\n        freqs_cis: Precomputed frequency tensor of shape (max_T, hs // 2, 2).\n        position_ids: Optional tensor of shape (B, T) specifying the positions for each token.\n\n    Returns:\n        Tuple of (q, k) with rotary embeddings applied.\n    \"\"\"\n    is_q_dtensor = isinstance(q, DTensor)\n    is_k_dtensor = isinstance(k, DTensor)\n    is_freqs_cis_dtensor = isinstance(freqs_cis, DTensor)\n\n    q_in = q.to_local() if is_q_dtensor else q\n    k_in = k.to_local() if is_k_dtensor else k\n    freqs_cis_in = freqs_cis.to_local() if is_freqs_cis_dtensor else freqs_cis\n\n    # Input dtype for restoration\n    input_dtype = q_in.dtype\n\n    _, T = q_in.shape[0], q_in.shape[1]\n\n    if position_ids is not None:\n        # freqs_cis_in: (max_T, hs//2, 2)\n        # position_ids: (B, T)\n        # Result: (B, T, hs//2, 2)\n        freqs_cis_in = freqs_cis_in[position_ids]\n    else:\n        # Assume positions 0..T-1\n        freqs_cis_in = freqs_cis_in[:T]\n\n    q_in = q_in.float().reshape(*q_in.shape[:-1], -1, 2)\n    k_in = k_in.float().reshape(*k_in.shape[:-1], -1, 2)\n\n    freqs_cis_res = _reshape_for_broadcast(freqs_cis_in, q_in)\n\n    # Perform manual \"complex\" multiplication\n    q_cos = q_in[..., 0] * freqs_cis_res[..., 0] - q_in[..., 1] * freqs_cis_res[..., 1]\n    q_sin = q_in[..., 0] * freqs_cis_res[..., 1] + q_in[..., 1] * freqs_cis_res[..., 0]\n    k_cos = k_in[..., 0] * freqs_cis_res[..., 0] - k_in[..., 1] * freqs_cis_res[..., 1]\n    k_sin = k_in[..., 0] * freqs_cis_res[..., 1] + k_in[..., 1] * freqs_cis_res[..., 0]\n\n    # Combine the results back into the interleaved format expected by q and k\n    q_out = (\n        torch.stack((q_cos, q_sin), dim=-1)\n        .reshape(q_in.shape)\n        .flatten(3)\n        .to(input_dtype)\n    )\n    k_out = (\n        torch.stack((k_cos, k_sin), dim=-1)\n        .reshape(k_in.shape)\n        .flatten(3)\n        .to(input_dtype)\n    )\n\n    # Wrap back to DTensor if inputs were DTensor\n    if is_q_dtensor:\n        q_out = DTensor.from_local(q_out, q.device_mesh, q.placements)\n    if is_k_dtensor:\n        k_out = DTensor.from_local(k_out, k.device_mesh, k.placements)\n\n    return q_out, k_out\n</code></pre>"},{"location":"reference/modules/model/blocks/rope/#optimus_dl.modules.model.blocks.rope.precompute_freqs_cis","title":"<code>precompute_freqs_cis(dim, end, theta=10000.0, scaling_config=None)</code>","text":"<p>Precompute the frequency tensor for complex exponential (cis) with optional scaling.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension of the head.</p> required <code>end</code> <code>int</code> <p>Maximum sequence length.</p> required <code>theta</code> <code>float</code> <p>Base frequency for the positional encoding.</p> <code>10000.0</code> <code>scaling_config</code> <code>dict | None</code> <p>Optional RoPE scaling configuration (e.g., YaRN, Llama3).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape (end, dim // 2, 2) representing the real and imaginary</p> <code>Tensor</code> <p>parts of the frequencies.</p> Source code in <code>optimus_dl/modules/model/blocks/rope.py</code> <pre><code>def precompute_freqs_cis(\n    dim: int, end: int, theta: float = 10000.0, scaling_config: dict | None = None\n) -&gt; torch.Tensor:\n    \"\"\"Precompute the frequency tensor for complex exponential (cis) with optional scaling.\n\n    Args:\n        dim: Dimension of the head.\n        end: Maximum sequence length.\n        theta: Base frequency for the positional encoding.\n        scaling_config: Optional RoPE scaling configuration (e.g., YaRN, Llama3).\n\n    Returns:\n        Tensor of shape (end, dim // 2, 2) representing the real and imaginary\n        parts of the frequencies.\n    \"\"\"\n    inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n\n    if scaling_config is not None:\n        rope_type = scaling_config.get(\"rope_type\")\n        if rope_type == \"yarn\":\n            factor = scaling_config.get(\"factor\", 1.0)\n            original_max_position_embeddings = scaling_config.get(\n                \"original_max_position_embeddings\", 8192\n            )\n            attention_factor = scaling_config.get(\"attention_factor\", 1.0)\n            beta_fast = scaling_config.get(\"beta_fast\", 32.0)\n            beta_slow = scaling_config.get(\"beta_slow\", 1.0)\n\n            def find_correction_dim(num_rotations, dim, base, max_position_embeddings):\n                return (\n                    dim\n                    * math.log(max_position_embeddings / (num_rotations * 2 * math.pi))\n                ) / (2 * math.log(base))\n\n            def find_correction_range(\n                low_rot, high_rot, dim, base, max_position_embeddings\n            ):\n                low = find_correction_dim(low_rot, dim, base, max_position_embeddings)\n                high = find_correction_dim(high_rot, dim, base, max_position_embeddings)\n                return max(math.floor(low), 0), min(math.ceil(high), dim - 1)\n\n            low, high = find_correction_range(\n                beta_fast, beta_slow, dim, theta, original_max_position_embeddings\n            )\n\n            inv_freq_extrapolation = inv_freq\n            inv_freq_interpolation = 1.0 / (\n                factor\n                * (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n            )\n\n            def linear_ramp_factor(min_val, max_val, dim_range):\n                if min_val == max_val:\n                    max_val += 0.001\n                linear_func = (\n                    torch.arange(dim_range, dtype=torch.float32) - min_val\n                ) / (max_val - min_val)\n                return torch.clamp(linear_func, 0, 1)\n\n            extrapolation_factor = 1.0 - linear_ramp_factor(low, high, dim // 2)\n            inv_freq = (\n                inv_freq_interpolation * (1.0 - extrapolation_factor)\n                + inv_freq_extrapolation * extrapolation_factor\n            )\n\n            t = torch.arange(end, device=inv_freq.device)\n            freqs = torch.outer(t, inv_freq).float()\n            return torch.stack(\n                (\n                    torch.cos(freqs) * attention_factor,\n                    torch.sin(freqs) * attention_factor,\n                ),\n                dim=-1,\n            )\n\n        elif rope_type == \"llama3\":\n            factor = scaling_config.get(\"factor\", 8.0)\n            low_freq_factor = scaling_config.get(\"low_freq_factor\", 1.0)\n            high_freq_factor = scaling_config.get(\"high_freq_factor\", 4.0)\n            old_context_len = scaling_config.get(\n                \"original_max_position_embeddings\", 8192\n            )\n\n            low_freq_wavelen = old_context_len / low_freq_factor\n            high_freq_wavelen = old_context_len / high_freq_factor\n\n            wavelens = 2 * math.pi / inv_freq\n\n            scaling_factors = torch.ones_like(inv_freq)\n\n            # Wavelengths &gt; low_freq_wavelen: apply full factor\n            mask_low = wavelens &gt; low_freq_wavelen\n            scaling_factors[mask_low] = factor\n\n            # Wavelengths in between: smooth interpolation\n            mask_mid = (wavelens &lt;= low_freq_wavelen) &amp; (wavelens &gt;= high_freq_wavelen)\n            smooth = (old_context_len / wavelens[mask_mid] - low_freq_factor) / (\n                high_freq_factor - low_freq_factor\n            )\n            scaling_factors[mask_mid] = 1.0 + smooth * (factor - 1.0)\n\n            inv_freq = inv_freq / scaling_factors\n\n    t = torch.arange(end, device=inv_freq.device)\n    freqs = torch.outer(t, inv_freq).float()\n    return torch.stack((torch.cos(freqs), torch.sin(freqs)), dim=-1)\n</code></pre>"},{"location":"reference/modules/model/blocks/transformer/","title":"transformer","text":""},{"location":"reference/modules/model/blocks/transformer/#optimus_dl.modules.model.blocks.transformer","title":"<code>optimus_dl.modules.model.blocks.transformer</code>","text":""},{"location":"reference/modules/model/blocks/transformer/#optimus_dl.modules.model.blocks.transformer.RotaryTransformerBlock","title":"<code>RotaryTransformerBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Unified Transformer block with RMSNorm, Rotary Attention, and SwiGLU MLP.</p> <p>Used by Llama and Qwen models. Supports optional Q/K normalization.</p> Source code in <code>optimus_dl/modules/model/blocks/transformer.py</code> <pre><code>class RotaryTransformerBlock(nn.Module):\n    \"\"\"Unified Transformer block with RMSNorm, Rotary Attention, and SwiGLU MLP.\n\n    Used by Llama and Qwen models. Supports optional Q/K normalization.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_embd: int,\n        n_head: int,\n        n_kv_head: int | None = None,\n        head_dim: int | None = None,\n        dropout: float = 0.0,\n        rmsnorm_eps: float = 1e-5,\n        bias: bool = False,\n        attention_bias: bool = False,\n        use_qk_norm: bool = False,\n        qk_norm_per_head: bool = True,\n        intermediate_size: int | None = None,\n        multiple_of: int = 256,\n        use_liger_rmsnorm: bool | None = None,\n        use_liger_swiglu: bool | None = None,\n    ):\n        super().__init__()\n        self.ln_1 = RMSNorm(n_embd, eps=rmsnorm_eps, use_liger=use_liger_rmsnorm)\n        self.attn = RotarySelfAttention(\n            n_embd=n_embd,\n            n_head=n_head,\n            n_kv_head=n_kv_head,\n            head_dim=head_dim,\n            dropout=dropout,\n            bias=attention_bias,\n            use_qk_norm=use_qk_norm,\n            qk_norm_per_head=qk_norm_per_head,\n            rmsnorm_eps=rmsnorm_eps,\n        )\n        self.ln_2 = RMSNorm(n_embd, eps=rmsnorm_eps, use_liger=use_liger_rmsnorm)\n        self.mlp = SwiGLUMLP(\n            n_embd=n_embd,\n            intermediate_size=intermediate_size,\n            multiple_of=multiple_of,\n            bias=bias,\n            use_liger=use_liger_swiglu,\n        )\n\n    def forward(\n        self,\n        *,\n        x: torch.Tensor,\n        freqs_cis: torch.Tensor,\n        seq_lens: torch.Tensor | None = None,\n        document_ids: torch.Tensor | None = None,\n        position_ids: torch.Tensor | None = None,\n        cu_seqlens: torch.Tensor | None = None,\n        max_seqlen: int | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the forward pass for the transformer block (pre-norm residual).\"\"\"\n        ln_1 = self.ln_1(x)\n        attn_out = self.attn(\n            ln_1,\n            freqs_cis=freqs_cis,\n            seq_lens=seq_lens,\n            document_ids=document_ids,\n            position_ids=position_ids,\n            cu_seqlens=cu_seqlens,\n            max_seqlen=max_seqlen,\n        )\n\n        x = x + attn_out\n        x = x + self.mlp(self.ln_2(x))\n        return x\n</code></pre>"},{"location":"reference/modules/model/blocks/transformer/#optimus_dl.modules.model.blocks.transformer.RotaryTransformerBlock.forward","title":"<code>forward(*, x, freqs_cis, seq_lens=None, document_ids=None, position_ids=None, cu_seqlens=None, max_seqlen=None)</code>","text":"<p>Compute the forward pass for the transformer block (pre-norm residual).</p> Source code in <code>optimus_dl/modules/model/blocks/transformer.py</code> <pre><code>def forward(\n    self,\n    *,\n    x: torch.Tensor,\n    freqs_cis: torch.Tensor,\n    seq_lens: torch.Tensor | None = None,\n    document_ids: torch.Tensor | None = None,\n    position_ids: torch.Tensor | None = None,\n    cu_seqlens: torch.Tensor | None = None,\n    max_seqlen: int | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the forward pass for the transformer block (pre-norm residual).\"\"\"\n    ln_1 = self.ln_1(x)\n    attn_out = self.attn(\n        ln_1,\n        freqs_cis=freqs_cis,\n        seq_lens=seq_lens,\n        document_ids=document_ids,\n        position_ids=position_ids,\n        cu_seqlens=cu_seqlens,\n        max_seqlen=max_seqlen,\n    )\n\n    x = x + attn_out\n    x = x + self.mlp(self.ln_2(x))\n    return x\n</code></pre>"},{"location":"reference/modules/model/presets/","title":"Index","text":""},{"location":"reference/modules/model/presets/#optimus_dl.modules.model.presets","title":"<code>optimus_dl.modules.model.presets</code>","text":""},{"location":"reference/modules/model/presets/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>hf_llama</code>: Preset for loading Hugging Face Llama models.</li> <li><code>hf_olmo3</code>: Preset for loading Hugging Face Olmo3 models.</li> <li><code>hf_qwen3</code>: Preset for loading Hugging Face Qwen3 models.</li> <li><code>utils</code>: Utility functions for loading Hugging Face models.</li> </ul>"},{"location":"reference/modules/model/presets/hf_llama/","title":"hf_llama","text":""},{"location":"reference/modules/model/presets/hf_llama/#optimus_dl.modules.model.presets.hf_llama","title":"<code>optimus_dl.modules.model.presets.hf_llama</code>","text":"<p>Preset for loading Hugging Face Llama models.</p>"},{"location":"reference/modules/model/presets/hf_llama/#optimus_dl.modules.model.presets.hf_llama.HFLlamaConfig","title":"<code>HFLlamaConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>LlamaConfig</code></p> <p>HFLlamaConfig(_name: str | None = None, block_size: int = 1024, vocab_size: int = 50304, n_layer: int = 12, n_head: int = 12, n_embd: int = 768, head_dim: int | None = None, dropout: float = 0.0, bias: bool = False, tie_word_embeddings: bool = True, shard_every_ith_layer: int = 1, padding_token_id: int | None = None, sequence_length: int = 16000, rmsnorm_eps: float = 1e-05, attention_bias: bool = False, n_kv_head: int | None = None, intermediate_size: int | None = None, multiple_of: int = 256, rope_theta: float = 10000.0, rope_scaling: dict | None = None, use_liger_rmsnorm: bool | None = None, use_liger_swiglu: bool | None = None, hf_model_name: str = 'meta-llama/Llama-2-7b-hf', load_weights: bool = True)</p> <p>Parameters:</p> Name Type Description Default <code>hf_model_name</code> <code>str</code> <code>'meta-llama/Llama-2-7b-hf'</code> <code>load_weights</code> <code>bool</code> <code>True</code> Source code in <code>optimus_dl/modules/model/presets/hf_llama.py</code> <pre><code>@dataclass\nclass HFLlamaConfig(LlamaConfig):\n    hf_model_name: str = \"meta-llama/Llama-2-7b-hf\"\n    load_weights: bool = (\n        True  # If True, will download and load weights. If False, just config is used (random init)\n    )\n</code></pre>"},{"location":"reference/modules/model/presets/hf_llama/#optimus_dl.modules.model.presets.hf_llama.make_hf_llama_model","title":"<code>make_hf_llama_model(cfg, **_)</code>","text":"<p>Create a Llama model loaded with weights from Hugging Face.</p> Source code in <code>optimus_dl/modules/model/presets/hf_llama.py</code> <pre><code>@register_model(\"preset_hfllama2\", HFLlamaConfig)\ndef make_hf_llama_model(cfg: HFLlamaConfig, **_):\n    \"\"\"Create a Llama model loaded with weights from Hugging Face.\"\"\"\n    logger.info(f\"Loading HF model: {cfg.hf_model_name}\")\n\n    # Load HF config\n    hf_config = AutoConfig.from_pretrained(cfg.hf_model_name)\n\n    # Update local config from HF config\n    update_config_from_hf(cfg, hf_config)\n\n    # Initialize local Llama model\n    model = Llama(cfg)\n\n    if not cfg.load_weights:\n        return model\n\n    # Load HF model weights\n    logger.info(\"Loading HF model weights...\")\n    hf_model = AutoModelForCausalLM.from_pretrained(\n        cfg.hf_model_name,\n        dtype=torch.float32,\n        low_cpu_mem_usage=True,\n    )\n    hf_sd = hf_model.state_dict()\n    mapper = WeightMapper(hf_sd, model.state_dict())\n\n    logger.info(\"Copying weights...\")\n\n    # Embeddings\n    mapper.copy(\"model.embed_tokens.weight\", \"transformer.wte.weight\")\n\n    # Layers\n    for i in range(cfg.n_layer):\n        # Attention\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.q_proj.weight\",\n            f\"transformer.h.{i}.attn.wq.weight\",\n            permute=True,\n            n_heads=cfg.n_head,\n            head_dim=cfg.head_dim,\n        )\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.k_proj.weight\",\n            f\"transformer.h.{i}.attn.wk.weight\",\n            permute=True,\n            n_heads=cfg.n_kv_head,\n            head_dim=cfg.head_dim,\n        )\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.v_proj.weight\",\n            f\"transformer.h.{i}.attn.wv.weight\",\n        )\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.o_proj.weight\",\n            f\"transformer.h.{i}.attn.wo.weight\",\n        )\n\n        # MLP\n        mapper.copy(\n            f\"model.layers.{i}.mlp.gate_proj.weight\", f\"transformer.h.{i}.mlp.w1.weight\"\n        )\n        mapper.copy(\n            f\"model.layers.{i}.mlp.up_proj.weight\", f\"transformer.h.{i}.mlp.w2.weight\"\n        )\n        mapper.copy(\n            f\"model.layers.{i}.mlp.down_proj.weight\",\n            f\"transformer.h.{i}.mlp.c_proj.weight\",\n        )\n\n        # Layer Norms\n        mapper.copy(\n            f\"model.layers.{i}.input_layernorm.weight\", f\"transformer.h.{i}.ln_1.weight\"\n        )\n        mapper.copy(\n            f\"model.layers.{i}.post_attention_layernorm.weight\",\n            f\"transformer.h.{i}.ln_2.weight\",\n        )\n\n    # Final Norm\n    mapper.copy(\"model.norm.weight\", \"transformer.ln_f.weight\")\n\n    # LM Head\n    mapper.copy(\"lm_head.weight\", \"lm_head.weight\")\n\n    # Validation\n    mapper.validate(tie_word_embeddings=cfg.tie_word_embeddings)\n\n    del hf_model\n    del hf_sd\n    import gc\n\n    gc.collect()\n\n    return model\n</code></pre>"},{"location":"reference/modules/model/presets/hf_olmo3/","title":"hf_olmo3","text":""},{"location":"reference/modules/model/presets/hf_olmo3/#optimus_dl.modules.model.presets.hf_olmo3","title":"<code>optimus_dl.modules.model.presets.hf_olmo3</code>","text":"<p>Preset for loading Hugging Face Olmo3 models.</p>"},{"location":"reference/modules/model/presets/hf_olmo3/#optimus_dl.modules.model.presets.hf_olmo3.HFOlmo3Config","title":"<code>HFOlmo3Config</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Olmo3Config</code></p> <p>HFOlmo3Config(_name: str | None = None, block_size: int = 1024, vocab_size: int = 50304, n_layer: int = 16, n_head: int = 12, n_embd: int = 768, head_dim: int | None = None, dropout: float = 0.0, bias: bool = False, tie_word_embeddings: bool = False, shard_every_ith_layer: int = 1, padding_token_id: int | None = None, sequence_length: int = 4096, rmsnorm_eps: float = 1e-06, rope_theta: float = 500000.0, rope_parameters: dict = , attention_bias: bool = False, n_kv_head: int | None = 4, intermediate_size: int | None = 1024, multiple_of: int = 256, sliding_window: int = 4096, layer_types: list[str] = , use_liger_rmsnorm: bool | None = None, use_liger_swiglu: bool | None = None, hf_model_name: str = 'allenai/Olmo-3-1025-7B', load_weights: bool = True) <p>Parameters:</p> Name Type Description Default <code>hf_model_name</code> <code>str</code> <code>'allenai/Olmo-3-1025-7B'</code> <code>load_weights</code> <code>bool</code> <code>True</code> Source code in <code>optimus_dl/modules/model/presets/hf_olmo3.py</code> <pre><code>@dataclass\nclass HFOlmo3Config(Olmo3Config):\n    hf_model_name: str = \"allenai/Olmo-3-1025-7B\"\n    load_weights: bool = True\n</code></pre>"},{"location":"reference/modules/model/presets/hf_olmo3/#optimus_dl.modules.model.presets.hf_olmo3.make_hf_olmo3_model","title":"<code>make_hf_olmo3_model(cfg, **_)</code>","text":"<p>Create an Olmo3 model loaded with weights from Hugging Face.</p> Source code in <code>optimus_dl/modules/model/presets/hf_olmo3.py</code> <pre><code>@register_model(\"preset_hfolmo3\", HFOlmo3Config)\ndef make_hf_olmo3_model(cfg: HFOlmo3Config, **_):\n    \"\"\"Create an Olmo3 model loaded with weights from Hugging Face.\"\"\"\n    logger.info(f\"Loading HF model: {cfg.hf_model_name}\")\n\n    # Load HF config\n    hf_config = AutoConfig.from_pretrained(cfg.hf_model_name, trust_remote_code=True)\n\n    # Update local config from HF config\n    update_config_from_hf(cfg, hf_config)\n    cfg.sliding_window = getattr(hf_config, \"sliding_window\", 4096)\n    cfg.layer_types = getattr(\n        hf_config,\n        \"layer_types\",\n        [\n            \"full_attention\",\n        ]\n        * cfg.n_layer,\n    )\n\n    # Extract rope_parameters robustly\n    rope_params = getattr(hf_config, \"rope_parameters\", None)\n    if rope_params:\n        # Standardize field names if they differ from what precompute_freqs_cis expects\n        cfg.rope_parameters = dict(rope_params)\n        if \"rope_theta\" in rope_params:\n            cfg.rope_theta = rope_params[\"rope_theta\"]\n\n    cfg.attention_bias = getattr(hf_config, \"attention_bias\", False)\n\n    # Initialize local Olmo3 model\n    model = Olmo3(cfg)\n\n    if not cfg.load_weights:\n        return model\n\n    # Load HF model weights\n    logger.info(\"Loading HF model weights...\")\n    hf_model = AutoModelForCausalLM.from_pretrained(\n        cfg.hf_model_name,\n        dtype=torch.float32,\n        low_cpu_mem_usage=True,\n        trust_remote_code=True,\n    )\n    hf_sd = hf_model.state_dict()\n    mapper = WeightMapper(hf_sd, model.state_dict())\n\n    logger.info(\"Copying weights...\")\n\n    # Embeddings\n    mapper.copy(\"model.embed_tokens.weight\", \"transformer.wte.weight\")\n\n    # Layers\n    for i in range(cfg.n_layer):\n        # Attention\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.q_proj.weight\",\n            f\"transformer.h.{i}.attn.wq.weight\",\n            permute=True,\n            n_heads=cfg.n_head,\n            head_dim=cfg.head_dim,\n        )\n        if cfg.attention_bias:\n            mapper.copy(\n                f\"model.layers.{i}.self_attn.q_proj.bias\",\n                f\"transformer.h.{i}.attn.wq.bias\",\n                permute=True,\n                n_heads=cfg.n_head,\n                head_dim=cfg.head_dim,\n            )\n\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.k_proj.weight\",\n            f\"transformer.h.{i}.attn.wk.weight\",\n            permute=True,\n            n_heads=cfg.n_kv_head,\n            head_dim=cfg.head_dim,\n        )\n        if cfg.attention_bias:\n            mapper.copy(\n                f\"model.layers.{i}.self_attn.k_proj.bias\",\n                f\"transformer.h.{i}.attn.wk.bias\",\n                permute=True,\n                n_heads=cfg.n_kv_head,\n                head_dim=cfg.head_dim,\n            )\n\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.v_proj.weight\",\n            f\"transformer.h.{i}.attn.wv.weight\",\n        )\n        if cfg.attention_bias:\n            mapper.copy(\n                f\"model.layers.{i}.self_attn.v_proj.bias\",\n                f\"transformer.h.{i}.attn.wv.bias\",\n            )\n\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.o_proj.weight\",\n            f\"transformer.h.{i}.attn.wo.weight\",\n        )\n        if cfg.attention_bias:\n            mapper.copy(\n                f\"model.layers.{i}.self_attn.o_proj.bias\",\n                f\"transformer.h.{i}.attn.wo.bias\",\n            )\n\n        # Q/K Norms\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.q_norm.weight\",\n            f\"transformer.h.{i}.attn.q_norm.weight\",\n            permute=True,\n            n_heads=cfg.n_head,\n            head_dim=cfg.head_dim,\n        )\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.k_norm.weight\",\n            f\"transformer.h.{i}.attn.k_norm.weight\",\n            permute=True,\n            n_heads=cfg.n_kv_head,\n            head_dim=cfg.head_dim,\n        )\n\n        # MLP\n        mapper.copy(\n            f\"model.layers.{i}.mlp.gate_proj.weight\", f\"transformer.h.{i}.mlp.w1.weight\"\n        )\n        mapper.copy(\n            f\"model.layers.{i}.mlp.up_proj.weight\", f\"transformer.h.{i}.mlp.w2.weight\"\n        )\n        mapper.copy(\n            f\"model.layers.{i}.mlp.down_proj.weight\",\n            f\"transformer.h.{i}.mlp.c_proj.weight\",\n        )\n\n        # Layer Norms (Refactored names)\n        mapper.copy(\n            f\"model.layers.{i}.post_attention_layernorm.weight\",\n            f\"transformer.h.{i}.post_attention_layernorm.weight\",\n        )\n        mapper.copy(\n            f\"model.layers.{i}.post_feedforward_layernorm.weight\",\n            f\"transformer.h.{i}.post_feedforward_layernorm.weight\",\n        )\n\n    # Final Norm\n    mapper.copy(\"model.norm.weight\", \"transformer.ln_f.weight\")\n\n    # LM Head\n    mapper.copy(\"lm_head.weight\", \"lm_head.weight\")\n\n    # Validation\n    mapper.validate(tie_word_embeddings=cfg.tie_word_embeddings)\n\n    del hf_model\n    del hf_sd\n    import gc\n\n    gc.collect()\n\n    return model\n</code></pre>"},{"location":"reference/modules/model/presets/hf_qwen3/","title":"hf_qwen3","text":""},{"location":"reference/modules/model/presets/hf_qwen3/#optimus_dl.modules.model.presets.hf_qwen3","title":"<code>optimus_dl.modules.model.presets.hf_qwen3</code>","text":"<p>Preset for loading Hugging Face Qwen3 models.</p>"},{"location":"reference/modules/model/presets/hf_qwen3/#optimus_dl.modules.model.presets.hf_qwen3.HFQwen3Config","title":"<code>HFQwen3Config</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Qwen3Config</code></p> <p>HFQwen3Config(_name: str | None = None, block_size: int = 1024, vocab_size: int = 50304, n_layer: int = 12, n_head: int = 12, n_embd: int = 768, head_dim: int | None = None, dropout: float = 0.0, bias: bool = False, tie_word_embeddings: bool = True, shard_every_ith_layer: int = 1, padding_token_id: int | None = None, sequence_length: int = 32768, rmsnorm_eps: float = 1e-06, rope_theta: float = 1000000.0, rope_scaling: dict | None = None, attention_bias: bool = True, n_kv_head: int | None = None, intermediate_size: int | None = None, multiple_of: int = 256, use_liger_rmsnorm: bool | None = None, use_liger_swiglu: bool | None = None, hf_model_name: str = 'Qwen/Qwen3-4B-Thinking-2507', load_weights: bool = True)</p> <p>Parameters:</p> Name Type Description Default <code>hf_model_name</code> <code>str</code> <code>'Qwen/Qwen3-4B-Thinking-2507'</code> <code>load_weights</code> <code>bool</code> <code>True</code> Source code in <code>optimus_dl/modules/model/presets/hf_qwen3.py</code> <pre><code>@dataclass\nclass HFQwen3Config(Qwen3Config):\n    hf_model_name: str = \"Qwen/Qwen3-4B-Thinking-2507\"\n    load_weights: bool = True\n</code></pre>"},{"location":"reference/modules/model/presets/hf_qwen3/#optimus_dl.modules.model.presets.hf_qwen3.make_hf_qwen3_model","title":"<code>make_hf_qwen3_model(cfg, **_)</code>","text":"<p>Create a Qwen3 model loaded with weights from Hugging Face.</p> Source code in <code>optimus_dl/modules/model/presets/hf_qwen3.py</code> <pre><code>@register_model(\"preset_hfqwen3\", HFQwen3Config)\ndef make_hf_qwen3_model(cfg: HFQwen3Config, **_):\n    \"\"\"Create a Qwen3 model loaded with weights from Hugging Face.\"\"\"\n    logger.info(f\"Loading HF model: {cfg.hf_model_name}\")\n\n    # Load HF config\n    hf_config = AutoConfig.from_pretrained(cfg.hf_model_name, trust_remote_code=True)\n\n    # Update local config from HF config\n    update_config_from_hf(cfg, hf_config)\n    cfg.attention_bias = getattr(hf_config, \"attention_bias\", False)\n\n    # Initialize local Qwen3 model\n    model = Qwen3(cfg)\n\n    if not cfg.load_weights:\n        return model\n\n    # Load HF model weights\n    logger.info(\"Loading HF model weights...\")\n    hf_model = AutoModelForCausalLM.from_pretrained(\n        cfg.hf_model_name,\n        dtype=torch.float32,\n        low_cpu_mem_usage=True,\n        trust_remote_code=True,\n    )\n    hf_sd = hf_model.state_dict()\n    mapper = WeightMapper(hf_sd, model.state_dict())\n\n    logger.info(\"Copying weights...\")\n\n    # Embeddings\n    mapper.copy(\"model.embed_tokens.weight\", \"transformer.wte.weight\")\n\n    # Layers\n    for i in range(cfg.n_layer):\n        # Attention\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.q_proj.weight\",\n            f\"transformer.h.{i}.attn.wq.weight\",\n            permute=True,\n            n_heads=cfg.n_head,\n            head_dim=cfg.head_dim,\n        )\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.q_proj.bias\",\n            f\"transformer.h.{i}.attn.wq.bias\",\n            permute=True,\n            n_heads=cfg.n_head,\n            head_dim=cfg.head_dim,\n        )\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.k_proj.weight\",\n            f\"transformer.h.{i}.attn.wk.weight\",\n            permute=True,\n            n_heads=cfg.n_kv_head,\n            head_dim=cfg.head_dim,\n        )\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.k_proj.bias\",\n            f\"transformer.h.{i}.attn.wk.bias\",\n            permute=True,\n            n_heads=cfg.n_kv_head,\n            head_dim=cfg.head_dim,\n        )\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.v_proj.weight\",\n            f\"transformer.h.{i}.attn.wv.weight\",\n        )\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.v_proj.bias\",\n            f\"transformer.h.{i}.attn.wv.bias\",\n        )\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.o_proj.weight\",\n            f\"transformer.h.{i}.attn.wo.weight\",\n        )\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.o_proj.bias\",\n            f\"transformer.h.{i}.attn.wo.bias\",\n        )\n\n        # Q/K Norms\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.q_norm.weight\",\n            f\"transformer.h.{i}.attn.q_norm.weight\",\n            permute=True,\n            n_heads=cfg.n_head,\n            head_dim=cfg.head_dim,\n        )\n        mapper.copy(\n            f\"model.layers.{i}.self_attn.k_norm.weight\",\n            f\"transformer.h.{i}.attn.k_norm.weight\",\n            permute=True,\n            n_heads=cfg.n_kv_head,\n            head_dim=cfg.head_dim,\n        )\n\n        # MLP\n        mapper.copy(\n            f\"model.layers.{i}.mlp.gate_proj.weight\", f\"transformer.h.{i}.mlp.w1.weight\"\n        )\n        mapper.copy(\n            f\"model.layers.{i}.mlp.up_proj.weight\", f\"transformer.h.{i}.mlp.w2.weight\"\n        )\n        mapper.copy(\n            f\"model.layers.{i}.mlp.down_proj.weight\",\n            f\"transformer.h.{i}.mlp.c_proj.weight\",\n        )\n\n        # Layer Norms\n        mapper.copy(\n            f\"model.layers.{i}.input_layernorm.weight\", f\"transformer.h.{i}.ln_1.weight\"\n        )\n        mapper.copy(\n            f\"model.layers.{i}.post_attention_layernorm.weight\",\n            f\"transformer.h.{i}.ln_2.weight\",\n        )\n\n    # Final Norm\n    mapper.copy(\"model.norm.weight\", \"transformer.ln_f.weight\")\n\n    # LM Head\n    mapper.copy(\"lm_head.weight\", \"lm_head.weight\")\n\n    # Validation\n    mapper.validate(tie_word_embeddings=cfg.tie_word_embeddings)\n\n    del hf_model\n    del hf_sd\n    import gc\n\n    gc.collect()\n\n    return model\n</code></pre>"},{"location":"reference/modules/model/presets/utils/","title":"utils","text":""},{"location":"reference/modules/model/presets/utils/#optimus_dl.modules.model.presets.utils","title":"<code>optimus_dl.modules.model.presets.utils</code>","text":"<p>Utility functions for loading Hugging Face models.</p>"},{"location":"reference/modules/model/presets/utils/#optimus_dl.modules.model.presets.utils.WeightMapper","title":"<code>WeightMapper</code>","text":"<p>Helper to map and copy weights from HF state dict to local model.</p> Source code in <code>optimus_dl/modules/model/presets/utils.py</code> <pre><code>class WeightMapper:\n    \"\"\"Helper to map and copy weights from HF state dict to local model.\"\"\"\n\n    def __init__(\n        self, hf_sd: dict[str, torch.Tensor], local_sd: dict[str, torch.Tensor]\n    ):\n        self.hf_sd = hf_sd\n        self.local_sd = local_sd\n        self.loaded_keys: set[str] = set()\n\n    def copy(\n        self,\n        src_key: str,\n        dest_key: str,\n        permute: bool = False,\n        n_heads: int | None = None,\n        head_dim: int | None = None,\n        transpose: bool = False,\n    ):\n        \"\"\"Copy weight from HF state dict to local state dict.\"\"\"\n        if src_key not in self.hf_sd:\n            if dest_key in self.local_sd:\n                logger.warning(f\"Missing key in HF model: {src_key}\")\n            return\n        w = self.hf_sd[src_key]\n\n        if transpose:\n            w = w.t()\n\n        if permute:\n            assert n_heads is not None and head_dim is not None\n            w = permute_rope_weight(w, n_heads, head_dim)\n\n        if dest_key not in self.local_sd:\n            logger.warning(f\"Extra key in HF model not in local model: {dest_key}\")\n            return\n\n        if self.local_sd[dest_key].shape != w.shape:\n            logger.warning(\n                f\"Shape mismatch for {dest_key}: {self.local_sd[dest_key].shape} vs {w.shape}. Attempting reshape.\"\n            )\n            w = w.view(self.local_sd[dest_key].shape)\n\n        self.local_sd[dest_key].copy_(w)\n        self.loaded_keys.add(dest_key)\n\n    def validate(\n        self,\n        tie_word_embeddings: bool = False,\n        ignore_patterns: list[str] | None = None,\n    ):\n        \"\"\"Validate that all expected keys were loaded.\"\"\"\n        expected_keys = set(self.local_sd.keys())\n        missing_keys = expected_keys - self.loaded_keys\n\n        # Filter out ignored patterns\n        if ignore_patterns:\n            missing_keys = {\n                k\n                for k in missing_keys\n                if not any(pattern in k for pattern in ignore_patterns)\n            }\n\n        # Common ignorable keys\n        missing_keys = {\n            k for k in missing_keys if \"inv_freq\" not in k and \"bias\" not in k\n        }\n\n        if tie_word_embeddings:\n            if (\n                \"transformer.wte.weight\" in self.loaded_keys\n                and \"lm_head.weight\" in missing_keys\n            ):\n                missing_keys.remove(\"lm_head.weight\")\n            if (\n                \"lm_head.weight\" in self.loaded_keys\n                and \"transformer.wte.weight\" in missing_keys\n            ):\n                missing_keys.remove(\"transformer.wte.weight\")\n\n        if missing_keys:\n            logger.warning(f\"Missing keys in loaded model: {missing_keys}\")\n        else:\n            logger.info(\"All weights loaded successfully.\")\n</code></pre>"},{"location":"reference/modules/model/presets/utils/#optimus_dl.modules.model.presets.utils.WeightMapper.copy","title":"<code>copy(src_key, dest_key, permute=False, n_heads=None, head_dim=None, transpose=False)</code>","text":"<p>Copy weight from HF state dict to local state dict.</p> Source code in <code>optimus_dl/modules/model/presets/utils.py</code> <pre><code>def copy(\n    self,\n    src_key: str,\n    dest_key: str,\n    permute: bool = False,\n    n_heads: int | None = None,\n    head_dim: int | None = None,\n    transpose: bool = False,\n):\n    \"\"\"Copy weight from HF state dict to local state dict.\"\"\"\n    if src_key not in self.hf_sd:\n        if dest_key in self.local_sd:\n            logger.warning(f\"Missing key in HF model: {src_key}\")\n        return\n    w = self.hf_sd[src_key]\n\n    if transpose:\n        w = w.t()\n\n    if permute:\n        assert n_heads is not None and head_dim is not None\n        w = permute_rope_weight(w, n_heads, head_dim)\n\n    if dest_key not in self.local_sd:\n        logger.warning(f\"Extra key in HF model not in local model: {dest_key}\")\n        return\n\n    if self.local_sd[dest_key].shape != w.shape:\n        logger.warning(\n            f\"Shape mismatch for {dest_key}: {self.local_sd[dest_key].shape} vs {w.shape}. Attempting reshape.\"\n        )\n        w = w.view(self.local_sd[dest_key].shape)\n\n    self.local_sd[dest_key].copy_(w)\n    self.loaded_keys.add(dest_key)\n</code></pre>"},{"location":"reference/modules/model/presets/utils/#optimus_dl.modules.model.presets.utils.WeightMapper.validate","title":"<code>validate(tie_word_embeddings=False, ignore_patterns=None)</code>","text":"<p>Validate that all expected keys were loaded.</p> Source code in <code>optimus_dl/modules/model/presets/utils.py</code> <pre><code>def validate(\n    self,\n    tie_word_embeddings: bool = False,\n    ignore_patterns: list[str] | None = None,\n):\n    \"\"\"Validate that all expected keys were loaded.\"\"\"\n    expected_keys = set(self.local_sd.keys())\n    missing_keys = expected_keys - self.loaded_keys\n\n    # Filter out ignored patterns\n    if ignore_patterns:\n        missing_keys = {\n            k\n            for k in missing_keys\n            if not any(pattern in k for pattern in ignore_patterns)\n        }\n\n    # Common ignorable keys\n    missing_keys = {\n        k for k in missing_keys if \"inv_freq\" not in k and \"bias\" not in k\n    }\n\n    if tie_word_embeddings:\n        if (\n            \"transformer.wte.weight\" in self.loaded_keys\n            and \"lm_head.weight\" in missing_keys\n        ):\n            missing_keys.remove(\"lm_head.weight\")\n        if (\n            \"lm_head.weight\" in self.loaded_keys\n            and \"transformer.wte.weight\" in missing_keys\n        ):\n            missing_keys.remove(\"transformer.wte.weight\")\n\n    if missing_keys:\n        logger.warning(f\"Missing keys in loaded model: {missing_keys}\")\n    else:\n        logger.info(\"All weights loaded successfully.\")\n</code></pre>"},{"location":"reference/modules/model/presets/utils/#optimus_dl.modules.model.presets.utils.permute_rope_weight","title":"<code>permute_rope_weight(w, n_heads, head_dim, interleaved=True)</code>","text":"<p>Permute weights for Rotary Positional Embeddings.</p> <p>HF typically uses a half-half split (first half of head_dim is cos, second is sin). Optimus-DL uses interleaved (cos, sin, cos, sin...).</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>Tensor</code> <p>Weight tensor of shape (n_heads * head_dim, input_dim) or (n_heads * head_dim,).</p> required <code>n_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>head_dim</code> <code>int</code> <p>Dimension of each head.</p> required <code>interleaved</code> <code>bool</code> <p>If True, permutes to interleaved format. If False, returns as is.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Permuted weight tensor.</p> Source code in <code>optimus_dl/modules/model/presets/utils.py</code> <pre><code>def permute_rope_weight(\n    w: torch.Tensor, n_heads: int, head_dim: int, interleaved: bool = True\n) -&gt; torch.Tensor:\n    \"\"\"Permute weights for Rotary Positional Embeddings.\n\n    HF typically uses a half-half split (first half of head_dim is cos, second is sin).\n    Optimus-DL uses interleaved (cos, sin, cos, sin...).\n\n    Args:\n        w: Weight tensor of shape (n_heads * head_dim, input_dim) or (n_heads * head_dim,).\n        n_heads: Number of attention heads.\n        head_dim: Dimension of each head.\n        interleaved: If True, permutes to interleaved format. If False, returns as is.\n\n    Returns:\n        Permuted weight tensor.\n    \"\"\"\n    if not interleaved:\n        return w\n\n    original_shape = w.shape\n    # Determine if weights are shared across heads (e.g. Q/K norm in some models)\n    effective_n_heads = n_heads\n    if w.shape[0] == head_dim:\n        effective_n_heads = 1\n    elif w.shape[0] != n_heads * head_dim:\n        # Fallback for unexpected shapes - don't permute if we can't reason about it\n        logger.warning(\n            f\"Unexpected shape for RoPE permutation: {w.shape}. Expected first dim to be {head_dim} or {n_heads * head_dim}. Skipping permutation.\"\n        )\n        return w\n\n    # Handle both weight (2D) and bias (1D)\n    if w.ndim == 1:\n        w = w.view(effective_n_heads, head_dim)\n        w1 = w[:, : head_dim // 2]\n        w2 = w[:, head_dim // 2 :]\n        w_new = torch.stack((w1, w2), dim=2)\n        return w_new.reshape(original_shape)\n\n    # 2D case: (output_dim, input_dim)\n    w = w.view(effective_n_heads, head_dim, -1)\n    w1 = w[:, : head_dim // 2, :]\n    w2 = w[:, head_dim // 2 :, :]\n    # Interleave: (x0, x_half, x1, x_half+1...)\n    w_new = torch.stack((w1, w2), dim=2)\n    return w_new.reshape(original_shape)\n</code></pre>"},{"location":"reference/modules/model/presets/utils/#optimus_dl.modules.model.presets.utils.update_config_from_hf","title":"<code>update_config_from_hf(optimus_cfg, hf_config, head_dim_fallback=None)</code>","text":"<p>Update Optimus-DL config from HF config with common attributes.</p> Source code in <code>optimus_dl/modules/model/presets/utils.py</code> <pre><code>def update_config_from_hf(\n    optimus_cfg: Any, hf_config: Any, head_dim_fallback: int | None = None\n):\n    \"\"\"Update Optimus-DL config from HF config with common attributes.\"\"\"\n    optimus_cfg.n_layer = hf_config.num_hidden_layers\n    optimus_cfg.n_head = hf_config.num_attention_heads\n    optimus_cfg.n_embd = hf_config.hidden_size\n    optimus_cfg.vocab_size = hf_config.vocab_size\n    optimus_cfg.sequence_length = getattr(hf_config, \"max_position_embeddings\", 2048)\n    optimus_cfg.block_size = optimus_cfg.sequence_length\n    optimus_cfg.rmsnorm_eps = getattr(hf_config, \"rms_norm_eps\", 1e-5)\n    optimus_cfg.intermediate_size = getattr(hf_config, \"intermediate_size\", None)\n    optimus_cfg.tie_word_embeddings = getattr(hf_config, \"tie_word_embeddings\", False)\n\n    # Handle rope_theta, checking both root and rope_scaling/rope_parameters\n    rope_theta = getattr(hf_config, \"rope_theta\", None)\n    if rope_theta is None:\n        for attr in [\"rope_scaling\", \"rope_parameters\"]:\n            val = getattr(hf_config, attr, None)\n            if isinstance(val, dict):\n                rope_theta = val.get(\"rope_theta\")\n                if rope_theta is not None:\n                    break\n\n    if hasattr(optimus_cfg, \"rope_theta\"):\n        optimus_cfg.rope_theta = rope_theta if rope_theta is not None else 10000.0\n\n    # Handle rope_scaling if it exists in local config\n    if hasattr(optimus_cfg, \"rope_scaling\"):\n        if hasattr(hf_config, \"rope_scaling\") and hf_config.rope_scaling:\n            if isinstance(hf_config.rope_scaling, dict):\n                optimus_cfg.rope_scaling = hf_config.rope_scaling\n        elif hasattr(hf_config, \"rope_parameters\") and hf_config.rope_parameters:\n            if isinstance(hf_config.rope_parameters, dict):\n                optimus_cfg.rope_scaling = hf_config.rope_parameters\n\n    # Handle rope_parameters if it exists in local config\n    if hasattr(optimus_cfg, \"rope_parameters\"):\n        if hasattr(hf_config, \"rope_parameters\") and hf_config.rope_parameters:\n            if isinstance(hf_config.rope_parameters, dict):\n                optimus_cfg.rope_parameters = hf_config.rope_parameters\n        elif hasattr(hf_config, \"rope_scaling\") and hf_config.rope_scaling:\n            if isinstance(hf_config.rope_scaling, dict):\n                optimus_cfg.rope_parameters = hf_config.rope_scaling\n\n    if hasattr(hf_config, \"num_key_value_heads\"):\n        optimus_cfg.n_kv_head = hf_config.num_key_value_heads\n    else:\n        optimus_cfg.n_kv_head = hf_config.num_attention_heads\n\n    if hasattr(hf_config, \"head_dim\") and isinstance(hf_config.head_dim, int):\n        optimus_cfg.head_dim = hf_config.head_dim\n    elif head_dim_fallback:\n        optimus_cfg.head_dim = head_dim_fallback\n    else:\n        optimus_cfg.head_dim = optimus_cfg.n_embd // optimus_cfg.n_head\n</code></pre>"},{"location":"reference/modules/model_transforms/","title":"Index","text":""},{"location":"reference/modules/model_transforms/#optimus_dl.modules.model_transforms","title":"<code>optimus_dl.modules.model_transforms</code>","text":""},{"location":"reference/modules/model_transforms/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>base</code>: Abstract base class for all model transformations.</li> <li><code>checkpoint</code>: Activation checkpointing (gradient checkpointing) transform using public PyTorch API.</li> <li><code>compile</code>: Configuration for torch.compile model transform.</li> <li><code>config</code>: Base configuration for model transforms.</li> <li><code>distributed</code>: Distributed model transforms for training.</li> <li><code>load_weights</code>: Configuration for load weights model transform.</li> <li><code>tensor_parallel</code>: Tensor Parallelism Transform.</li> </ul>"},{"location":"reference/modules/model_transforms/base/","title":"base","text":""},{"location":"reference/modules/model_transforms/base/#optimus_dl.modules.model_transforms.base","title":"<code>optimus_dl.modules.model_transforms.base</code>","text":""},{"location":"reference/modules/model_transforms/base/#optimus_dl.modules.model_transforms.base.BaseModelTransform","title":"<code>BaseModelTransform</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all model transformations.</p> <p>Model transforms are applied after the model is built but before training begins. They modify the model's structure, wrapping it with distributed wrappers (DDP, FSDP), applying graph compilation (torch.compile), or injecting activation checkpointing.</p> <p>Transforms are registered in the <code>model_transform</code> registry and can be chained together in the configuration.</p> Source code in <code>optimus_dl/modules/model_transforms/base.py</code> <pre><code>class BaseModelTransform(ABC):\n    \"\"\"Abstract base class for all model transformations.\n\n    Model transforms are applied after the model is built but before training\n    begins. They modify the model's structure, wrapping it with distributed\n    wrappers (DDP, FSDP), applying graph compilation (torch.compile), or\n    injecting activation checkpointing.\n\n    Transforms are registered in the `model_transform` registry and can be\n    chained together in the configuration.\n    \"\"\"\n\n    def __init__(self, cfg: Any = None, **kwargs):\n        \"\"\"Initialize the transform.\n\n        Args:\n            cfg: Configuration object for the transform.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        self.cfg = cfg\n\n    @abstractmethod\n    def apply(self, model: BaseModel, **kwargs) -&gt; BaseModel:\n        \"\"\"Apply the transformation to the given model.\n\n        Args:\n            model: The model to transform.\n            **kwargs: Additional arguments (e.g., collective, device).\n\n        Returns:\n            The transformed model (either modified in-place or a new wrapper).\n        \"\"\"\n        pass\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(cfg={self.cfg})\"\n</code></pre>"},{"location":"reference/modules/model_transforms/base/#optimus_dl.modules.model_transforms.base.BaseModelTransform.__init__","title":"<code>__init__(cfg=None, **kwargs)</code>","text":"<p>Initialize the transform.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>Any</code> <p>Configuration object for the transform.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>optimus_dl/modules/model_transforms/base.py</code> <pre><code>def __init__(self, cfg: Any = None, **kwargs):\n    \"\"\"Initialize the transform.\n\n    Args:\n        cfg: Configuration object for the transform.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    self.cfg = cfg\n</code></pre>"},{"location":"reference/modules/model_transforms/base/#optimus_dl.modules.model_transforms.base.BaseModelTransform.apply","title":"<code>apply(model, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Apply the transformation to the given model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel</code> <p>The model to transform.</p> required <code>**kwargs</code> <p>Additional arguments (e.g., collective, device).</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseModel</code> <p>The transformed model (either modified in-place or a new wrapper).</p> Source code in <code>optimus_dl/modules/model_transforms/base.py</code> <pre><code>@abstractmethod\ndef apply(self, model: BaseModel, **kwargs) -&gt; BaseModel:\n    \"\"\"Apply the transformation to the given model.\n\n    Args:\n        model: The model to transform.\n        **kwargs: Additional arguments (e.g., collective, device).\n\n    Returns:\n        The transformed model (either modified in-place or a new wrapper).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/modules/model_transforms/checkpoint/","title":"checkpoint","text":""},{"location":"reference/modules/model_transforms/checkpoint/#optimus_dl.modules.model_transforms.checkpoint","title":"<code>optimus_dl.modules.model_transforms.checkpoint</code>","text":"<p>Activation checkpointing (gradient checkpointing) transform using public PyTorch API.</p>"},{"location":"reference/modules/model_transforms/checkpoint/#optimus_dl.modules.model_transforms.checkpoint.ActivationCheckpointConfig","title":"<code>ActivationCheckpointConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ModelTransformConfig</code></p> <p>Configuration for activation checkpointing.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>layer_classes</code> <code>list[str] | None</code> <code>None</code> <code>use_reentrant</code> <code>bool</code> <code>False</code> <code>ops_to_save</code> <code>list[str] | None</code> <code>None</code> Source code in <code>optimus_dl/modules/model_transforms/checkpoint.py</code> <pre><code>@dataclass\nclass ActivationCheckpointConfig(ModelTransformConfig):\n    \"\"\"Configuration for activation checkpointing.\n\n    Attributes:\n        layer_classes: List of layer class names to wrap (e.g., [\"LlamaBlock\"]).\n        use_reentrant: If True, uses the legacy reentrant checkpointing. False\n            is recommended for modern PyTorch and FSDP integration.\n        ops_to_save: Optional list of specific operations to always save (not recompute).\n    \"\"\"\n\n    # List of layer class names to wrap (e.g. [\"LlamaBlock\", \"GPTBlock\"])\n    layer_classes: list[str] | None = None\n\n    # Whether to use reentrant checkpointing.\n    # False is generally recommended for newer PyTorch versions and FSDP.\n    use_reentrant: bool = False\n\n    ops_to_save: list[str] | None = None\n</code></pre>"},{"location":"reference/modules/model_transforms/checkpoint/#optimus_dl.modules.model_transforms.checkpoint.ActivationCheckpointTransform","title":"<code>ActivationCheckpointTransform</code>","text":"<p>               Bases: <code>BaseModelTransform</code></p> <p>Transform that injects activation checkpointing into a model.</p> <p>Recursively searches the model for instances of specified <code>layer_classes</code> and wraps them with <code>CheckpointWrapper</code>. This is a crucial optimization for fitting large models or long sequences into GPU memory.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>ActivationCheckpointConfig</code> <p>Activation checkpointing configuration.</p> required Source code in <code>optimus_dl/modules/model_transforms/checkpoint.py</code> <pre><code>@register_model_transform(\"activation_checkpoint\", ActivationCheckpointConfig)\nclass ActivationCheckpointTransform(BaseModelTransform):\n    \"\"\"Transform that injects activation checkpointing into a model.\n\n    Recursively searches the model for instances of specified `layer_classes`\n    and wraps them with `CheckpointWrapper`. This is a crucial optimization for\n    fitting large models or long sequences into GPU memory.\n\n    Args:\n        cfg: Activation checkpointing configuration.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: ActivationCheckpointConfig,\n        **kwargs: Any,\n    ):\n        super().__init__(cfg, **kwargs)\n\n    def apply(self, model: BaseModel, **kwargs) -&gt; BaseModel:\n        \"\"\"Find and wrap target layers in the model.\"\"\"\n        logger.info(\"Applying activation checkpointing (torch.utils.checkpoint)\")\n\n        if not self.cfg.layer_classes:\n            logger.warning(\n                \"No layer classes specified for activation checkpointing. \"\n                \"Please specify 'layer_classes' in the config (e.g. ['LlamaBlock']).\"\n            )\n            return model\n\n        target_classes = set(self.cfg.layer_classes)\n        ops_to_save = [\n            eval(op, {\"__builtins__\": None}, {\"torch\": torch})\n            for op in (self.cfg.ops_to_save or [])\n        ]\n        replaced_count = self._replace_modules(\n            model,\n            target_classes,\n            use_reentrant=self.cfg.use_reentrant,\n            ops_to_save=ops_to_save,\n        )\n\n        if replaced_count == 0:\n            logger.warning(f\"No modules matching {target_classes} found to checkpoint.\")\n        else:\n            logger.info(\n                f\"Applied activation checkpointing to {replaced_count} layers of types: {target_classes}\"\n            )\n\n        return model\n\n    def _replace_modules(\n        self,\n        model: nn.Module,\n        target_classes: set,\n        use_reentrant: bool,\n        ops_to_save: list,\n    ) -&gt; int:\n        \"\"\"Recursively replace target modules with CheckpointWrapper.\"\"\"\n        count = 0\n        for name, child in model.named_children():\n            if child.__class__.__name__ in target_classes:\n                # Replace the module\n                logger.debug(f\"Wrapping {name} ({child.__class__.__name__})\")\n                wrapped_child = CheckpointWrapper(\n                    child, use_reentrant=use_reentrant, ops_to_save=ops_to_save\n                )\n                setattr(model, name, wrapped_child)\n                count += 1\n            else:\n                # Recurse\n                count += self._replace_modules(\n                    child, target_classes, use_reentrant, ops_to_save\n                )\n        return count\n</code></pre>"},{"location":"reference/modules/model_transforms/checkpoint/#optimus_dl.modules.model_transforms.checkpoint.ActivationCheckpointTransform.apply","title":"<code>apply(model, **kwargs)</code>","text":"<p>Find and wrap target layers in the model.</p> Source code in <code>optimus_dl/modules/model_transforms/checkpoint.py</code> <pre><code>def apply(self, model: BaseModel, **kwargs) -&gt; BaseModel:\n    \"\"\"Find and wrap target layers in the model.\"\"\"\n    logger.info(\"Applying activation checkpointing (torch.utils.checkpoint)\")\n\n    if not self.cfg.layer_classes:\n        logger.warning(\n            \"No layer classes specified for activation checkpointing. \"\n            \"Please specify 'layer_classes' in the config (e.g. ['LlamaBlock']).\"\n        )\n        return model\n\n    target_classes = set(self.cfg.layer_classes)\n    ops_to_save = [\n        eval(op, {\"__builtins__\": None}, {\"torch\": torch})\n        for op in (self.cfg.ops_to_save or [])\n    ]\n    replaced_count = self._replace_modules(\n        model,\n        target_classes,\n        use_reentrant=self.cfg.use_reentrant,\n        ops_to_save=ops_to_save,\n    )\n\n    if replaced_count == 0:\n        logger.warning(f\"No modules matching {target_classes} found to checkpoint.\")\n    else:\n        logger.info(\n            f\"Applied activation checkpointing to {replaced_count} layers of types: {target_classes}\"\n        )\n\n    return model\n</code></pre>"},{"location":"reference/modules/model_transforms/checkpoint/#optimus_dl.modules.model_transforms.checkpoint.CheckpointWrapper","title":"<code>CheckpointWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>Module wrapper that applies activation checkpointing to its child.</p> <p>During the forward pass, this module uses <code>torch.utils.checkpoint.checkpoint</code> to trade compute for memory: activations are discarded after the forward pass and recomputed during the backward pass.</p> Source code in <code>optimus_dl/modules/model_transforms/checkpoint.py</code> <pre><code>class CheckpointWrapper(nn.Module):\n    \"\"\"Module wrapper that applies activation checkpointing to its child.\n\n    During the forward pass, this module uses `torch.utils.checkpoint.checkpoint`\n    to trade compute for memory: activations are discarded after the forward\n    pass and recomputed during the backward pass.\n    \"\"\"\n\n    def __init__(\n        self, module: nn.Module, ops_to_save: list, use_reentrant: bool = False\n    ):\n        super().__init__()\n        self.module = module\n        self.use_reentrant = use_reentrant\n\n        def policy_fn(_, op, *__, **___):\n            if op in ops_to_save:\n                return CheckpointPolicy.MUST_SAVE\n            else:\n                return CheckpointPolicy.PREFER_RECOMPUTE\n\n        self.policy_fn = policy_fn\n\n    def forward(self, *args, **kwargs):\n        \"\"\"Forward pass with activation checkpointing.\"\"\"\n        # torch.utils.checkpoint.checkpoint requires a function as the first argument.\n        # We pass self.module (which is callable).\n        # Note: 'use_reentrant' argument is available in modern PyTorch.\n        return checkpoint(\n            self.module,\n            *args,\n            use_reentrant=self.use_reentrant,\n            context_fn=functools.partial(\n                create_selective_checkpoint_contexts, self.policy_fn\n            ),\n            **kwargs,\n        )\n</code></pre>"},{"location":"reference/modules/model_transforms/checkpoint/#optimus_dl.modules.model_transforms.checkpoint.CheckpointWrapper.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Forward pass with activation checkpointing.</p> Source code in <code>optimus_dl/modules/model_transforms/checkpoint.py</code> <pre><code>def forward(self, *args, **kwargs):\n    \"\"\"Forward pass with activation checkpointing.\"\"\"\n    # torch.utils.checkpoint.checkpoint requires a function as the first argument.\n    # We pass self.module (which is callable).\n    # Note: 'use_reentrant' argument is available in modern PyTorch.\n    return checkpoint(\n        self.module,\n        *args,\n        use_reentrant=self.use_reentrant,\n        context_fn=functools.partial(\n            create_selective_checkpoint_contexts, self.policy_fn\n        ),\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/modules/model_transforms/compile/","title":"compile","text":""},{"location":"reference/modules/model_transforms/compile/#optimus_dl.modules.model_transforms.compile","title":"<code>optimus_dl.modules.model_transforms.compile</code>","text":""},{"location":"reference/modules/model_transforms/compile/#optimus_dl.modules.model_transforms.compile.CompileTransform","title":"<code>CompileTransform</code>","text":"<p>               Bases: <code>BaseModelTransform</code></p> <p>Model transform that applies torch.compile to the entire model.</p> <p>Graph compilation can significantly improve performance by fusing kernels and reducing CPU overhead. It should typically be the last transform applied.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>Any</code> <p>Compilation configuration.</p> <code>None</code> Source code in <code>optimus_dl/modules/model_transforms/compile.py</code> <pre><code>@register_model_transform(\"compile\", CompileTransformConfig)\nclass CompileTransform(BaseModelTransform):\n    \"\"\"Model transform that applies torch.compile to the entire model.\n\n    Graph compilation can significantly improve performance by fusing kernels\n    and reducing CPU overhead. It should typically be the last transform\n    applied.\n\n    Args:\n        cfg: Compilation configuration.\n    \"\"\"\n\n    def apply(self, model: BaseModel, **kwargs) -&gt; BaseModel:\n        \"\"\"Apply torch.compile to the model.\n\n        Args:\n            model: The model to compile.\n            **kwargs: Unused.\n\n        Returns:\n            The compiled model wrapper.\n        \"\"\"\n        import torch._functorch.config\n\n        compile_kwargs = self.cfg.compile_kwargs if self.cfg else {}\n        torch._functorch.config.activation_memory_budget = (\n            self.cfg.activation_memory_budget\n        )\n\n        logger.info(f\"Applying torch.compile with args: {compile_kwargs}\")\n        model = torch.compile(model, **compile_kwargs)\n\n        return model\n</code></pre>"},{"location":"reference/modules/model_transforms/compile/#optimus_dl.modules.model_transforms.compile.CompileTransform.apply","title":"<code>apply(model, **kwargs)</code>","text":"<p>Apply torch.compile to the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel</code> <p>The model to compile.</p> required <code>**kwargs</code> <p>Unused.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseModel</code> <p>The compiled model wrapper.</p> Source code in <code>optimus_dl/modules/model_transforms/compile.py</code> <pre><code>def apply(self, model: BaseModel, **kwargs) -&gt; BaseModel:\n    \"\"\"Apply torch.compile to the model.\n\n    Args:\n        model: The model to compile.\n        **kwargs: Unused.\n\n    Returns:\n        The compiled model wrapper.\n    \"\"\"\n    import torch._functorch.config\n\n    compile_kwargs = self.cfg.compile_kwargs if self.cfg else {}\n    torch._functorch.config.activation_memory_budget = (\n        self.cfg.activation_memory_budget\n    )\n\n    logger.info(f\"Applying torch.compile with args: {compile_kwargs}\")\n    model = torch.compile(model, **compile_kwargs)\n\n    return model\n</code></pre>"},{"location":"reference/modules/model_transforms/compile/#optimus_dl.modules.model_transforms.compile.CompileTransformConfig","title":"<code>CompileTransformConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ModelTransformConfig</code></p> <p>Configuration for torch.compile model transform.</p> <p>Parameters:</p> Name Type Description Default <code>compile_kwargs</code> <code>dict</code> <p>Arguments for torch.compile. See https://pytorch.org/docs/stable/generated/torch.compile.html</p> <code>&lt;class 'dict'&gt;</code> <code>activation_memory_budget</code> <code>float</code> <p>Activation memory budget for torch.compile. See https://pytorch.org/blog/activation-checkpointing-techniques/</p> <code>1.0</code> Source code in <code>optimus_dl/modules/model_transforms/compile.py</code> <pre><code>@dataclass\nclass CompileTransformConfig(ModelTransformConfig):\n    \"\"\"Configuration for torch.compile model transform.\"\"\"\n\n    compile_kwargs: dict = field(\n        default_factory=dict,\n        metadata={\n            \"description\": \"Arguments for torch.compile. See https://pytorch.org/docs/stable/generated/torch.compile.html\"\n        },\n    )\n    activation_memory_budget: float = field(\n        default=1.0,\n        metadata={\n            \"description\": \"Activation memory budget for torch.compile. See https://pytorch.org/blog/activation-checkpointing-techniques/\"\n        },\n    )\n</code></pre>"},{"location":"reference/modules/model_transforms/config/","title":"config","text":""},{"location":"reference/modules/model_transforms/config/#optimus_dl.modules.model_transforms.config","title":"<code>optimus_dl.modules.model_transforms.config</code>","text":""},{"location":"reference/modules/model_transforms/config/#optimus_dl.modules.model_transforms.config.ModelTransformConfig","title":"<code>ModelTransformConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfig</code></p> <p>Base configuration for model transforms.</p> Source code in <code>optimus_dl/modules/model_transforms/config.py</code> <pre><code>@dataclass\nclass ModelTransformConfig(RegistryConfig):\n    \"\"\"Base configuration for model transforms.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/modules/model_transforms/distributed/","title":"distributed","text":""},{"location":"reference/modules/model_transforms/distributed/#optimus_dl.modules.model_transforms.distributed","title":"<code>optimus_dl.modules.model_transforms.distributed</code>","text":"<p>Distributed model transforms for training.</p>"},{"location":"reference/modules/model_transforms/distributed/#optimus_dl.modules.model_transforms.distributed.BaseDistributedTransform","title":"<code>BaseDistributedTransform</code>","text":"<p>               Bases: <code>BaseModelTransform</code></p> <p>Base class for distributed model transforms.</p> <p>Provides common access to the collective and device information.</p> Source code in <code>optimus_dl/modules/model_transforms/distributed.py</code> <pre><code>class BaseDistributedTransform(BaseModelTransform):\n    \"\"\"Base class for distributed model transforms.\n\n    Provides common access to the collective and device information.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: ModelTransformConfig,\n        collective: Collective,\n        device: torch.device,\n        **kwargs: Any,\n    ):\n        super().__init__(cfg, **kwargs)\n        self.collective = collective\n        self.device = device\n</code></pre>"},{"location":"reference/modules/model_transforms/distributed/#optimus_dl.modules.model_transforms.distributed.DDPTransform","title":"<code>DDPTransform</code>","text":"<p>               Bases: <code>BaseDistributedTransform</code></p> <p>Transform that wraps a model with Distributed Data Parallel.</p> <p>DDP replicates the model on each device and synchronizes gradients during the backward pass.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DDPTransformConfig</code> <p>DDP configuration.</p> required <code>collective</code> <code>Collective</code> <p>Distributed collective.</p> required <code>device</code> <code>device</code> <p>Target compute device.</p> required Source code in <code>optimus_dl/modules/model_transforms/distributed.py</code> <pre><code>@register_model_transform(\"ddp\", DDPTransformConfig)\nclass DDPTransform(BaseDistributedTransform):\n    \"\"\"Transform that wraps a model with Distributed Data Parallel.\n\n    DDP replicates the model on each device and synchronizes gradients during\n    the backward pass.\n\n    Args:\n        cfg: DDP configuration.\n        collective: Distributed collective.\n        device: Target compute device.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: DDPTransformConfig,\n        collective: Collective,\n        device: torch.device,\n        **kwargs: Any,\n    ):\n        super().__init__(cfg, collective, device, **kwargs)\n\n        self.collective = collective\n\n    def apply(self, model: BaseModel, **kwargs) -&gt; BaseModel:\n        \"\"\"Apply DDP wrapping to the model.\"\"\"\n        if self.collective.world_size &lt;= 1:\n            logger.info(\"Single rank detected, skipping DDP wrapping\")\n            return model\n\n        logger.info(\"Wrapping model with DDP\")\n\n        # Move model to device\n        model = model.to(self.device)\n\n        # Wrap with DDP\n        ddp_model = DDPWrappedModel(\n            model,\n            process_group=self.collective.process_group,\n            device_ids=(\n                [self.collective.local_rank] if self.device.type == \"cuda\" else None\n            ),\n            find_unused_parameters=self.cfg.find_unused_parameters,\n            gradient_as_bucket_view=self.cfg.gradient_as_bucket_view,\n            static_graph=self.cfg.static_graph,\n        )\n\n        return ddp_model\n</code></pre>"},{"location":"reference/modules/model_transforms/distributed/#optimus_dl.modules.model_transforms.distributed.DDPTransform.apply","title":"<code>apply(model, **kwargs)</code>","text":"<p>Apply DDP wrapping to the model.</p> Source code in <code>optimus_dl/modules/model_transforms/distributed.py</code> <pre><code>def apply(self, model: BaseModel, **kwargs) -&gt; BaseModel:\n    \"\"\"Apply DDP wrapping to the model.\"\"\"\n    if self.collective.world_size &lt;= 1:\n        logger.info(\"Single rank detected, skipping DDP wrapping\")\n        return model\n\n    logger.info(\"Wrapping model with DDP\")\n\n    # Move model to device\n    model = model.to(self.device)\n\n    # Wrap with DDP\n    ddp_model = DDPWrappedModel(\n        model,\n        process_group=self.collective.process_group,\n        device_ids=(\n            [self.collective.local_rank] if self.device.type == \"cuda\" else None\n        ),\n        find_unused_parameters=self.cfg.find_unused_parameters,\n        gradient_as_bucket_view=self.cfg.gradient_as_bucket_view,\n        static_graph=self.cfg.static_graph,\n    )\n\n    return ddp_model\n</code></pre>"},{"location":"reference/modules/model_transforms/distributed/#optimus_dl.modules.model_transforms.distributed.DDPTransformConfig","title":"<code>DDPTransformConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ModelTransformConfig</code></p> <p>Configuration for Distributed Data Parallel (DDP).</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>find_unused_parameters</code> <code>bool</code> <code>False</code> <code>gradient_as_bucket_view</code> <code>bool</code> <code>True</code> <code>static_graph</code> <code>bool</code> <code>False</code> Source code in <code>optimus_dl/modules/model_transforms/distributed.py</code> <pre><code>@dataclass\nclass DDPTransformConfig(ModelTransformConfig):\n    \"\"\"Configuration for Distributed Data Parallel (DDP).\n\n    Attributes:\n        find_unused_parameters: Whether to traverse the graph to find unused\n            parameters during backward.\n        gradient_as_bucket_view: If True, uses views for gradient buckets to\n            save memory.\n        static_graph: Whether the computation graph is static across iterations.\n    \"\"\"\n\n    find_unused_parameters: bool = False\n    gradient_as_bucket_view: bool = True\n    static_graph: bool = False\n</code></pre>"},{"location":"reference/modules/model_transforms/distributed/#optimus_dl.modules.model_transforms.distributed.DDPWrappedModel","title":"<code>DDPWrappedModel</code>","text":"<p>               Bases: <code>DistributedDataParallel</code>, <code>BaseModel</code></p> <p>A wrapper for DDP that implements the BaseModel interface.</p> Source code in <code>optimus_dl/modules/model_transforms/distributed.py</code> <pre><code>class DDPWrappedModel(DDP, BaseModel):\n    \"\"\"A wrapper for DDP that implements the BaseModel interface.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def make_parameter_groups(self):\n        \"\"\"Delegate parameter grouping to the inner module.\"\"\"\n        return self.module.make_parameter_groups()\n\n    def fully_shard(self, **fsdp_kwargs):\n        \"\"\"Delegate sharding to the inner module.\"\"\"\n        return self.module.fully_shard(**fsdp_kwargs)\n\n    def accumulation_context(self, is_last_microbatch):\n        \"\"\"Context manager for gradient accumulation (disables synchronization).\"\"\"\n        return nullcontext() if is_last_microbatch else self.no_sync()\n</code></pre>"},{"location":"reference/modules/model_transforms/distributed/#optimus_dl.modules.model_transforms.distributed.DDPWrappedModel.accumulation_context","title":"<code>accumulation_context(is_last_microbatch)</code>","text":"<p>Context manager for gradient accumulation (disables synchronization).</p> Source code in <code>optimus_dl/modules/model_transforms/distributed.py</code> <pre><code>def accumulation_context(self, is_last_microbatch):\n    \"\"\"Context manager for gradient accumulation (disables synchronization).\"\"\"\n    return nullcontext() if is_last_microbatch else self.no_sync()\n</code></pre>"},{"location":"reference/modules/model_transforms/distributed/#optimus_dl.modules.model_transforms.distributed.DDPWrappedModel.fully_shard","title":"<code>fully_shard(**fsdp_kwargs)</code>","text":"<p>Delegate sharding to the inner module.</p> Source code in <code>optimus_dl/modules/model_transforms/distributed.py</code> <pre><code>def fully_shard(self, **fsdp_kwargs):\n    \"\"\"Delegate sharding to the inner module.\"\"\"\n    return self.module.fully_shard(**fsdp_kwargs)\n</code></pre>"},{"location":"reference/modules/model_transforms/distributed/#optimus_dl.modules.model_transforms.distributed.DDPWrappedModel.make_parameter_groups","title":"<code>make_parameter_groups()</code>","text":"<p>Delegate parameter grouping to the inner module.</p> Source code in <code>optimus_dl/modules/model_transforms/distributed.py</code> <pre><code>def make_parameter_groups(self):\n    \"\"\"Delegate parameter grouping to the inner module.\"\"\"\n    return self.module.make_parameter_groups()\n</code></pre>"},{"location":"reference/modules/model_transforms/distributed/#optimus_dl.modules.model_transforms.distributed.FullyShardTransform","title":"<code>FullyShardTransform</code>","text":"<p>               Bases: <code>BaseDistributedTransform</code></p> <p>Transform that wraps a model with FSDP2 (Fully Sharded Data Parallel).</p> <p>FSDP2 shards model parameters, gradients, and optimizer states across ranks, enabling the training of models much larger than the memory of a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>FullyShardTransformConfig</code> <p>FSDP2 configuration.</p> required <code>collective</code> <code>Collective</code> <p>Distributed collective (MeshCollective required).</p> required <code>device</code> <code>device</code> <p>Target compute device.</p> required Source code in <code>optimus_dl/modules/model_transforms/distributed.py</code> <pre><code>@register_model_transform(\"fully_shard\", FullyShardTransformConfig)\nclass FullyShardTransform(BaseDistributedTransform):\n    \"\"\"Transform that wraps a model with FSDP2 (Fully Sharded Data Parallel).\n\n    FSDP2 shards model parameters, gradients, and optimizer states across ranks,\n    enabling the training of models much larger than the memory of a single GPU.\n\n    Args:\n        cfg: FSDP2 configuration.\n        collective: Distributed collective (MeshCollective required).\n        device: Target compute device.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: FullyShardTransformConfig,\n        collective: Collective,\n        device: torch.device,\n        **kwargs: Any,\n    ):\n        super().__init__(cfg, collective, device, **kwargs)\n        self.mesh = None\n        if self.collective.world_size &gt; 1:\n            self.mesh = self._create_hybrid_mesh()\n\n    def apply(self, model: BaseModel, **kwargs) -&gt; BaseModel:\n        \"\"\"Apply FSDP2 sharding to the model.\"\"\"\n        if self.collective.world_size &lt;= 1:\n            logger.info(\"Single rank detected, skipping FSDP2 wrapping\")\n            return model\n\n        logger.info(\"Wrapping model with FSDP2 (fully_shard)\")\n\n        # Move model to device\n        model = model.to(self.device)\n\n        # Configure FSDP2 options\n        fsdp_kwargs = {}\n\n        # Add mesh if available\n        if self.mesh is not None:\n            fsdp_kwargs[\"mesh\"] = self.mesh\n\n        # Set reshard_after_forward\n        fsdp_kwargs[\"reshard_after_forward\"] = self.cfg.reshard_after_forward\n\n        # Configure mixed precision policy\n        if self.cfg.mixed_precision is not None:\n            mp_config = self.cfg.mixed_precision\n\n            # Convert string dtype names to torch dtypes\n            param_dtype = (\n                self._str_to_dtype(mp_config.param_dtype)\n                if mp_config.param_dtype\n                else None\n            )\n            reduce_dtype = (\n                self._str_to_dtype(mp_config.reduce_dtype)\n                if mp_config.reduce_dtype\n                else None\n            )\n            output_dtype = (\n                self._str_to_dtype(mp_config.output_dtype)\n                if mp_config.output_dtype\n                else None\n            )\n\n            mp_policy = MixedPrecisionPolicy(\n                param_dtype=param_dtype,\n                reduce_dtype=reduce_dtype,\n                output_dtype=output_dtype,\n                cast_forward_inputs=mp_config.cast_forward_inputs,\n            )\n            fsdp_kwargs[\"mp_policy\"] = mp_policy\n            logger.info(\n                f\"Configured mixed precision: param={param_dtype}, reduce={reduce_dtype}, output={output_dtype}\"\n            )\n\n        # Configure offloading policy\n        if self.cfg.offload is not None and self.cfg.offload.cpu_offload:\n            offload_policy = CPUOffloadPolicy(pin_memory=self.cfg.offload.pin_memory)\n            fsdp_kwargs[\"offload_policy\"] = offload_policy\n            logger.info(\n                f\"Configured CPU offloading with pin_memory={self.cfg.offload.pin_memory}\"\n            )\n\n        # Apply fully_shard to the model\n        model.fully_shard(**fsdp_kwargs)\n        fsdp_model = fully_shard(model, **fsdp_kwargs)\n\n        @contextmanager\n        def accumulation_context(is_last_microbatch):\n            \"\"\"Context manager for FSDP gradient accumulation.\"\"\"\n            if self.cfg.sync_grad_accum:\n                fsdp_model.set_requires_gradient_sync(True)\n                yield\n                return\n\n            if is_last_microbatch:\n                fsdp_model.set_requires_gradient_sync(True)\n            else:\n                fsdp_model.set_requires_gradient_sync(False)\n            yield\n            fsdp_model.set_requires_gradient_sync(True)\n\n        # The return type will be the FSDP-wrapped model\n        fsdp_model.accumulation_context = accumulation_context\n        return fsdp_model\n\n    def _str_to_dtype(self, dtype_str: str) -&gt; torch.dtype:\n        \"\"\"Convert string dtype name to torch.dtype.\"\"\"\n        dtype_map = {\n            \"float32\": torch.float32,\n            \"float16\": torch.float16,\n            \"bfloat16\": torch.bfloat16,\n            \"float64\": torch.float64,\n            \"double\": torch.float64,\n            \"half\": torch.float16,\n            \"fp32\": torch.float32,\n            \"fp16\": torch.float16,\n            \"bf16\": torch.bfloat16,\n        }\n\n        if dtype_str not in dtype_map:\n            raise ValueError(\n                f\"Unsupported dtype: {dtype_str}. Supported dtypes: {list(dtype_map.keys())}\"\n            )\n\n        return dtype_map[dtype_str]\n\n    def _create_hybrid_mesh(self):\n        \"\"\"Create a hybrid sharding mesh (HSDP) from the collective's DP mesh.\"\"\"\n        if not isinstance(self.collective, MeshCollective):\n            raise ValueError(\"Hybrid sharding requires MeshCollective\")\n\n        mesh = self.collective.dp_mesh\n        if not self.cfg.use_hybrid_sharding:\n            mesh = mesh._flatten()\n        return mesh\n</code></pre>"},{"location":"reference/modules/model_transforms/distributed/#optimus_dl.modules.model_transforms.distributed.FullyShardTransform.apply","title":"<code>apply(model, **kwargs)</code>","text":"<p>Apply FSDP2 sharding to the model.</p> Source code in <code>optimus_dl/modules/model_transforms/distributed.py</code> <pre><code>def apply(self, model: BaseModel, **kwargs) -&gt; BaseModel:\n    \"\"\"Apply FSDP2 sharding to the model.\"\"\"\n    if self.collective.world_size &lt;= 1:\n        logger.info(\"Single rank detected, skipping FSDP2 wrapping\")\n        return model\n\n    logger.info(\"Wrapping model with FSDP2 (fully_shard)\")\n\n    # Move model to device\n    model = model.to(self.device)\n\n    # Configure FSDP2 options\n    fsdp_kwargs = {}\n\n    # Add mesh if available\n    if self.mesh is not None:\n        fsdp_kwargs[\"mesh\"] = self.mesh\n\n    # Set reshard_after_forward\n    fsdp_kwargs[\"reshard_after_forward\"] = self.cfg.reshard_after_forward\n\n    # Configure mixed precision policy\n    if self.cfg.mixed_precision is not None:\n        mp_config = self.cfg.mixed_precision\n\n        # Convert string dtype names to torch dtypes\n        param_dtype = (\n            self._str_to_dtype(mp_config.param_dtype)\n            if mp_config.param_dtype\n            else None\n        )\n        reduce_dtype = (\n            self._str_to_dtype(mp_config.reduce_dtype)\n            if mp_config.reduce_dtype\n            else None\n        )\n        output_dtype = (\n            self._str_to_dtype(mp_config.output_dtype)\n            if mp_config.output_dtype\n            else None\n        )\n\n        mp_policy = MixedPrecisionPolicy(\n            param_dtype=param_dtype,\n            reduce_dtype=reduce_dtype,\n            output_dtype=output_dtype,\n            cast_forward_inputs=mp_config.cast_forward_inputs,\n        )\n        fsdp_kwargs[\"mp_policy\"] = mp_policy\n        logger.info(\n            f\"Configured mixed precision: param={param_dtype}, reduce={reduce_dtype}, output={output_dtype}\"\n        )\n\n    # Configure offloading policy\n    if self.cfg.offload is not None and self.cfg.offload.cpu_offload:\n        offload_policy = CPUOffloadPolicy(pin_memory=self.cfg.offload.pin_memory)\n        fsdp_kwargs[\"offload_policy\"] = offload_policy\n        logger.info(\n            f\"Configured CPU offloading with pin_memory={self.cfg.offload.pin_memory}\"\n        )\n\n    # Apply fully_shard to the model\n    model.fully_shard(**fsdp_kwargs)\n    fsdp_model = fully_shard(model, **fsdp_kwargs)\n\n    @contextmanager\n    def accumulation_context(is_last_microbatch):\n        \"\"\"Context manager for FSDP gradient accumulation.\"\"\"\n        if self.cfg.sync_grad_accum:\n            fsdp_model.set_requires_gradient_sync(True)\n            yield\n            return\n\n        if is_last_microbatch:\n            fsdp_model.set_requires_gradient_sync(True)\n        else:\n            fsdp_model.set_requires_gradient_sync(False)\n        yield\n        fsdp_model.set_requires_gradient_sync(True)\n\n    # The return type will be the FSDP-wrapped model\n    fsdp_model.accumulation_context = accumulation_context\n    return fsdp_model\n</code></pre>"},{"location":"reference/modules/model_transforms/distributed/#optimus_dl.modules.model_transforms.distributed.FullyShardTransformConfig","title":"<code>FullyShardTransformConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ModelTransformConfig</code></p> <p>Configuration for FSDP2 (fully_shard) transform.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>reshard_after_forward</code> <code>bool | int</code> <code>False</code> <code>mixed_precision</code> <code>MixedPrecisionConfig | None</code> <code>None</code> <code>offload</code> <code>OffloadConfig | None</code> <code>None</code> <code>use_hybrid_sharding</code> <code>bool</code> <code>True</code> <code>sync_grad_accum</code> <code>bool</code> <code>False</code> Source code in <code>optimus_dl/modules/model_transforms/distributed.py</code> <pre><code>@dataclass\nclass FullyShardTransformConfig(ModelTransformConfig):\n    \"\"\"Configuration for FSDP2 (fully_shard) transform.\n\n    Attributes:\n        reshard_after_forward: Whether to discard parameters after forward pass.\n        mixed_precision: Mixed precision policy configuration.\n        offload: CPU offloading policy configuration.\n        use_hybrid_sharding: If True, uses Hybrid Sharding (shard within node,\n            replicate across nodes).\n        sync_grad_accum: If True, always synchronizes gradients during accumulation.\n    \"\"\"\n\n    # Whether to reshard parameters after forward pass\n    reshard_after_forward: bool | int = False\n    # Mixed precision configuration\n    mixed_precision: MixedPrecisionConfig | None = None\n    # Offloading configuration\n    offload: OffloadConfig | None = None\n    # Whether to use hybrid sharding (HSDP): shard within nodes, replicate across nodes\n    use_hybrid_sharding: bool = True\n\n    sync_grad_accum: bool = False\n</code></pre>"},{"location":"reference/modules/model_transforms/distributed/#optimus_dl.modules.model_transforms.distributed.MixedPrecisionConfig","title":"<code>MixedPrecisionConfig</code>  <code>dataclass</code>","text":"<p>Configuration for FSDP mixed precision policy.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>param_dtype</code> <code>str | None</code> <code>None</code> <code>reduce_dtype</code> <code>str | None</code> <code>None</code> <code>output_dtype</code> <code>str | None</code> <code>None</code> <code>cast_forward_inputs</code> <code>bool</code> <code>True</code> Source code in <code>optimus_dl/modules/model_transforms/distributed.py</code> <pre><code>@dataclass\nclass MixedPrecisionConfig:\n    \"\"\"Configuration for FSDP mixed precision policy.\n\n    Attributes:\n        param_dtype: Datatype for parameter storage (e.g., 'bfloat16').\n        reduce_dtype: Datatype for gradient reduction (e.g., 'float32').\n        output_dtype: Datatype for forward pass outputs.\n        cast_forward_inputs: If True, automatically casts inputs to param_dtype.\n    \"\"\"\n\n    # Parameter storage dtype (e.g., float16, bfloat16, float32)\n    param_dtype: str | None = None\n    # Gradient reduction dtype (e.g., float16, bfloat16, float32)\n    reduce_dtype: str | None = None\n    # Output dtype for forward pass (e.g., float16, bfloat16, float32)\n    output_dtype: str | None = None\n    # Whether to cast forward inputs to the specified dtype\n    cast_forward_inputs: bool = True\n</code></pre>"},{"location":"reference/modules/model_transforms/distributed/#optimus_dl.modules.model_transforms.distributed.OffloadConfig","title":"<code>OffloadConfig</code>  <code>dataclass</code>","text":"<p>Configuration for FSDP offloading policy.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>cpu_offload</code> <code>bool</code> <code>False</code> <code>pin_memory</code> <code>bool</code> <code>True</code> Source code in <code>optimus_dl/modules/model_transforms/distributed.py</code> <pre><code>@dataclass\nclass OffloadConfig:\n    \"\"\"Configuration for FSDP offloading policy.\n\n    Attributes:\n        cpu_offload: If True, offloads parameters to CPU memory.\n        pin_memory: If True, pins CPU memory for faster transfers.\n    \"\"\"\n\n    # Whether to enable CPU offloading\n    cpu_offload: bool = False\n    # Whether to pin memory for CPU offloaded parameters (only relevant if cpu_offload=True)\n    pin_memory: bool = True\n</code></pre>"},{"location":"reference/modules/model_transforms/load_weights/","title":"load_weights","text":""},{"location":"reference/modules/model_transforms/load_weights/#optimus_dl.modules.model_transforms.load_weights","title":"<code>optimus_dl.modules.model_transforms.load_weights</code>","text":""},{"location":"reference/modules/model_transforms/load_weights/#optimus_dl.modules.model_transforms.load_weights.LoadWeightsTransform","title":"<code>LoadWeightsTransform</code>","text":"<p>               Bases: <code>BaseModelTransform</code></p> Source code in <code>optimus_dl/modules/model_transforms/load_weights.py</code> <pre><code>@register_model_transform(\"load_weights\", LoadWeightsTransformConfig)\nclass LoadWeightsTransform(BaseModelTransform):\n    def apply(\n        self,\n        model: BaseModel,\n        is_restart: bool,\n        checkpoint_manager: CheckpointManager,\n        **kwargs,\n    ) -&gt; BaseModel:\n        \"\"\"Load weights from a specified checkpoint path into the model.\n\n        Args:\n            model: The model to load weights into.\n            is_restart: A boolean indicating if the current run is a restart.\n            checkpoint_manager: The checkpoint manager instance.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            The model with loaded weights.\n        \"\"\"\n        if self.cfg.skip_on_restart and is_restart:\n            logger.info(\n                \"Skipping 'load_weights' transform because it's a restart and \"\n                \"'skip_on_restart' is True.\"\n            )\n            return model\n\n        if self.cfg.checkpoint_path is None:\n            logger.warning(\n                \"No 'checkpoint_path' specified for 'load_weights' transform. Skipping.\"\n            )\n            return model\n\n        logger.info(f\"Loading weights from: {self.cfg.checkpoint_path}\")\n\n        checkpoint_manager.load_model_state_dict(model, self.cfg.checkpoint_path)\n\n        logger.info(\"Successfully loaded weights.\")\n        return model\n</code></pre>"},{"location":"reference/modules/model_transforms/load_weights/#optimus_dl.modules.model_transforms.load_weights.LoadWeightsTransform.apply","title":"<code>apply(model, is_restart, checkpoint_manager, **kwargs)</code>","text":"<p>Load weights from a specified checkpoint path into the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel</code> <p>The model to load weights into.</p> required <code>is_restart</code> <code>bool</code> <p>A boolean indicating if the current run is a restart.</p> required <code>checkpoint_manager</code> <code>CheckpointManager</code> <p>The checkpoint manager instance.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseModel</code> <p>The model with loaded weights.</p> Source code in <code>optimus_dl/modules/model_transforms/load_weights.py</code> <pre><code>def apply(\n    self,\n    model: BaseModel,\n    is_restart: bool,\n    checkpoint_manager: CheckpointManager,\n    **kwargs,\n) -&gt; BaseModel:\n    \"\"\"Load weights from a specified checkpoint path into the model.\n\n    Args:\n        model: The model to load weights into.\n        is_restart: A boolean indicating if the current run is a restart.\n        checkpoint_manager: The checkpoint manager instance.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        The model with loaded weights.\n    \"\"\"\n    if self.cfg.skip_on_restart and is_restart:\n        logger.info(\n            \"Skipping 'load_weights' transform because it's a restart and \"\n            \"'skip_on_restart' is True.\"\n        )\n        return model\n\n    if self.cfg.checkpoint_path is None:\n        logger.warning(\n            \"No 'checkpoint_path' specified for 'load_weights' transform. Skipping.\"\n        )\n        return model\n\n    logger.info(f\"Loading weights from: {self.cfg.checkpoint_path}\")\n\n    checkpoint_manager.load_model_state_dict(model, self.cfg.checkpoint_path)\n\n    logger.info(\"Successfully loaded weights.\")\n    return model\n</code></pre>"},{"location":"reference/modules/model_transforms/load_weights/#optimus_dl.modules.model_transforms.load_weights.LoadWeightsTransformConfig","title":"<code>LoadWeightsTransformConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ModelTransformConfig</code></p> <p>Configuration for load weights model transform.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>str</code> <p>Path to checkpoint to load weights from.</p> <code>'???'</code> <code>skip_on_restart</code> <code>bool</code> <p>Skip loading weights if this run is a restart.</p> <code>True</code> Source code in <code>optimus_dl/modules/model_transforms/load_weights.py</code> <pre><code>@dataclass\nclass LoadWeightsTransformConfig(ModelTransformConfig):\n    \"\"\"\n    Configuration for load weights model transform.\n    \"\"\"\n\n    checkpoint_path: str = field(\n        default=MISSING,\n        metadata={\n            \"description\": \"Path to checkpoint to load weights from.\",\n        },\n    )\n\n    skip_on_restart: bool = field(\n        default=True,\n        metadata={\n            \"description\": \"Skip loading weights if this run is a restart.\",\n        },\n    )\n</code></pre>"},{"location":"reference/modules/model_transforms/tensor_parallel/","title":"tensor_parallel","text":""},{"location":"reference/modules/model_transforms/tensor_parallel/#optimus_dl.modules.model_transforms.tensor_parallel","title":"<code>optimus_dl.modules.model_transforms.tensor_parallel</code>","text":"<p>Tensor Parallelism Transform.</p>"},{"location":"reference/modules/model_transforms/tensor_parallel/#optimus_dl.modules.model_transforms.tensor_parallel.TensorParallelConfig","title":"<code>TensorParallelConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ModelTransformConfig</code></p> <p>Configuration for Tensor Parallelism.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>custom_model_kwargs</code> <code>dict</code> <p>dict() -&gt; new empty dictionary dict(mapping) -&gt; new dictionary initialized from a mapping object's     (key, value) pairs dict(iterable) -&gt; new dictionary initialized as if via:     d = {}     for k, v in iterable:         d[k] = v dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs     in the keyword argument list.  For example:  dict(one=1, two=2)</p> <code>&lt;class 'dict'&gt;</code> Source code in <code>optimus_dl/modules/model_transforms/tensor_parallel.py</code> <pre><code>@dataclass\nclass TensorParallelConfig(ModelTransformConfig):\n    \"\"\"Configuration for Tensor Parallelism.\n\n    Attributes:\n        custom_model_kwargs: Additional keyword arguments passed to the model's\n            `apply_tp` method (e.g., sequence_parallel=True).\n    \"\"\"\n\n    custom_model_kwargs: dict = field(default_factory=dict)\n</code></pre>"},{"location":"reference/modules/model_transforms/tensor_parallel/#optimus_dl.modules.model_transforms.tensor_parallel.TensorParallelTransform","title":"<code>TensorParallelTransform</code>","text":"<p>               Bases: <code>BaseModelTransform</code></p> <p>Transform that applies Tensor Parallelism to a model.</p> <p>This transform delegates the actual sharding logic to the model's <code>apply_tp</code> method, providing it with the appropriate Tensor Parallel device mesh from  the global collective.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>TensorParallelConfig</code> <p>Tensor parallel configuration.</p> required <code>collective</code> <code>Collective</code> <p>Distributed collective (MeshCollective required).</p> required Source code in <code>optimus_dl/modules/model_transforms/tensor_parallel.py</code> <pre><code>@register_model_transform(\"tensor_parallel\", TensorParallelConfig)\nclass TensorParallelTransform(BaseModelTransform):\n    \"\"\"Transform that applies Tensor Parallelism to a model.\n\n    This transform delegates the actual sharding logic to the model's `apply_tp`\n    method, providing it with the appropriate Tensor Parallel device mesh from\n     the global collective.\n\n    Args:\n        cfg: Tensor parallel configuration.\n        collective: Distributed collective (MeshCollective required).\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: TensorParallelConfig,\n        collective: Collective,\n        **kwargs: Any,\n    ):\n        super().__init__(cfg, **kwargs)\n        self.collective = collective\n\n    def apply(self, model: BaseModel, **kwargs) -&gt; BaseModel:\n        \"\"\"Apply the tensor parallel plan to the model.\"\"\"\n        if not isinstance(self.collective, MeshCollective):\n            logger.warning(\"TensorParallel requires MeshCollective. Skipping.\")\n            return model\n\n        tp_mesh = self.collective.tp_mesh\n        if tp_mesh is None:\n            logger.info(\"No TP mesh found (tp_size=1). Skipping Tensor Parallelism.\")\n            return model\n\n        logger.info(f\"Applying Tensor Parallelism with mesh: {tp_mesh}\")\n\n        # Get the parallelization plan from the model\n        model.apply_tp(tp_mesh, **self.cfg.custom_model_kwargs)\n\n        logger.info(\"Tensor Parallelism applied successfully.\")\n        return model\n</code></pre>"},{"location":"reference/modules/model_transforms/tensor_parallel/#optimus_dl.modules.model_transforms.tensor_parallel.TensorParallelTransform.apply","title":"<code>apply(model, **kwargs)</code>","text":"<p>Apply the tensor parallel plan to the model.</p> Source code in <code>optimus_dl/modules/model_transforms/tensor_parallel.py</code> <pre><code>def apply(self, model: BaseModel, **kwargs) -&gt; BaseModel:\n    \"\"\"Apply the tensor parallel plan to the model.\"\"\"\n    if not isinstance(self.collective, MeshCollective):\n        logger.warning(\"TensorParallel requires MeshCollective. Skipping.\")\n        return model\n\n    tp_mesh = self.collective.tp_mesh\n    if tp_mesh is None:\n        logger.info(\"No TP mesh found (tp_size=1). Skipping Tensor Parallelism.\")\n        return model\n\n    logger.info(f\"Applying Tensor Parallelism with mesh: {tp_mesh}\")\n\n    # Get the parallelization plan from the model\n    model.apply_tp(tp_mesh, **self.cfg.custom_model_kwargs)\n\n    logger.info(\"Tensor Parallelism applied successfully.\")\n    return model\n</code></pre>"},{"location":"reference/modules/optim/","title":"Index","text":""},{"location":"reference/modules/optim/#optimus_dl.modules.optim","title":"<code>optimus_dl.modules.optim</code>","text":""},{"location":"reference/modules/optim/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>adamw</code>: AdamW optimizer</li> <li><code>config</code>: General optimizer config</li> <li><code>muon</code>: Muon optimizer</li> <li><code>soap</code>: SOAP optimizer</li> </ul>"},{"location":"reference/modules/optim/adamw/","title":"adamw","text":""},{"location":"reference/modules/optim/adamw/#optimus_dl.modules.optim.adamw","title":"<code>optimus_dl.modules.optim.adamw</code>","text":"<p>AdamW optimizer</p>"},{"location":"reference/modules/optim/adamw/#optimus_dl.modules.optim.adamw.AdamWConfig","title":"<code>AdamWConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for AdamW optimizer.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <code>0.001</code> <code>betas</code> <code>tuple[float, float]</code> <code>(0.9, 0.999)</code> <code>eps</code> <code>float</code> <code>1e-08</code> <code>weight_decay</code> <code>float</code> <code>0.01</code> <code>amsgrad</code> <code>bool</code> <code>False</code> <code>maximize</code> <code>bool</code> <code>False</code> <code>foreach</code> <code>bool | None</code> <code>None</code> <code>capturable</code> <code>bool</code> <code>False</code> <code>differentiable</code> <code>bool</code> <code>False</code> <code>fused</code> <code>bool</code> <code>True</code> Source code in <code>optimus_dl/modules/optim/adamw.py</code> <pre><code>@dataclass\nclass AdamWConfig(RegistryConfigStrict):\n    \"\"\"Configuration for AdamW optimizer.\n\n    Attributes:\n        lr: Learning rate.\n        betas: Coefficients for computing running averages of gradient and its square.\n        eps: Term added to denominator for numerical stability.\n        weight_decay: Weight decay (L2 penalty) coefficient.\n        amsgrad: Whether to use the AMSGrad variant.\n        maximize: Whether to maximize the objective or minimize it.\n        foreach: Whether to use the faster 'foreach' implementation.\n        capturable: Whether this instance is safe to capture in a CUDA graph.\n        differentiable: Whether autograd should occur through the optimizer step.\n        fused: Whether to use the fused kernel implementation (recommended for GPU).\n    \"\"\"\n\n    lr: float = 1e-3\n    betas: tuple[float, float] = (0.9, 0.999)\n    eps: float = 1e-8\n    weight_decay: float = 1e-2\n    amsgrad: bool = False\n    maximize: bool = False\n    foreach: bool | None = None\n    capturable: bool = False\n    differentiable: bool = False\n    fused: bool = True\n</code></pre>"},{"location":"reference/modules/optim/adamw/#optimus_dl.modules.optim.adamw.make_adamw","title":"<code>make_adamw(cfg, params, **_)</code>","text":"<p>Instantiate a PyTorch AdamW optimizer from the given configuration.</p> Source code in <code>optimus_dl/modules/optim/adamw.py</code> <pre><code>@register_optimizer(\"adamw\", AdamWConfig)\ndef make_adamw(cfg, params, **_):\n    \"\"\"Instantiate a PyTorch AdamW optimizer from the given configuration.\"\"\"\n    return torch.optim.AdamW(\n        params=params,\n        lr=cfg.lr,\n        betas=cfg.betas,\n        eps=cfg.eps,\n        weight_decay=cfg.weight_decay,\n        amsgrad=cfg.amsgrad,\n        maximize=cfg.maximize,\n        foreach=cfg.foreach,\n        capturable=cfg.capturable,\n        differentiable=cfg.differentiable,\n        fused=cfg.fused,\n    )\n</code></pre>"},{"location":"reference/modules/optim/config/","title":"config","text":""},{"location":"reference/modules/optim/config/#optimus_dl.modules.optim.config","title":"<code>optimus_dl.modules.optim.config</code>","text":"<p>General optimizer config</p>"},{"location":"reference/modules/optim/config/#optimus_dl.modules.optim.config.AmpConfig","title":"<code>AmpConfig</code>  <code>dataclass</code>","text":"<p>AmpConfig(enabled: bool = False, dtype: str = 'torch.bfloat16', enable_scaler: bool = '\\({eval: \\'\"\\){.dtype}\" == \"torch.float16\"\\'}', init_scale: float = 65536, growth_factor: float = 2.0, backoff_factor: float = 0.5, growth_interval: int = 2000)</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <code>False</code> <code>dtype</code> <code>str</code> <code>'torch.bfloat16'</code> <code>enable_scaler</code> <code>bool</code> <code>'${eval: \\'\"${.dtype}\" == \"torch.float16\"\\'}'</code> <code>init_scale</code> <code>float</code> <code>65536</code> <code>growth_factor</code> <code>float</code> <code>2.0</code> <code>backoff_factor</code> <code>float</code> <code>0.5</code> <code>growth_interval</code> <code>int</code> <code>2000</code> Source code in <code>optimus_dl/modules/optim/config.py</code> <pre><code>@dataclass\nclass AmpConfig:\n    enabled: bool = False\n    dtype: str = \"torch.bfloat16\"\n\n    enable_scaler: bool = '${eval: \\'\"${.dtype}\" == \"torch.float16\"\\'}'\n    init_scale: float = 2**16\n    growth_factor: float = 2.0\n    backoff_factor: float = 0.5\n    growth_interval: int = 2000\n</code></pre>"},{"location":"reference/modules/optim/config/#optimus_dl.modules.optim.config.OptimizationConfig","title":"<code>OptimizationConfig</code>  <code>dataclass</code>","text":"<p>OptimizationConfig(optimizer: optimus_dl.core.registry.RegistryConfig, iterations: int = 1000, acc_steps: int = 1, clip_grad_norm: float | None = None, amp: optimus_dl.modules.optim.config.AmpConfig = ) <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>RegistryConfig</code> required <code>iterations</code> <code>int</code> <p>Total train steps</p> <code>1000</code> <code>acc_steps</code> <code>int</code> <p>Steps to accumulate gradient</p> <code>1</code> <code>clip_grad_norm</code> <code>float | None</code> <p>Clip gradient norm</p> <code>None</code> <code>amp</code> <code>AmpConfig</code> <p>AmpConfig(enabled: bool = False, dtype: str = 'torch.bfloat16', enable_scaler: bool = '\\({eval: \\'\"\\){.dtype}\" == \"torch.float16\"\\'}', init_scale: float = 65536, growth_factor: float = 2.0, backoff_factor: float = 0.5, growth_interval: int = 2000)</p> <code>&lt;dynamic&gt;</code> Source code in <code>optimus_dl/modules/optim/config.py</code> <pre><code>@dataclass\nclass OptimizationConfig:\n    optimizer: RegistryConfig\n\n    iterations: int = field(default=1000, metadata={\"description\": \"Total train steps\"})\n    acc_steps: int = field(\n        default=1, metadata={\"description\": \"Steps to accumulate gradient\"}\n    )\n    clip_grad_norm: float | None = field(\n        default=None, metadata={\"description\": \"Clip gradient norm\"}\n    )\n    amp: AmpConfig = field(default_factory=AmpConfig)\n</code></pre>"},{"location":"reference/modules/optim/muon/","title":"muon","text":""},{"location":"reference/modules/optim/muon/#optimus_dl.modules.optim.muon","title":"<code>optimus_dl.modules.optim.muon</code>","text":"<p>Muon optimizer</p>"},{"location":"reference/modules/optim/muon/#optimus_dl.modules.optim.muon.MuonConfig","title":"<code>MuonConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for Muon optimizer.</p> <p>Muon is a momentum-based optimizer that uses Newton-Schulz iteration for preconditioning. It's designed for efficient training of large models.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <code>0.001</code> <code>weight_decay</code> <code>float</code> <code>0.1</code> <code>momentum</code> <code>float</code> <code>0.95</code> <code>nesterov</code> <code>bool</code> <code>True</code> <code>ns_coefficients</code> <code>tuple[float, float, float]</code> <code>(3.4445, -4.775, 2.0315)</code> <code>eps</code> <code>float</code> <code>1e-07</code> <code>ns_steps</code> <code>int</code> <code>5</code> <code>adjust_lr_fn</code> <code>str | None</code> <code>None</code> Source code in <code>optimus_dl/modules/optim/muon.py</code> <pre><code>@dataclass\nclass MuonConfig(RegistryConfigStrict):\n    \"\"\"Configuration for Muon optimizer.\n\n    Muon is a momentum-based optimizer that uses Newton-Schulz iteration for\n    preconditioning. It's designed for efficient training of large models.\n\n    Attributes:\n        lr: Learning rate for parameter updates.\n        weight_decay: Weight decay (L2 penalty) coefficient applied to parameters.\n        momentum: Momentum factor for the moving average of gradients.\n        nesterov: Whether to use Nesterov momentum.\n        ns_coefficients: Coefficients (a, b, c) for Newton-Schulz iteration algorithm.\n            These control the convergence and stability of the preconditioning step.\n            Default values are tuned for typical use cases.\n        eps: Small constant added for numerical stability in computations.\n        ns_steps: Number of Newton-Schulz iteration steps to perform for preconditioning.\n            Higher values can improve accuracy but increase computational cost.\n        adjust_lr_fn: Optional learning rate adjustment function name. If provided,\n            applies dynamic learning rate scaling during training.\n    \"\"\"\n\n    lr: float = 1e-3\n    weight_decay: float = 0.1\n    momentum: float = 0.95\n    nesterov: bool = True\n    ns_coefficients: tuple[float, float, float] = (DEFAULT_A, DEFAULT_B, DEFAULT_C)\n    eps: float = EPS\n    ns_steps: int = DEFAULT_NS_STEPS\n    adjust_lr_fn: str | None = None\n</code></pre>"},{"location":"reference/modules/optim/soap/","title":"soap","text":""},{"location":"reference/modules/optim/soap/#optimus_dl.modules.optim.soap","title":"<code>optimus_dl.modules.optim.soap</code>","text":"<p>SOAP optimizer</p>"},{"location":"reference/modules/optim/soap/#optimus_dl.modules.optim.soap.SOAP","title":"<code>SOAP</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>Implements SOAP algorithm (https://arxiv.org/abs/2409.11321).</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>`Iterable[nn.parameter.Parameter]`</code> <p>Iterable of parameters to optimize or dictionaries defining parameter groups.</p> required <code>lr</code> <code>`float`, *optional*, defaults to 0.003</code> <p>The learning rate to use.</p> <code>0.003</code> <code>betas</code> <code>`Tuple[float,float]`, *optional*, defaults to `(0.95, 0.95)`</code> <p>Adam's betas parameters (b1, b2).</p> <code>(0.95, 0.95)</code> <code>shampoo_beta</code> <code>`float`, *optional*, defaults to -1</code> <p>If &gt;= 0, use this beta for the preconditioner (L and R in paper, state['GG'] below) moving average instead of betas[1].</p> <code>-1</code> <code>eps</code> <code>`float`, *optional*, defaults to 1e-08</code> <p>Adam's epsilon for numerical stability.</p> <code>1e-08</code> <code>weight_decay</code> <code>`float`, *optional*, defaults to 0.01</code> <p>weight decay coefficient.</p> <code>0.01</code> <code>precondition_frequency</code> <code>`int`, *optional*, defaults to 10</code> <p>How often to update the preconditioner.</p> <code>10</code> <code>max_precond_dim</code> <code>`int`, *optional*, defaults to 10000</code> <p>Maximum dimension of the preconditioner. Set to 10000, so that we exclude most common vocab sizes while including layers.</p> <code>10000</code> <code>merge_dims</code> <code>`bool`, *optional*, defaults to `False`</code> <p>Whether or not to merge dimensions of the preconditioner.</p> <code>False</code> <code>precondition_1d</code> <code>`bool`, *optional*, defaults to `False`</code> <p>Whether or not to precondition 1D gradients.</p> <code>False</code> <code>normalize_grads</code> <code>`bool`, *optional*, defaults to `False`</code> <p>Whether or not to normalize gradients per layer. Helps at large precondition_frequency (~100 in our experiments), but hurts performance at small precondition_frequency (~10 in our experiments).</p> <code>False</code> <code>data_format</code> <code>`str`, *optional*, defaults to `channels_first`</code> <p>Data format of the input for convolutional layers. Should be \"channels_last\" for data_format of NHWC and \"channels_first\" for NCHW.</p> <code>'channels_first'</code> <code>correct_bias</code> <code>`bool`, *optional*, defaults to `True`</code> <p>Whether or not to use bias correction in Adam.</p> <code>True</code> Source code in <code>optimus_dl/modules/optim/soap.py</code> <pre><code>class SOAP(optim.Optimizer):\n    \"\"\"\n    Implements SOAP algorithm (https://arxiv.org/abs/2409.11321).\n\n    Parameters:\n        params (`Iterable[nn.parameter.Parameter]`):\n            Iterable of parameters to optimize or dictionaries defining parameter groups.\n        lr (`float`, *optional*, defaults to 0.003):\n            The learning rate to use.\n        betas (`Tuple[float,float]`, *optional*, defaults to `(0.95, 0.95)`):\n            Adam's betas parameters (b1, b2).\n        shampoo_beta (`float`, *optional*, defaults to -1):\n            If &gt;= 0, use this beta for the preconditioner (L and R in paper, state['GG'] below) moving average instead of betas[1].\n        eps (`float`, *optional*, defaults to 1e-08):\n            Adam's epsilon for numerical stability.\n        weight_decay (`float`, *optional*, defaults to 0.01): weight decay coefficient.\n        precondition_frequency (`int`, *optional*, defaults to 10):\n            How often to update the preconditioner.\n        max_precond_dim (`int`, *optional*, defaults to 10000):\n            Maximum dimension of the preconditioner.\n            Set to 10000, so that we exclude most common vocab sizes while including layers.\n        merge_dims (`bool`, *optional*, defaults to `False`):\n            Whether or not to merge dimensions of the preconditioner.\n        precondition_1d (`bool`, *optional*, defaults to `False`):\n            Whether or not to precondition 1D gradients.\n        normalize_grads (`bool`, *optional*, defaults to `False`):\n            Whether or not to normalize gradients per layer.\n            Helps at large precondition_frequency (~100 in our experiments),\n            but hurts performance at small precondition_frequency (~10 in our experiments).\n        data_format (`str`, *optional*, defaults to `channels_first`):\n            Data format of the input for convolutional layers.\n            Should be \"channels_last\" for data_format of NHWC and \"channels_first\" for NCHW.\n        correct_bias (`bool`, *optional*, defaults to `True`):\n            Whether or not to use bias correction in Adam.\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr: float = 3e-3,\n        betas=(0.95, 0.95),\n        shampoo_beta: float = -1,\n        eps: float = 1e-8,\n        weight_decay: float = 0.01,\n        precondition_frequency: int = 10,\n        max_precond_dim: int = 10000,  #\n        merge_dims: bool = False,  # Merge dimensions till the product of the dimensions is less than or equal to max_precond_dim.\n        precondition_1d: bool = False,\n        normalize_grads: bool = False,\n        data_format: str = \"channels_first\",\n        correct_bias: bool = True,\n    ):\n        defaults = {\n            \"lr\": lr,\n            \"betas\": betas,\n            \"shampoo_beta\": shampoo_beta,\n            \"eps\": eps,\n            \"weight_decay\": weight_decay,\n            \"precondition_frequency\": precondition_frequency,\n            \"max_precond_dim\": max_precond_dim,\n            \"merge_dims\": merge_dims,\n            \"precondition_1d\": precondition_1d,\n            \"normalize_grads\": normalize_grads,\n            \"correct_bias\": correct_bias,\n        }\n        super().__init__(params, defaults)\n        self._data_format = data_format\n\n    def merge_dims(self, grad, max_precond_dim):\n        \"\"\"\n        Merges dimensions of the gradient tensor till the product of the dimensions is less than or equal to max_precond_dim.\n        \"\"\"\n        assert self._data_format in [\"channels_first\", \"channels_last\"]\n        if self._data_format == \"channels_last\" and grad.dim() == 4:\n            grad = grad.permute(0, 3, 1, 2)\n        shape = grad.shape\n        new_shape = []\n\n        curr_shape = 1\n        for sh in shape:\n            temp_shape = curr_shape * sh\n            if temp_shape &gt; max_precond_dim:\n                if curr_shape &gt; 1:\n                    new_shape.append(curr_shape)\n                    curr_shape = sh\n                else:\n                    new_shape.append(sh)\n                    curr_shape = 1\n            else:\n                curr_shape = temp_shape\n\n        if curr_shape &gt; 1 or len(new_shape) == 0:\n            new_shape.append(curr_shape)\n\n        new_grad = grad.reshape(new_shape)\n        return new_grad\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"\n        Performs a single optimization step.\n\n        Arguments:\n            closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.\n        \"\"\"\n        if closure is None:\n            loss = None\n        else:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n\n                state = self.state[p]\n\n                if \"step\" not in state:\n                    state[\"step\"] = 0\n\n                # State initialization\n                if \"exp_avg\" not in state:\n                    # Exponential moving average of gradient values\n                    state[\"exp_avg\"] = torch.zeros_like(grad)\n                    # Exponential moving average of squared gradient values\n                    state[\"exp_avg_sq\"] = torch.zeros_like(grad)\n\n                if \"Q\" not in state:\n                    self.init_preconditioner(\n                        grad,\n                        state,\n                        precondition_frequency=group[\"precondition_frequency\"],\n                        precondition_1d=group[\"precondition_1d\"],\n                        shampoo_beta=(\n                            group[\"shampoo_beta\"]\n                            if group[\"shampoo_beta\"] &gt;= 0\n                            else group[\"betas\"][1]\n                        ),\n                        max_precond_dim=group[\"max_precond_dim\"],\n                        merge_dims=group[\"merge_dims\"],\n                    )\n                    self.update_preconditioner(\n                        grad,\n                        state,\n                        max_precond_dim=group[\"max_precond_dim\"],\n                        merge_dims=group[\"merge_dims\"],\n                        precondition_1d=group[\"precondition_1d\"],\n                    )\n                    continue  # first step is skipped so that we never use the current gradients in the projection.\n\n                # Projecting gradients to the eigenbases of Shampoo's preconditioner\n                # i.e. projecting to the eigenbases of matrices in state['GG']\n                grad_projected = self.project(\n                    grad,\n                    state,\n                    merge_dims=group[\"merge_dims\"],\n                    max_precond_dim=group[\"max_precond_dim\"],\n                )\n\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n                beta1, beta2 = group[\"betas\"]\n\n                state[\"step\"] += 1\n\n                # Decay the first and second moment running average coefficient\n                # In-place operations to update the averages at the same time\n                exp_avg.mul_(beta1).add_(grad_projected, alpha=(1.0 - beta1))\n                exp_avg_sq.mul_(beta2).add_(\n                    grad_projected.square(), alpha=(1.0 - beta2)\n                )\n\n                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n\n                # Projecting the exponential moving average of gradients to the eigenbases of Shampoo's preconditioner\n                # i.e. projecting to the eigenbases of matrices in state['GG']\n                # exp_avg_projected = self.project(exp_avg, state, merge_dims=group[\"merge_dims\"],\n                #                                  max_precond_dim=group['max_precond_dim'])\n                exp_avg_projected = exp_avg\n\n                step_size = group[\"lr\"]\n                if group[\"correct_bias\"]:\n                    bias_correction1 = 1.0 - beta1 ** (state[\"step\"])\n                    bias_correction2 = 1.0 - beta2 ** (state[\"step\"])\n                    step_size = step_size * (bias_correction2**0.5) / bias_correction1\n\n                # Projecting back the preconditioned (by Adam) exponential moving average of gradients\n                # to the original space\n                norm_grad = self.project_back(\n                    exp_avg_projected / denom,\n                    state,\n                    merge_dims=group[\"merge_dims\"],\n                    max_precond_dim=group[\"max_precond_dim\"],\n                )\n\n                if group[\"normalize_grads\"]:\n                    norm_grad = norm_grad / (1e-30 + torch.mean(norm_grad**2) ** 0.5)\n\n                p.add_(norm_grad, alpha=-step_size)\n\n                # From AdamW code: Just adding the square of the weights to the loss function is *not*\n                # the correct way of using L2 regularization/weight decay with Adam,\n                # since that will interact with the m and v parameters in strange ways.\n                #\n                # Instead we want to decay the weights in a manner that doesn't interact\n                # with the m/v parameters. This is equivalent to adding the square\n                # of the weights to the loss with plain (non-momentum) SGD.\n                # Add weight decay at the end (fixed version)\n                if group[\"weight_decay\"] &gt; 0.0:\n                    p.add_(p, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))\n\n                # Update is done after the gradient step to avoid using current gradients in the projection.\n                self.update_preconditioner(\n                    grad,\n                    state,\n                    max_precond_dim=group[\"max_precond_dim\"],\n                    merge_dims=group[\"merge_dims\"],\n                    precondition_1d=group[\"precondition_1d\"],\n                )\n\n        return loss\n\n    def init_preconditioner(\n        self,\n        grad,\n        state,\n        precondition_frequency=10,\n        shampoo_beta=0.95,\n        max_precond_dim=10000,\n        precondition_1d=False,\n        merge_dims=False,\n    ):\n        \"\"\"\n        Initializes the preconditioner matrices (L and R in the paper).\n        \"\"\"\n        state[\"GG\"] = (\n            []\n        )  # Will hold all the preconditioner matrices (L and R in the paper).\n        if grad.dim() == 1:\n            if not precondition_1d or grad.shape[0] &gt; max_precond_dim:\n                state[\"GG\"].append([])\n            else:\n                state[\"GG\"].append(\n                    torch.zeros(grad.shape[0], grad.shape[0], device=grad.device)\n                )\n        else:\n            if merge_dims:\n                grad = self.merge_dims(grad, max_precond_dim)\n\n            for sh in grad.shape:\n                if sh &gt; max_precond_dim:\n                    state[\"GG\"].append([])\n                else:\n                    state[\"GG\"].append(torch.zeros(sh, sh, device=grad.device))\n\n        state[\"Q\"] = None  # Will hold all the eigenbases of the preconditioner.\n        state[\"precondition_frequency\"] = precondition_frequency\n        state[\"shampoo_beta\"] = shampoo_beta\n\n    def project(self, grad, state, merge_dims=False, max_precond_dim=10000):\n        \"\"\"\n        Projects the gradient to the eigenbases of the preconditioner.\n        \"\"\"\n        original_shape = grad.shape\n        if merge_dims:\n            if grad.dim() == 4 and self._data_format == \"channels_last\":\n                permuted_shape = grad.permute(0, 3, 1, 2).shape\n            grad = self.merge_dims(grad, max_precond_dim)\n\n        for mat in state[\"Q\"]:\n            if len(mat) &gt; 0:\n                grad = torch.tensordot(\n                    grad,\n                    mat,\n                    dims=[[0], [0]],\n                )\n            else:\n                permute_order = list(range(1, len(grad.shape))) + [0]\n                grad = grad.permute(permute_order)\n\n        if merge_dims:\n            if self._data_format == \"channels_last\" and len(original_shape) == 4:\n                grad = grad.reshape(permuted_shape).permute(0, 2, 3, 1)\n            else:\n                grad = grad.reshape(original_shape)\n        return grad\n\n    def update_preconditioner(\n        self,\n        grad,\n        state,\n        max_precond_dim=10000,\n        merge_dims=False,\n        precondition_1d=False,\n    ):\n        \"\"\"\n        Updates the preconditioner matrices and the eigenbases (L, R, Q_L, Q_R in the paper).\n        \"\"\"\n        if state[\"Q\"] is not None:\n            state[\"exp_avg\"] = self.project_back(\n                state[\"exp_avg\"],\n                state,\n                merge_dims=merge_dims,\n                max_precond_dim=max_precond_dim,\n            )\n        if grad.dim() == 1:\n            if precondition_1d and grad.shape[0] &lt;= max_precond_dim:\n                state[\"GG\"][0].lerp_(\n                    grad.unsqueeze(1) @ grad.unsqueeze(0), 1 - state[\"shampoo_beta\"]\n                )\n        else:\n            if merge_dims:\n                new_grad = self.merge_dims(grad, max_precond_dim)\n                for idx, sh in enumerate(new_grad.shape):\n                    if sh &lt;= max_precond_dim:\n                        outer_product = torch.tensordot(\n                            new_grad,\n                            new_grad,\n                            dims=[\n                                [\n                                    *chain(\n                                        range(idx), range(idx + 1, len(new_grad.shape))\n                                    )\n                                ]\n                            ]\n                            * 2,\n                        )\n                        state[\"GG\"][idx].lerp_(outer_product, 1 - state[\"shampoo_beta\"])\n            else:\n                for idx, sh in enumerate(grad.shape):\n                    if sh &lt;= max_precond_dim:\n                        outer_product = torch.tensordot(\n                            grad,\n                            grad,\n                            # Contracts across all dimensions except for k.\n                            dims=[[*chain(range(idx), range(idx + 1, len(grad.shape)))]]\n                            * 2,\n                        )\n                        state[\"GG\"][idx].lerp_(outer_product, 1 - state[\"shampoo_beta\"])\n\n        if state[\"Q\"] is None:\n            state[\"Q\"] = self.get_orthogonal_matrix(state[\"GG\"])\n        if state[\"step\"] &gt; 0 and state[\"step\"] % state[\"precondition_frequency\"] == 0:\n            state[\"Q\"] = self.get_orthogonal_matrix_QR(\n                state, max_precond_dim, merge_dims\n            )\n            # state['Q'] = self.get_fast_QR(state, max_precond_dim, merge_dims)\n\n        if state[\"step\"] &gt; 0:\n            state[\"exp_avg\"] = self.project(\n                state[\"exp_avg\"],\n                state,\n                merge_dims=merge_dims,\n                max_precond_dim=max_precond_dim,\n            )\n\n    def project_back(self, grad, state, merge_dims=False, max_precond_dim=10000):\n        \"\"\"\n        Projects the gradient back to the original space.\n        \"\"\"\n        original_shape = grad.shape\n        if merge_dims:\n            if self._data_format == \"channels_last\" and grad.dim() == 4:\n                permuted_shape = grad.permute(0, 3, 1, 2).shape\n            grad = self.merge_dims(grad, max_precond_dim)\n        for mat in state[\"Q\"]:\n            if len(mat) &gt; 0:\n                grad = torch.tensordot(\n                    grad,\n                    mat,\n                    dims=[[0], [1]],\n                )\n            else:\n                permute_order = list(range(1, len(grad.shape))) + [0]\n                grad = grad.permute(permute_order)\n\n        if merge_dims:\n            if self._data_format == \"channels_last\" and len(original_shape) == 4:\n                grad = grad.reshape(permuted_shape).permute(0, 2, 3, 1)\n            else:\n                grad = grad.reshape(original_shape)\n        return grad\n\n    def get_orthogonal_matrix(self, mat):\n        \"\"\"\n        Computes the eigenbases of the preconditioner using torch.linalg.eigh decomposition.\n        \"\"\"\n        matrix = []\n        for m in mat:\n            if len(m) == 0:\n                matrix.append([])\n                continue\n            if m.data.dtype != torch.float:\n                float_data = False\n                original_type = m.data.dtype\n                original_device = m.data.device\n                matrix.append(m.data.float())\n            else:\n                float_data = True\n                matrix.append(m.data)\n\n        final = []\n        for m in matrix:\n            if len(m) == 0:\n                final.append([])\n                continue\n            try:\n                _, Q = torch.linalg.eigh(\n                    m + 1e-30 * torch.eye(m.shape[0], device=m.device)\n                )\n            except:  # noqa: E722\n                _, Q = torch.linalg.eigh(\n                    m.to(torch.float64) + 1e-30 * torch.eye(m.shape[0], device=m.device)\n                )\n                Q = Q.to(m.dtype)\n            Q = torch.flip(Q, [1])\n\n            if not float_data:\n                Q = Q.to(original_device).type(original_type)\n            final.append(Q)\n        return final\n\n    def get_orthogonal_matrix_QR(self, state, max_precond_dim=10000, merge_dims=False):\n        \"\"\"\n        Computes the eigenbases of the preconditioner using one round of power iteration\n        followed by torch.linalg.qr decomposition.\n        \"\"\"\n        precond_list = state[\"GG\"]\n        orth_list = state[\"Q\"]\n\n        matrix = []\n        orth_matrix = []\n        for m, o in zip(precond_list, orth_list, strict=False):\n            if len(m) == 0:\n                matrix.append([])\n                orth_matrix.append([])\n                continue\n            if m.data.dtype != torch.float:\n                float_data = False\n                original_type = m.data.dtype\n                original_device = m.data.device\n                matrix.append(m.data.float())\n                orth_matrix.append(o.data.float())\n            else:\n                float_data = True\n                matrix.append(m.data.float())\n                orth_matrix.append(o.data.float())\n\n        orig_shape = state[\"exp_avg_sq\"].shape\n        if self._data_format == \"channels_last\" and len(orig_shape) == 4:\n            permuted_shape = state[\"exp_avg_sq\"].permute(0, 3, 1, 2).shape\n        if merge_dims:\n            exp_avg_sq = self.merge_dims(state[\"exp_avg_sq\"], max_precond_dim)\n        else:\n            exp_avg_sq = state[\"exp_avg_sq\"]\n\n        final = []\n        for ind, (m, o) in enumerate(zip(matrix, orth_matrix, strict=False)):\n            if len(m) == 0:\n                final.append([])\n                continue\n            est_eig = torch.diag(o.T @ m @ o)\n            sort_idx = torch.argsort(est_eig, descending=True)\n            exp_avg_sq = exp_avg_sq.index_select(ind, sort_idx)\n            o = o[:, sort_idx]\n            power_iter = m @ o\n            Q, _ = torch.linalg.qr(power_iter)\n\n            if not float_data:\n                Q = Q.to(original_device).type(original_type)\n            final.append(Q)\n\n        if merge_dims:\n            if self._data_format == \"channels_last\" and len(orig_shape) == 4:\n                exp_avg_sq = exp_avg_sq.reshape(permuted_shape).permute(0, 2, 3, 1)\n            else:\n                exp_avg_sq = exp_avg_sq.reshape(orig_shape)\n\n        state[\"exp_avg_sq\"] = exp_avg_sq\n        return final\n</code></pre>"},{"location":"reference/modules/optim/soap/#optimus_dl.modules.optim.soap.SOAP.get_orthogonal_matrix","title":"<code>get_orthogonal_matrix(mat)</code>","text":"<p>Computes the eigenbases of the preconditioner using torch.linalg.eigh decomposition.</p> Source code in <code>optimus_dl/modules/optim/soap.py</code> <pre><code>def get_orthogonal_matrix(self, mat):\n    \"\"\"\n    Computes the eigenbases of the preconditioner using torch.linalg.eigh decomposition.\n    \"\"\"\n    matrix = []\n    for m in mat:\n        if len(m) == 0:\n            matrix.append([])\n            continue\n        if m.data.dtype != torch.float:\n            float_data = False\n            original_type = m.data.dtype\n            original_device = m.data.device\n            matrix.append(m.data.float())\n        else:\n            float_data = True\n            matrix.append(m.data)\n\n    final = []\n    for m in matrix:\n        if len(m) == 0:\n            final.append([])\n            continue\n        try:\n            _, Q = torch.linalg.eigh(\n                m + 1e-30 * torch.eye(m.shape[0], device=m.device)\n            )\n        except:  # noqa: E722\n            _, Q = torch.linalg.eigh(\n                m.to(torch.float64) + 1e-30 * torch.eye(m.shape[0], device=m.device)\n            )\n            Q = Q.to(m.dtype)\n        Q = torch.flip(Q, [1])\n\n        if not float_data:\n            Q = Q.to(original_device).type(original_type)\n        final.append(Q)\n    return final\n</code></pre>"},{"location":"reference/modules/optim/soap/#optimus_dl.modules.optim.soap.SOAP.get_orthogonal_matrix_QR","title":"<code>get_orthogonal_matrix_QR(state, max_precond_dim=10000, merge_dims=False)</code>","text":"<p>Computes the eigenbases of the preconditioner using one round of power iteration followed by torch.linalg.qr decomposition.</p> Source code in <code>optimus_dl/modules/optim/soap.py</code> <pre><code>def get_orthogonal_matrix_QR(self, state, max_precond_dim=10000, merge_dims=False):\n    \"\"\"\n    Computes the eigenbases of the preconditioner using one round of power iteration\n    followed by torch.linalg.qr decomposition.\n    \"\"\"\n    precond_list = state[\"GG\"]\n    orth_list = state[\"Q\"]\n\n    matrix = []\n    orth_matrix = []\n    for m, o in zip(precond_list, orth_list, strict=False):\n        if len(m) == 0:\n            matrix.append([])\n            orth_matrix.append([])\n            continue\n        if m.data.dtype != torch.float:\n            float_data = False\n            original_type = m.data.dtype\n            original_device = m.data.device\n            matrix.append(m.data.float())\n            orth_matrix.append(o.data.float())\n        else:\n            float_data = True\n            matrix.append(m.data.float())\n            orth_matrix.append(o.data.float())\n\n    orig_shape = state[\"exp_avg_sq\"].shape\n    if self._data_format == \"channels_last\" and len(orig_shape) == 4:\n        permuted_shape = state[\"exp_avg_sq\"].permute(0, 3, 1, 2).shape\n    if merge_dims:\n        exp_avg_sq = self.merge_dims(state[\"exp_avg_sq\"], max_precond_dim)\n    else:\n        exp_avg_sq = state[\"exp_avg_sq\"]\n\n    final = []\n    for ind, (m, o) in enumerate(zip(matrix, orth_matrix, strict=False)):\n        if len(m) == 0:\n            final.append([])\n            continue\n        est_eig = torch.diag(o.T @ m @ o)\n        sort_idx = torch.argsort(est_eig, descending=True)\n        exp_avg_sq = exp_avg_sq.index_select(ind, sort_idx)\n        o = o[:, sort_idx]\n        power_iter = m @ o\n        Q, _ = torch.linalg.qr(power_iter)\n\n        if not float_data:\n            Q = Q.to(original_device).type(original_type)\n        final.append(Q)\n\n    if merge_dims:\n        if self._data_format == \"channels_last\" and len(orig_shape) == 4:\n            exp_avg_sq = exp_avg_sq.reshape(permuted_shape).permute(0, 2, 3, 1)\n        else:\n            exp_avg_sq = exp_avg_sq.reshape(orig_shape)\n\n    state[\"exp_avg_sq\"] = exp_avg_sq\n    return final\n</code></pre>"},{"location":"reference/modules/optim/soap/#optimus_dl.modules.optim.soap.SOAP.init_preconditioner","title":"<code>init_preconditioner(grad, state, precondition_frequency=10, shampoo_beta=0.95, max_precond_dim=10000, precondition_1d=False, merge_dims=False)</code>","text":"<p>Initializes the preconditioner matrices (L and R in the paper).</p> Source code in <code>optimus_dl/modules/optim/soap.py</code> <pre><code>def init_preconditioner(\n    self,\n    grad,\n    state,\n    precondition_frequency=10,\n    shampoo_beta=0.95,\n    max_precond_dim=10000,\n    precondition_1d=False,\n    merge_dims=False,\n):\n    \"\"\"\n    Initializes the preconditioner matrices (L and R in the paper).\n    \"\"\"\n    state[\"GG\"] = (\n        []\n    )  # Will hold all the preconditioner matrices (L and R in the paper).\n    if grad.dim() == 1:\n        if not precondition_1d or grad.shape[0] &gt; max_precond_dim:\n            state[\"GG\"].append([])\n        else:\n            state[\"GG\"].append(\n                torch.zeros(grad.shape[0], grad.shape[0], device=grad.device)\n            )\n    else:\n        if merge_dims:\n            grad = self.merge_dims(grad, max_precond_dim)\n\n        for sh in grad.shape:\n            if sh &gt; max_precond_dim:\n                state[\"GG\"].append([])\n            else:\n                state[\"GG\"].append(torch.zeros(sh, sh, device=grad.device))\n\n    state[\"Q\"] = None  # Will hold all the eigenbases of the preconditioner.\n    state[\"precondition_frequency\"] = precondition_frequency\n    state[\"shampoo_beta\"] = shampoo_beta\n</code></pre>"},{"location":"reference/modules/optim/soap/#optimus_dl.modules.optim.soap.SOAP.merge_dims","title":"<code>merge_dims(grad, max_precond_dim)</code>","text":"<p>Merges dimensions of the gradient tensor till the product of the dimensions is less than or equal to max_precond_dim.</p> Source code in <code>optimus_dl/modules/optim/soap.py</code> <pre><code>def merge_dims(self, grad, max_precond_dim):\n    \"\"\"\n    Merges dimensions of the gradient tensor till the product of the dimensions is less than or equal to max_precond_dim.\n    \"\"\"\n    assert self._data_format in [\"channels_first\", \"channels_last\"]\n    if self._data_format == \"channels_last\" and grad.dim() == 4:\n        grad = grad.permute(0, 3, 1, 2)\n    shape = grad.shape\n    new_shape = []\n\n    curr_shape = 1\n    for sh in shape:\n        temp_shape = curr_shape * sh\n        if temp_shape &gt; max_precond_dim:\n            if curr_shape &gt; 1:\n                new_shape.append(curr_shape)\n                curr_shape = sh\n            else:\n                new_shape.append(sh)\n                curr_shape = 1\n        else:\n            curr_shape = temp_shape\n\n    if curr_shape &gt; 1 or len(new_shape) == 0:\n        new_shape.append(curr_shape)\n\n    new_grad = grad.reshape(new_shape)\n    return new_grad\n</code></pre>"},{"location":"reference/modules/optim/soap/#optimus_dl.modules.optim.soap.SOAP.project","title":"<code>project(grad, state, merge_dims=False, max_precond_dim=10000)</code>","text":"<p>Projects the gradient to the eigenbases of the preconditioner.</p> Source code in <code>optimus_dl/modules/optim/soap.py</code> <pre><code>def project(self, grad, state, merge_dims=False, max_precond_dim=10000):\n    \"\"\"\n    Projects the gradient to the eigenbases of the preconditioner.\n    \"\"\"\n    original_shape = grad.shape\n    if merge_dims:\n        if grad.dim() == 4 and self._data_format == \"channels_last\":\n            permuted_shape = grad.permute(0, 3, 1, 2).shape\n        grad = self.merge_dims(grad, max_precond_dim)\n\n    for mat in state[\"Q\"]:\n        if len(mat) &gt; 0:\n            grad = torch.tensordot(\n                grad,\n                mat,\n                dims=[[0], [0]],\n            )\n        else:\n            permute_order = list(range(1, len(grad.shape))) + [0]\n            grad = grad.permute(permute_order)\n\n    if merge_dims:\n        if self._data_format == \"channels_last\" and len(original_shape) == 4:\n            grad = grad.reshape(permuted_shape).permute(0, 2, 3, 1)\n        else:\n            grad = grad.reshape(original_shape)\n    return grad\n</code></pre>"},{"location":"reference/modules/optim/soap/#optimus_dl.modules.optim.soap.SOAP.project_back","title":"<code>project_back(grad, state, merge_dims=False, max_precond_dim=10000)</code>","text":"<p>Projects the gradient back to the original space.</p> Source code in <code>optimus_dl/modules/optim/soap.py</code> <pre><code>def project_back(self, grad, state, merge_dims=False, max_precond_dim=10000):\n    \"\"\"\n    Projects the gradient back to the original space.\n    \"\"\"\n    original_shape = grad.shape\n    if merge_dims:\n        if self._data_format == \"channels_last\" and grad.dim() == 4:\n            permuted_shape = grad.permute(0, 3, 1, 2).shape\n        grad = self.merge_dims(grad, max_precond_dim)\n    for mat in state[\"Q\"]:\n        if len(mat) &gt; 0:\n            grad = torch.tensordot(\n                grad,\n                mat,\n                dims=[[0], [1]],\n            )\n        else:\n            permute_order = list(range(1, len(grad.shape))) + [0]\n            grad = grad.permute(permute_order)\n\n    if merge_dims:\n        if self._data_format == \"channels_last\" and len(original_shape) == 4:\n            grad = grad.reshape(permuted_shape).permute(0, 2, 3, 1)\n        else:\n            grad = grad.reshape(original_shape)\n    return grad\n</code></pre>"},{"location":"reference/modules/optim/soap/#optimus_dl.modules.optim.soap.SOAP.step","title":"<code>step(closure=None)</code>","text":"<p>Performs a single optimization step.</p> <p>Parameters:</p> Name Type Description Default <code>closure</code> <code>`Callable`, *optional*</code> <p>A closure that reevaluates the model and returns the loss.</p> <code>None</code> Source code in <code>optimus_dl/modules/optim/soap.py</code> <pre><code>@torch.no_grad()\ndef step(self, closure=None):\n    \"\"\"\n    Performs a single optimization step.\n\n    Arguments:\n        closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.\n    \"\"\"\n    if closure is None:\n        loss = None\n    else:\n        loss = closure()\n\n    for group in self.param_groups:\n        for p in group[\"params\"]:\n            if p.grad is None:\n                continue\n            grad = p.grad\n\n            state = self.state[p]\n\n            if \"step\" not in state:\n                state[\"step\"] = 0\n\n            # State initialization\n            if \"exp_avg\" not in state:\n                # Exponential moving average of gradient values\n                state[\"exp_avg\"] = torch.zeros_like(grad)\n                # Exponential moving average of squared gradient values\n                state[\"exp_avg_sq\"] = torch.zeros_like(grad)\n\n            if \"Q\" not in state:\n                self.init_preconditioner(\n                    grad,\n                    state,\n                    precondition_frequency=group[\"precondition_frequency\"],\n                    precondition_1d=group[\"precondition_1d\"],\n                    shampoo_beta=(\n                        group[\"shampoo_beta\"]\n                        if group[\"shampoo_beta\"] &gt;= 0\n                        else group[\"betas\"][1]\n                    ),\n                    max_precond_dim=group[\"max_precond_dim\"],\n                    merge_dims=group[\"merge_dims\"],\n                )\n                self.update_preconditioner(\n                    grad,\n                    state,\n                    max_precond_dim=group[\"max_precond_dim\"],\n                    merge_dims=group[\"merge_dims\"],\n                    precondition_1d=group[\"precondition_1d\"],\n                )\n                continue  # first step is skipped so that we never use the current gradients in the projection.\n\n            # Projecting gradients to the eigenbases of Shampoo's preconditioner\n            # i.e. projecting to the eigenbases of matrices in state['GG']\n            grad_projected = self.project(\n                grad,\n                state,\n                merge_dims=group[\"merge_dims\"],\n                max_precond_dim=group[\"max_precond_dim\"],\n            )\n\n            exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n            beta1, beta2 = group[\"betas\"]\n\n            state[\"step\"] += 1\n\n            # Decay the first and second moment running average coefficient\n            # In-place operations to update the averages at the same time\n            exp_avg.mul_(beta1).add_(grad_projected, alpha=(1.0 - beta1))\n            exp_avg_sq.mul_(beta2).add_(\n                grad_projected.square(), alpha=(1.0 - beta2)\n            )\n\n            denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n\n            # Projecting the exponential moving average of gradients to the eigenbases of Shampoo's preconditioner\n            # i.e. projecting to the eigenbases of matrices in state['GG']\n            # exp_avg_projected = self.project(exp_avg, state, merge_dims=group[\"merge_dims\"],\n            #                                  max_precond_dim=group['max_precond_dim'])\n            exp_avg_projected = exp_avg\n\n            step_size = group[\"lr\"]\n            if group[\"correct_bias\"]:\n                bias_correction1 = 1.0 - beta1 ** (state[\"step\"])\n                bias_correction2 = 1.0 - beta2 ** (state[\"step\"])\n                step_size = step_size * (bias_correction2**0.5) / bias_correction1\n\n            # Projecting back the preconditioned (by Adam) exponential moving average of gradients\n            # to the original space\n            norm_grad = self.project_back(\n                exp_avg_projected / denom,\n                state,\n                merge_dims=group[\"merge_dims\"],\n                max_precond_dim=group[\"max_precond_dim\"],\n            )\n\n            if group[\"normalize_grads\"]:\n                norm_grad = norm_grad / (1e-30 + torch.mean(norm_grad**2) ** 0.5)\n\n            p.add_(norm_grad, alpha=-step_size)\n\n            # From AdamW code: Just adding the square of the weights to the loss function is *not*\n            # the correct way of using L2 regularization/weight decay with Adam,\n            # since that will interact with the m and v parameters in strange ways.\n            #\n            # Instead we want to decay the weights in a manner that doesn't interact\n            # with the m/v parameters. This is equivalent to adding the square\n            # of the weights to the loss with plain (non-momentum) SGD.\n            # Add weight decay at the end (fixed version)\n            if group[\"weight_decay\"] &gt; 0.0:\n                p.add_(p, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))\n\n            # Update is done after the gradient step to avoid using current gradients in the projection.\n            self.update_preconditioner(\n                grad,\n                state,\n                max_precond_dim=group[\"max_precond_dim\"],\n                merge_dims=group[\"merge_dims\"],\n                precondition_1d=group[\"precondition_1d\"],\n            )\n\n    return loss\n</code></pre>"},{"location":"reference/modules/optim/soap/#optimus_dl.modules.optim.soap.SOAP.update_preconditioner","title":"<code>update_preconditioner(grad, state, max_precond_dim=10000, merge_dims=False, precondition_1d=False)</code>","text":"<p>Updates the preconditioner matrices and the eigenbases (L, R, Q_L, Q_R in the paper).</p> Source code in <code>optimus_dl/modules/optim/soap.py</code> <pre><code>def update_preconditioner(\n    self,\n    grad,\n    state,\n    max_precond_dim=10000,\n    merge_dims=False,\n    precondition_1d=False,\n):\n    \"\"\"\n    Updates the preconditioner matrices and the eigenbases (L, R, Q_L, Q_R in the paper).\n    \"\"\"\n    if state[\"Q\"] is not None:\n        state[\"exp_avg\"] = self.project_back(\n            state[\"exp_avg\"],\n            state,\n            merge_dims=merge_dims,\n            max_precond_dim=max_precond_dim,\n        )\n    if grad.dim() == 1:\n        if precondition_1d and grad.shape[0] &lt;= max_precond_dim:\n            state[\"GG\"][0].lerp_(\n                grad.unsqueeze(1) @ grad.unsqueeze(0), 1 - state[\"shampoo_beta\"]\n            )\n    else:\n        if merge_dims:\n            new_grad = self.merge_dims(grad, max_precond_dim)\n            for idx, sh in enumerate(new_grad.shape):\n                if sh &lt;= max_precond_dim:\n                    outer_product = torch.tensordot(\n                        new_grad,\n                        new_grad,\n                        dims=[\n                            [\n                                *chain(\n                                    range(idx), range(idx + 1, len(new_grad.shape))\n                                )\n                            ]\n                        ]\n                        * 2,\n                    )\n                    state[\"GG\"][idx].lerp_(outer_product, 1 - state[\"shampoo_beta\"])\n        else:\n            for idx, sh in enumerate(grad.shape):\n                if sh &lt;= max_precond_dim:\n                    outer_product = torch.tensordot(\n                        grad,\n                        grad,\n                        # Contracts across all dimensions except for k.\n                        dims=[[*chain(range(idx), range(idx + 1, len(grad.shape)))]]\n                        * 2,\n                    )\n                    state[\"GG\"][idx].lerp_(outer_product, 1 - state[\"shampoo_beta\"])\n\n    if state[\"Q\"] is None:\n        state[\"Q\"] = self.get_orthogonal_matrix(state[\"GG\"])\n    if state[\"step\"] &gt; 0 and state[\"step\"] % state[\"precondition_frequency\"] == 0:\n        state[\"Q\"] = self.get_orthogonal_matrix_QR(\n            state, max_precond_dim, merge_dims\n        )\n        # state['Q'] = self.get_fast_QR(state, max_precond_dim, merge_dims)\n\n    if state[\"step\"] &gt; 0:\n        state[\"exp_avg\"] = self.project(\n            state[\"exp_avg\"],\n            state,\n            merge_dims=merge_dims,\n            max_precond_dim=max_precond_dim,\n        )\n</code></pre>"},{"location":"reference/modules/optim/soap/#optimus_dl.modules.optim.soap.SoapConfig","title":"<code>SoapConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Configuration for SOAP optimizer. Refer to <code>optimus_dl.modules.optim.soap.SOAP</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <code>0.003</code> <code>betas</code> <code>tuple[float, float]</code> <code>(0.95, 0.95)</code> <code>shampoo_beta</code> <code>float</code> <code>-1</code> <code>eps</code> <code>float</code> <code>1e-08</code> <code>weight_decay</code> <code>float</code> <code>0.01</code> <code>precondition_frequency</code> <code>int</code> <code>10</code> <code>max_precond_dim</code> <code>int</code> <code>10000</code> <code>merge_dims</code> <code>bool</code> <code>False</code> <code>precondition_1d</code> <code>bool</code> <code>False</code> <code>normalize_grads</code> <code>bool</code> <code>False</code> <code>data_format</code> <code>str</code> <code>'channels_first'</code> <code>correct_bias</code> <code>bool</code> <code>True</code> Source code in <code>optimus_dl/modules/optim/soap.py</code> <pre><code>@dataclass\nclass SoapConfig(RegistryConfigStrict):\n    \"\"\"\n    Configuration for SOAP optimizer.\n    Refer to `optimus_dl.modules.optim.soap.SOAP` for more details.\n    \"\"\"\n\n    lr: float = 3e-3\n    betas: tuple[float, float] = (0.95, 0.95)\n    shampoo_beta: float = -1\n    eps: float = 1e-8\n    weight_decay: float = 0.01\n    precondition_frequency: int = 10\n    max_precond_dim: int = 10000\n    merge_dims: bool = False\n    precondition_1d: bool = False\n    normalize_grads: bool = False\n    data_format: str = \"channels_first\"\n    correct_bias: bool = True\n</code></pre>"},{"location":"reference/modules/tokenizer/","title":"Index","text":""},{"location":"reference/modules/tokenizer/#optimus_dl.modules.tokenizer","title":"<code>optimus_dl.modules.tokenizer</code>","text":""},{"location":"reference/modules/tokenizer/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>base</code>: Abstract base class for all tokenizers.</li> <li><code>config</code>: </li> <li><code>implementations</code>: </li> </ul>"},{"location":"reference/modules/tokenizer/base/","title":"base","text":""},{"location":"reference/modules/tokenizer/base/#optimus_dl.modules.tokenizer.base","title":"<code>optimus_dl.modules.tokenizer.base</code>","text":""},{"location":"reference/modules/tokenizer/base/#optimus_dl.modules.tokenizer.base.BaseTokenizer","title":"<code>BaseTokenizer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all tokenizers.</p> <p>Defines the standard interface for encoding strings to token IDs and decoding IDs back to text.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>Configuration object for the tokenizer.</p> Source code in <code>optimus_dl/modules/tokenizer/base.py</code> <pre><code>class BaseTokenizer(ABC):\n    \"\"\"Abstract base class for all tokenizers.\n\n    Defines the standard interface for encoding strings to token IDs and\n    decoding IDs back to text.\n\n    Attributes:\n        config: Configuration object for the tokenizer.\n    \"\"\"\n\n    def __init__(self, config: BaseTokenizerConfig):\n        self.config = config\n\n    @abstractmethod\n    def encode(self, text: str) -&gt; list[int]:\n        \"\"\"Convert a text string into a list of integer token IDs.\"\"\"\n        pass\n\n    @abstractmethod\n    def decode(self, ids: list[int]) -&gt; str:\n        \"\"\"Convert a list of token IDs back into a text string.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def vocab_size(self) -&gt; int:\n        \"\"\"Total size of the tokenizer's vocabulary.\"\"\"\n        pass\n\n    @property\n    def bos_token_id(self) -&gt; int | None:\n        \"\"\"ID of the Beginning-of-Sequence token, if any.\"\"\"\n        return None\n\n    @property\n    def eos_token_id(self) -&gt; int | None:\n        \"\"\"ID of the End-of-Sequence token, if any.\"\"\"\n        return None\n\n    def save_pretrained(self, save_directory: str):\n        \"\"\"Save tokenizer configuration to a directory.\"\"\"\n        save_directory_path = pathlib.Path(save_directory)\n        save_directory_path.mkdir(parents=True, exist_ok=True)\n        with open(save_directory_path / \"tokenizer_config.json\", \"w\") as f:\n            yaml.dump(self.config, f)\n\n    def apply_chat_template(\n        self,\n        conversation: list[dict[str, str]],\n        tokenize: bool = True,\n        add_generation_prompt: bool = True,\n    ) -&gt; str | list[int]:\n        \"\"\"Apply a chat template (e.g., Llama-2-chat) to a conversation history.\n\n        Args:\n            conversation: List of messages (e.g., [{\"role\": \"user\", \"content\": \"...\"}]).\n            tokenize: Whether to return token IDs (True) or the raw string (False).\n            add_generation_prompt: Whether to append the assistant's response prefix.\n\n        Returns:\n            Formatted string or list of token IDs.\n        \"\"\"\n        raise NotImplementedError(\n            \"apply_chat_template not implemented for this tokenizer\"\n        )\n</code></pre>"},{"location":"reference/modules/tokenizer/base/#optimus_dl.modules.tokenizer.base.BaseTokenizer.bos_token_id","title":"<code>bos_token_id</code>  <code>property</code>","text":"<p>ID of the Beginning-of-Sequence token, if any.</p>"},{"location":"reference/modules/tokenizer/base/#optimus_dl.modules.tokenizer.base.BaseTokenizer.eos_token_id","title":"<code>eos_token_id</code>  <code>property</code>","text":"<p>ID of the End-of-Sequence token, if any.</p>"},{"location":"reference/modules/tokenizer/base/#optimus_dl.modules.tokenizer.base.BaseTokenizer.vocab_size","title":"<code>vocab_size</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Total size of the tokenizer's vocabulary.</p>"},{"location":"reference/modules/tokenizer/base/#optimus_dl.modules.tokenizer.base.BaseTokenizer.apply_chat_template","title":"<code>apply_chat_template(conversation, tokenize=True, add_generation_prompt=True)</code>","text":"<p>Apply a chat template (e.g., Llama-2-chat) to a conversation history.</p> <p>Parameters:</p> Name Type Description Default <code>conversation</code> <code>list[dict[str, str]]</code> <p>List of messages (e.g., [{\"role\": \"user\", \"content\": \"...\"}]).</p> required <code>tokenize</code> <code>bool</code> <p>Whether to return token IDs (True) or the raw string (False).</p> <code>True</code> <code>add_generation_prompt</code> <code>bool</code> <p>Whether to append the assistant's response prefix.</p> <code>True</code> <p>Returns:</p> Type Description <code>str | list[int]</code> <p>Formatted string or list of token IDs.</p> Source code in <code>optimus_dl/modules/tokenizer/base.py</code> <pre><code>def apply_chat_template(\n    self,\n    conversation: list[dict[str, str]],\n    tokenize: bool = True,\n    add_generation_prompt: bool = True,\n) -&gt; str | list[int]:\n    \"\"\"Apply a chat template (e.g., Llama-2-chat) to a conversation history.\n\n    Args:\n        conversation: List of messages (e.g., [{\"role\": \"user\", \"content\": \"...\"}]).\n        tokenize: Whether to return token IDs (True) or the raw string (False).\n        add_generation_prompt: Whether to append the assistant's response prefix.\n\n    Returns:\n        Formatted string or list of token IDs.\n    \"\"\"\n    raise NotImplementedError(\n        \"apply_chat_template not implemented for this tokenizer\"\n    )\n</code></pre>"},{"location":"reference/modules/tokenizer/base/#optimus_dl.modules.tokenizer.base.BaseTokenizer.decode","title":"<code>decode(ids)</code>  <code>abstractmethod</code>","text":"<p>Convert a list of token IDs back into a text string.</p> Source code in <code>optimus_dl/modules/tokenizer/base.py</code> <pre><code>@abstractmethod\ndef decode(self, ids: list[int]) -&gt; str:\n    \"\"\"Convert a list of token IDs back into a text string.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/modules/tokenizer/base/#optimus_dl.modules.tokenizer.base.BaseTokenizer.encode","title":"<code>encode(text)</code>  <code>abstractmethod</code>","text":"<p>Convert a text string into a list of integer token IDs.</p> Source code in <code>optimus_dl/modules/tokenizer/base.py</code> <pre><code>@abstractmethod\ndef encode(self, text: str) -&gt; list[int]:\n    \"\"\"Convert a text string into a list of integer token IDs.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/modules/tokenizer/base/#optimus_dl.modules.tokenizer.base.BaseTokenizer.save_pretrained","title":"<code>save_pretrained(save_directory)</code>","text":"<p>Save tokenizer configuration to a directory.</p> Source code in <code>optimus_dl/modules/tokenizer/base.py</code> <pre><code>def save_pretrained(self, save_directory: str):\n    \"\"\"Save tokenizer configuration to a directory.\"\"\"\n    save_directory_path = pathlib.Path(save_directory)\n    save_directory_path.mkdir(parents=True, exist_ok=True)\n    with open(save_directory_path / \"tokenizer_config.json\", \"w\") as f:\n        yaml.dump(self.config, f)\n</code></pre>"},{"location":"reference/modules/tokenizer/config/","title":"config","text":""},{"location":"reference/modules/tokenizer/config/#optimus_dl.modules.tokenizer.config","title":"<code>optimus_dl.modules.tokenizer.config</code>","text":""},{"location":"reference/modules/tokenizer/config/#optimus_dl.modules.tokenizer.config.BaseTokenizerConfig","title":"<code>BaseTokenizerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfig</code></p> <p>BaseTokenizerConfig(_name: str | None = None, add_bos: bool = True, add_eos: bool = True)</p> <p>Parameters:</p> Name Type Description Default <code>add_bos</code> <code>bool</code> <code>True</code> <code>add_eos</code> <code>bool</code> <code>True</code> Source code in <code>optimus_dl/modules/tokenizer/config.py</code> <pre><code>@dataclass\nclass BaseTokenizerConfig(RegistryConfig):\n    add_bos: bool = True\n    add_eos: bool = True\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/","title":"Index","text":""},{"location":"reference/modules/tokenizer/implementations/#optimus_dl.modules.tokenizer.implementations","title":"<code>optimus_dl.modules.tokenizer.implementations</code>","text":""},{"location":"reference/modules/tokenizer/implementations/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>char</code>: Configuration for character/byte-level tokenizer.</li> <li><code>huggingface</code>: Configuration for Hugging Face tokenizers.</li> <li><code>inline_tokens</code>: Configuration for explicitly specified tokens tokenizer.</li> <li><code>tiktoken</code>: Configuration for Tiktoken tokenizers.</li> </ul>"},{"location":"reference/modules/tokenizer/implementations/char/","title":"char","text":""},{"location":"reference/modules/tokenizer/implementations/char/#optimus_dl.modules.tokenizer.implementations.char","title":"<code>optimus_dl.modules.tokenizer.implementations.char</code>","text":""},{"location":"reference/modules/tokenizer/implementations/char/#optimus_dl.modules.tokenizer.implementations.char.CharTokenizer","title":"<code>CharTokenizer</code>","text":"<p>               Bases: <code>BaseTokenizer</code></p> <p>Simple byte-level UTF-8 tokenizer.</p> <p>Converts text to raw UTF-8 bytes and adds optional BOS/EOS tokens. Detokenization skips the special token IDs and decodes the remainder as UTF-8.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>CharTokenizerConfig</code> <p>Character tokenizer configuration.</p> required Source code in <code>optimus_dl/modules/tokenizer/implementations/char.py</code> <pre><code>@register_tokenizer(\"char_tokenize\", CharTokenizerConfig)\nclass CharTokenizer(BaseTokenizer):\n    \"\"\"Simple byte-level UTF-8 tokenizer.\n\n    Converts text to raw UTF-8 bytes and adds optional BOS/EOS tokens.\n    Detokenization skips the special token IDs and decodes the remainder\n    as UTF-8.\n\n    Args:\n        config: Character tokenizer configuration.\n    \"\"\"\n\n    def __init__(self, config: CharTokenizerConfig, **kwargs):\n        super().__init__(config)\n\n    def encode(self, text: str) -&gt; list[int]:\n        \"\"\"Convert text to UTF-8 bytes and add special tokens.\"\"\"\n        input_ids = list(text.encode(\"utf-8\"))\n\n        if self.config.add_bos:\n            if self.bos_token_id is None:\n                raise ValueError(\n                    \"Tokenizer does not have a BOS token ID, but add_bos is True.\"\n                )\n            input_ids.insert(0, self.bos_token_id)\n        if self.config.add_eos:\n            if self.eos_token_id is None:\n                raise ValueError(\n                    \"Tokenizer does not have an EOS token ID, but add_eos is True.\"\n                )\n            input_ids.append(self.eos_token_id)\n        return input_ids\n\n    def decode(self, ids: list[int]) -&gt; str:\n        \"\"\"Filter out special IDs and decode bytes to UTF-8.\"\"\"\n        # Filter out special tokens\n        bytes_list = []\n        for id in ids:\n            if 0 &lt;= id &lt; 256:\n                bytes_list.append(id)\n        return bytes(bytes_list).decode(\"utf-8\", errors=\"replace\")\n\n    @property\n    def vocab_size(self) -&gt; int:\n        \"\"\"Vocabulary size including BOS/EOS tokens.\"\"\"\n        return max(self.config.vocab_size, (self.config.eos_token_id or 0) + 1)\n\n    @property\n    def bos_token_id(self):\n        \"\"\"BOS token ID from config.\"\"\"\n        return self.config.bos_token_id\n\n    @property\n    def eos_token_id(self):\n        \"\"\"EOS token ID from config.\"\"\"\n        return self.config.eos_token_id\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/char/#optimus_dl.modules.tokenizer.implementations.char.CharTokenizer.bos_token_id","title":"<code>bos_token_id</code>  <code>property</code>","text":"<p>BOS token ID from config.</p>"},{"location":"reference/modules/tokenizer/implementations/char/#optimus_dl.modules.tokenizer.implementations.char.CharTokenizer.eos_token_id","title":"<code>eos_token_id</code>  <code>property</code>","text":"<p>EOS token ID from config.</p>"},{"location":"reference/modules/tokenizer/implementations/char/#optimus_dl.modules.tokenizer.implementations.char.CharTokenizer.vocab_size","title":"<code>vocab_size</code>  <code>property</code>","text":"<p>Vocabulary size including BOS/EOS tokens.</p>"},{"location":"reference/modules/tokenizer/implementations/char/#optimus_dl.modules.tokenizer.implementations.char.CharTokenizer.decode","title":"<code>decode(ids)</code>","text":"<p>Filter out special IDs and decode bytes to UTF-8.</p> Source code in <code>optimus_dl/modules/tokenizer/implementations/char.py</code> <pre><code>def decode(self, ids: list[int]) -&gt; str:\n    \"\"\"Filter out special IDs and decode bytes to UTF-8.\"\"\"\n    # Filter out special tokens\n    bytes_list = []\n    for id in ids:\n        if 0 &lt;= id &lt; 256:\n            bytes_list.append(id)\n    return bytes(bytes_list).decode(\"utf-8\", errors=\"replace\")\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/char/#optimus_dl.modules.tokenizer.implementations.char.CharTokenizer.encode","title":"<code>encode(text)</code>","text":"<p>Convert text to UTF-8 bytes and add special tokens.</p> Source code in <code>optimus_dl/modules/tokenizer/implementations/char.py</code> <pre><code>def encode(self, text: str) -&gt; list[int]:\n    \"\"\"Convert text to UTF-8 bytes and add special tokens.\"\"\"\n    input_ids = list(text.encode(\"utf-8\"))\n\n    if self.config.add_bos:\n        if self.bos_token_id is None:\n            raise ValueError(\n                \"Tokenizer does not have a BOS token ID, but add_bos is True.\"\n            )\n        input_ids.insert(0, self.bos_token_id)\n    if self.config.add_eos:\n        if self.eos_token_id is None:\n            raise ValueError(\n                \"Tokenizer does not have an EOS token ID, but add_eos is True.\"\n            )\n        input_ids.append(self.eos_token_id)\n    return input_ids\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/char/#optimus_dl.modules.tokenizer.implementations.char.CharTokenizerConfig","title":"<code>CharTokenizerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseTokenizerConfig</code></p> <p>Configuration for character/byte-level tokenizer.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <code>256</code> <code>bos_token_id</code> <code>int</code> <code>256</code> <code>eos_token_id</code> <code>int</code> <code>257</code> Source code in <code>optimus_dl/modules/tokenizer/implementations/char.py</code> <pre><code>@dataclass\nclass CharTokenizerConfig(BaseTokenizerConfig):\n    \"\"\"Configuration for character/byte-level tokenizer.\n\n    Attributes:\n        vocab_size: Number of unique byte values (usually 256).\n        bos_token_id: ID for the Beginning-of-Sequence token.\n        eos_token_id: ID for the End-of-Sequence token.\n    \"\"\"\n\n    vocab_size: int = 256  # 0-255 bytes + special tokens\n    bos_token_id: int = 256\n    eos_token_id: int = 257\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/huggingface/","title":"huggingface","text":""},{"location":"reference/modules/tokenizer/implementations/huggingface/#optimus_dl.modules.tokenizer.implementations.huggingface","title":"<code>optimus_dl.modules.tokenizer.implementations.huggingface</code>","text":""},{"location":"reference/modules/tokenizer/implementations/huggingface/#optimus_dl.modules.tokenizer.implementations.huggingface.HFTokenizer","title":"<code>HFTokenizer</code>","text":"<p>               Bases: <code>BaseTokenizer</code></p> <p>Wrapper for Hugging Face AutoTokenizer.</p> <p>Integrates standard Hub tokenizers into the framework. It handles:</p> <ul> <li>Pretrained Loading: Automatically downloads and caches tokenizers.</li> <li>Special Tokens: Manages BOS/EOS injection based on config.</li> <li>Chat Templates: Supports generating formatted conversation strings.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>HFTokenizerConfig</code> <p>Hugging Face tokenizer configuration.</p> required Source code in <code>optimus_dl/modules/tokenizer/implementations/huggingface.py</code> <pre><code>@register_tokenizer(\"transformers\", HFTokenizerConfig)\nclass HFTokenizer(BaseTokenizer):\n    \"\"\"Wrapper for Hugging Face AutoTokenizer.\n\n    Integrates standard Hub tokenizers into the framework. It handles:\n\n    - **Pretrained Loading**: Automatically downloads and caches tokenizers.\n    - **Special Tokens**: Manages BOS/EOS injection based on config.\n    - **Chat Templates**: Supports generating formatted conversation strings.\n\n    Args:\n        config: Hugging Face tokenizer configuration.\n    \"\"\"\n\n    def __init__(self, config: HFTokenizerConfig, **kwargs):\n        super().__init__(config)\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            config.name, trust_remote_code=config.trust_remote_code\n        )\n\n    def encode(self, text: str) -&gt; list[int]:\n        \"\"\"Convert text to IDs using the Hub tokenizer.\"\"\"\n        ids = self.tokenizer.encode(text, add_special_tokens=False)\n\n        if self.config.add_bos and self.bos_token_id is not None:\n            ids = [self.bos_token_id] + ids\n        if self.config.add_eos and self.eos_token_id is not None:\n            ids = ids + [self.eos_token_id]\n\n        return ids\n\n    def decode(self, ids: list[int]) -&gt; str:\n        \"\"\"Detokenize IDs into text.\"\"\"\n        return self.tokenizer.decode(ids)\n\n    @property\n    def vocab_size(self) -&gt; int:\n        \"\"\"Vocabulary size from Hub tokenizer.\"\"\"\n        return self.tokenizer.vocab_size\n\n    @property\n    def bos_token_id(self):\n        \"\"\"BOS ID from Hub tokenizer.\"\"\"\n        return self.tokenizer.bos_token_id\n\n    @property\n    def eos_token_id(self):\n        \"\"\"EOS ID from Hub tokenizer.\"\"\"\n        return self.tokenizer.eos_token_id\n\n    def save_pretrained(self, save_directory: str):\n        \"\"\"Delegate saving to the underlying transformers tokenizer.\"\"\"\n        self.tokenizer.save_pretrained(save_directory)\n\n    def apply_chat_template(\n        self,\n        conversation: list[dict[str, str]],\n        tokenize: bool = True,\n        add_generation_prompt: bool = True,\n    ) -&gt; str | list[int]:\n        \"\"\"Apply the Hub tokenizer's chat template to a conversation.\"\"\"\n        if (\n            not hasattr(self.tokenizer, \"apply_chat_template\")\n            or not self.tokenizer.chat_template\n        ):\n            raise ValueError(\"Tokenizer does not support chat template\")\n\n        return self.tokenizer.apply_chat_template(\n            conversation,\n            tokenize=tokenize,\n            add_generation_prompt=add_generation_prompt,\n        )\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/huggingface/#optimus_dl.modules.tokenizer.implementations.huggingface.HFTokenizer.bos_token_id","title":"<code>bos_token_id</code>  <code>property</code>","text":"<p>BOS ID from Hub tokenizer.</p>"},{"location":"reference/modules/tokenizer/implementations/huggingface/#optimus_dl.modules.tokenizer.implementations.huggingface.HFTokenizer.eos_token_id","title":"<code>eos_token_id</code>  <code>property</code>","text":"<p>EOS ID from Hub tokenizer.</p>"},{"location":"reference/modules/tokenizer/implementations/huggingface/#optimus_dl.modules.tokenizer.implementations.huggingface.HFTokenizer.vocab_size","title":"<code>vocab_size</code>  <code>property</code>","text":"<p>Vocabulary size from Hub tokenizer.</p>"},{"location":"reference/modules/tokenizer/implementations/huggingface/#optimus_dl.modules.tokenizer.implementations.huggingface.HFTokenizer.apply_chat_template","title":"<code>apply_chat_template(conversation, tokenize=True, add_generation_prompt=True)</code>","text":"<p>Apply the Hub tokenizer's chat template to a conversation.</p> Source code in <code>optimus_dl/modules/tokenizer/implementations/huggingface.py</code> <pre><code>def apply_chat_template(\n    self,\n    conversation: list[dict[str, str]],\n    tokenize: bool = True,\n    add_generation_prompt: bool = True,\n) -&gt; str | list[int]:\n    \"\"\"Apply the Hub tokenizer's chat template to a conversation.\"\"\"\n    if (\n        not hasattr(self.tokenizer, \"apply_chat_template\")\n        or not self.tokenizer.chat_template\n    ):\n        raise ValueError(\"Tokenizer does not support chat template\")\n\n    return self.tokenizer.apply_chat_template(\n        conversation,\n        tokenize=tokenize,\n        add_generation_prompt=add_generation_prompt,\n    )\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/huggingface/#optimus_dl.modules.tokenizer.implementations.huggingface.HFTokenizer.decode","title":"<code>decode(ids)</code>","text":"<p>Detokenize IDs into text.</p> Source code in <code>optimus_dl/modules/tokenizer/implementations/huggingface.py</code> <pre><code>def decode(self, ids: list[int]) -&gt; str:\n    \"\"\"Detokenize IDs into text.\"\"\"\n    return self.tokenizer.decode(ids)\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/huggingface/#optimus_dl.modules.tokenizer.implementations.huggingface.HFTokenizer.encode","title":"<code>encode(text)</code>","text":"<p>Convert text to IDs using the Hub tokenizer.</p> Source code in <code>optimus_dl/modules/tokenizer/implementations/huggingface.py</code> <pre><code>def encode(self, text: str) -&gt; list[int]:\n    \"\"\"Convert text to IDs using the Hub tokenizer.\"\"\"\n    ids = self.tokenizer.encode(text, add_special_tokens=False)\n\n    if self.config.add_bos and self.bos_token_id is not None:\n        ids = [self.bos_token_id] + ids\n    if self.config.add_eos and self.eos_token_id is not None:\n        ids = ids + [self.eos_token_id]\n\n    return ids\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/huggingface/#optimus_dl.modules.tokenizer.implementations.huggingface.HFTokenizer.save_pretrained","title":"<code>save_pretrained(save_directory)</code>","text":"<p>Delegate saving to the underlying transformers tokenizer.</p> Source code in <code>optimus_dl/modules/tokenizer/implementations/huggingface.py</code> <pre><code>def save_pretrained(self, save_directory: str):\n    \"\"\"Delegate saving to the underlying transformers tokenizer.\"\"\"\n    self.tokenizer.save_pretrained(save_directory)\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/huggingface/#optimus_dl.modules.tokenizer.implementations.huggingface.HFTokenizerConfig","title":"<code>HFTokenizerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseTokenizerConfig</code></p> <p>Configuration for Hugging Face tokenizers.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <code>'gpt2'</code> <code>trust_remote_code</code> <code>bool</code> <code>False</code> Source code in <code>optimus_dl/modules/tokenizer/implementations/huggingface.py</code> <pre><code>@dataclass\nclass HFTokenizerConfig(BaseTokenizerConfig):\n    \"\"\"Configuration for Hugging Face tokenizers.\n\n    Attributes:\n        name: Name or path of the pretrained tokenizer on Hugging Face Hub.\n        trust_remote_code: If True, allows executing code from the model repo.\n    \"\"\"\n\n    name: str = \"gpt2\"\n    trust_remote_code: bool = False\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/inline_tokens/","title":"inline_tokens","text":""},{"location":"reference/modules/tokenizer/implementations/inline_tokens/#optimus_dl.modules.tokenizer.implementations.inline_tokens","title":"<code>optimus_dl.modules.tokenizer.implementations.inline_tokens</code>","text":""},{"location":"reference/modules/tokenizer/implementations/inline_tokens/#optimus_dl.modules.tokenizer.implementations.inline_tokens.InlineTokensTokenizer","title":"<code>InlineTokensTokenizer</code>","text":"<p>               Bases: <code>BaseTokenizer</code></p> <p>Inline sequence tokenizer based on an explicitly provided list of tokens.</p> <p>Uses regex-based greedy longest-match parsing to tokenize arbitrary strings without whitespace assumptions, handling unknown text chunks based on the configured strategy.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>InlineTokensTokenizerConfig</code> <p>Tokenizer configuration containing the vocabulary and UNK strategy.</p> required Source code in <code>optimus_dl/modules/tokenizer/implementations/inline_tokens.py</code> <pre><code>@register_tokenizer(\"inline_tokens_tokenizer\", InlineTokensTokenizerConfig)\nclass InlineTokensTokenizer(BaseTokenizer):\n    \"\"\"Inline sequence tokenizer based on an explicitly provided list of tokens.\n\n    Uses regex-based greedy longest-match parsing to tokenize arbitrary strings\n    without whitespace assumptions, handling unknown text chunks based on\n    the configured strategy.\n\n    Args:\n        config: Tokenizer configuration containing the vocabulary and UNK strategy.\n    \"\"\"\n\n    def __init__(self, config: InlineTokensTokenizerConfig, **kwargs):\n        super().__init__(config)\n        self.config = config\n        self.tokens = config.tokens\n\n        self._token_to_id = {token: i for i, token in enumerate(self.tokens)}\n\n        # Sort tokens by length (longest first) for greedy regex matching.\n        # This prevents partial matches if vocab contains both \"A\" and \"AB\".\n        sorted_tokens = sorted(self.tokens, key=len, reverse=True)\n        escaped_tokens = [re.escape(token) for token in sorted_tokens]\n\n        # A capturing group in re.split() keeps the matched tokens and\n        # returns the unmatched chunks between them.\n        self._tokenizer_pattern = re.compile(f\"({'|'.join(escaped_tokens)})\")\n\n        self._vocab_size = len(self.tokens)\n\n        self._bos_token_id = self._vocab_size\n        self._vocab_size += 1\n\n        self._eos_token_id = self._vocab_size\n        self._vocab_size += 1\n\n        self._unk_token_id = None\n        if config.unk_strategy == UnkStrategy.UNK:\n            self._unk_token_id = self._vocab_size\n            self._vocab_size += 1\n\n    def encode(self, text: str) -&gt; list[int]:\n        \"\"\"Convert text into token IDs and add special tokens using regex.\"\"\"\n        if self.config.add_bos:\n            token_ids = [self._bos_token_id]\n        else:\n            token_ids = []\n\n        # re.split() with a capturing group yields alternating un-matched and matched strings\n        for chunk in self._tokenizer_pattern.split(text):\n            if not chunk:\n                continue  # Ignore empty string remnants from splits\n\n            if chunk in self._token_to_id:\n                token_ids.append(self._token_to_id[chunk])\n            else:\n                # If a chunk doesn't match a vocab token, it's treated as unknown.\n                if self.config.unk_strategy == UnkStrategy.RAISE:\n                    raise ValueError(\n                        f\"Unknown token/characters encountered: '{chunk}', '{text}'\"\n                    )\n                elif self.config.unk_strategy == UnkStrategy.UNK:\n                    assert self._unk_token_id is not None\n                    token_ids.append(self._unk_token_id)\n                elif self.config.unk_strategy == UnkStrategy.IGNORE:\n                    continue\n\n        if self.config.add_eos:\n            token_ids.append(self._eos_token_id)\n        return token_ids\n\n    def decode(self, ids: list[int]) -&gt; str:\n        \"\"\"Filter out special IDs and decode back to a string.\"\"\"\n        decoded_tokens = []\n\n        for token_id in ids:\n            # Filter out BOS and EOS\n            if token_id in (self._bos_token_id, self._eos_token_id):\n                continue\n\n            # Handle UNK\n            if self._unk_token_id is not None and token_id == self._unk_token_id:\n                decoded_tokens.append(\"&lt;UNK&gt;\")\n                continue\n\n            # Decode standard tokens\n            if 0 &lt;= token_id &lt; len(self.tokens):\n                decoded_tokens.append(self.tokens[token_id])\n            else:\n                raise ValueError(f\"Invalid token ID: {token_id}\")\n\n        # Because we don't assume whitespace during tokenization, we just concatenate\n        return \"\".join(decoded_tokens)\n\n    @property\n    def vocab_size(self) -&gt; int:\n        \"\"\"Vocabulary size including BOS/EOS/UNK tokens.\"\"\"\n        return self._vocab_size\n\n    @property\n    def bos_token_id(self):\n        \"\"\"BOS token ID from config.\"\"\"\n        return self._bos_token_id\n\n    @property\n    def eos_token_id(self):\n        \"\"\"EOS token ID from config.\"\"\"\n        return self._eos_token_id\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/inline_tokens/#optimus_dl.modules.tokenizer.implementations.inline_tokens.InlineTokensTokenizer.bos_token_id","title":"<code>bos_token_id</code>  <code>property</code>","text":"<p>BOS token ID from config.</p>"},{"location":"reference/modules/tokenizer/implementations/inline_tokens/#optimus_dl.modules.tokenizer.implementations.inline_tokens.InlineTokensTokenizer.eos_token_id","title":"<code>eos_token_id</code>  <code>property</code>","text":"<p>EOS token ID from config.</p>"},{"location":"reference/modules/tokenizer/implementations/inline_tokens/#optimus_dl.modules.tokenizer.implementations.inline_tokens.InlineTokensTokenizer.vocab_size","title":"<code>vocab_size</code>  <code>property</code>","text":"<p>Vocabulary size including BOS/EOS/UNK tokens.</p>"},{"location":"reference/modules/tokenizer/implementations/inline_tokens/#optimus_dl.modules.tokenizer.implementations.inline_tokens.InlineTokensTokenizer.decode","title":"<code>decode(ids)</code>","text":"<p>Filter out special IDs and decode back to a string.</p> Source code in <code>optimus_dl/modules/tokenizer/implementations/inline_tokens.py</code> <pre><code>def decode(self, ids: list[int]) -&gt; str:\n    \"\"\"Filter out special IDs and decode back to a string.\"\"\"\n    decoded_tokens = []\n\n    for token_id in ids:\n        # Filter out BOS and EOS\n        if token_id in (self._bos_token_id, self._eos_token_id):\n            continue\n\n        # Handle UNK\n        if self._unk_token_id is not None and token_id == self._unk_token_id:\n            decoded_tokens.append(\"&lt;UNK&gt;\")\n            continue\n\n        # Decode standard tokens\n        if 0 &lt;= token_id &lt; len(self.tokens):\n            decoded_tokens.append(self.tokens[token_id])\n        else:\n            raise ValueError(f\"Invalid token ID: {token_id}\")\n\n    # Because we don't assume whitespace during tokenization, we just concatenate\n    return \"\".join(decoded_tokens)\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/inline_tokens/#optimus_dl.modules.tokenizer.implementations.inline_tokens.InlineTokensTokenizer.encode","title":"<code>encode(text)</code>","text":"<p>Convert text into token IDs and add special tokens using regex.</p> Source code in <code>optimus_dl/modules/tokenizer/implementations/inline_tokens.py</code> <pre><code>def encode(self, text: str) -&gt; list[int]:\n    \"\"\"Convert text into token IDs and add special tokens using regex.\"\"\"\n    if self.config.add_bos:\n        token_ids = [self._bos_token_id]\n    else:\n        token_ids = []\n\n    # re.split() with a capturing group yields alternating un-matched and matched strings\n    for chunk in self._tokenizer_pattern.split(text):\n        if not chunk:\n            continue  # Ignore empty string remnants from splits\n\n        if chunk in self._token_to_id:\n            token_ids.append(self._token_to_id[chunk])\n        else:\n            # If a chunk doesn't match a vocab token, it's treated as unknown.\n            if self.config.unk_strategy == UnkStrategy.RAISE:\n                raise ValueError(\n                    f\"Unknown token/characters encountered: '{chunk}', '{text}'\"\n                )\n            elif self.config.unk_strategy == UnkStrategy.UNK:\n                assert self._unk_token_id is not None\n                token_ids.append(self._unk_token_id)\n            elif self.config.unk_strategy == UnkStrategy.IGNORE:\n                continue\n\n    if self.config.add_eos:\n        token_ids.append(self._eos_token_id)\n    return token_ids\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/inline_tokens/#optimus_dl.modules.tokenizer.implementations.inline_tokens.InlineTokensTokenizerConfig","title":"<code>InlineTokensTokenizerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseTokenizerConfig</code></p> <p>Configuration for explicitly specified tokens tokenizer.</p> <p>Attributes:</p> Name Type Description <code>bos_token</code> <code>list[str]</code> <p>Beginning-of-Sequence token.</p> <code>eos_token</code> <code>list[str]</code> <p>End-of-Sequence token.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[str]</code> <code>'???'</code> <code>unk_strategy</code> <code>UnkStrategy</code> <code>&lt;UnkStrategy.RAISE: 'raise'&gt;</code> Source code in <code>optimus_dl/modules/tokenizer/implementations/inline_tokens.py</code> <pre><code>@dataclass\nclass InlineTokensTokenizerConfig(BaseTokenizerConfig):\n    \"\"\"Configuration for explicitly specified tokens tokenizer.\n\n    Attributes:\n        tokens: List of all tokens\n        bos_token: Beginning-of-Sequence token.\n        eos_token: End-of-Sequence token.\n        unk_strategy: How to deal with unknown tokens. Can be ignore, raise or unk.\n        Unk will replace the unknown token with the unk token.\n    \"\"\"\n\n    tokens: list[str] = MISSING\n    unk_strategy: UnkStrategy = UnkStrategy.RAISE\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/tiktoken/","title":"tiktoken","text":""},{"location":"reference/modules/tokenizer/implementations/tiktoken/#optimus_dl.modules.tokenizer.implementations.tiktoken","title":"<code>optimus_dl.modules.tokenizer.implementations.tiktoken</code>","text":""},{"location":"reference/modules/tokenizer/implementations/tiktoken/#optimus_dl.modules.tokenizer.implementations.tiktoken.TiktokenConfig","title":"<code>TiktokenConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseTokenizerConfig</code></p> <p>Configuration for Tiktoken tokenizers.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <code>'gpt2'</code> Source code in <code>optimus_dl/modules/tokenizer/implementations/tiktoken.py</code> <pre><code>@dataclass\nclass TiktokenConfig(BaseTokenizerConfig):\n    \"\"\"Configuration for Tiktoken tokenizers.\n\n    Attributes:\n        name: Name of the tiktoken encoding (e.g., 'gpt2', 'cl100k_base').\n    \"\"\"\n\n    name: str = \"gpt2\"\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/tiktoken/#optimus_dl.modules.tokenizer.implementations.tiktoken.TiktokenTokenizer","title":"<code>TiktokenTokenizer</code>","text":"<p>               Bases: <code>BaseTokenizer</code></p> <p>Wrapper for OpenAI's tiktoken library.</p> <p>Provides extremely fast Byte-Pair Encoding (BPE) for GPT-style models.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TiktokenConfig</code> <p>Tiktoken tokenizer configuration.</p> required Source code in <code>optimus_dl/modules/tokenizer/implementations/tiktoken.py</code> <pre><code>@register_tokenizer(\"tiktoken\", TiktokenConfig)\nclass TiktokenTokenizer(BaseTokenizer):\n    \"\"\"Wrapper for OpenAI's tiktoken library.\n\n    Provides extremely fast Byte-Pair Encoding (BPE) for GPT-style models.\n\n    Args:\n        config: Tiktoken tokenizer configuration.\n    \"\"\"\n\n    def __init__(self, config: TiktokenConfig, **kwargs):\n        super().__init__(config)\n        self.encoding = tiktoken.get_encoding(config.name)\n\n    def encode(self, text: str) -&gt; list[int]:\n        \"\"\"Convert text to IDs, allowing all special tokens.\"\"\"\n        # Using allowed_special=\"all\" to permit special tokens in input text\n        ids = self.encoding.encode(text, allowed_special=\"all\")\n\n        if self.config.add_bos and self.bos_token_id is not None:\n            ids = [self.bos_token_id] + ids\n\n        if self.config.add_eos and self.eos_token_id is not None:\n            ids = ids + [self.eos_token_id]\n\n        return ids\n\n    def decode(self, ids: list[int]) -&gt; str:\n        \"\"\"Detokenize IDs into text.\"\"\"\n        return self.encoding.decode(ids)\n\n    @property\n    def vocab_size(self) -&gt; int:\n        \"\"\"Total number of tokens in the encoding.\"\"\"\n        return self.encoding.n_vocab\n\n    @property\n    def eos_token_id(self):\n        \"\"\"EOT token ID used as EOS.\"\"\"\n        return self.encoding.eot_token\n\n    @property\n    def bos_token_id(self):\n        \"\"\"EOT token ID used as BOS (tiktoken default).\"\"\"\n        return self.encoding.eot_token\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/tiktoken/#optimus_dl.modules.tokenizer.implementations.tiktoken.TiktokenTokenizer.bos_token_id","title":"<code>bos_token_id</code>  <code>property</code>","text":"<p>EOT token ID used as BOS (tiktoken default).</p>"},{"location":"reference/modules/tokenizer/implementations/tiktoken/#optimus_dl.modules.tokenizer.implementations.tiktoken.TiktokenTokenizer.eos_token_id","title":"<code>eos_token_id</code>  <code>property</code>","text":"<p>EOT token ID used as EOS.</p>"},{"location":"reference/modules/tokenizer/implementations/tiktoken/#optimus_dl.modules.tokenizer.implementations.tiktoken.TiktokenTokenizer.vocab_size","title":"<code>vocab_size</code>  <code>property</code>","text":"<p>Total number of tokens in the encoding.</p>"},{"location":"reference/modules/tokenizer/implementations/tiktoken/#optimus_dl.modules.tokenizer.implementations.tiktoken.TiktokenTokenizer.decode","title":"<code>decode(ids)</code>","text":"<p>Detokenize IDs into text.</p> Source code in <code>optimus_dl/modules/tokenizer/implementations/tiktoken.py</code> <pre><code>def decode(self, ids: list[int]) -&gt; str:\n    \"\"\"Detokenize IDs into text.\"\"\"\n    return self.encoding.decode(ids)\n</code></pre>"},{"location":"reference/modules/tokenizer/implementations/tiktoken/#optimus_dl.modules.tokenizer.implementations.tiktoken.TiktokenTokenizer.encode","title":"<code>encode(text)</code>","text":"<p>Convert text to IDs, allowing all special tokens.</p> Source code in <code>optimus_dl/modules/tokenizer/implementations/tiktoken.py</code> <pre><code>def encode(self, text: str) -&gt; list[int]:\n    \"\"\"Convert text to IDs, allowing all special tokens.\"\"\"\n    # Using allowed_special=\"all\" to permit special tokens in input text\n    ids = self.encoding.encode(text, allowed_special=\"all\")\n\n    if self.config.add_bos and self.bos_token_id is not None:\n        ids = [self.bos_token_id] + ids\n\n    if self.config.add_eos and self.eos_token_id is not None:\n        ids = ids + [self.eos_token_id]\n\n    return ids\n</code></pre>"},{"location":"reference/recipe/","title":"Index","text":""},{"location":"reference/recipe/#optimus_dl.recipe","title":"<code>optimus_dl.recipe</code>","text":""},{"location":"reference/recipe/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>eval</code>: Evaluation recipe for LLM Baselines models.</li> <li><code>metrics</code>: Metrics evaluation recipe module.</li> <li><code>mixins</code>: Shared mixins that can be used across different recipe types.</li> <li><code>pretokenize</code>: </li> <li><code>serve</code>: </li> <li><code>train</code>: </li> </ul>"},{"location":"reference/recipe/eval/","title":"Index","text":""},{"location":"reference/recipe/eval/#optimus_dl.recipe.eval","title":"<code>optimus_dl.recipe.eval</code>","text":"<p>Evaluation recipe for LLM Baselines models.</p>"},{"location":"reference/recipe/eval/#optimus_dl.recipe.eval.EvalConfig","title":"<code>EvalConfig</code>  <code>dataclass</code>","text":"<p>Main evaluation configuration.</p> <p>Parameters:</p> Name Type Description Default <code>common</code> <code>EvalCommonConfig</code> <p>Common evaluation configuration.</p> <code>&lt;dynamic&gt;</code> <code>lm_eval</code> <code>LMEvalConfig</code> <p>Configuration for lm_eval harness evaluation.</p> <code>&lt;dynamic&gt;</code> Source code in <code>optimus_dl/recipe/eval/config.py</code> <pre><code>@dataclass\nclass EvalConfig:\n    \"\"\"Main evaluation configuration.\"\"\"\n\n    common: EvalCommonConfig = field(default_factory=EvalCommonConfig)\n    lm_eval: LMEvalConfig = field(default_factory=LMEvalConfig)\n\n    def __post_init__(self):\n        \"\"\"Validate configuration.\"\"\"\n        if self.common.checkpoint_path == MISSING:\n            raise ValueError(\"checkpoint_path is required\")\n\n        # Convert checkpoint_path to Path for validation\n        checkpoint_path = Path(self.common.checkpoint_path)\n        if not (checkpoint_path.exists() or checkpoint_path.parent.exists()):\n            raise ValueError(f\"Checkpoint path does not exist: {checkpoint_path}\")\n</code></pre>"},{"location":"reference/recipe/eval/#optimus_dl.recipe.eval.EvalConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate configuration.</p> Source code in <code>optimus_dl/recipe/eval/config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate configuration.\"\"\"\n    if self.common.checkpoint_path == MISSING:\n        raise ValueError(\"checkpoint_path is required\")\n\n    # Convert checkpoint_path to Path for validation\n    checkpoint_path = Path(self.common.checkpoint_path)\n    if not (checkpoint_path.exists() or checkpoint_path.parent.exists()):\n        raise ValueError(f\"Checkpoint path does not exist: {checkpoint_path}\")\n</code></pre>"},{"location":"reference/recipe/eval/#optimus_dl.recipe.eval.EvalRecipe","title":"<code>EvalRecipe</code>","text":"<p>Recipe for evaluating LLM Baselines models using lm_eval harness.</p> <p>Orchestrates the evaluation process: 1.  Loads a model from a checkpoint or configuration. 2.  Wraps it in a <code>LLMBaselinesModel</code> adapter for <code>lm_eval</code>. 3.  Runs specified evaluation tasks (e.g., Hellaswag, MMLU). 4.  Saves results to JSON.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>EvalConfig</code> <p>Evaluation configuration.</p> required Source code in <code>optimus_dl/recipe/eval/base.py</code> <pre><code>class EvalRecipe:\n    \"\"\"Recipe for evaluating LLM Baselines models using lm_eval harness.\n\n    Orchestrates the evaluation process:\n    1.  Loads a model from a checkpoint or configuration.\n    2.  Wraps it in a `LLMBaselinesModel` adapter for `lm_eval`.\n    3.  Runs specified evaluation tasks (e.g., Hellaswag, MMLU).\n    4.  Saves results to JSON.\n\n    Args:\n        cfg: Evaluation configuration.\n    \"\"\"\n\n    def __init__(self, cfg: EvalConfig):\n        \"\"\"Initialize evaluation recipe.\"\"\"\n        self.cfg = cfg\n        self.model = None\n        self.tokenizer = None\n\n        # Initialize builders using composition\n        # ModelBuilder needs a dummy config, but it won't be used since we use build_model_from_checkpoint\n        self.model_builder = ModelBuilder(None, [])\n        self.checkpoint_manager = CheckpointManager(None)\n\n        # Direct tokenizer build config\n        self.tokenizer_config = cfg.common.tokenizer\n\n    def build_eval_model(self, collective: Collective) -&gt; LLMBaselinesModel:\n        \"\"\"Build and load the model for evaluation.\n\n        Loads the model from the configured checkpoint path, initializes the\n        tokenizer, and wraps them in the `LLMBaselinesModel` adapter.\n\n        Args:\n            collective: Distributed collective for model initialization context.\n\n        Returns:\n            An initialized `LLMBaselinesModel` ready for `lm_eval`.\n        \"\"\"\n        if self.model is None:\n            assert (self.cfg.common.checkpoint_path is not None) ^ (\n                self.cfg.common.model is not None\n            ), \"Either checkpoint_path or model must be specified, but not both\"\n\n            device = collective.default_device\n            if self.cfg.common.checkpoint_path is not None:\n                logger.info(\n                    f\"Loading model from checkpoint: {self.cfg.common.checkpoint_path}\"\n                )\n                base_model, _ = self.checkpoint_manager.build_model_from_checkpoint(\n                    checkpoint_path=self.cfg.common.checkpoint_path, device=device\n                )\n            else:\n                logger.info(\"Building model from config\")\n                base_model = self.model_builder.build_model(\n                    model_config=self.cfg.common.model,\n                    collective=collective,\n                )\n\n            # Build tokenizer directly\n            self.tokenizer = build_tokenizer(self.tokenizer_config)\n\n            # Wrap in LLMBaselinesModel for lm_eval compatibility\n            base_model.eval()\n            self.model = LLMBaselinesModel(\n                model=base_model.to(device),\n                tokenizer=self.tokenizer,\n                tokenizer_config=self.tokenizer_config,\n                device=device,\n            )\n\n        return self.model\n\n    def run_lm_eval(self) -&gt; dict:\n        \"\"\"Run standard benchmarks using the lm_eval harness.\n\n        Sets up the distributed environment (even if single-device), builds the\n        model, executes the tasks, and saves results.\n\n        Returns:\n            Dictionary containing evaluation metrics and results.\n        \"\"\"\n        try:\n            from lm_eval import evaluator\n        except ImportError as err:\n            raise ImportError(\n                \"lm_eval is required for evaluation. Install with: pip install lm_eval\"\n            ) from err\n\n        # Build model\n        collective = build_best_collective(\n            device=None if self.cfg.common.use_gpu else torch.device(\"cpu\"),\n            config=DistributedConfig(),\n        )\n        model = self.build_eval_model(collective=collective)\n\n        logger.info(f\"Running lm_eval on tasks: {self.cfg.lm_eval.tasks}\")\n        logger.info(f\"Few-shot examples: {self.cfg.lm_eval.num_fewshot}\")\n\n        # Convert tasks to proper format for lm_eval\n        raw_tasks = self.cfg.lm_eval.tasks\n        if isinstance(raw_tasks, str):\n            tasks = [raw_tasks]\n        else:\n            # Convert to list of strings, handling any nested structure\n            tasks = []\n            for task in raw_tasks:\n                if isinstance(task, str):\n                    tasks.append(task)\n                else:\n                    tasks.append(str(task))\n\n        # Run evaluation\n        results = evaluator.simple_evaluate(\n            model=model,\n            tasks=tasks,\n            num_fewshot=self.cfg.lm_eval.num_fewshot,\n            batch_size=self.cfg.lm_eval.batch_size,\n            limit=self.cfg.lm_eval.limit,\n            device=collective.default_device,\n            use_cache=None,  # Disable caching for now\n            # verbosity=\"INFO\",\n        )\n\n        if results is None:\n            raise RuntimeError(\"Evaluation returned no results\")\n\n        # Save results if output path is specified\n        if self.cfg.lm_eval.output_path:\n            import json\n\n            output_path = Path(self.cfg.lm_eval.output_path)\n            output_path.parent.mkdir(parents=True, exist_ok=True)\n\n            with open(output_path, \"w\") as f:\n                json.dump(results, f, indent=2)\n            logger.info(f\"Results saved to: {output_path}\")\n\n        return results\n</code></pre>"},{"location":"reference/recipe/eval/#optimus_dl.recipe.eval.EvalRecipe.__init__","title":"<code>__init__(cfg)</code>","text":"<p>Initialize evaluation recipe.</p> Source code in <code>optimus_dl/recipe/eval/base.py</code> <pre><code>def __init__(self, cfg: EvalConfig):\n    \"\"\"Initialize evaluation recipe.\"\"\"\n    self.cfg = cfg\n    self.model = None\n    self.tokenizer = None\n\n    # Initialize builders using composition\n    # ModelBuilder needs a dummy config, but it won't be used since we use build_model_from_checkpoint\n    self.model_builder = ModelBuilder(None, [])\n    self.checkpoint_manager = CheckpointManager(None)\n\n    # Direct tokenizer build config\n    self.tokenizer_config = cfg.common.tokenizer\n</code></pre>"},{"location":"reference/recipe/eval/#optimus_dl.recipe.eval.EvalRecipe.build_eval_model","title":"<code>build_eval_model(collective)</code>","text":"<p>Build and load the model for evaluation.</p> <p>Loads the model from the configured checkpoint path, initializes the tokenizer, and wraps them in the <code>LLMBaselinesModel</code> adapter.</p> <p>Parameters:</p> Name Type Description Default <code>collective</code> <code>Collective</code> <p>Distributed collective for model initialization context.</p> required <p>Returns:</p> Type Description <code>LLMBaselinesModel</code> <p>An initialized <code>LLMBaselinesModel</code> ready for <code>lm_eval</code>.</p> Source code in <code>optimus_dl/recipe/eval/base.py</code> <pre><code>def build_eval_model(self, collective: Collective) -&gt; LLMBaselinesModel:\n    \"\"\"Build and load the model for evaluation.\n\n    Loads the model from the configured checkpoint path, initializes the\n    tokenizer, and wraps them in the `LLMBaselinesModel` adapter.\n\n    Args:\n        collective: Distributed collective for model initialization context.\n\n    Returns:\n        An initialized `LLMBaselinesModel` ready for `lm_eval`.\n    \"\"\"\n    if self.model is None:\n        assert (self.cfg.common.checkpoint_path is not None) ^ (\n            self.cfg.common.model is not None\n        ), \"Either checkpoint_path or model must be specified, but not both\"\n\n        device = collective.default_device\n        if self.cfg.common.checkpoint_path is not None:\n            logger.info(\n                f\"Loading model from checkpoint: {self.cfg.common.checkpoint_path}\"\n            )\n            base_model, _ = self.checkpoint_manager.build_model_from_checkpoint(\n                checkpoint_path=self.cfg.common.checkpoint_path, device=device\n            )\n        else:\n            logger.info(\"Building model from config\")\n            base_model = self.model_builder.build_model(\n                model_config=self.cfg.common.model,\n                collective=collective,\n            )\n\n        # Build tokenizer directly\n        self.tokenizer = build_tokenizer(self.tokenizer_config)\n\n        # Wrap in LLMBaselinesModel for lm_eval compatibility\n        base_model.eval()\n        self.model = LLMBaselinesModel(\n            model=base_model.to(device),\n            tokenizer=self.tokenizer,\n            tokenizer_config=self.tokenizer_config,\n            device=device,\n        )\n\n    return self.model\n</code></pre>"},{"location":"reference/recipe/eval/#optimus_dl.recipe.eval.EvalRecipe.run_lm_eval","title":"<code>run_lm_eval()</code>","text":"<p>Run standard benchmarks using the lm_eval harness.</p> <p>Sets up the distributed environment (even if single-device), builds the model, executes the tasks, and saves results.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing evaluation metrics and results.</p> Source code in <code>optimus_dl/recipe/eval/base.py</code> <pre><code>def run_lm_eval(self) -&gt; dict:\n    \"\"\"Run standard benchmarks using the lm_eval harness.\n\n    Sets up the distributed environment (even if single-device), builds the\n    model, executes the tasks, and saves results.\n\n    Returns:\n        Dictionary containing evaluation metrics and results.\n    \"\"\"\n    try:\n        from lm_eval import evaluator\n    except ImportError as err:\n        raise ImportError(\n            \"lm_eval is required for evaluation. Install with: pip install lm_eval\"\n        ) from err\n\n    # Build model\n    collective = build_best_collective(\n        device=None if self.cfg.common.use_gpu else torch.device(\"cpu\"),\n        config=DistributedConfig(),\n    )\n    model = self.build_eval_model(collective=collective)\n\n    logger.info(f\"Running lm_eval on tasks: {self.cfg.lm_eval.tasks}\")\n    logger.info(f\"Few-shot examples: {self.cfg.lm_eval.num_fewshot}\")\n\n    # Convert tasks to proper format for lm_eval\n    raw_tasks = self.cfg.lm_eval.tasks\n    if isinstance(raw_tasks, str):\n        tasks = [raw_tasks]\n    else:\n        # Convert to list of strings, handling any nested structure\n        tasks = []\n        for task in raw_tasks:\n            if isinstance(task, str):\n                tasks.append(task)\n            else:\n                tasks.append(str(task))\n\n    # Run evaluation\n    results = evaluator.simple_evaluate(\n        model=model,\n        tasks=tasks,\n        num_fewshot=self.cfg.lm_eval.num_fewshot,\n        batch_size=self.cfg.lm_eval.batch_size,\n        limit=self.cfg.lm_eval.limit,\n        device=collective.default_device,\n        use_cache=None,  # Disable caching for now\n        # verbosity=\"INFO\",\n    )\n\n    if results is None:\n        raise RuntimeError(\"Evaluation returned no results\")\n\n    # Save results if output path is specified\n    if self.cfg.lm_eval.output_path:\n        import json\n\n        output_path = Path(self.cfg.lm_eval.output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        with open(output_path, \"w\") as f:\n            json.dump(results, f, indent=2)\n        logger.info(f\"Results saved to: {output_path}\")\n\n    return results\n</code></pre>"},{"location":"reference/recipe/eval/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>base</code>: Evaluation recipe for LLM Baselines models.</li> <li><code>config</code>: Configuration for evaluation recipe.</li> </ul>"},{"location":"reference/recipe/eval/base/","title":"base","text":""},{"location":"reference/recipe/eval/base/#optimus_dl.recipe.eval.base","title":"<code>optimus_dl.recipe.eval.base</code>","text":"<p>Evaluation recipe for LLM Baselines models.</p>"},{"location":"reference/recipe/eval/base/#optimus_dl.recipe.eval.base.EvalRecipe","title":"<code>EvalRecipe</code>","text":"<p>Recipe for evaluating LLM Baselines models using lm_eval harness.</p> <p>Orchestrates the evaluation process: 1.  Loads a model from a checkpoint or configuration. 2.  Wraps it in a <code>LLMBaselinesModel</code> adapter for <code>lm_eval</code>. 3.  Runs specified evaluation tasks (e.g., Hellaswag, MMLU). 4.  Saves results to JSON.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>EvalConfig</code> <p>Evaluation configuration.</p> required Source code in <code>optimus_dl/recipe/eval/base.py</code> <pre><code>class EvalRecipe:\n    \"\"\"Recipe for evaluating LLM Baselines models using lm_eval harness.\n\n    Orchestrates the evaluation process:\n    1.  Loads a model from a checkpoint or configuration.\n    2.  Wraps it in a `LLMBaselinesModel` adapter for `lm_eval`.\n    3.  Runs specified evaluation tasks (e.g., Hellaswag, MMLU).\n    4.  Saves results to JSON.\n\n    Args:\n        cfg: Evaluation configuration.\n    \"\"\"\n\n    def __init__(self, cfg: EvalConfig):\n        \"\"\"Initialize evaluation recipe.\"\"\"\n        self.cfg = cfg\n        self.model = None\n        self.tokenizer = None\n\n        # Initialize builders using composition\n        # ModelBuilder needs a dummy config, but it won't be used since we use build_model_from_checkpoint\n        self.model_builder = ModelBuilder(None, [])\n        self.checkpoint_manager = CheckpointManager(None)\n\n        # Direct tokenizer build config\n        self.tokenizer_config = cfg.common.tokenizer\n\n    def build_eval_model(self, collective: Collective) -&gt; LLMBaselinesModel:\n        \"\"\"Build and load the model for evaluation.\n\n        Loads the model from the configured checkpoint path, initializes the\n        tokenizer, and wraps them in the `LLMBaselinesModel` adapter.\n\n        Args:\n            collective: Distributed collective for model initialization context.\n\n        Returns:\n            An initialized `LLMBaselinesModel` ready for `lm_eval`.\n        \"\"\"\n        if self.model is None:\n            assert (self.cfg.common.checkpoint_path is not None) ^ (\n                self.cfg.common.model is not None\n            ), \"Either checkpoint_path or model must be specified, but not both\"\n\n            device = collective.default_device\n            if self.cfg.common.checkpoint_path is not None:\n                logger.info(\n                    f\"Loading model from checkpoint: {self.cfg.common.checkpoint_path}\"\n                )\n                base_model, _ = self.checkpoint_manager.build_model_from_checkpoint(\n                    checkpoint_path=self.cfg.common.checkpoint_path, device=device\n                )\n            else:\n                logger.info(\"Building model from config\")\n                base_model = self.model_builder.build_model(\n                    model_config=self.cfg.common.model,\n                    collective=collective,\n                )\n\n            # Build tokenizer directly\n            self.tokenizer = build_tokenizer(self.tokenizer_config)\n\n            # Wrap in LLMBaselinesModel for lm_eval compatibility\n            base_model.eval()\n            self.model = LLMBaselinesModel(\n                model=base_model.to(device),\n                tokenizer=self.tokenizer,\n                tokenizer_config=self.tokenizer_config,\n                device=device,\n            )\n\n        return self.model\n\n    def run_lm_eval(self) -&gt; dict:\n        \"\"\"Run standard benchmarks using the lm_eval harness.\n\n        Sets up the distributed environment (even if single-device), builds the\n        model, executes the tasks, and saves results.\n\n        Returns:\n            Dictionary containing evaluation metrics and results.\n        \"\"\"\n        try:\n            from lm_eval import evaluator\n        except ImportError as err:\n            raise ImportError(\n                \"lm_eval is required for evaluation. Install with: pip install lm_eval\"\n            ) from err\n\n        # Build model\n        collective = build_best_collective(\n            device=None if self.cfg.common.use_gpu else torch.device(\"cpu\"),\n            config=DistributedConfig(),\n        )\n        model = self.build_eval_model(collective=collective)\n\n        logger.info(f\"Running lm_eval on tasks: {self.cfg.lm_eval.tasks}\")\n        logger.info(f\"Few-shot examples: {self.cfg.lm_eval.num_fewshot}\")\n\n        # Convert tasks to proper format for lm_eval\n        raw_tasks = self.cfg.lm_eval.tasks\n        if isinstance(raw_tasks, str):\n            tasks = [raw_tasks]\n        else:\n            # Convert to list of strings, handling any nested structure\n            tasks = []\n            for task in raw_tasks:\n                if isinstance(task, str):\n                    tasks.append(task)\n                else:\n                    tasks.append(str(task))\n\n        # Run evaluation\n        results = evaluator.simple_evaluate(\n            model=model,\n            tasks=tasks,\n            num_fewshot=self.cfg.lm_eval.num_fewshot,\n            batch_size=self.cfg.lm_eval.batch_size,\n            limit=self.cfg.lm_eval.limit,\n            device=collective.default_device,\n            use_cache=None,  # Disable caching for now\n            # verbosity=\"INFO\",\n        )\n\n        if results is None:\n            raise RuntimeError(\"Evaluation returned no results\")\n\n        # Save results if output path is specified\n        if self.cfg.lm_eval.output_path:\n            import json\n\n            output_path = Path(self.cfg.lm_eval.output_path)\n            output_path.parent.mkdir(parents=True, exist_ok=True)\n\n            with open(output_path, \"w\") as f:\n                json.dump(results, f, indent=2)\n            logger.info(f\"Results saved to: {output_path}\")\n\n        return results\n</code></pre>"},{"location":"reference/recipe/eval/base/#optimus_dl.recipe.eval.base.EvalRecipe.__init__","title":"<code>__init__(cfg)</code>","text":"<p>Initialize evaluation recipe.</p> Source code in <code>optimus_dl/recipe/eval/base.py</code> <pre><code>def __init__(self, cfg: EvalConfig):\n    \"\"\"Initialize evaluation recipe.\"\"\"\n    self.cfg = cfg\n    self.model = None\n    self.tokenizer = None\n\n    # Initialize builders using composition\n    # ModelBuilder needs a dummy config, but it won't be used since we use build_model_from_checkpoint\n    self.model_builder = ModelBuilder(None, [])\n    self.checkpoint_manager = CheckpointManager(None)\n\n    # Direct tokenizer build config\n    self.tokenizer_config = cfg.common.tokenizer\n</code></pre>"},{"location":"reference/recipe/eval/base/#optimus_dl.recipe.eval.base.EvalRecipe.build_eval_model","title":"<code>build_eval_model(collective)</code>","text":"<p>Build and load the model for evaluation.</p> <p>Loads the model from the configured checkpoint path, initializes the tokenizer, and wraps them in the <code>LLMBaselinesModel</code> adapter.</p> <p>Parameters:</p> Name Type Description Default <code>collective</code> <code>Collective</code> <p>Distributed collective for model initialization context.</p> required <p>Returns:</p> Type Description <code>LLMBaselinesModel</code> <p>An initialized <code>LLMBaselinesModel</code> ready for <code>lm_eval</code>.</p> Source code in <code>optimus_dl/recipe/eval/base.py</code> <pre><code>def build_eval_model(self, collective: Collective) -&gt; LLMBaselinesModel:\n    \"\"\"Build and load the model for evaluation.\n\n    Loads the model from the configured checkpoint path, initializes the\n    tokenizer, and wraps them in the `LLMBaselinesModel` adapter.\n\n    Args:\n        collective: Distributed collective for model initialization context.\n\n    Returns:\n        An initialized `LLMBaselinesModel` ready for `lm_eval`.\n    \"\"\"\n    if self.model is None:\n        assert (self.cfg.common.checkpoint_path is not None) ^ (\n            self.cfg.common.model is not None\n        ), \"Either checkpoint_path or model must be specified, but not both\"\n\n        device = collective.default_device\n        if self.cfg.common.checkpoint_path is not None:\n            logger.info(\n                f\"Loading model from checkpoint: {self.cfg.common.checkpoint_path}\"\n            )\n            base_model, _ = self.checkpoint_manager.build_model_from_checkpoint(\n                checkpoint_path=self.cfg.common.checkpoint_path, device=device\n            )\n        else:\n            logger.info(\"Building model from config\")\n            base_model = self.model_builder.build_model(\n                model_config=self.cfg.common.model,\n                collective=collective,\n            )\n\n        # Build tokenizer directly\n        self.tokenizer = build_tokenizer(self.tokenizer_config)\n\n        # Wrap in LLMBaselinesModel for lm_eval compatibility\n        base_model.eval()\n        self.model = LLMBaselinesModel(\n            model=base_model.to(device),\n            tokenizer=self.tokenizer,\n            tokenizer_config=self.tokenizer_config,\n            device=device,\n        )\n\n    return self.model\n</code></pre>"},{"location":"reference/recipe/eval/base/#optimus_dl.recipe.eval.base.EvalRecipe.run_lm_eval","title":"<code>run_lm_eval()</code>","text":"<p>Run standard benchmarks using the lm_eval harness.</p> <p>Sets up the distributed environment (even if single-device), builds the model, executes the tasks, and saves results.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing evaluation metrics and results.</p> Source code in <code>optimus_dl/recipe/eval/base.py</code> <pre><code>def run_lm_eval(self) -&gt; dict:\n    \"\"\"Run standard benchmarks using the lm_eval harness.\n\n    Sets up the distributed environment (even if single-device), builds the\n    model, executes the tasks, and saves results.\n\n    Returns:\n        Dictionary containing evaluation metrics and results.\n    \"\"\"\n    try:\n        from lm_eval import evaluator\n    except ImportError as err:\n        raise ImportError(\n            \"lm_eval is required for evaluation. Install with: pip install lm_eval\"\n        ) from err\n\n    # Build model\n    collective = build_best_collective(\n        device=None if self.cfg.common.use_gpu else torch.device(\"cpu\"),\n        config=DistributedConfig(),\n    )\n    model = self.build_eval_model(collective=collective)\n\n    logger.info(f\"Running lm_eval on tasks: {self.cfg.lm_eval.tasks}\")\n    logger.info(f\"Few-shot examples: {self.cfg.lm_eval.num_fewshot}\")\n\n    # Convert tasks to proper format for lm_eval\n    raw_tasks = self.cfg.lm_eval.tasks\n    if isinstance(raw_tasks, str):\n        tasks = [raw_tasks]\n    else:\n        # Convert to list of strings, handling any nested structure\n        tasks = []\n        for task in raw_tasks:\n            if isinstance(task, str):\n                tasks.append(task)\n            else:\n                tasks.append(str(task))\n\n    # Run evaluation\n    results = evaluator.simple_evaluate(\n        model=model,\n        tasks=tasks,\n        num_fewshot=self.cfg.lm_eval.num_fewshot,\n        batch_size=self.cfg.lm_eval.batch_size,\n        limit=self.cfg.lm_eval.limit,\n        device=collective.default_device,\n        use_cache=None,  # Disable caching for now\n        # verbosity=\"INFO\",\n    )\n\n    if results is None:\n        raise RuntimeError(\"Evaluation returned no results\")\n\n    # Save results if output path is specified\n    if self.cfg.lm_eval.output_path:\n        import json\n\n        output_path = Path(self.cfg.lm_eval.output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        with open(output_path, \"w\") as f:\n            json.dump(results, f, indent=2)\n        logger.info(f\"Results saved to: {output_path}\")\n\n    return results\n</code></pre>"},{"location":"reference/recipe/eval/config/","title":"config","text":""},{"location":"reference/recipe/eval/config/#optimus_dl.recipe.eval.config","title":"<code>optimus_dl.recipe.eval.config</code>","text":"<p>Configuration for evaluation recipe.</p>"},{"location":"reference/recipe/eval/config/#optimus_dl.recipe.eval.config.EvalCommonConfig","title":"<code>EvalCommonConfig</code>  <code>dataclass</code>","text":"<p>Common evaluation configuration.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>str | None</code> <p>Path to checkpoint directory or metadata file</p> <code>'???'</code> <code>model</code> <code>Any</code> <p>Model to build (if you want to load model not from checkpoint)</p> <code>None</code> <code>use_gpu</code> <code>bool</code> <p>Use gpu if available</p> <code>True</code> <code>seed</code> <code>int</code> <p>Random seed for evaluation</p> <code>42</code> <code>tokenizer</code> <code>Any</code> <code>'???'</code> Source code in <code>optimus_dl/recipe/eval/config.py</code> <pre><code>@dataclass\nclass EvalCommonConfig:\n    \"\"\"Common evaluation configuration.\"\"\"\n\n    checkpoint_path: str | None = field(\n        default=MISSING,\n        metadata={\"description\": \"Path to checkpoint directory or metadata file\"},\n    )\n    model: Any = field(\n        default=None,\n        metadata={\n            \"description\": \"Model to build (if you want to load model not from checkpoint)\"\n        },\n    )\n    use_gpu: bool = field(\n        default=True,\n        metadata={\"description\": \"Use gpu if available\"},\n    )\n    seed: int = field(\n        default=42, metadata={\"description\": \"Random seed for evaluation\"}\n    )\n    tokenizer: Any = MISSING\n</code></pre>"},{"location":"reference/recipe/eval/config/#optimus_dl.recipe.eval.config.EvalConfig","title":"<code>EvalConfig</code>  <code>dataclass</code>","text":"<p>Main evaluation configuration.</p> <p>Parameters:</p> Name Type Description Default <code>common</code> <code>EvalCommonConfig</code> <p>Common evaluation configuration.</p> <code>&lt;dynamic&gt;</code> <code>lm_eval</code> <code>LMEvalConfig</code> <p>Configuration for lm_eval harness evaluation.</p> <code>&lt;dynamic&gt;</code> Source code in <code>optimus_dl/recipe/eval/config.py</code> <pre><code>@dataclass\nclass EvalConfig:\n    \"\"\"Main evaluation configuration.\"\"\"\n\n    common: EvalCommonConfig = field(default_factory=EvalCommonConfig)\n    lm_eval: LMEvalConfig = field(default_factory=LMEvalConfig)\n\n    def __post_init__(self):\n        \"\"\"Validate configuration.\"\"\"\n        if self.common.checkpoint_path == MISSING:\n            raise ValueError(\"checkpoint_path is required\")\n\n        # Convert checkpoint_path to Path for validation\n        checkpoint_path = Path(self.common.checkpoint_path)\n        if not (checkpoint_path.exists() or checkpoint_path.parent.exists()):\n            raise ValueError(f\"Checkpoint path does not exist: {checkpoint_path}\")\n</code></pre>"},{"location":"reference/recipe/eval/config/#optimus_dl.recipe.eval.config.EvalConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate configuration.</p> Source code in <code>optimus_dl/recipe/eval/config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate configuration.\"\"\"\n    if self.common.checkpoint_path == MISSING:\n        raise ValueError(\"checkpoint_path is required\")\n\n    # Convert checkpoint_path to Path for validation\n    checkpoint_path = Path(self.common.checkpoint_path)\n    if not (checkpoint_path.exists() or checkpoint_path.parent.exists()):\n        raise ValueError(f\"Checkpoint path does not exist: {checkpoint_path}\")\n</code></pre>"},{"location":"reference/recipe/eval/config/#optimus_dl.recipe.eval.config.LMEvalConfig","title":"<code>LMEvalConfig</code>  <code>dataclass</code>","text":"<p>Configuration for lm_eval harness evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>list[str]</code> <p>List of lm_eval tasks to evaluate on</p> <code>['hellaswag']</code> <code>num_fewshot</code> <code>int</code> <p>Number of few-shot examples</p> <code>0</code> <code>batch_size</code> <code>int</code> <p>Batch size for evaluation</p> <code>1</code> <code>limit</code> <code>int | None</code> <p>Limit number of examples per task</p> <code>None</code> <code>output_path</code> <code>str | None</code> <p>Path to save evaluation results</p> <code>None</code> Source code in <code>optimus_dl/recipe/eval/config.py</code> <pre><code>@dataclass\nclass LMEvalConfig:\n    \"\"\"Configuration for lm_eval harness evaluation.\"\"\"\n\n    tasks: list[str] = field(\n        default_factory=lambda: [\"hellaswag\"],\n        metadata={\"description\": \"List of lm_eval tasks to evaluate on\"},\n    )\n    num_fewshot: int = field(\n        default=0, metadata={\"description\": \"Number of few-shot examples\"}\n    )\n    batch_size: int = field(\n        default=1, metadata={\"description\": \"Batch size for evaluation\"}\n    )\n    limit: int | None = field(\n        default=None, metadata={\"description\": \"Limit number of examples per task\"}\n    )\n    output_path: str | None = field(\n        default=None, metadata={\"description\": \"Path to save evaluation results\"}\n    )\n</code></pre>"},{"location":"reference/recipe/metrics/","title":"Index","text":""},{"location":"reference/recipe/metrics/#optimus_dl.recipe.metrics","title":"<code>optimus_dl.recipe.metrics</code>","text":"<p>Metrics evaluation recipe module.</p>"},{"location":"reference/recipe/metrics/#optimus_dl.recipe.metrics.MetricsConfig","title":"<code>MetricsConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Complete metrics evaluation configuration.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>dict</code> <p>dict() -&gt; new empty dictionary dict(mapping) -&gt; new dictionary initialized from a mapping object's     (key, value) pairs dict(iterable) -&gt; new dictionary initialized as if via:     d = {}     for k, v in iterable:         d[k] = v dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs     in the keyword argument list.  For example:  dict(one=1, two=2)</p> <code>&lt;class 'dict'&gt;</code> <code>common</code> <code>MetricsRecipeConfig</code> <p>Configuration for metrics evaluation recipe common settings.</p> <code>&lt;dynamic&gt;</code> <code>model</code> <code>ModelConfig | None</code> <code>None</code> <code>data</code> <code>DataConfig</code> <code>'???'</code> <code>criterion</code> <code>CriterionConfig</code> <code>'???'</code> <code>metrics</code> <code>dict[str, list[dict]]</code> <p>Metric configurations mapped by dataset name</p> <code>&lt;class 'dict'&gt;</code> <code>model_transforms</code> <code>list[ModelTransformConfig]</code> <p>List of model transforms to apply</p> <code>&lt;dynamic&gt;</code> <code>loggers</code> <code>list[MetricsLoggerConfig] | None</code> <code>None</code> <code>model_builder</code> <code>Any</code> <code>ModelBuilderConfig(_name='base')</code> <code>criterion_builder</code> <code>Any</code> <code>CriterionBuilderConfig(_name='base')</code> <code>data_builder</code> <code>Any</code> <code>DataBuilderConfig(_name='base')</code> <code>checkpoint_manager</code> <code>Any</code> <code>CheckpointManagerConfig(_name='base')</code> <code>logger_manager</code> <code>Any</code> <code>LoggerManagerConfig(_name='base')</code> <code>evaluator</code> <code>Any</code> <code>EvaluatorConfig(_name='base')</code> Source code in <code>optimus_dl/recipe/metrics/config.py</code> <pre><code>@dataclass\nclass MetricsConfig(RegistryConfigStrict):\n    \"\"\"Complete metrics evaluation configuration.\"\"\"\n\n    args: dict = field(default_factory=dict)\n    common: MetricsRecipeConfig = field(default_factory=MetricsRecipeConfig)\n\n    model: ModelConfig | None = field(default=None)\n    data: DataConfig = field(default=MISSING)\n    criterion: CriterionConfig = field(default=MISSING)\n\n    # Metrics configuration for MetricEngine, mapped by dataset name\n    metrics: dict[str, list[dict]] = field(\n        default_factory=dict,\n        metadata={\"description\": \"Metric configurations mapped by dataset name\"},\n    )\n\n    # Model transforms configuration\n    model_transforms: list[ModelTransformConfig] = field(\n        default_factory=list,\n        metadata={\"description\": \"List of model transforms to apply\"},\n    )\n\n    # Logging\n    loggers: list[MetricsLoggerConfig] | None = field(default=None)\n\n    # Dependency Injection Configs\n    model_builder: Any = field(default_factory=lambda: ModelBuilderConfig(_name=\"base\"))\n    criterion_builder: Any = field(\n        default_factory=lambda: CriterionBuilderConfig(_name=\"base\")\n    )\n    data_builder: Any = field(default_factory=lambda: DataBuilderConfig(_name=\"base\"))\n    checkpoint_manager: Any = field(\n        default_factory=lambda: CheckpointManagerConfig(_name=\"base\")\n    )\n    logger_manager: Any = field(\n        default_factory=lambda: LoggerManagerConfig(_name=\"base\")\n    )\n    evaluator: Any = field(default_factory=lambda: EvaluatorConfig(_name=\"base\"))\n</code></pre>"},{"location":"reference/recipe/metrics/#optimus_dl.recipe.metrics.MetricsRecipe","title":"<code>MetricsRecipe</code>","text":"<p>Recipe for evaluating models using the internal Metrics system.</p> <p>Handles building the model, data pipelines, and executing the evaluation loop for all provided datasets, reporting metrics via the MetricEngine.</p> Source code in <code>optimus_dl/recipe/metrics/base.py</code> <pre><code>class MetricsRecipe:\n    \"\"\"Recipe for evaluating models using the internal Metrics system.\n\n    Handles building the model, data pipelines, and executing the evaluation loop\n    for all provided datasets, reporting metrics via the MetricEngine.\n    \"\"\"\n\n    def __init__(self, cfg: MetricsConfig):\n        self.cfg = cfg\n\n        # Initialize builders via composition\n        self.model_builder = build_component(\n            \"model_builder\",\n            cfg.model_builder,\n            cast_to=ModelBuilder,\n            model_transforms=cfg.model_transforms,\n        )\n        self.data_builder = build_component(\n            \"data_builder\",\n            cfg.data_builder,\n            cast_to=DataBuilder,\n            data_config=cfg.data,\n            data_seed=cfg.common.data_seed,\n            tokenizer_config=cfg.common.tokenizer,\n        )\n        self.criterion_builder = build_component(\n            \"criterion_builder\",\n            cfg.criterion_builder,\n            cast_to=CriterionBuilder,\n            criterion_config=cfg.criterion,\n        )\n        self.checkpoint_manager = build_component(\n            \"checkpoint_manager\",\n            cfg.checkpoint_manager,\n            cast_to=CheckpointManager,\n        )\n        self.logger_manager: LoggerManager = build_logger_manager(\n            cfg.logger_manager, loggers_config=cfg.loggers\n        )\n        self.evaluator: Evaluator = build_evaluator(\n            cfg.evaluator,\n            eval_iterations=cfg.common.max_iterations,\n        )\n        self.tokenizer = None\n\n    def run(self) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"Run the complete evaluation pipeline.\"\"\"\n        set_seed(self.cfg.common.seed)\n\n        # Setup device and distributed collective\n        device, collective = setup_device_and_collective(\n            use_gpu=self.cfg.common.use_gpu, config=self.cfg.common.distributed\n        )\n\n        logger.info(f\"Starting Metrics Evaluation on {device}\")\n\n        # 1. Build Model\n        # Try loading from checkpoint if provided, else build from model config\n        if self.cfg.common.checkpoint_path:\n            logger.info(\n                f\"Loading model from checkpoint: {self.cfg.common.checkpoint_path}\"\n            )\n            model, _ = self.checkpoint_manager.build_model_from_checkpoint(\n                checkpoint_path=self.cfg.common.checkpoint_path, device=device\n            )\n        else:\n            assert (\n                self.cfg.model is not None\n            ), \"Model config required if no checkpoint path provided\"\n            model = self.model_builder.build_model(\n                model_config=self.cfg.model,\n                collective=collective,\n            )\n\n        model.eval()\n        model.to(device)\n\n        # 2. Build Criterion\n        criterion: BaseCriterion = self.criterion_builder.build_criterion(\n            collective=collective\n        )\n\n        # 3. Build Data\n        eval_datapipeline = self.data_builder.build_eval_data(\n            device=device, collective=collective\n        )\n\n        # 4. Setup Loggers\n        if collective.is_master:\n            self.logger_manager.build_loggers()\n            self.logger_manager.setup_loggers(\n                self.cfg.common.name, OmegaConf.to_container(self.cfg, resolve=True)\n            )\n\n        all_results = {}\n\n        try:\n            # 5. Run evaluation using Evaluator component\n            all_results = self.evaluator.run_evaluation(\n                model=model,\n                criterion=criterion,\n                eval_data_dict=eval_datapipeline,\n                max_iterations=self.cfg.common.max_iterations,\n                collective=collective,\n                all_metrics_configs=self.cfg.metrics,\n                metrics_prefix=\"metrics\",\n                show_progress=True,\n            )\n\n            # 6. Log results to loggers\n            if collective.is_master:\n                for eval_name, eval_metrics in all_results.items():\n                    self.logger_manager.log_metrics_to_loggers(\n                        eval_metrics, step=0, group=f\"eval/{eval_name}\"\n                    )\n\n        finally:\n            if collective.is_master:\n                self.logger_manager.close_loggers()\n\n        return all_results\n</code></pre>"},{"location":"reference/recipe/metrics/#optimus_dl.recipe.metrics.MetricsRecipe.run","title":"<code>run()</code>","text":"<p>Run the complete evaluation pipeline.</p> Source code in <code>optimus_dl/recipe/metrics/base.py</code> <pre><code>def run(self) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"Run the complete evaluation pipeline.\"\"\"\n    set_seed(self.cfg.common.seed)\n\n    # Setup device and distributed collective\n    device, collective = setup_device_and_collective(\n        use_gpu=self.cfg.common.use_gpu, config=self.cfg.common.distributed\n    )\n\n    logger.info(f\"Starting Metrics Evaluation on {device}\")\n\n    # 1. Build Model\n    # Try loading from checkpoint if provided, else build from model config\n    if self.cfg.common.checkpoint_path:\n        logger.info(\n            f\"Loading model from checkpoint: {self.cfg.common.checkpoint_path}\"\n        )\n        model, _ = self.checkpoint_manager.build_model_from_checkpoint(\n            checkpoint_path=self.cfg.common.checkpoint_path, device=device\n        )\n    else:\n        assert (\n            self.cfg.model is not None\n        ), \"Model config required if no checkpoint path provided\"\n        model = self.model_builder.build_model(\n            model_config=self.cfg.model,\n            collective=collective,\n        )\n\n    model.eval()\n    model.to(device)\n\n    # 2. Build Criterion\n    criterion: BaseCriterion = self.criterion_builder.build_criterion(\n        collective=collective\n    )\n\n    # 3. Build Data\n    eval_datapipeline = self.data_builder.build_eval_data(\n        device=device, collective=collective\n    )\n\n    # 4. Setup Loggers\n    if collective.is_master:\n        self.logger_manager.build_loggers()\n        self.logger_manager.setup_loggers(\n            self.cfg.common.name, OmegaConf.to_container(self.cfg, resolve=True)\n        )\n\n    all_results = {}\n\n    try:\n        # 5. Run evaluation using Evaluator component\n        all_results = self.evaluator.run_evaluation(\n            model=model,\n            criterion=criterion,\n            eval_data_dict=eval_datapipeline,\n            max_iterations=self.cfg.common.max_iterations,\n            collective=collective,\n            all_metrics_configs=self.cfg.metrics,\n            metrics_prefix=\"metrics\",\n            show_progress=True,\n        )\n\n        # 6. Log results to loggers\n        if collective.is_master:\n            for eval_name, eval_metrics in all_results.items():\n                self.logger_manager.log_metrics_to_loggers(\n                    eval_metrics, step=0, group=f\"eval/{eval_name}\"\n                )\n\n    finally:\n        if collective.is_master:\n            self.logger_manager.close_loggers()\n\n    return all_results\n</code></pre>"},{"location":"reference/recipe/metrics/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>base</code>: Metrics evaluation recipe using the internal MetricEngine.</li> <li><code>config</code>: Configuration for metrics evaluation recipe.</li> </ul>"},{"location":"reference/recipe/metrics/base/","title":"base","text":""},{"location":"reference/recipe/metrics/base/#optimus_dl.recipe.metrics.base","title":"<code>optimus_dl.recipe.metrics.base</code>","text":"<p>Metrics evaluation recipe using the internal MetricEngine.</p>"},{"location":"reference/recipe/metrics/base/#optimus_dl.recipe.metrics.base.MetricsRecipe","title":"<code>MetricsRecipe</code>","text":"<p>Recipe for evaluating models using the internal Metrics system.</p> <p>Handles building the model, data pipelines, and executing the evaluation loop for all provided datasets, reporting metrics via the MetricEngine.</p> Source code in <code>optimus_dl/recipe/metrics/base.py</code> <pre><code>class MetricsRecipe:\n    \"\"\"Recipe for evaluating models using the internal Metrics system.\n\n    Handles building the model, data pipelines, and executing the evaluation loop\n    for all provided datasets, reporting metrics via the MetricEngine.\n    \"\"\"\n\n    def __init__(self, cfg: MetricsConfig):\n        self.cfg = cfg\n\n        # Initialize builders via composition\n        self.model_builder = build_component(\n            \"model_builder\",\n            cfg.model_builder,\n            cast_to=ModelBuilder,\n            model_transforms=cfg.model_transforms,\n        )\n        self.data_builder = build_component(\n            \"data_builder\",\n            cfg.data_builder,\n            cast_to=DataBuilder,\n            data_config=cfg.data,\n            data_seed=cfg.common.data_seed,\n            tokenizer_config=cfg.common.tokenizer,\n        )\n        self.criterion_builder = build_component(\n            \"criterion_builder\",\n            cfg.criterion_builder,\n            cast_to=CriterionBuilder,\n            criterion_config=cfg.criterion,\n        )\n        self.checkpoint_manager = build_component(\n            \"checkpoint_manager\",\n            cfg.checkpoint_manager,\n            cast_to=CheckpointManager,\n        )\n        self.logger_manager: LoggerManager = build_logger_manager(\n            cfg.logger_manager, loggers_config=cfg.loggers\n        )\n        self.evaluator: Evaluator = build_evaluator(\n            cfg.evaluator,\n            eval_iterations=cfg.common.max_iterations,\n        )\n        self.tokenizer = None\n\n    def run(self) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"Run the complete evaluation pipeline.\"\"\"\n        set_seed(self.cfg.common.seed)\n\n        # Setup device and distributed collective\n        device, collective = setup_device_and_collective(\n            use_gpu=self.cfg.common.use_gpu, config=self.cfg.common.distributed\n        )\n\n        logger.info(f\"Starting Metrics Evaluation on {device}\")\n\n        # 1. Build Model\n        # Try loading from checkpoint if provided, else build from model config\n        if self.cfg.common.checkpoint_path:\n            logger.info(\n                f\"Loading model from checkpoint: {self.cfg.common.checkpoint_path}\"\n            )\n            model, _ = self.checkpoint_manager.build_model_from_checkpoint(\n                checkpoint_path=self.cfg.common.checkpoint_path, device=device\n            )\n        else:\n            assert (\n                self.cfg.model is not None\n            ), \"Model config required if no checkpoint path provided\"\n            model = self.model_builder.build_model(\n                model_config=self.cfg.model,\n                collective=collective,\n            )\n\n        model.eval()\n        model.to(device)\n\n        # 2. Build Criterion\n        criterion: BaseCriterion = self.criterion_builder.build_criterion(\n            collective=collective\n        )\n\n        # 3. Build Data\n        eval_datapipeline = self.data_builder.build_eval_data(\n            device=device, collective=collective\n        )\n\n        # 4. Setup Loggers\n        if collective.is_master:\n            self.logger_manager.build_loggers()\n            self.logger_manager.setup_loggers(\n                self.cfg.common.name, OmegaConf.to_container(self.cfg, resolve=True)\n            )\n\n        all_results = {}\n\n        try:\n            # 5. Run evaluation using Evaluator component\n            all_results = self.evaluator.run_evaluation(\n                model=model,\n                criterion=criterion,\n                eval_data_dict=eval_datapipeline,\n                max_iterations=self.cfg.common.max_iterations,\n                collective=collective,\n                all_metrics_configs=self.cfg.metrics,\n                metrics_prefix=\"metrics\",\n                show_progress=True,\n            )\n\n            # 6. Log results to loggers\n            if collective.is_master:\n                for eval_name, eval_metrics in all_results.items():\n                    self.logger_manager.log_metrics_to_loggers(\n                        eval_metrics, step=0, group=f\"eval/{eval_name}\"\n                    )\n\n        finally:\n            if collective.is_master:\n                self.logger_manager.close_loggers()\n\n        return all_results\n</code></pre>"},{"location":"reference/recipe/metrics/base/#optimus_dl.recipe.metrics.base.MetricsRecipe.run","title":"<code>run()</code>","text":"<p>Run the complete evaluation pipeline.</p> Source code in <code>optimus_dl/recipe/metrics/base.py</code> <pre><code>def run(self) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"Run the complete evaluation pipeline.\"\"\"\n    set_seed(self.cfg.common.seed)\n\n    # Setup device and distributed collective\n    device, collective = setup_device_and_collective(\n        use_gpu=self.cfg.common.use_gpu, config=self.cfg.common.distributed\n    )\n\n    logger.info(f\"Starting Metrics Evaluation on {device}\")\n\n    # 1. Build Model\n    # Try loading from checkpoint if provided, else build from model config\n    if self.cfg.common.checkpoint_path:\n        logger.info(\n            f\"Loading model from checkpoint: {self.cfg.common.checkpoint_path}\"\n        )\n        model, _ = self.checkpoint_manager.build_model_from_checkpoint(\n            checkpoint_path=self.cfg.common.checkpoint_path, device=device\n        )\n    else:\n        assert (\n            self.cfg.model is not None\n        ), \"Model config required if no checkpoint path provided\"\n        model = self.model_builder.build_model(\n            model_config=self.cfg.model,\n            collective=collective,\n        )\n\n    model.eval()\n    model.to(device)\n\n    # 2. Build Criterion\n    criterion: BaseCriterion = self.criterion_builder.build_criterion(\n        collective=collective\n    )\n\n    # 3. Build Data\n    eval_datapipeline = self.data_builder.build_eval_data(\n        device=device, collective=collective\n    )\n\n    # 4. Setup Loggers\n    if collective.is_master:\n        self.logger_manager.build_loggers()\n        self.logger_manager.setup_loggers(\n            self.cfg.common.name, OmegaConf.to_container(self.cfg, resolve=True)\n        )\n\n    all_results = {}\n\n    try:\n        # 5. Run evaluation using Evaluator component\n        all_results = self.evaluator.run_evaluation(\n            model=model,\n            criterion=criterion,\n            eval_data_dict=eval_datapipeline,\n            max_iterations=self.cfg.common.max_iterations,\n            collective=collective,\n            all_metrics_configs=self.cfg.metrics,\n            metrics_prefix=\"metrics\",\n            show_progress=True,\n        )\n\n        # 6. Log results to loggers\n        if collective.is_master:\n            for eval_name, eval_metrics in all_results.items():\n                self.logger_manager.log_metrics_to_loggers(\n                    eval_metrics, step=0, group=f\"eval/{eval_name}\"\n                )\n\n    finally:\n        if collective.is_master:\n            self.logger_manager.close_loggers()\n\n    return all_results\n</code></pre>"},{"location":"reference/recipe/metrics/config/","title":"config","text":""},{"location":"reference/recipe/metrics/config/#optimus_dl.recipe.metrics.config","title":"<code>optimus_dl.recipe.metrics.config</code>","text":"<p>Configuration for metrics evaluation recipe.</p>"},{"location":"reference/recipe/metrics/config/#optimus_dl.recipe.metrics.config.MetricsConfig","title":"<code>MetricsConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Complete metrics evaluation configuration.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>dict</code> <p>dict() -&gt; new empty dictionary dict(mapping) -&gt; new dictionary initialized from a mapping object's     (key, value) pairs dict(iterable) -&gt; new dictionary initialized as if via:     d = {}     for k, v in iterable:         d[k] = v dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs     in the keyword argument list.  For example:  dict(one=1, two=2)</p> <code>&lt;class 'dict'&gt;</code> <code>common</code> <code>MetricsRecipeConfig</code> <p>Configuration for metrics evaluation recipe common settings.</p> <code>&lt;dynamic&gt;</code> <code>model</code> <code>ModelConfig | None</code> <code>None</code> <code>data</code> <code>DataConfig</code> <code>'???'</code> <code>criterion</code> <code>CriterionConfig</code> <code>'???'</code> <code>metrics</code> <code>dict[str, list[dict]]</code> <p>Metric configurations mapped by dataset name</p> <code>&lt;class 'dict'&gt;</code> <code>model_transforms</code> <code>list[ModelTransformConfig]</code> <p>List of model transforms to apply</p> <code>&lt;dynamic&gt;</code> <code>loggers</code> <code>list[MetricsLoggerConfig] | None</code> <code>None</code> <code>model_builder</code> <code>Any</code> <code>ModelBuilderConfig(_name='base')</code> <code>criterion_builder</code> <code>Any</code> <code>CriterionBuilderConfig(_name='base')</code> <code>data_builder</code> <code>Any</code> <code>DataBuilderConfig(_name='base')</code> <code>checkpoint_manager</code> <code>Any</code> <code>CheckpointManagerConfig(_name='base')</code> <code>logger_manager</code> <code>Any</code> <code>LoggerManagerConfig(_name='base')</code> <code>evaluator</code> <code>Any</code> <code>EvaluatorConfig(_name='base')</code> Source code in <code>optimus_dl/recipe/metrics/config.py</code> <pre><code>@dataclass\nclass MetricsConfig(RegistryConfigStrict):\n    \"\"\"Complete metrics evaluation configuration.\"\"\"\n\n    args: dict = field(default_factory=dict)\n    common: MetricsRecipeConfig = field(default_factory=MetricsRecipeConfig)\n\n    model: ModelConfig | None = field(default=None)\n    data: DataConfig = field(default=MISSING)\n    criterion: CriterionConfig = field(default=MISSING)\n\n    # Metrics configuration for MetricEngine, mapped by dataset name\n    metrics: dict[str, list[dict]] = field(\n        default_factory=dict,\n        metadata={\"description\": \"Metric configurations mapped by dataset name\"},\n    )\n\n    # Model transforms configuration\n    model_transforms: list[ModelTransformConfig] = field(\n        default_factory=list,\n        metadata={\"description\": \"List of model transforms to apply\"},\n    )\n\n    # Logging\n    loggers: list[MetricsLoggerConfig] | None = field(default=None)\n\n    # Dependency Injection Configs\n    model_builder: Any = field(default_factory=lambda: ModelBuilderConfig(_name=\"base\"))\n    criterion_builder: Any = field(\n        default_factory=lambda: CriterionBuilderConfig(_name=\"base\")\n    )\n    data_builder: Any = field(default_factory=lambda: DataBuilderConfig(_name=\"base\"))\n    checkpoint_manager: Any = field(\n        default_factory=lambda: CheckpointManagerConfig(_name=\"base\")\n    )\n    logger_manager: Any = field(\n        default_factory=lambda: LoggerManagerConfig(_name=\"base\")\n    )\n    evaluator: Any = field(default_factory=lambda: EvaluatorConfig(_name=\"base\"))\n</code></pre>"},{"location":"reference/recipe/metrics/config/#optimus_dl.recipe.metrics.config.MetricsRecipeConfig","title":"<code>MetricsRecipeConfig</code>  <code>dataclass</code>","text":"<p>Configuration for metrics evaluation recipe common settings.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Experiment name for loggers</p> <code>'metrics-eval'</code> <code>seed</code> <code>int</code> <code>42</code> <code>data_seed</code> <code>int</code> <code>42</code> <code>output_path</code> <code>str</code> <p>Base directory for outputs (logs, etc.)</p> <code>'outputs/metrics'</code> <code>checkpoint_path</code> <code>str | None</code> <p>Path to checkpoint to load from</p> <code>None</code> <code>use_gpu</code> <code>bool</code> <code>True</code> <code>distributed</code> <code>DistributedConfig</code> <p>Configuration for distributed training topologies.</p> <p>Attributes:     tp_size: Degree of Tensor Parallelism (number of GPUs to shard each layer across).     sharding_world_size: Size of FSDP sharding groups. If None, defaults to         the number of GPUs per node (intra-node sharding).</p> <code>&lt;dynamic&gt;</code> <code>max_iterations</code> <code>int | None</code> <p>Max number of batches to process per dataset</p> <code>None</code> <code>tokenizer</code> <code>Any</code> <p>Tokenizer configuration</p> <code>None</code> Source code in <code>optimus_dl/recipe/metrics/config.py</code> <pre><code>@dataclass\nclass MetricsRecipeConfig:\n    \"\"\"Configuration for metrics evaluation recipe common settings.\"\"\"\n\n    # Experiment name\n    name: str = field(\n        default=\"metrics-eval\",\n        metadata={\"description\": \"Experiment name for loggers\"},\n    )\n\n    # Reproducibility\n    seed: int = field(default=42)\n    data_seed: int = field(default=42)\n\n    # Output\n    output_path: str = field(\n        default=\"outputs/metrics\",\n        metadata={\"description\": \"Base directory for outputs (logs, etc.)\"},\n    )\n\n    # Checkpointing\n    checkpoint_path: str | None = field(\n        default=None,\n        metadata={\"description\": \"Path to checkpoint to load from\"},\n    )\n\n    # Distributed\n    use_gpu: bool = True\n    distributed: DistributedConfig = field(default_factory=DistributedConfig)\n\n    # Evaluation limit\n    max_iterations: int | None = field(\n        default=None,\n        metadata={\"description\": \"Max number of batches to process per dataset\"},\n    )\n\n    # Tokenizer\n    tokenizer: Any = field(\n        default=None, metadata={\"description\": \"Tokenizer configuration\"}\n    )\n</code></pre>"},{"location":"reference/recipe/mixins/","title":"Index","text":""},{"location":"reference/recipe/mixins/#optimus_dl.recipe.mixins","title":"<code>optimus_dl.recipe.mixins</code>","text":"<p>Shared mixins that can be used across different recipe types.</p>"},{"location":"reference/recipe/mixins/#optimus_dl.recipe.mixins.ModelBuilder","title":"<code>ModelBuilder</code>","text":"<p>Mixin for building models and applying transformations.</p> <p>Encapsulates the logic for: 1.  Instantiating a <code>BaseModel</code> from a configuration object. 2.  Sequentially applying a list of <code>ModelTransforms</code> (e.g., FSDP, DDP, compile). 3.  Logging model statistics (parameter count).</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>ModelBuilderConfig</code> <p>Builder configuration.</p> required <code>model_transforms</code> <code>list[ModelTransformConfig] | None</code> <p>List of configurations for transforms to apply.</p> <code>None</code> Source code in <code>optimus_dl/recipe/mixins/model_builder.py</code> <pre><code>class ModelBuilder:\n    \"\"\"Mixin for building models and applying transformations.\n\n    Encapsulates the logic for:\n    1.  Instantiating a `BaseModel` from a configuration object.\n    2.  Sequentially applying a list of `ModelTransforms` (e.g., FSDP, DDP, compile).\n    3.  Logging model statistics (parameter count).\n\n    Args:\n        cfg: Builder configuration.\n        model_transforms: List of configurations for transforms to apply.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: ModelBuilderConfig,\n        model_transforms: list[ModelTransformConfig] | None = None,\n        **kwargs: Any,\n    ):\n        self.model_transforms = model_transforms or []\n\n    def build_model(\n        self, model_config: ModelConfig | None, collective: Collective, **kwargs\n    ) -&gt; BaseModel:\n        \"\"\"Build the model and apply all configured transforms.\n\n        Args:\n            model_config: Configuration for the model architecture.\n            collective: Distributed collective for transforms that need it.\n            **kwargs: Additional arguments passed to model constructor and transforms.\n\n        Returns:\n            The fully constructed and transformed model.\n        \"\"\"\n        if model_config is None:\n            raise ValueError(\n                \"model_config is None. Use build_model_from_checkpoint for evaluation.\"\n            )\n\n        model = build_model(model_config, **kwargs)\n        num_param_before = get_num_parameters(model)\n        logger.info(f\"Params num (before model transforms): {num_param_before:,}\")\n        log_averaged(\"model/num_params_before_transforms\", num_param_before)\n        assert isinstance(model, BaseModel)\n\n        # Apply model transforms (including distributed setup)\n        model = self._apply_model_transforms(\n            model, collective=collective, device=collective.default_device, **kwargs\n        )\n        num_param_after = get_num_parameters(model)\n        logger.info(f\"Model \\n{model}\")\n        logger.info(f\"Params num (after model transforms): {num_param_after:,}\")\n        log_averaged(\"model/num_params_after_transforms\", num_param_after)\n\n        return model\n\n    def _apply_model_transforms(self, model: BaseModel, **kwargs) -&gt; BaseModel:\n        \"\"\"Iteratively apply all configured model transforms.\n\n        Args:\n            model: The base model.\n            **kwargs: Context arguments (device, collective, etc.).\n\n        Returns:\n            The transformed model.\n        \"\"\"\n        for transform_cfg in self.model_transforms:\n            try:\n                transform = build_model_transform(transform_cfg, **kwargs)\n                if transform is not None:\n                    logger.info(f\"Applying model transform: {transform}\")\n                    model = transform.apply(model, **kwargs)\n                else:\n                    logger.warning(\n                        f\"Failed to build model transform from config: {transform_cfg}\"\n                    )\n            except Exception as e:\n                logger.error(f\"Failed to apply model transform {transform_cfg}: {e}\")\n                raise\n\n        return model\n</code></pre>"},{"location":"reference/recipe/mixins/#optimus_dl.recipe.mixins.ModelBuilder.build_model","title":"<code>build_model(model_config, collective, **kwargs)</code>","text":"<p>Build the model and apply all configured transforms.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>ModelConfig | None</code> <p>Configuration for the model architecture.</p> required <code>collective</code> <code>Collective</code> <p>Distributed collective for transforms that need it.</p> required <code>**kwargs</code> <p>Additional arguments passed to model constructor and transforms.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseModel</code> <p>The fully constructed and transformed model.</p> Source code in <code>optimus_dl/recipe/mixins/model_builder.py</code> <pre><code>def build_model(\n    self, model_config: ModelConfig | None, collective: Collective, **kwargs\n) -&gt; BaseModel:\n    \"\"\"Build the model and apply all configured transforms.\n\n    Args:\n        model_config: Configuration for the model architecture.\n        collective: Distributed collective for transforms that need it.\n        **kwargs: Additional arguments passed to model constructor and transforms.\n\n    Returns:\n        The fully constructed and transformed model.\n    \"\"\"\n    if model_config is None:\n        raise ValueError(\n            \"model_config is None. Use build_model_from_checkpoint for evaluation.\"\n        )\n\n    model = build_model(model_config, **kwargs)\n    num_param_before = get_num_parameters(model)\n    logger.info(f\"Params num (before model transforms): {num_param_before:,}\")\n    log_averaged(\"model/num_params_before_transforms\", num_param_before)\n    assert isinstance(model, BaseModel)\n\n    # Apply model transforms (including distributed setup)\n    model = self._apply_model_transforms(\n        model, collective=collective, device=collective.default_device, **kwargs\n    )\n    num_param_after = get_num_parameters(model)\n    logger.info(f\"Model \\n{model}\")\n    logger.info(f\"Params num (after model transforms): {num_param_after:,}\")\n    log_averaged(\"model/num_params_after_transforms\", num_param_after)\n\n    return model\n</code></pre>"},{"location":"reference/recipe/mixins/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>model_builder</code>: Model builder mixin for building and transforming models with checkpoint loading.</li> </ul>"},{"location":"reference/recipe/mixins/model_builder/","title":"model_builder","text":""},{"location":"reference/recipe/mixins/model_builder/#optimus_dl.recipe.mixins.model_builder","title":"<code>optimus_dl.recipe.mixins.model_builder</code>","text":"<p>Model builder mixin for building and transforming models with checkpoint loading.</p>"},{"location":"reference/recipe/mixins/model_builder/#optimus_dl.recipe.mixins.model_builder.ModelBuilder","title":"<code>ModelBuilder</code>","text":"<p>Mixin for building models and applying transformations.</p> <p>Encapsulates the logic for: 1.  Instantiating a <code>BaseModel</code> from a configuration object. 2.  Sequentially applying a list of <code>ModelTransforms</code> (e.g., FSDP, DDP, compile). 3.  Logging model statistics (parameter count).</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>ModelBuilderConfig</code> <p>Builder configuration.</p> required <code>model_transforms</code> <code>list[ModelTransformConfig] | None</code> <p>List of configurations for transforms to apply.</p> <code>None</code> Source code in <code>optimus_dl/recipe/mixins/model_builder.py</code> <pre><code>class ModelBuilder:\n    \"\"\"Mixin for building models and applying transformations.\n\n    Encapsulates the logic for:\n    1.  Instantiating a `BaseModel` from a configuration object.\n    2.  Sequentially applying a list of `ModelTransforms` (e.g., FSDP, DDP, compile).\n    3.  Logging model statistics (parameter count).\n\n    Args:\n        cfg: Builder configuration.\n        model_transforms: List of configurations for transforms to apply.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: ModelBuilderConfig,\n        model_transforms: list[ModelTransformConfig] | None = None,\n        **kwargs: Any,\n    ):\n        self.model_transforms = model_transforms or []\n\n    def build_model(\n        self, model_config: ModelConfig | None, collective: Collective, **kwargs\n    ) -&gt; BaseModel:\n        \"\"\"Build the model and apply all configured transforms.\n\n        Args:\n            model_config: Configuration for the model architecture.\n            collective: Distributed collective for transforms that need it.\n            **kwargs: Additional arguments passed to model constructor and transforms.\n\n        Returns:\n            The fully constructed and transformed model.\n        \"\"\"\n        if model_config is None:\n            raise ValueError(\n                \"model_config is None. Use build_model_from_checkpoint for evaluation.\"\n            )\n\n        model = build_model(model_config, **kwargs)\n        num_param_before = get_num_parameters(model)\n        logger.info(f\"Params num (before model transforms): {num_param_before:,}\")\n        log_averaged(\"model/num_params_before_transforms\", num_param_before)\n        assert isinstance(model, BaseModel)\n\n        # Apply model transforms (including distributed setup)\n        model = self._apply_model_transforms(\n            model, collective=collective, device=collective.default_device, **kwargs\n        )\n        num_param_after = get_num_parameters(model)\n        logger.info(f\"Model \\n{model}\")\n        logger.info(f\"Params num (after model transforms): {num_param_after:,}\")\n        log_averaged(\"model/num_params_after_transforms\", num_param_after)\n\n        return model\n\n    def _apply_model_transforms(self, model: BaseModel, **kwargs) -&gt; BaseModel:\n        \"\"\"Iteratively apply all configured model transforms.\n\n        Args:\n            model: The base model.\n            **kwargs: Context arguments (device, collective, etc.).\n\n        Returns:\n            The transformed model.\n        \"\"\"\n        for transform_cfg in self.model_transforms:\n            try:\n                transform = build_model_transform(transform_cfg, **kwargs)\n                if transform is not None:\n                    logger.info(f\"Applying model transform: {transform}\")\n                    model = transform.apply(model, **kwargs)\n                else:\n                    logger.warning(\n                        f\"Failed to build model transform from config: {transform_cfg}\"\n                    )\n            except Exception as e:\n                logger.error(f\"Failed to apply model transform {transform_cfg}: {e}\")\n                raise\n\n        return model\n</code></pre>"},{"location":"reference/recipe/mixins/model_builder/#optimus_dl.recipe.mixins.model_builder.ModelBuilder.build_model","title":"<code>build_model(model_config, collective, **kwargs)</code>","text":"<p>Build the model and apply all configured transforms.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>ModelConfig | None</code> <p>Configuration for the model architecture.</p> required <code>collective</code> <code>Collective</code> <p>Distributed collective for transforms that need it.</p> required <code>**kwargs</code> <p>Additional arguments passed to model constructor and transforms.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseModel</code> <p>The fully constructed and transformed model.</p> Source code in <code>optimus_dl/recipe/mixins/model_builder.py</code> <pre><code>def build_model(\n    self, model_config: ModelConfig | None, collective: Collective, **kwargs\n) -&gt; BaseModel:\n    \"\"\"Build the model and apply all configured transforms.\n\n    Args:\n        model_config: Configuration for the model architecture.\n        collective: Distributed collective for transforms that need it.\n        **kwargs: Additional arguments passed to model constructor and transforms.\n\n    Returns:\n        The fully constructed and transformed model.\n    \"\"\"\n    if model_config is None:\n        raise ValueError(\n            \"model_config is None. Use build_model_from_checkpoint for evaluation.\"\n        )\n\n    model = build_model(model_config, **kwargs)\n    num_param_before = get_num_parameters(model)\n    logger.info(f\"Params num (before model transforms): {num_param_before:,}\")\n    log_averaged(\"model/num_params_before_transforms\", num_param_before)\n    assert isinstance(model, BaseModel)\n\n    # Apply model transforms (including distributed setup)\n    model = self._apply_model_transforms(\n        model, collective=collective, device=collective.default_device, **kwargs\n    )\n    num_param_after = get_num_parameters(model)\n    logger.info(f\"Model \\n{model}\")\n    logger.info(f\"Params num (after model transforms): {num_param_after:,}\")\n    log_averaged(\"model/num_params_after_transforms\", num_param_after)\n\n    return model\n</code></pre>"},{"location":"reference/recipe/mixins/model_builder/#optimus_dl.recipe.mixins.model_builder.ModelBuilderConfig","title":"<code>ModelBuilderConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfig</code></p> <p>Configuration for ModelBuilder.</p> Source code in <code>optimus_dl/recipe/mixins/model_builder.py</code> <pre><code>@dataclass\nclass ModelBuilderConfig(RegistryConfig):\n    \"\"\"Configuration for ModelBuilder.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/recipe/pretokenize/","title":"Index","text":""},{"location":"reference/recipe/pretokenize/#optimus_dl.recipe.pretokenize","title":"<code>optimus_dl.recipe.pretokenize</code>","text":""},{"location":"reference/recipe/pretokenize/#optimus_dl.recipe.pretokenize.DataPrepConfig","title":"<code>DataPrepConfig</code>  <code>dataclass</code>","text":"<p>DataPrepConfig(dataset: optimus_dl.recipe.pretokenize.config.DatasetConfig = , processing: optimus_dl.recipe.pretokenize.config.ProcessingConfig = , output: optimus_dl.recipe.pretokenize.config.OutputConfig = , tokenizer: Any = '???') <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DatasetConfig</code> <p>DatasetConfig(repo_id: str = '???', split: str = 'train', config_name: str | None = None, cache_dir: str | None = None, file_pattern: str | None = None)</p> <code>&lt;dynamic&gt;</code> <code>processing</code> <code>ProcessingConfig</code> <p>ProcessingConfig(shard_size_mb: int = 512, shuffle_buffer_size: int = 10000, text_column: str = 'text', seed: int = 42, dtype: str = 'uint16', num_proc: int = 1)</p> <code>&lt;dynamic&gt;</code> <code>output</code> <code>OutputConfig</code> <p>OutputConfig(dir: str = '???', name: str = 'dataset')</p> <code>&lt;dynamic&gt;</code> <code>tokenizer</code> <code>Any</code> <code>'???'</code> Source code in <code>optimus_dl/recipe/pretokenize/config.py</code> <pre><code>@dataclass\nclass DataPrepConfig:\n    dataset: DatasetConfig = field(default_factory=DatasetConfig)\n    processing: ProcessingConfig = field(default_factory=ProcessingConfig)\n    output: OutputConfig = field(default_factory=OutputConfig)\n    tokenizer: Any = MISSING\n</code></pre>"},{"location":"reference/recipe/pretokenize/#optimus_dl.recipe.pretokenize.DataPrepRecipe","title":"<code>DataPrepRecipe</code>","text":"<p>Recipe for preparing and tokenizing datasets.</p> <p>Orchestrates the entire ETL pipeline: 1.  Extract: Finds files from a Hugging Face Hub repository using <code>FileFinder</code>. 2.  Transform: Tokenizes text documents in parallel using <code>TokenProcessor</code>. 3.  Load: Writes tokenized data into sharded numpy files using <code>Sharder</code>.</p> <p>Handles resumption from interruptions via atomic checkpointing.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DataPrepConfig</code> <p>Data preparation configuration.</p> required Source code in <code>optimus_dl/recipe/pretokenize/recipe.py</code> <pre><code>class DataPrepRecipe:\n    \"\"\"Recipe for preparing and tokenizing datasets.\n\n    Orchestrates the entire ETL pipeline:\n    1.  **Extract**: Finds files from a Hugging Face Hub repository using `FileFinder`.\n    2.  **Transform**: Tokenizes text documents in parallel using `TokenProcessor`.\n    3.  **Load**: Writes tokenized data into sharded numpy files using `Sharder`.\n\n    Handles resumption from interruptions via atomic checkpointing.\n\n    Args:\n        config: Data preparation configuration.\n    \"\"\"\n\n    def __init__(self, config: DataPrepConfig):\n        self.config = config\n        self.output_dir = Path(config.output.dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        self.sharder = Sharder(config.output, config.processing)\n        self.checkpointer = CheckpointManager(self.output_dir)\n\n        self._check_tokenizer()\n\n    def _check_tokenizer(self):\n        \"\"\"Builds the tokenizer and validates its vocab size against the chosen dtype.\"\"\"\n        tokenizer = build(\"tokenizer\", self.config.tokenizer)\n        assert isinstance(tokenizer, BaseTokenizer)\n\n        # Validate that the tokenizer vocab size fits within the chosen dtype\n        max_val = np.iinfo(self.sharder.dtype).max\n        if tokenizer.vocab_size &gt; max_val:\n            raise ValueError(\n                f\"Tokenizer vocab size ({tokenizer.vocab_size}) exceeds the maximum value \"\n                f\"for the chosen dtype '{self.sharder.dtype}' ({max_val}). \"\n                \"Please use a larger dtype (e.g., uint32).\"\n            )\n\n    def run(self):\n        \"\"\"Executes the data preparation pipeline.\n\n        Finds files, resumes from checkpoint if available, and processes data\n        until completion. Finalizes by writing the `index.json`.\n        \"\"\"\n        file_finder = FileFinder(self.config.dataset, self.config.processing.seed)\n        files = file_finder.get_files()\n        if not files:\n            logger.error(\"No files found to process. Aborting.\")\n            return\n\n        logger.info(f\"Found {len(files)} files to process.\")\n        processor = TokenProcessor(files, self.config)\n\n        # Load checkpoint if one exists\n        checkpoint = self.checkpointer.load()\n        if checkpoint:\n            logger.info(\"Resuming from a checkpoint.\")\n            processor.load_state(checkpoint.processor_state)\n            self.sharder.load_state(checkpoint.sharder_state)\n\n        # Setup progress bars\n        file_pbar = tqdm(\n            total=len(files),\n            desc=\"Files\",\n            unit=\"file\",\n            initial=processor.progress,\n            position=0,\n        )\n        token_pbar = tqdm(\n            desc=\"Tokens\", unit=\"tok\", initial=self.sharder.total_tokens, position=1\n        )\n\n        last_file_progress = processor.progress\n\n        try:\n            for doc_tokens in processor:\n                # Update file progress bar\n                new_file_progress = processor.progress\n                if new_file_progress &gt; last_file_progress:\n                    file_pbar.update(new_file_progress - last_file_progress)\n                    last_file_progress = new_file_progress\n\n                initial_total_tokens = self.sharder.total_tokens\n\n                # Add document to sharder and check if a flush occurred\n                shard_was_flushed = self.sharder.add(doc_tokens)\n\n                # Update token progress bar\n                token_pbar.update(self.sharder.total_tokens - initial_total_tokens)\n\n                if shard_was_flushed:\n                    # A shard was just written, which is a good time to save a checkpoint\n                    file_pbar.set_description(\n                        f\"Files (Saved shard {self.sharder.shard_idx-1})\"\n                    )\n                    logger.debug(f\"Shard flushed at file index {processor.progress}.\")\n                    state = CheckpointState(\n                        processor_state=processor.get_state(),\n                        sharder_state=self.sharder.get_state(),\n                    )\n                    self.checkpointer.save(state)\n\n            # Finalize the process\n            file_pbar.set_description(\"Finalizing index...\")\n            self.sharder.finalize(self._get_final_config())\n            self.checkpointer.clean()\n            file_pbar.set_description(\"Processing Complete\")\n\n        except KeyboardInterrupt:\n            logger.info(\"Interruption detected. Saving final checkpoint...\")\n            # Ensure the current state is saved upon interruption\n            state = CheckpointState(\n                processor_state=processor.get_state(),\n                sharder_state=self.sharder.get_state(),\n            )\n            self.checkpointer.save(state)\n            logger.info(\"Checkpoint saved. To resume, run the script again.\")\n        finally:\n            file_pbar.close()\n            token_pbar.close()\n\n    def _get_final_config(self) -&gt; dict[str, Any]:\n        \"\"\"Constructs the configuration to be saved in the final index.json.\"\"\"\n        return {\n            \"dataset\": self.config.dataset.repo_id,\n            \"split\": self.config.dataset.split,\n            \"dtype\": self.config.processing.dtype,\n            \"tokenizer\": (\n                omegaconf.OmegaConf.to_container(self.config.tokenizer, resolve=True)\n                if omegaconf.OmegaConf.is_config(self.config.tokenizer)\n                else omegaconf.OmegaConf.to_container(\n                    omegaconf.OmegaConf.structured(self.config.tokenizer), resolve=True\n                )\n            ),\n        }\n</code></pre>"},{"location":"reference/recipe/pretokenize/#optimus_dl.recipe.pretokenize.DataPrepRecipe.run","title":"<code>run()</code>","text":"<p>Executes the data preparation pipeline.</p> <p>Finds files, resumes from checkpoint if available, and processes data until completion. Finalizes by writing the <code>index.json</code>.</p> Source code in <code>optimus_dl/recipe/pretokenize/recipe.py</code> <pre><code>def run(self):\n    \"\"\"Executes the data preparation pipeline.\n\n    Finds files, resumes from checkpoint if available, and processes data\n    until completion. Finalizes by writing the `index.json`.\n    \"\"\"\n    file_finder = FileFinder(self.config.dataset, self.config.processing.seed)\n    files = file_finder.get_files()\n    if not files:\n        logger.error(\"No files found to process. Aborting.\")\n        return\n\n    logger.info(f\"Found {len(files)} files to process.\")\n    processor = TokenProcessor(files, self.config)\n\n    # Load checkpoint if one exists\n    checkpoint = self.checkpointer.load()\n    if checkpoint:\n        logger.info(\"Resuming from a checkpoint.\")\n        processor.load_state(checkpoint.processor_state)\n        self.sharder.load_state(checkpoint.sharder_state)\n\n    # Setup progress bars\n    file_pbar = tqdm(\n        total=len(files),\n        desc=\"Files\",\n        unit=\"file\",\n        initial=processor.progress,\n        position=0,\n    )\n    token_pbar = tqdm(\n        desc=\"Tokens\", unit=\"tok\", initial=self.sharder.total_tokens, position=1\n    )\n\n    last_file_progress = processor.progress\n\n    try:\n        for doc_tokens in processor:\n            # Update file progress bar\n            new_file_progress = processor.progress\n            if new_file_progress &gt; last_file_progress:\n                file_pbar.update(new_file_progress - last_file_progress)\n                last_file_progress = new_file_progress\n\n            initial_total_tokens = self.sharder.total_tokens\n\n            # Add document to sharder and check if a flush occurred\n            shard_was_flushed = self.sharder.add(doc_tokens)\n\n            # Update token progress bar\n            token_pbar.update(self.sharder.total_tokens - initial_total_tokens)\n\n            if shard_was_flushed:\n                # A shard was just written, which is a good time to save a checkpoint\n                file_pbar.set_description(\n                    f\"Files (Saved shard {self.sharder.shard_idx-1})\"\n                )\n                logger.debug(f\"Shard flushed at file index {processor.progress}.\")\n                state = CheckpointState(\n                    processor_state=processor.get_state(),\n                    sharder_state=self.sharder.get_state(),\n                )\n                self.checkpointer.save(state)\n\n        # Finalize the process\n        file_pbar.set_description(\"Finalizing index...\")\n        self.sharder.finalize(self._get_final_config())\n        self.checkpointer.clean()\n        file_pbar.set_description(\"Processing Complete\")\n\n    except KeyboardInterrupt:\n        logger.info(\"Interruption detected. Saving final checkpoint...\")\n        # Ensure the current state is saved upon interruption\n        state = CheckpointState(\n            processor_state=processor.get_state(),\n            sharder_state=self.sharder.get_state(),\n        )\n        self.checkpointer.save(state)\n        logger.info(\"Checkpoint saved. To resume, run the script again.\")\n    finally:\n        file_pbar.close()\n        token_pbar.close()\n</code></pre>"},{"location":"reference/recipe/pretokenize/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>checkpoint</code>: Manages saving and loading of data preparation checkpoints.</li> <li><code>config</code>: Configuration for data preparation recipe.</li> <li><code>processor</code>: Handles the tokenization of source files using a high-performance parallel pipeline.</li> <li><code>recipe</code>: Recipe for preparing and tokenizing datasets.</li> <li><code>sharder</code>: Handles writing tokenized documents into sized-shards on disk</li> <li><code>source</code>: Handles finding and reading data from various sources.</li> </ul>"},{"location":"reference/recipe/pretokenize/checkpoint/","title":"checkpoint","text":""},{"location":"reference/recipe/pretokenize/checkpoint/#optimus_dl.recipe.pretokenize.checkpoint","title":"<code>optimus_dl.recipe.pretokenize.checkpoint</code>","text":"<p>Manages saving and loading of data preparation checkpoints.</p>"},{"location":"reference/recipe/pretokenize/checkpoint/#optimus_dl.recipe.pretokenize.checkpoint.CheckpointManager","title":"<code>CheckpointManager</code>","text":"<p>Handles the loading and saving of checkpoints to ensure atomicity.</p> Source code in <code>optimus_dl/recipe/pretokenize/checkpoint.py</code> <pre><code>class CheckpointManager:\n    \"\"\"Handles the loading and saving of checkpoints to ensure atomicity.\"\"\"\n\n    def __init__(self, output_dir: Path):\n        self.checkpoint_path = output_dir / \"checkpoint.pkl\"\n        self.tmp_path = output_dir / \"checkpoint.tmp\"\n\n    def save(self, state: CheckpointState):\n        \"\"\"Saves the current processing state to disk atomically.\n\n        Args:\n            state: The checkpoint state object to save.\n        \"\"\"\n        with open(self.tmp_path, \"wb\") as f:\n            pickle.dump(state, f)\n        shutil.move(self.tmp_path, self.checkpoint_path)\n        logger.debug(f\"Saved checkpoint to {self.checkpoint_path}\")\n\n    def load(self) -&gt; CheckpointState | None:\n        \"\"\"Loads the processing state from disk if a checkpoint exists.\n\n        Returns:\n            The loaded CheckpointState, or None if no valid checkpoint is found.\n        \"\"\"\n        if self.checkpoint_path.exists():\n            logger.info(f\"Loading checkpoint from {self.checkpoint_path}\")\n            try:\n                with open(self.checkpoint_path, \"rb\") as f:\n                    state = pickle.load(f)\n                if isinstance(state, CheckpointState):\n                    return state\n                logger.warning(\"Checkpoint file is invalid or outdated, ignoring.\")\n                return None\n            except Exception as e:\n                logger.warning(\n                    f\"Failed to load checkpoint: {e}. Starting from scratch.\"\n                )\n                return None\n        return None\n\n    def clean(self):\n        \"\"\"Removes the checkpoint file if it exists.\"\"\"\n        if self.checkpoint_path.exists():\n            self.checkpoint_path.unlink()\n            logger.debug(\"Removed checkpoint file.\")\n</code></pre>"},{"location":"reference/recipe/pretokenize/checkpoint/#optimus_dl.recipe.pretokenize.checkpoint.CheckpointManager.clean","title":"<code>clean()</code>","text":"<p>Removes the checkpoint file if it exists.</p> Source code in <code>optimus_dl/recipe/pretokenize/checkpoint.py</code> <pre><code>def clean(self):\n    \"\"\"Removes the checkpoint file if it exists.\"\"\"\n    if self.checkpoint_path.exists():\n        self.checkpoint_path.unlink()\n        logger.debug(\"Removed checkpoint file.\")\n</code></pre>"},{"location":"reference/recipe/pretokenize/checkpoint/#optimus_dl.recipe.pretokenize.checkpoint.CheckpointManager.load","title":"<code>load()</code>","text":"<p>Loads the processing state from disk if a checkpoint exists.</p> <p>Returns:</p> Type Description <code>CheckpointState | None</code> <p>The loaded CheckpointState, or None if no valid checkpoint is found.</p> Source code in <code>optimus_dl/recipe/pretokenize/checkpoint.py</code> <pre><code>def load(self) -&gt; CheckpointState | None:\n    \"\"\"Loads the processing state from disk if a checkpoint exists.\n\n    Returns:\n        The loaded CheckpointState, or None if no valid checkpoint is found.\n    \"\"\"\n    if self.checkpoint_path.exists():\n        logger.info(f\"Loading checkpoint from {self.checkpoint_path}\")\n        try:\n            with open(self.checkpoint_path, \"rb\") as f:\n                state = pickle.load(f)\n            if isinstance(state, CheckpointState):\n                return state\n            logger.warning(\"Checkpoint file is invalid or outdated, ignoring.\")\n            return None\n        except Exception as e:\n            logger.warning(\n                f\"Failed to load checkpoint: {e}. Starting from scratch.\"\n            )\n            return None\n    return None\n</code></pre>"},{"location":"reference/recipe/pretokenize/checkpoint/#optimus_dl.recipe.pretokenize.checkpoint.CheckpointManager.save","title":"<code>save(state)</code>","text":"<p>Saves the current processing state to disk atomically.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>CheckpointState</code> <p>The checkpoint state object to save.</p> required Source code in <code>optimus_dl/recipe/pretokenize/checkpoint.py</code> <pre><code>def save(self, state: CheckpointState):\n    \"\"\"Saves the current processing state to disk atomically.\n\n    Args:\n        state: The checkpoint state object to save.\n    \"\"\"\n    with open(self.tmp_path, \"wb\") as f:\n        pickle.dump(state, f)\n    shutil.move(self.tmp_path, self.checkpoint_path)\n    logger.debug(f\"Saved checkpoint to {self.checkpoint_path}\")\n</code></pre>"},{"location":"reference/recipe/pretokenize/checkpoint/#optimus_dl.recipe.pretokenize.checkpoint.CheckpointState","title":"<code>CheckpointState</code>  <code>dataclass</code>","text":"<p>Represents the state to be saved in a checkpoint.</p> <p>This provides a clear structure for what is being saved and loaded.</p> <p>Attributes:</p> Name Type Description <code>rng_state</code> <code>dict[str, Any]</code> <p>Random number generator state (from <code>random.getstate()</code>).</p> <p>Parameters:</p> Name Type Description Default <code>processor_state</code> <code>dict[str, Any]</code> required <code>sharder_state</code> <code>dict[str, Any]</code> required Source code in <code>optimus_dl/recipe/pretokenize/checkpoint.py</code> <pre><code>@dataclass\nclass CheckpointState:\n    \"\"\"Represents the state to be saved in a checkpoint.\n\n    This provides a clear structure for what is being saved and loaded.\n\n    Attributes:\n        processor_state: State dictionary from the TokenProcessor.\n        sharder_state: State dictionary from the Sharder.\n        rng_state: Random number generator state (from `random.getstate()`).\n    \"\"\"\n\n    processor_state: dict[str, Any]\n    sharder_state: dict[str, Any]\n</code></pre>"},{"location":"reference/recipe/pretokenize/config/","title":"config","text":""},{"location":"reference/recipe/pretokenize/config/#optimus_dl.recipe.pretokenize.config","title":"<code>optimus_dl.recipe.pretokenize.config</code>","text":"<p>Configuration for data preparation recipe.</p>"},{"location":"reference/recipe/pretokenize/config/#optimus_dl.recipe.pretokenize.config.DataPrepConfig","title":"<code>DataPrepConfig</code>  <code>dataclass</code>","text":"<p>DataPrepConfig(dataset: optimus_dl.recipe.pretokenize.config.DatasetConfig = , processing: optimus_dl.recipe.pretokenize.config.ProcessingConfig = , output: optimus_dl.recipe.pretokenize.config.OutputConfig = , tokenizer: Any = '???') <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DatasetConfig</code> <p>DatasetConfig(repo_id: str = '???', split: str = 'train', config_name: str | None = None, cache_dir: str | None = None, file_pattern: str | None = None)</p> <code>&lt;dynamic&gt;</code> <code>processing</code> <code>ProcessingConfig</code> <p>ProcessingConfig(shard_size_mb: int = 512, shuffle_buffer_size: int = 10000, text_column: str = 'text', seed: int = 42, dtype: str = 'uint16', num_proc: int = 1)</p> <code>&lt;dynamic&gt;</code> <code>output</code> <code>OutputConfig</code> <p>OutputConfig(dir: str = '???', name: str = 'dataset')</p> <code>&lt;dynamic&gt;</code> <code>tokenizer</code> <code>Any</code> <code>'???'</code> Source code in <code>optimus_dl/recipe/pretokenize/config.py</code> <pre><code>@dataclass\nclass DataPrepConfig:\n    dataset: DatasetConfig = field(default_factory=DatasetConfig)\n    processing: ProcessingConfig = field(default_factory=ProcessingConfig)\n    output: OutputConfig = field(default_factory=OutputConfig)\n    tokenizer: Any = MISSING\n</code></pre>"},{"location":"reference/recipe/pretokenize/config/#optimus_dl.recipe.pretokenize.config.DatasetConfig","title":"<code>DatasetConfig</code>  <code>dataclass</code>","text":"<p>DatasetConfig(repo_id: str = '???', split: str = 'train', config_name: str | None = None, cache_dir: str | None = None, file_pattern: str | None = None)</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <code>'???'</code> <code>split</code> <code>str</code> <code>'train'</code> <code>config_name</code> <code>str | None</code> <code>None</code> <code>cache_dir</code> <code>str | None</code> <code>None</code> <code>file_pattern</code> <code>str | None</code> <code>None</code> Source code in <code>optimus_dl/recipe/pretokenize/config.py</code> <pre><code>@dataclass\nclass DatasetConfig:\n    repo_id: str = MISSING\n    split: str = \"train\"\n    config_name: str | None = None\n    cache_dir: str | None = None\n    file_pattern: str | None = None  # To filter files if needed\n</code></pre>"},{"location":"reference/recipe/pretokenize/config/#optimus_dl.recipe.pretokenize.config.OutputConfig","title":"<code>OutputConfig</code>  <code>dataclass</code>","text":"<p>OutputConfig(dir: str = '???', name: str = 'dataset')</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <code>'???'</code> <code>name</code> <code>str</code> <code>'dataset'</code> Source code in <code>optimus_dl/recipe/pretokenize/config.py</code> <pre><code>@dataclass\nclass OutputConfig:\n    dir: str = MISSING\n    name: str = \"dataset\"  # Prefix for shards?\n</code></pre>"},{"location":"reference/recipe/pretokenize/config/#optimus_dl.recipe.pretokenize.config.ProcessingConfig","title":"<code>ProcessingConfig</code>  <code>dataclass</code>","text":"<p>ProcessingConfig(shard_size_mb: int = 512, shuffle_buffer_size: int = 10000, text_column: str = 'text', seed: int = 42, dtype: str = 'uint16', num_proc: int = 1)</p> <p>Parameters:</p> Name Type Description Default <code>shard_size_mb</code> <code>int</code> <code>512</code> <code>shuffle_buffer_size</code> <code>int</code> <code>10000</code> <code>text_column</code> <code>str</code> <code>'text'</code> <code>seed</code> <code>int</code> <code>42</code> <code>dtype</code> <code>str</code> <code>'uint16'</code> <code>num_proc</code> <code>int</code> <code>1</code> Source code in <code>optimus_dl/recipe/pretokenize/config.py</code> <pre><code>@dataclass\nclass ProcessingConfig:\n    shard_size_mb: int = 512\n    shuffle_buffer_size: int = 10000\n    text_column: str = \"text\"\n    seed: int = 42\n    dtype: str = \"uint16\"  # uint16 or uint32\n    num_proc: int = 1\n</code></pre>"},{"location":"reference/recipe/pretokenize/processor/","title":"processor","text":""},{"location":"reference/recipe/pretokenize/processor/#optimus_dl.recipe.pretokenize.processor","title":"<code>optimus_dl.recipe.pretokenize.processor</code>","text":"<p>Handles the tokenization of source files using a high-performance parallel pipeline. Architecture:   [Downloader Process] -&gt; (File Paths) -&gt; [Reader Process] -&gt; (Raw Batches) -&gt; [Tokenizer Processes] -&gt; (Token Batches) -&gt; [Main Process]</p>"},{"location":"reference/recipe/pretokenize/processor/#optimus_dl.recipe.pretokenize.processor.DownloaderMessage","title":"<code>DownloaderMessage</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>DownloaderMessage(file_idx, file_path)</p> <p>Parameters:</p> Name Type Description Default <code>file_idx</code> <code>int</code> <code>None</code> <code>file_path</code> <code>str</code> <code>None</code> Source code in <code>optimus_dl/recipe/pretokenize/processor.py</code> <pre><code>class DownloaderMessage(NamedTuple):\n    file_idx: int\n    file_path: str\n</code></pre>"},{"location":"reference/recipe/pretokenize/processor/#optimus_dl.recipe.pretokenize.processor.ReaderMessage","title":"<code>ReaderMessage</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>ReaderMessage(file_idx, doc_idx, sort_doc_id, text)</p> <p>Parameters:</p> Name Type Description Default <code>file_idx</code> <code>int</code> <code>None</code> <code>doc_idx</code> <code>int</code> <code>None</code> <code>sort_doc_id</code> <code>int</code> <code>None</code> <code>text</code> <code>str</code> <code>None</code> Source code in <code>optimus_dl/recipe/pretokenize/processor.py</code> <pre><code>class ReaderMessage(NamedTuple):\n    file_idx: int\n    doc_idx: int\n    sort_doc_id: int\n    text: str\n</code></pre>"},{"location":"reference/recipe/pretokenize/processor/#optimus_dl.recipe.pretokenize.processor.TokenProcessor","title":"<code>TokenProcessor</code>","text":"<p>A resumable, parallel tokenization pipeline.</p> <p>Manages a multi-stage pipeline: 1. Downloader (Pre-fetch) 2. Reader (Disk I/O) 3. Tokenizers (CPU)</p> <p>Outputs are re-ordered to ensure determinism for resumption.</p> Source code in <code>optimus_dl/recipe/pretokenize/processor.py</code> <pre><code>class TokenProcessor:\n    \"\"\"A resumable, parallel tokenization pipeline.\n\n    Manages a multi-stage pipeline:\n    1. Downloader (Pre-fetch)\n    2. Reader (Disk I/O)\n    3. Tokenizers (CPU)\n\n    Outputs are re-ordered to ensure determinism for resumption.\n    \"\"\"\n\n    def __init__(self, files: list[str], config: DataPrepConfig):\n        self.files = files\n        self.config = config\n        self.tokenizer = build(\"tokenizer\", config.tokenizer)\n        self.dataset_config = config.dataset\n        self.processing_config = config.processing\n        self.num_proc = self.processing_config.num_proc\n        self.shuffle_buffer_size = self.processing_config.shuffle_buffer_size\n        self.seed = self.processing_config.seed\n\n        # Pipeline internals\n        self.ctx = multiprocessing.get_context(\"spawn\")\n        self.processes = []\n        self.queues = {}\n\n        self.yielded_doc_idx = None\n        self.yielded_file_idx = None\n        self.total_docs_yielded = 0\n\n    def get_state(self) -&gt; dict[str, Any]:\n        return {\n            \"yielded_doc_idx\": self.yielded_doc_idx,\n            \"yielded_file_idx\": self.yielded_file_idx,\n            \"total_docs_yielded\": self.total_docs_yielded,\n        }\n\n    def load_state(self, state: dict[str, Any]):\n        self.yielded_doc_idx = state.get(\"yielded_doc_idx\", 0)\n        self.yielded_file_idx = state.get(\"yielded_file_idx\", 0)\n        self.total_docs_yielded = state.get(\"total_docs_yielded\", 0)\n\n        logger.info(\n            f\"Resuming from checkpoint. {self.yielded_doc_idx = } {self.yielded_file_idx = } {self.total_docs_yielded = }\"\n        )\n\n    def __iter__(self) -&gt; Generator[list[int], None, None]:\n        # Clean up any previous run\n        self._stop_pipeline()\n\n        # Generator that yields ordered batches from the parallel pipeline\n        if self.num_proc == 0:\n            pipeline_gen = self._run_sequential()\n        else:\n            pipeline_gen = self._start_pipeline_generator()\n\n        try:\n            for result in pipeline_gen:\n                self.total_docs_yielded += 1\n                self.yielded_doc_idx = result.doc_idx\n                self.yielded_file_idx = result.file_idx\n                yield result.tokens\n        finally:\n            self._stop_pipeline()\n\n    def _start_pipeline_generator(self) -&gt; Generator[TokenizedMessage, None, None]:\n        \"\"\"\n        Sets up the multiprocessing pipeline and yields re-ordered batches.\n        Updates `self.file_idx` and `self.doc_idx_in_file` as data flows through.\n        \"\"\"\n        # 1. Create Queues\n        # Limited size to control RAM\n        self.queues[\"files\"] = self.ctx.Queue(maxsize=3)\n        self.queues[\"documents\"] = self.ctx.Queue(maxsize=1024)\n        self.queues[\"tokens\"] = self.ctx.Queue(maxsize=1024)\n\n        # 2. Start Downloader\n        p_down = self.ctx.Process(\n            target=_downloader_worker,\n            args=(\n                self.files,\n                self.dataset_config,\n                self.queues[\n                    \"files\"\n                ],  # Start from the beginning of the current block context\n                self.yielded_file_idx,\n            ),\n            name=\"Downloader\",\n            daemon=True,\n        )\n        p_down.start()\n        self.processes.append(p_down)\n\n        # 3. Start Reader\n        actual_num_tok = max(1, self.num_proc)\n        p_read = self.ctx.Process(\n            target=_reader_worker,\n            args=(\n                self.queues[\"files\"],\n                self.queues[\"documents\"],\n                self.processing_config,\n                self.dataset_config,\n                self.yielded_doc_idx,  # Skip docs if resuming within a file\n                self.shuffle_buffer_size,\n                actual_num_tok,\n                self.seed,\n            ),\n            name=\"Reader\",\n            daemon=True,\n        )\n        p_read.start()\n        self.processes.append(p_read)\n\n        # 4. Start Tokenizers\n        for i in range(actual_num_tok):\n            p_tok = self.ctx.Process(\n                target=_tokenizer_worker,\n                args=(\n                    self.queues[\"documents\"],\n                    self.queues[\"tokens\"],\n                    self.tokenizer,\n                ),\n                name=f\"Tokenizer-{i}\",\n                daemon=True,\n            )\n            p_tok.start()\n            self.processes.append(p_tok)\n\n        # 5. Re-ordering Loop (Generator)\n        return self._consume_and_reorder(actual_num_tok)\n\n    def _run_sequential(self) -&gt; Generator[TokenizedMessage, None, None]:\n        \"\"\"\n        Runs the pipeline sequentially in the main process (for testing/debugging).\n        Reuses the exact same worker functions but with standard Queues and direct calls.\n        \"\"\"\n        # 1. Create Queues (Standard queue.Queue for sequential execution)\n        self.queues[\"files\"] = queue.Queue()\n        self.queues[\"documents\"] = queue.Queue()\n        self.queues[\"tokens\"] = queue.Queue()\n\n        # 2. Run Downloader\n        _downloader_worker(\n            self.files,\n            self.dataset_config,\n            self.queues[\"files\"],\n            self.yielded_file_idx,\n        )\n        logger.info(\"Downloader completed.\")\n\n        # 3. Run Reader\n        _reader_worker(\n            self.queues[\"files\"],\n            self.queues[\"documents\"],\n            self.processing_config,\n            self.dataset_config,\n            self.yielded_doc_idx,\n            self.shuffle_buffer_size,\n            1,\n            self.seed,\n        )\n        logger.info(\"Reader completed.\")\n\n        # 4. Run Tokenizer (Single worker for sequential)\n        _tokenizer_worker(\n            self.queues[\"documents\"],\n            self.queues[\"tokens\"],\n            self.tokenizer,\n        )\n        logger.info(\"Tokenizer completed.\")\n\n        # 5. Consume Results (1 worker means 1 sentinel)\n        return self._consume_and_reorder(num_workers=1)\n\n    def _consume_and_reorder(\n        self, num_workers: int\n    ) -&gt; Generator[TokenizedMessage, None, None]:\n        \"\"\"\n        Consumes from token_batches queue.\n        Ensures strict ordering by batch_id: 0, 1, 2...\n        \"\"\"\n        next_expected_id = 0\n        reorder_heap = []  # Min-heap of (sort_doc_id, data)\n        finished_workers = 0\n\n        while True:\n            # Check for dead workers\n            # (Simplification: we assume they handle their own errors or we catch the None sentinel)\n\n            item = self.queues[\"tokens\"].get()\n\n            if isinstance(item, Exception | KeyboardInterrupt):\n                logger.warning(f\"Got exception from worker. {item!r}\")\n                raise item\n\n            if item is None:\n                logger.info(f\"Worker {finished_workers} completed.\")\n                finished_workers += 1\n                if finished_workers &gt;= num_workers:\n                    logger.info(\"All workers completed.\")\n                    break\n                continue\n\n            heapq.heappush(reorder_heap, (item.sort_doc_id, item))\n\n            while reorder_heap and reorder_heap[0][0] == next_expected_id:\n                _, item = heapq.heappop(reorder_heap)\n                next_expected_id += 1\n\n                yield item\n\n        logger.info(\"Pipeline completed.\")\n\n    def _stop_pipeline(self):\n        \"\"\"Terminates all workers.\"\"\"\n        logger.info(\"Stopping pipeline...\")\n        for p in self.processes:\n            if p.is_alive():\n                p.terminate()\n                p.join()\n        self.processes = []\n\n        # Clear queues\n        for q in self.queues.values():\n            if not isinstance(q, queue.Queue):\n                q.close()\n                q.join_thread()\n        self.queues = {}\n\n    @property\n    def progress(self) -&gt; int:\n        return self.yielded_file_idx or 0\n</code></pre>"},{"location":"reference/recipe/pretokenize/processor/#optimus_dl.recipe.pretokenize.processor.TokenizedMessage","title":"<code>TokenizedMessage</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>TokenizedMessage(file_idx, doc_idx, sort_doc_id, tokens)</p> <p>Parameters:</p> Name Type Description Default <code>file_idx</code> <code>int</code> <code>None</code> <code>doc_idx</code> <code>int</code> <code>None</code> <code>sort_doc_id</code> <code>int</code> <code>None</code> <code>tokens</code> <code>list[int]</code> <code>None</code> Source code in <code>optimus_dl/recipe/pretokenize/processor.py</code> <pre><code>class TokenizedMessage(NamedTuple):\n    file_idx: int\n    doc_idx: int\n    sort_doc_id: int\n    tokens: list[int]\n</code></pre>"},{"location":"reference/recipe/pretokenize/recipe/","title":"recipe","text":""},{"location":"reference/recipe/pretokenize/recipe/#optimus_dl.recipe.pretokenize.recipe","title":"<code>optimus_dl.recipe.pretokenize.recipe</code>","text":"<p>Main recipe for preparing and tokenizing datasets from the Hugging Face Hub.</p>"},{"location":"reference/recipe/pretokenize/recipe/#optimus_dl.recipe.pretokenize.recipe.DataPrepRecipe","title":"<code>DataPrepRecipe</code>","text":"<p>Recipe for preparing and tokenizing datasets.</p> <p>Orchestrates the entire ETL pipeline: 1.  Extract: Finds files from a Hugging Face Hub repository using <code>FileFinder</code>. 2.  Transform: Tokenizes text documents in parallel using <code>TokenProcessor</code>. 3.  Load: Writes tokenized data into sharded numpy files using <code>Sharder</code>.</p> <p>Handles resumption from interruptions via atomic checkpointing.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DataPrepConfig</code> <p>Data preparation configuration.</p> required Source code in <code>optimus_dl/recipe/pretokenize/recipe.py</code> <pre><code>class DataPrepRecipe:\n    \"\"\"Recipe for preparing and tokenizing datasets.\n\n    Orchestrates the entire ETL pipeline:\n    1.  **Extract**: Finds files from a Hugging Face Hub repository using `FileFinder`.\n    2.  **Transform**: Tokenizes text documents in parallel using `TokenProcessor`.\n    3.  **Load**: Writes tokenized data into sharded numpy files using `Sharder`.\n\n    Handles resumption from interruptions via atomic checkpointing.\n\n    Args:\n        config: Data preparation configuration.\n    \"\"\"\n\n    def __init__(self, config: DataPrepConfig):\n        self.config = config\n        self.output_dir = Path(config.output.dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        self.sharder = Sharder(config.output, config.processing)\n        self.checkpointer = CheckpointManager(self.output_dir)\n\n        self._check_tokenizer()\n\n    def _check_tokenizer(self):\n        \"\"\"Builds the tokenizer and validates its vocab size against the chosen dtype.\"\"\"\n        tokenizer = build(\"tokenizer\", self.config.tokenizer)\n        assert isinstance(tokenizer, BaseTokenizer)\n\n        # Validate that the tokenizer vocab size fits within the chosen dtype\n        max_val = np.iinfo(self.sharder.dtype).max\n        if tokenizer.vocab_size &gt; max_val:\n            raise ValueError(\n                f\"Tokenizer vocab size ({tokenizer.vocab_size}) exceeds the maximum value \"\n                f\"for the chosen dtype '{self.sharder.dtype}' ({max_val}). \"\n                \"Please use a larger dtype (e.g., uint32).\"\n            )\n\n    def run(self):\n        \"\"\"Executes the data preparation pipeline.\n\n        Finds files, resumes from checkpoint if available, and processes data\n        until completion. Finalizes by writing the `index.json`.\n        \"\"\"\n        file_finder = FileFinder(self.config.dataset, self.config.processing.seed)\n        files = file_finder.get_files()\n        if not files:\n            logger.error(\"No files found to process. Aborting.\")\n            return\n\n        logger.info(f\"Found {len(files)} files to process.\")\n        processor = TokenProcessor(files, self.config)\n\n        # Load checkpoint if one exists\n        checkpoint = self.checkpointer.load()\n        if checkpoint:\n            logger.info(\"Resuming from a checkpoint.\")\n            processor.load_state(checkpoint.processor_state)\n            self.sharder.load_state(checkpoint.sharder_state)\n\n        # Setup progress bars\n        file_pbar = tqdm(\n            total=len(files),\n            desc=\"Files\",\n            unit=\"file\",\n            initial=processor.progress,\n            position=0,\n        )\n        token_pbar = tqdm(\n            desc=\"Tokens\", unit=\"tok\", initial=self.sharder.total_tokens, position=1\n        )\n\n        last_file_progress = processor.progress\n\n        try:\n            for doc_tokens in processor:\n                # Update file progress bar\n                new_file_progress = processor.progress\n                if new_file_progress &gt; last_file_progress:\n                    file_pbar.update(new_file_progress - last_file_progress)\n                    last_file_progress = new_file_progress\n\n                initial_total_tokens = self.sharder.total_tokens\n\n                # Add document to sharder and check if a flush occurred\n                shard_was_flushed = self.sharder.add(doc_tokens)\n\n                # Update token progress bar\n                token_pbar.update(self.sharder.total_tokens - initial_total_tokens)\n\n                if shard_was_flushed:\n                    # A shard was just written, which is a good time to save a checkpoint\n                    file_pbar.set_description(\n                        f\"Files (Saved shard {self.sharder.shard_idx-1})\"\n                    )\n                    logger.debug(f\"Shard flushed at file index {processor.progress}.\")\n                    state = CheckpointState(\n                        processor_state=processor.get_state(),\n                        sharder_state=self.sharder.get_state(),\n                    )\n                    self.checkpointer.save(state)\n\n            # Finalize the process\n            file_pbar.set_description(\"Finalizing index...\")\n            self.sharder.finalize(self._get_final_config())\n            self.checkpointer.clean()\n            file_pbar.set_description(\"Processing Complete\")\n\n        except KeyboardInterrupt:\n            logger.info(\"Interruption detected. Saving final checkpoint...\")\n            # Ensure the current state is saved upon interruption\n            state = CheckpointState(\n                processor_state=processor.get_state(),\n                sharder_state=self.sharder.get_state(),\n            )\n            self.checkpointer.save(state)\n            logger.info(\"Checkpoint saved. To resume, run the script again.\")\n        finally:\n            file_pbar.close()\n            token_pbar.close()\n\n    def _get_final_config(self) -&gt; dict[str, Any]:\n        \"\"\"Constructs the configuration to be saved in the final index.json.\"\"\"\n        return {\n            \"dataset\": self.config.dataset.repo_id,\n            \"split\": self.config.dataset.split,\n            \"dtype\": self.config.processing.dtype,\n            \"tokenizer\": (\n                omegaconf.OmegaConf.to_container(self.config.tokenizer, resolve=True)\n                if omegaconf.OmegaConf.is_config(self.config.tokenizer)\n                else omegaconf.OmegaConf.to_container(\n                    omegaconf.OmegaConf.structured(self.config.tokenizer), resolve=True\n                )\n            ),\n        }\n</code></pre>"},{"location":"reference/recipe/pretokenize/recipe/#optimus_dl.recipe.pretokenize.recipe.DataPrepRecipe.run","title":"<code>run()</code>","text":"<p>Executes the data preparation pipeline.</p> <p>Finds files, resumes from checkpoint if available, and processes data until completion. Finalizes by writing the <code>index.json</code>.</p> Source code in <code>optimus_dl/recipe/pretokenize/recipe.py</code> <pre><code>def run(self):\n    \"\"\"Executes the data preparation pipeline.\n\n    Finds files, resumes from checkpoint if available, and processes data\n    until completion. Finalizes by writing the `index.json`.\n    \"\"\"\n    file_finder = FileFinder(self.config.dataset, self.config.processing.seed)\n    files = file_finder.get_files()\n    if not files:\n        logger.error(\"No files found to process. Aborting.\")\n        return\n\n    logger.info(f\"Found {len(files)} files to process.\")\n    processor = TokenProcessor(files, self.config)\n\n    # Load checkpoint if one exists\n    checkpoint = self.checkpointer.load()\n    if checkpoint:\n        logger.info(\"Resuming from a checkpoint.\")\n        processor.load_state(checkpoint.processor_state)\n        self.sharder.load_state(checkpoint.sharder_state)\n\n    # Setup progress bars\n    file_pbar = tqdm(\n        total=len(files),\n        desc=\"Files\",\n        unit=\"file\",\n        initial=processor.progress,\n        position=0,\n    )\n    token_pbar = tqdm(\n        desc=\"Tokens\", unit=\"tok\", initial=self.sharder.total_tokens, position=1\n    )\n\n    last_file_progress = processor.progress\n\n    try:\n        for doc_tokens in processor:\n            # Update file progress bar\n            new_file_progress = processor.progress\n            if new_file_progress &gt; last_file_progress:\n                file_pbar.update(new_file_progress - last_file_progress)\n                last_file_progress = new_file_progress\n\n            initial_total_tokens = self.sharder.total_tokens\n\n            # Add document to sharder and check if a flush occurred\n            shard_was_flushed = self.sharder.add(doc_tokens)\n\n            # Update token progress bar\n            token_pbar.update(self.sharder.total_tokens - initial_total_tokens)\n\n            if shard_was_flushed:\n                # A shard was just written, which is a good time to save a checkpoint\n                file_pbar.set_description(\n                    f\"Files (Saved shard {self.sharder.shard_idx-1})\"\n                )\n                logger.debug(f\"Shard flushed at file index {processor.progress}.\")\n                state = CheckpointState(\n                    processor_state=processor.get_state(),\n                    sharder_state=self.sharder.get_state(),\n                )\n                self.checkpointer.save(state)\n\n        # Finalize the process\n        file_pbar.set_description(\"Finalizing index...\")\n        self.sharder.finalize(self._get_final_config())\n        self.checkpointer.clean()\n        file_pbar.set_description(\"Processing Complete\")\n\n    except KeyboardInterrupt:\n        logger.info(\"Interruption detected. Saving final checkpoint...\")\n        # Ensure the current state is saved upon interruption\n        state = CheckpointState(\n            processor_state=processor.get_state(),\n            sharder_state=self.sharder.get_state(),\n        )\n        self.checkpointer.save(state)\n        logger.info(\"Checkpoint saved. To resume, run the script again.\")\n    finally:\n        file_pbar.close()\n        token_pbar.close()\n</code></pre>"},{"location":"reference/recipe/pretokenize/sharder/","title":"sharder","text":""},{"location":"reference/recipe/pretokenize/sharder/#optimus_dl.recipe.pretokenize.sharder","title":"<code>optimus_dl.recipe.pretokenize.sharder</code>","text":"<p>Handles writing tokenized documents into sized-shards on disk and creating the final index file.</p>"},{"location":"reference/recipe/pretokenize/sharder/#optimus_dl.recipe.pretokenize.sharder.Sharder","title":"<code>Sharder</code>","text":"<p>Manages the creation of sharded dataset files.</p> <p>Accumulates tokenized documents in memory until a size threshold is reached, then flushes them to disk as numpy arrays (<code>.npy</code>). Also tracks metadata for each shard to generate a global index file.</p> <p>Features:</p> <ul> <li>Buffering: Minimizes disk I/O by batching writes.</li> <li>Size-based Splitting: Creates shards of approx. equal size (e.g., 512MB).</li> <li>Metadata Tracking: Records token counts and file paths for the index.</li> <li>Checkpoint Support: Can serialize internal state to resume processing.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>output_config</code> <code>OutputConfig</code> <p>Output directory and naming configuration.</p> required <code>proc_config</code> <code>ProcessingConfig</code> <p>Processing settings (shard size, dtype).</p> required Source code in <code>optimus_dl/recipe/pretokenize/sharder.py</code> <pre><code>class Sharder:\n    \"\"\"Manages the creation of sharded dataset files.\n\n    Accumulates tokenized documents in memory until a size threshold is reached,\n    then flushes them to disk as numpy arrays (`.npy`). Also tracks metadata\n    for each shard to generate a global index file.\n\n    Features:\n\n    - **Buffering**: Minimizes disk I/O by batching writes.\n    - **Size-based Splitting**: Creates shards of approx. equal size (e.g., 512MB).\n    - **Metadata Tracking**: Records token counts and file paths for the index.\n    - **Checkpoint Support**: Can serialize internal state to resume processing.\n\n    Args:\n        output_config: Output directory and naming configuration.\n        proc_config: Processing settings (shard size, dtype).\n    \"\"\"\n\n    def __init__(self, output_config: OutputConfig, proc_config: ProcessingConfig):\n        self.output_dir = Path(output_config.dir)\n        self.output_name = output_config.name\n        self.max_shard_bytes = proc_config.shard_size_mb * 1024 * 1024\n\n        # Determine dtype for tokens\n        self.dtype = np.uint16 if proc_config.dtype == \"uint16\" else np.uint32\n\n        # Internal state for the current shard\n        self.current_shard_tokens: list[int] = []\n        self.current_shard_doc_lens: list[int] = []\n        self.current_shard_size_bytes = 0\n\n        # Overall state\n        self.shard_idx = 0\n        self.file_metadata: list[dict[str, Any]] = []\n        self.total_tokens = 0\n\n    def get_state(self) -&gt; dict[str, Any]:\n        \"\"\"Returns the sharder's current state for checkpointing.\"\"\"\n        return {\n            \"shard_idx\": self.shard_idx,\n            \"file_metadata\": self.file_metadata,\n            \"total_tokens\": self.total_tokens,\n            \"current_shard_tokens\": deepcopy(self.current_shard_tokens),\n            \"current_shard_doc_lens\": deepcopy(self.current_shard_doc_lens),\n        }\n\n    def load_state(self, state: dict[str, Any]):\n        \"\"\"Restores the sharder's state from a checkpoint.\"\"\"\n        self.shard_idx = state.get(\"shard_idx\", 0)\n        self.file_metadata = state.get(\"file_metadata\", [])\n        self.total_tokens = state.get(\"total_tokens\", 0)\n        self.current_shard_tokens = state.get(\"current_shard_tokens\", [])\n        self.current_shard_doc_lens = state.get(\"current_shard_doc_lens\", [])\n\n        itemsize = np.dtype(self.dtype).itemsize\n        self.current_shard_size_bytes = len(self.current_shard_tokens) * itemsize\n\n    def add(self, doc_tokens: list[int]) -&gt; bool:\n        \"\"\"Add a tokenized document to the current shard.\n\n        If adding the document exceeds the maximum shard size, the current shard\n        is flushed to disk first.\n\n        Args:\n            doc_tokens: A list of integers representing the tokenized document.\n\n        Returns:\n            True if a shard was flushed, False otherwise.\n        \"\"\"\n        doc_len = len(doc_tokens)\n        doc_bytes = doc_len * np.dtype(self.dtype).itemsize\n\n        if (\n            self.current_shard_size_bytes + doc_bytes &gt; self.max_shard_bytes\n            and self.current_shard_tokens\n        ):\n            self.flush()\n            # After flushing, the new doc becomes the first in the new shard\n            self.current_shard_tokens.extend(doc_tokens)\n            self.current_shard_doc_lens.append(doc_len)\n            self.current_shard_size_bytes += doc_bytes\n            return True\n\n        # Default case: add to current shard\n        self.current_shard_tokens.extend(doc_tokens)\n        self.current_shard_doc_lens.append(doc_len)\n        self.current_shard_size_bytes += doc_bytes\n        return False\n\n    def flush(self):\n        \"\"\"Write the current accumulated tokens to a new shard file.\n\n        Saves two files:\n\n        - `name_XXXXX.npy`: The flat token array.\n        - `name_XXXXX_lens.npy`: Array of document lengths for reconstruction.\n        \"\"\"\n        if not self.current_shard_tokens:\n            return\n\n        shard_name = f\"{self.output_name}_{self.shard_idx:010d}.npy\"\n        shard_path = self.output_dir / shard_name\n\n        num_tokens_in_shard = len(self.current_shard_tokens)\n        num_docs_in_shard = len(self.current_shard_doc_lens)\n\n        logger.info(\n            f\"Saving shard {shard_name} ({num_tokens_in_shard:,} tokens, {num_docs_in_shard:,} docs)...\"\n        )\n\n        token_arr = np.array(self.current_shard_tokens, dtype=self.dtype)\n        np.save(shard_path, token_arr)\n\n        lens_name = f\"{self.output_name}_{self.shard_idx:010d}_lens.npy\"\n        lens_path = self.output_dir / lens_name\n        lens_arr = np.array(self.current_shard_doc_lens, dtype=np.uint32)\n        np.save(lens_path, lens_arr)\n\n        metadata = {\n            \"file\": shard_name,\n            \"lens_file\": lens_name,\n            \"num_tokens\": num_tokens_in_shard,\n            \"num_docs\": num_docs_in_shard,\n            \"shard_idx\": self.shard_idx,\n        }\n        self.file_metadata.append(metadata)\n        self.total_tokens += num_tokens_in_shard\n\n        logger.debug(f\"Saved shard {shard_name} to disk.\")\n\n        # Reset current shard state\n        self.shard_idx += 1\n        self.current_shard_tokens = []\n        self.current_shard_doc_lens = []\n        self.current_shard_size_bytes = 0\n\n    def finalize(self, final_config: dict[str, Any]):\n        \"\"\"Flush remaining data and write the global index file.\n\n        The index file (`index.json`) contains metadata for all shards and the\n        processing configuration, enabling the dataset to be loaded later.\n\n        Args:\n            final_config: Configuration dictionary to embed in the index.\n        \"\"\"\n        self.flush()  # Flush any remaining tokens\n\n        index_data = {\n            \"files\": self.file_metadata,\n            \"total_tokens\": self.total_tokens,\n            \"config\": final_config,\n        }\n\n        index_path = self.output_dir / \"index.json\"\n        with open(index_path, \"w\") as f:\n            json.dump(index_data, f, indent=2)\n\n        logger.info(f\"Done! Total tokens: {self.total_tokens:,}\")\n        logger.info(f\"Metadata saved to {index_path}\")\n</code></pre>"},{"location":"reference/recipe/pretokenize/sharder/#optimus_dl.recipe.pretokenize.sharder.Sharder.add","title":"<code>add(doc_tokens)</code>","text":"<p>Add a tokenized document to the current shard.</p> <p>If adding the document exceeds the maximum shard size, the current shard is flushed to disk first.</p> <p>Parameters:</p> Name Type Description Default <code>doc_tokens</code> <code>list[int]</code> <p>A list of integers representing the tokenized document.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if a shard was flushed, False otherwise.</p> Source code in <code>optimus_dl/recipe/pretokenize/sharder.py</code> <pre><code>def add(self, doc_tokens: list[int]) -&gt; bool:\n    \"\"\"Add a tokenized document to the current shard.\n\n    If adding the document exceeds the maximum shard size, the current shard\n    is flushed to disk first.\n\n    Args:\n        doc_tokens: A list of integers representing the tokenized document.\n\n    Returns:\n        True if a shard was flushed, False otherwise.\n    \"\"\"\n    doc_len = len(doc_tokens)\n    doc_bytes = doc_len * np.dtype(self.dtype).itemsize\n\n    if (\n        self.current_shard_size_bytes + doc_bytes &gt; self.max_shard_bytes\n        and self.current_shard_tokens\n    ):\n        self.flush()\n        # After flushing, the new doc becomes the first in the new shard\n        self.current_shard_tokens.extend(doc_tokens)\n        self.current_shard_doc_lens.append(doc_len)\n        self.current_shard_size_bytes += doc_bytes\n        return True\n\n    # Default case: add to current shard\n    self.current_shard_tokens.extend(doc_tokens)\n    self.current_shard_doc_lens.append(doc_len)\n    self.current_shard_size_bytes += doc_bytes\n    return False\n</code></pre>"},{"location":"reference/recipe/pretokenize/sharder/#optimus_dl.recipe.pretokenize.sharder.Sharder.finalize","title":"<code>finalize(final_config)</code>","text":"<p>Flush remaining data and write the global index file.</p> <p>The index file (<code>index.json</code>) contains metadata for all shards and the processing configuration, enabling the dataset to be loaded later.</p> <p>Parameters:</p> Name Type Description Default <code>final_config</code> <code>dict[str, Any]</code> <p>Configuration dictionary to embed in the index.</p> required Source code in <code>optimus_dl/recipe/pretokenize/sharder.py</code> <pre><code>def finalize(self, final_config: dict[str, Any]):\n    \"\"\"Flush remaining data and write the global index file.\n\n    The index file (`index.json`) contains metadata for all shards and the\n    processing configuration, enabling the dataset to be loaded later.\n\n    Args:\n        final_config: Configuration dictionary to embed in the index.\n    \"\"\"\n    self.flush()  # Flush any remaining tokens\n\n    index_data = {\n        \"files\": self.file_metadata,\n        \"total_tokens\": self.total_tokens,\n        \"config\": final_config,\n    }\n\n    index_path = self.output_dir / \"index.json\"\n    with open(index_path, \"w\") as f:\n        json.dump(index_data, f, indent=2)\n\n    logger.info(f\"Done! Total tokens: {self.total_tokens:,}\")\n    logger.info(f\"Metadata saved to {index_path}\")\n</code></pre>"},{"location":"reference/recipe/pretokenize/sharder/#optimus_dl.recipe.pretokenize.sharder.Sharder.flush","title":"<code>flush()</code>","text":"<p>Write the current accumulated tokens to a new shard file.</p> <p>Saves two files:</p> <ul> <li><code>name_XXXXX.npy</code>: The flat token array.</li> <li><code>name_XXXXX_lens.npy</code>: Array of document lengths for reconstruction.</li> </ul> Source code in <code>optimus_dl/recipe/pretokenize/sharder.py</code> <pre><code>def flush(self):\n    \"\"\"Write the current accumulated tokens to a new shard file.\n\n    Saves two files:\n\n    - `name_XXXXX.npy`: The flat token array.\n    - `name_XXXXX_lens.npy`: Array of document lengths for reconstruction.\n    \"\"\"\n    if not self.current_shard_tokens:\n        return\n\n    shard_name = f\"{self.output_name}_{self.shard_idx:010d}.npy\"\n    shard_path = self.output_dir / shard_name\n\n    num_tokens_in_shard = len(self.current_shard_tokens)\n    num_docs_in_shard = len(self.current_shard_doc_lens)\n\n    logger.info(\n        f\"Saving shard {shard_name} ({num_tokens_in_shard:,} tokens, {num_docs_in_shard:,} docs)...\"\n    )\n\n    token_arr = np.array(self.current_shard_tokens, dtype=self.dtype)\n    np.save(shard_path, token_arr)\n\n    lens_name = f\"{self.output_name}_{self.shard_idx:010d}_lens.npy\"\n    lens_path = self.output_dir / lens_name\n    lens_arr = np.array(self.current_shard_doc_lens, dtype=np.uint32)\n    np.save(lens_path, lens_arr)\n\n    metadata = {\n        \"file\": shard_name,\n        \"lens_file\": lens_name,\n        \"num_tokens\": num_tokens_in_shard,\n        \"num_docs\": num_docs_in_shard,\n        \"shard_idx\": self.shard_idx,\n    }\n    self.file_metadata.append(metadata)\n    self.total_tokens += num_tokens_in_shard\n\n    logger.debug(f\"Saved shard {shard_name} to disk.\")\n\n    # Reset current shard state\n    self.shard_idx += 1\n    self.current_shard_tokens = []\n    self.current_shard_doc_lens = []\n    self.current_shard_size_bytes = 0\n</code></pre>"},{"location":"reference/recipe/pretokenize/sharder/#optimus_dl.recipe.pretokenize.sharder.Sharder.get_state","title":"<code>get_state()</code>","text":"<p>Returns the sharder's current state for checkpointing.</p> Source code in <code>optimus_dl/recipe/pretokenize/sharder.py</code> <pre><code>def get_state(self) -&gt; dict[str, Any]:\n    \"\"\"Returns the sharder's current state for checkpointing.\"\"\"\n    return {\n        \"shard_idx\": self.shard_idx,\n        \"file_metadata\": self.file_metadata,\n        \"total_tokens\": self.total_tokens,\n        \"current_shard_tokens\": deepcopy(self.current_shard_tokens),\n        \"current_shard_doc_lens\": deepcopy(self.current_shard_doc_lens),\n    }\n</code></pre>"},{"location":"reference/recipe/pretokenize/sharder/#optimus_dl.recipe.pretokenize.sharder.Sharder.load_state","title":"<code>load_state(state)</code>","text":"<p>Restores the sharder's state from a checkpoint.</p> Source code in <code>optimus_dl/recipe/pretokenize/sharder.py</code> <pre><code>def load_state(self, state: dict[str, Any]):\n    \"\"\"Restores the sharder's state from a checkpoint.\"\"\"\n    self.shard_idx = state.get(\"shard_idx\", 0)\n    self.file_metadata = state.get(\"file_metadata\", [])\n    self.total_tokens = state.get(\"total_tokens\", 0)\n    self.current_shard_tokens = state.get(\"current_shard_tokens\", [])\n    self.current_shard_doc_lens = state.get(\"current_shard_doc_lens\", [])\n\n    itemsize = np.dtype(self.dtype).itemsize\n    self.current_shard_size_bytes = len(self.current_shard_tokens) * itemsize\n</code></pre>"},{"location":"reference/recipe/pretokenize/source/","title":"source","text":""},{"location":"reference/recipe/pretokenize/source/#optimus_dl.recipe.pretokenize.source","title":"<code>optimus_dl.recipe.pretokenize.source</code>","text":"<p>Handles finding and reading data from various sources.</p>"},{"location":"reference/recipe/pretokenize/source/#optimus_dl.recipe.pretokenize.source.FileFinder","title":"<code>FileFinder</code>","text":"<p>Discovers files from a Hugging Face Hub dataset repository.</p> <p>This class handles the logic for listing files in a dataset repo, filtering them by split/pattern, and optionally parsing <code>README.md</code> metadata to identify split-specific files (common in modern HF datasets).</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DatasetConfig</code> <p>Dataset configuration.</p> required <code>seed</code> <code>int</code> <p>Random seed for shuffling file list order.</p> required Source code in <code>optimus_dl/recipe/pretokenize/source.py</code> <pre><code>class FileFinder:\n    \"\"\"Discovers files from a Hugging Face Hub dataset repository.\n\n    This class handles the logic for listing files in a dataset repo, filtering\n    them by split/pattern, and optionally parsing `README.md` metadata to identify\n    split-specific files (common in modern HF datasets).\n\n    Args:\n        config: Dataset configuration.\n        seed: Random seed for shuffling file list order.\n    \"\"\"\n\n    def __init__(self, config: DatasetConfig, seed: int):\n        self.config = config\n        self.seed = seed\n\n    def get_files(self) -&gt; list[str]:\n        \"\"\"Retrieve and filter the list of files to process.\n\n        First attempts to use metadata from `README.md` to find files for the\n        requested split/config. If that fails or is not applicable, falls back\n        to file name pattern matching.\n\n        Returns:\n            List of file paths relative to the repository root.\n        \"\"\"\n        logger.info(\n            f\"Listing files for {self.config.repo_id} split={self.config.split}\"\n        )\n        all_files = list_repo_files(repo_id=self.config.repo_id, repo_type=\"dataset\")\n        logger.info(f\"Found {len(all_files)} files before filtering.\")\n\n        if self.config.file_pattern is not None:\n            logger.info(f\"Filtering files based on pattern: {self.config.file_pattern}\")\n            files = self._filter_files(all_files, pattern=self.config.file_pattern)\n        else:\n            logger.info(\n                f\"Filtering files based on metadata for split '{self.config.split}' and config_name '{self.config.config_name}'\"\n            )\n            files = self._get_files_from_metadata(all_files)\n\n            if not files:\n                logger.info(\n                    \"No metadata file found. Falling back to simple file name filtering.\"\n                )\n                assert (\n                    self.config.config_name is None\n                ), \"config_name is not supported without metadata file\"\n\n                patterns = [\"data/*\"]\n                if self.config.split and self.config.split != \"all\":\n                    patterns = [\n                        f\"data/{self.config.split}-*\",\n                        f\"data/{self.config.split}_*\",\n                        f\"data/{self.config.split}/*\",\n                    ]\n                files = []\n                for pattern in patterns:\n                    files += self._filter_files(all_files, pattern=pattern)\n\n            if not files:\n                logger.warning(\n                    f\"No files found after filtering for split '{self.config.split}'. {all_files = }\"\n                )\n                return []\n\n        # Shuffle the files for better distribution in the shuffle buffer\n        random.seed(self.seed)\n        random.shuffle(files)\n\n        logger.info(f\"Found {len(files)} files for processing.\")\n        return files\n\n    def _get_files_from_metadata(self, all_files: list[str]) -&gt; list[str] | None:\n        \"\"\"Parse dataset metadata from README.md to find relevant files.\"\"\"\n        if \"README.md\" not in all_files:\n            return None\n\n        try:\n            readme_path = hf_hub_download(\n                repo_id=self.config.repo_id,\n                filename=\"README.md\",\n                repo_type=\"dataset\",\n                cache_dir=self.config.cache_dir,\n            )\n            with open(readme_path, encoding=\"utf-8\") as f:\n                content = f.read()\n        except Exception as e:\n            logger.warning(f\"Could not download or read README.md: {e}\")\n            return None\n\n        # Extract YAML front matter\n        if not content.startswith(\"---\"):\n            logger.warning(\n                \"README.md does not contain YAML front matter (content.startswith('---')).\"\n            )\n            return None\n\n        parts = content.split(\"---\")\n        if len(parts) &lt; 3:\n            logger.warning(\n                \"README.md does not contain valid YAML front matter (content.split('---')).\"\n            )\n            return None\n\n        yaml_content = parts[1]\n        try:\n            metadata = yaml.safe_load(yaml_content)\n        except yaml.YAMLError as e:\n            logger.warning(f\"Failed to parse YAML from README.md: {e}\")\n            return None\n\n        if not isinstance(metadata, dict) or \"configs\" not in metadata:\n            logger.warning(f\"Invalid metadata format in README.md: {metadata}\")\n            return None\n\n        split_info = next(\n            (\n                s\n                for s in metadata[\"configs\"]\n                if s.get(\"config_name\") == self.config.config_name\n            ),\n            None,\n        )\n\n        if not split_info:\n            logger.warning(\n                f\"No split info found in README.md {self.config.config_name = }, {metadata['configs'] = }\"\n            )\n            return None\n\n        patterns = [\n            pattern[\"path\"]\n            for pattern in split_info[\"data_files\"]\n            if pattern[\"split\"] == self.config.split or pattern[\"split\"] == \"all\"\n        ]\n\n        matched_files = []\n        for pattern in tqdm(patterns, desc=\"Matching patterns\", leave=False):\n            matched = fnmatch.filter(all_files, pattern)\n            matched_files.extend(matched)\n\n        return matched_files\n\n    def _filter_files(self, all_files: list[str], pattern=None) -&gt; list[str]:\n        \"\"\"Filters files based on extension, split, and pattern.\"\"\"\n        filtered = []\n\n        for f in tqdm(all_files, desc=\"Filtering files\", unit=\"file\", leave=False):\n            if not f.endswith((\".parquet\", \".jsonl\", \".json\")):\n                continue\n            if pattern and not fnmatch.fnmatch(f, pattern):\n                continue\n            filtered.append(f)\n\n        filtered.sort()  # Sort for deterministic order before shuffling\n        return filtered\n</code></pre>"},{"location":"reference/recipe/pretokenize/source/#optimus_dl.recipe.pretokenize.source.FileFinder.get_files","title":"<code>get_files()</code>","text":"<p>Retrieve and filter the list of files to process.</p> <p>First attempts to use metadata from <code>README.md</code> to find files for the requested split/config. If that fails or is not applicable, falls back to file name pattern matching.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of file paths relative to the repository root.</p> Source code in <code>optimus_dl/recipe/pretokenize/source.py</code> <pre><code>def get_files(self) -&gt; list[str]:\n    \"\"\"Retrieve and filter the list of files to process.\n\n    First attempts to use metadata from `README.md` to find files for the\n    requested split/config. If that fails or is not applicable, falls back\n    to file name pattern matching.\n\n    Returns:\n        List of file paths relative to the repository root.\n    \"\"\"\n    logger.info(\n        f\"Listing files for {self.config.repo_id} split={self.config.split}\"\n    )\n    all_files = list_repo_files(repo_id=self.config.repo_id, repo_type=\"dataset\")\n    logger.info(f\"Found {len(all_files)} files before filtering.\")\n\n    if self.config.file_pattern is not None:\n        logger.info(f\"Filtering files based on pattern: {self.config.file_pattern}\")\n        files = self._filter_files(all_files, pattern=self.config.file_pattern)\n    else:\n        logger.info(\n            f\"Filtering files based on metadata for split '{self.config.split}' and config_name '{self.config.config_name}'\"\n        )\n        files = self._get_files_from_metadata(all_files)\n\n        if not files:\n            logger.info(\n                \"No metadata file found. Falling back to simple file name filtering.\"\n            )\n            assert (\n                self.config.config_name is None\n            ), \"config_name is not supported without metadata file\"\n\n            patterns = [\"data/*\"]\n            if self.config.split and self.config.split != \"all\":\n                patterns = [\n                    f\"data/{self.config.split}-*\",\n                    f\"data/{self.config.split}_*\",\n                    f\"data/{self.config.split}/*\",\n                ]\n            files = []\n            for pattern in patterns:\n                files += self._filter_files(all_files, pattern=pattern)\n\n        if not files:\n            logger.warning(\n                f\"No files found after filtering for split '{self.config.split}'. {all_files = }\"\n            )\n            return []\n\n    # Shuffle the files for better distribution in the shuffle buffer\n    random.seed(self.seed)\n    random.shuffle(files)\n\n    logger.info(f\"Found {len(files)} files for processing.\")\n    return files\n</code></pre>"},{"location":"reference/recipe/pretokenize/source/#optimus_dl.recipe.pretokenize.source.FileReader","title":"<code>FileReader</code>","text":"<p>Reads raw text documents from different file formats.</p> <p>Supports reading text columns from:</p> <ul> <li>Parquet files (<code>.parquet</code>)</li> <li>JSON Lines files (<code>.jsonl</code>)</li> <li>JSON files (<code>.json</code>)</li> </ul> <p>Handles automatic downloading from the Hub if files are remote.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ProcessingConfig</code> <p>Processing configuration (defines text column name).</p> required <code>dataset_config</code> <code>DatasetConfig</code> <p>Dataset configuration (defines cache dir, repo ID).</p> required Source code in <code>optimus_dl/recipe/pretokenize/source.py</code> <pre><code>class FileReader:\n    \"\"\"Reads raw text documents from different file formats.\n\n    Supports reading text columns from:\n\n    - Parquet files (`.parquet`)\n    - JSON Lines files (`.jsonl`)\n    - JSON files (`.json`)\n\n    Handles automatic downloading from the Hub if files are remote.\n\n    Args:\n        config: Processing configuration (defines text column name).\n        dataset_config: Dataset configuration (defines cache dir, repo ID).\n    \"\"\"\n\n    def __init__(self, config: ProcessingConfig, dataset_config: DatasetConfig):\n        self.text_column = config.text_column\n        self.dataset_config = dataset_config\n\n    def read_texts(self, file_path: str) -&gt; Generator[str, None, None]:\n        \"\"\"Download and read a file, yielding text documents one by one.\n\n        Args:\n            file_path: Path to the file in the repo.\n\n        Yields:\n            String content of each document found in the file.\n        \"\"\"\n        local_path = hf_hub_download(\n            repo_id=self.dataset_config.repo_id,\n            filename=file_path,\n            repo_type=\"dataset\",\n            cache_dir=self.dataset_config.cache_dir,\n        )\n        local_path = Path(local_path)\n        assert local_path.exists(), f\"File not found: {local_path}\"\n\n        if file_path.endswith(\".parquet\"):\n            yield from self._read_parquet(local_path)\n        elif file_path.endswith((\".jsonl\", \".json\")):\n            yield from self._read_jsonl(local_path)\n\n    def _read_parquet(self, local_path: Path) -&gt; Generator[str, None, None]:\n        \"\"\"Reads texts from a Parquet file using streaming.\"\"\"\n        try:\n            import pyarrow.parquet as pq\n\n            # Use iter_batches to stream the file instead of loading it entirely\n            parquet_file = pq.ParquetFile(local_path)\n            total_rows = parquet_file.metadata.num_rows\n\n            with tqdm(\n                total=total_rows,\n                desc=f\"Reading {local_path.name} (streaming)\",\n                unit=\"row\",\n                leave=False,\n                disable=True,\n            ) as pbar:\n                for batch in parquet_file.iter_batches(\n                    columns=[self.text_column], batch_size=100\n                ):\n                    # batch is a RecordBatch, convert to dict or pandas\n                    # We can access columns directly as arrays\n                    column_data = batch[self.text_column]\n                    # Iterate over the PyArrow array efficiently\n                    for item in column_data:\n                        # item is a pyarrow scalar, convert to python string\n                        text = item.as_py()\n                        if isinstance(text, str) and text:\n                            yield text\n                    pbar.update(batch.num_rows)\n\n        except ImportError:\n            logger.warning(\n                \"PyArrow not available, falling back to non-streaming pandas read.\"\n            )\n            df = pd.read_parquet(local_path)\n            if self.text_column in df.columns:\n                for text in tqdm(\n                    df[self.text_column],\n                    desc=f\"Reading {local_path.name} (inefficient)\",\n                    unit=\"row\",\n                    leave=False,\n                ):\n                    if isinstance(text, str) and text:\n                        yield text\n\n    def _read_jsonl(self, local_path: Path) -&gt; Generator[str, None, None]:\n        \"\"\"Reads texts from a JSONL file.\"\"\"\n        file_size = local_path.stat().st_size\n\n        with tqdm(\n            total=file_size,\n            desc=f\"Reading {local_path.name}\",\n            unit=\"B\",\n            unit_scale=True,\n            leave=False,\n            disable=True,\n        ) as pbar:\n            with open(local_path, encoding=\"utf-8\") as f:\n                for line in f:\n                    pbar.update(len(line))\n                    item = json.loads(line)\n\n                    if isinstance(item, dict):\n                        text = item.get(self.text_column, \"\")\n                        if isinstance(text, str) and text:\n                            yield text\n                    elif isinstance(item, list):\n                        for sub_item in item:\n                            if isinstance(sub_item, dict):\n                                text = sub_item.get(self.text_column, \"\")\n                                if isinstance(text, str) and text:\n                                    yield text\n</code></pre>"},{"location":"reference/recipe/pretokenize/source/#optimus_dl.recipe.pretokenize.source.FileReader.read_texts","title":"<code>read_texts(file_path)</code>","text":"<p>Download and read a file, yielding text documents one by one.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the file in the repo.</p> required <p>Yields:</p> Type Description <code>str</code> <p>String content of each document found in the file.</p> Source code in <code>optimus_dl/recipe/pretokenize/source.py</code> <pre><code>def read_texts(self, file_path: str) -&gt; Generator[str, None, None]:\n    \"\"\"Download and read a file, yielding text documents one by one.\n\n    Args:\n        file_path: Path to the file in the repo.\n\n    Yields:\n        String content of each document found in the file.\n    \"\"\"\n    local_path = hf_hub_download(\n        repo_id=self.dataset_config.repo_id,\n        filename=file_path,\n        repo_type=\"dataset\",\n        cache_dir=self.dataset_config.cache_dir,\n    )\n    local_path = Path(local_path)\n    assert local_path.exists(), f\"File not found: {local_path}\"\n\n    if file_path.endswith(\".parquet\"):\n        yield from self._read_parquet(local_path)\n    elif file_path.endswith((\".jsonl\", \".json\")):\n        yield from self._read_jsonl(local_path)\n</code></pre>"},{"location":"reference/recipe/serve/","title":"Index","text":""},{"location":"reference/recipe/serve/#optimus_dl.recipe.serve","title":"<code>optimus_dl.recipe.serve</code>","text":""},{"location":"reference/recipe/serve/#optimus_dl.recipe.serve.ServeConfig","title":"<code>ServeConfig</code>  <code>dataclass</code>","text":"<p>ServeConfig(serve: optimus_dl.recipe.serve.config.ServeRecipeConfig = , common: optimus_dl.recipe.serve.config.ServeCommonConfig = ) <p>Parameters:</p> Name Type Description Default <code>serve</code> <code>ServeRecipeConfig</code> <p>ServeRecipeConfig(port: int = 8000, host: str = '0.0.0.0')</p> <code>&lt;dynamic&gt;</code> <code>common</code> <code>ServeCommonConfig</code> <p>ServeCommonConfig(checkpoint_path: str | None = None, model: Any = None, tokenizer: optimus_dl.modules.tokenizer.config.BaseTokenizerConfig = '???', device: str = 'auto')</p> <code>&lt;dynamic&gt;</code> Source code in <code>optimus_dl/recipe/serve/config.py</code> <pre><code>@dataclass\nclass ServeConfig:\n    serve: ServeRecipeConfig = field(default_factory=ServeRecipeConfig)\n    common: ServeCommonConfig = field(default_factory=ServeCommonConfig)\n</code></pre>"},{"location":"reference/recipe/serve/#optimus_dl.recipe.serve.ServeRecipe","title":"<code>ServeRecipe</code>","text":"<p>Recipe for serving LLM Baselines models via simple HTTP API.</p> <p>This class loads a model from a checkpoint or config, initializes the tokenizer, and starts an HTTP server compatible with OpenAI clients.</p> Source code in <code>optimus_dl/recipe/serve/base.py</code> <pre><code>class ServeRecipe:\n    \"\"\"Recipe for serving LLM Baselines models via simple HTTP API.\n\n    This class loads a model from a checkpoint or config, initializes the\n    tokenizer, and starts an HTTP server compatible with OpenAI clients.\n    \"\"\"\n\n    def __init__(self, cfg: ServeConfig):\n        self.cfg = cfg\n        self.model = None\n        self.tokenizer = None\n        self.device = None\n\n        # Initialize builder with empty config as we load from checkpoint\n        chkp_cfg = CheckpointManagerConfig()\n        self.checkpoint_manager = CheckpointManager(chkp_cfg)\n\n        modelb_cfg = ModelBuilderConfig()\n        self.model_builder = ModelBuilder(modelb_cfg)\n\n    def setup(self):\n        \"\"\"Load model weights and tokenizer, and configure the device.\"\"\"\n        # Setup device\n        if self.cfg.common.device == \"auto\":\n            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = torch.device(self.cfg.common.device)\n\n        logger.info(f\"Using device: {self.device}\")\n\n        # Build collective for potential distributed init\n        collective = build_best_collective(\n            device=None if self.device.type == \"cuda\" else torch.device(\"cpu\"),\n            config=DistributedConfig(),\n        )\n\n        assert (self.cfg.common.checkpoint_path is not None) ^ (\n            self.cfg.common.model is not None\n        ), \"Either checkpoint_path or model must be specified, but not both\"\n\n        if self.cfg.common.checkpoint_path is not None:\n            logger.info(\n                f\"Loading model from checkpoint: {self.cfg.common.checkpoint_path}\"\n            )\n            self.model, _ = self.checkpoint_manager.build_model_from_checkpoint(\n                checkpoint_path=self.cfg.common.checkpoint_path, device=self.device\n            )\n        else:\n            logger.info(\"Building model from config\")\n            self.model = self.model_builder.build_model(\n                model_config=self.cfg.common.model,\n                collective=collective,\n            )\n\n        self.model.to(self.device)\n        self.model.eval()\n\n        # Build tokenizer\n        self.tokenizer = build_tokenizer(self.cfg.common.tokenizer)\n        logger.info(\"Model and tokenizer loaded\")\n\n    @torch.no_grad()\n    def _debug_tokens_log(self, input_ids):\n        \"\"\"Log tokens for debugging.\"\"\"\n        tokens = []\n        for token in input_ids.cpu().reshape(-1):\n            token = token.item()\n            tokens.append(f\"{token}:'{self.tokenizer.decode([token])}'\")\n        logger.debug(f\"Input tokens: {' '.join(tokens)}\")\n\n    @torch.no_grad()\n    def generate_stream(\n        self,\n        prompt_or_messages: str | list[dict],\n        max_new_tokens: int = 50,\n        temperature: float = 1.0,\n        top_k: int | None = None,\n    ):\n        \"\"\"Generate text continuation yielding chunks.\n\n        Handles tokenization (including chat templates), inference loop,\n        sampling, and detokenization delta logic for streaming.\n\n        Args:\n            prompt_or_messages: Input string or list of chat messages.\n            max_new_tokens: Maximum number of tokens to generate.\n            temperature: Sampling temperature (0.0 for greedy).\n            top_k: Optional top-k sampling.\n\n        Yields:\n            String chunks of generated text.\n        \"\"\"\n        if isinstance(prompt_or_messages, list):\n            # Apply chat template\n            input_ids_list = self.tokenizer.apply_chat_template(\n                prompt_or_messages, tokenize=True, add_generation_prompt=True\n            )\n            input_ids = torch.tensor(\n                input_ids_list, dtype=torch.long, device=self.device\n            ).unsqueeze(0)\n        else:\n            if isinstance(prompt_or_messages, list):\n                # Handle list of strings? Simple server assumes single string prompt\n                prompt_or_messages = prompt_or_messages[0]\n\n            input_ids = torch.tensor(\n                self.tokenizer.encode(prompt_or_messages),\n                dtype=torch.long,\n                device=self.device,\n            ).unsqueeze(0)\n\n        self._debug_tokens_log(input_ids)\n\n        generated_ids = []\n        last_text = \"\"\n\n        for _ in range(max_new_tokens):\n            # Crop context if needed\n            if input_ids.size(1) &gt; self.model.config.sequence_length:\n                input_cond = input_ids[:, -self.model.config.sequence_length :]\n            else:\n                input_cond = input_ids\n\n            outputs = self.model(input_cond)\n            logits = outputs[\"logits\"][:, -1, :]\n\n            if temperature &gt; 0:\n                logits = logits / temperature\n                if top_k is not None:\n                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                    logits[logits &lt; v[:, [-1]]] = -float(\"Inf\")\n                probs = F.softmax(logits, dim=-1)\n                next_token = torch.multinomial(probs, num_samples=1)\n            else:\n                next_token = torch.argmax(logits, dim=-1, keepdim=True)\n\n            input_ids = torch.cat([input_ids, next_token], dim=1)\n            generated_ids.append(next_token.item())\n\n            # Simple streaming: decode all and yield diff\n            # This is inefficient but safe for bytes/utf-8 boundaries\n            current_text = self.tokenizer.decode(generated_ids)\n            new_text = current_text[len(last_text) :]\n\n            if new_text:\n                yield new_text\n                last_text = current_text\n\n            if (\n                hasattr(self.cfg.common.tokenizer, \"eos_token_id\")\n                and next_token.item() == self.cfg.common.tokenizer.eos_token_id\n            ):\n                break\n\n    def generate(\n        self,\n        prompt_or_messages: str | list[dict],\n        max_new_tokens: int = 50,\n        temperature: float = 1.0,\n        top_k: int | None = None,\n    ) -&gt; str:\n        \"\"\"Generate full text continuation.\n\n        Wrapper around `generate_stream` that accumulates all chunks.\n        \"\"\"\n        return \"\".join(\n            list(\n                self.generate_stream(\n                    prompt_or_messages, max_new_tokens, temperature, top_k\n                )\n            )\n        )\n\n    def run(self):\n        \"\"\"Start the HTTP server.\"\"\"\n        self.setup()\n\n        server_address = (self.cfg.serve.host, self.cfg.serve.port)\n        httpd = HTTPServer(server_address, RequestHandler)\n        httpd.recipe = self\n\n        logger.info(f\"Serving at http://{self.cfg.serve.host}:{self.cfg.serve.port}\")\n\n        # Example payloads\n        text_completion_ex = json.dumps(\n            {\n                \"prompt\": \"Once upon a time\",\n                \"max_tokens\": 20,\n                \"temperature\": 0.8,\n            }\n        )\n        logger.info(\n            f\"Text Completion Example:\\ncurl -X POST http://{self.cfg.serve.host}:{self.cfg.serve.port}/v1/completions -d '{text_completion_ex}'\"\n        )\n\n        chat_completion_ex = json.dumps(\n            {\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                    {\"role\": \"user\", \"content\": \"Hello!\"},\n                ],\n                \"max_tokens\": 20,\n                \"temperature\": 0.8,\n                \"stream\": True,\n            }\n        )\n        logger.info(\n            f\"Chat Streaming Example:\\ncurl -X POST http://{self.cfg.serve.host}:{self.cfg.serve.port}/v1/chat/completions -d '{chat_completion_ex}'\"\n        )\n\n        try:\n            httpd.serve_forever()\n        except KeyboardInterrupt:\n            pass\n        httpd.server_close()\n        logger.info(\"Server stopped\")\n</code></pre>"},{"location":"reference/recipe/serve/#optimus_dl.recipe.serve.ServeRecipe.generate","title":"<code>generate(prompt_or_messages, max_new_tokens=50, temperature=1.0, top_k=None)</code>","text":"<p>Generate full text continuation.</p> <p>Wrapper around <code>generate_stream</code> that accumulates all chunks.</p> Source code in <code>optimus_dl/recipe/serve/base.py</code> <pre><code>def generate(\n    self,\n    prompt_or_messages: str | list[dict],\n    max_new_tokens: int = 50,\n    temperature: float = 1.0,\n    top_k: int | None = None,\n) -&gt; str:\n    \"\"\"Generate full text continuation.\n\n    Wrapper around `generate_stream` that accumulates all chunks.\n    \"\"\"\n    return \"\".join(\n        list(\n            self.generate_stream(\n                prompt_or_messages, max_new_tokens, temperature, top_k\n            )\n        )\n    )\n</code></pre>"},{"location":"reference/recipe/serve/#optimus_dl.recipe.serve.ServeRecipe.generate_stream","title":"<code>generate_stream(prompt_or_messages, max_new_tokens=50, temperature=1.0, top_k=None)</code>","text":"<p>Generate text continuation yielding chunks.</p> <p>Handles tokenization (including chat templates), inference loop, sampling, and detokenization delta logic for streaming.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_or_messages</code> <code>str | list[dict]</code> <p>Input string or list of chat messages.</p> required <code>max_new_tokens</code> <code>int</code> <p>Maximum number of tokens to generate.</p> <code>50</code> <code>temperature</code> <code>float</code> <p>Sampling temperature (0.0 for greedy).</p> <code>1.0</code> <code>top_k</code> <code>int | None</code> <p>Optional top-k sampling.</p> <code>None</code> <p>Yields:</p> Type Description <p>String chunks of generated text.</p> Source code in <code>optimus_dl/recipe/serve/base.py</code> <pre><code>@torch.no_grad()\ndef generate_stream(\n    self,\n    prompt_or_messages: str | list[dict],\n    max_new_tokens: int = 50,\n    temperature: float = 1.0,\n    top_k: int | None = None,\n):\n    \"\"\"Generate text continuation yielding chunks.\n\n    Handles tokenization (including chat templates), inference loop,\n    sampling, and detokenization delta logic for streaming.\n\n    Args:\n        prompt_or_messages: Input string or list of chat messages.\n        max_new_tokens: Maximum number of tokens to generate.\n        temperature: Sampling temperature (0.0 for greedy).\n        top_k: Optional top-k sampling.\n\n    Yields:\n        String chunks of generated text.\n    \"\"\"\n    if isinstance(prompt_or_messages, list):\n        # Apply chat template\n        input_ids_list = self.tokenizer.apply_chat_template(\n            prompt_or_messages, tokenize=True, add_generation_prompt=True\n        )\n        input_ids = torch.tensor(\n            input_ids_list, dtype=torch.long, device=self.device\n        ).unsqueeze(0)\n    else:\n        if isinstance(prompt_or_messages, list):\n            # Handle list of strings? Simple server assumes single string prompt\n            prompt_or_messages = prompt_or_messages[0]\n\n        input_ids = torch.tensor(\n            self.tokenizer.encode(prompt_or_messages),\n            dtype=torch.long,\n            device=self.device,\n        ).unsqueeze(0)\n\n    self._debug_tokens_log(input_ids)\n\n    generated_ids = []\n    last_text = \"\"\n\n    for _ in range(max_new_tokens):\n        # Crop context if needed\n        if input_ids.size(1) &gt; self.model.config.sequence_length:\n            input_cond = input_ids[:, -self.model.config.sequence_length :]\n        else:\n            input_cond = input_ids\n\n        outputs = self.model(input_cond)\n        logits = outputs[\"logits\"][:, -1, :]\n\n        if temperature &gt; 0:\n            logits = logits / temperature\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits &lt; v[:, [-1]]] = -float(\"Inf\")\n            probs = F.softmax(logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1)\n        else:\n            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n\n        input_ids = torch.cat([input_ids, next_token], dim=1)\n        generated_ids.append(next_token.item())\n\n        # Simple streaming: decode all and yield diff\n        # This is inefficient but safe for bytes/utf-8 boundaries\n        current_text = self.tokenizer.decode(generated_ids)\n        new_text = current_text[len(last_text) :]\n\n        if new_text:\n            yield new_text\n            last_text = current_text\n\n        if (\n            hasattr(self.cfg.common.tokenizer, \"eos_token_id\")\n            and next_token.item() == self.cfg.common.tokenizer.eos_token_id\n        ):\n            break\n</code></pre>"},{"location":"reference/recipe/serve/#optimus_dl.recipe.serve.ServeRecipe.run","title":"<code>run()</code>","text":"<p>Start the HTTP server.</p> Source code in <code>optimus_dl/recipe/serve/base.py</code> <pre><code>def run(self):\n    \"\"\"Start the HTTP server.\"\"\"\n    self.setup()\n\n    server_address = (self.cfg.serve.host, self.cfg.serve.port)\n    httpd = HTTPServer(server_address, RequestHandler)\n    httpd.recipe = self\n\n    logger.info(f\"Serving at http://{self.cfg.serve.host}:{self.cfg.serve.port}\")\n\n    # Example payloads\n    text_completion_ex = json.dumps(\n        {\n            \"prompt\": \"Once upon a time\",\n            \"max_tokens\": 20,\n            \"temperature\": 0.8,\n        }\n    )\n    logger.info(\n        f\"Text Completion Example:\\ncurl -X POST http://{self.cfg.serve.host}:{self.cfg.serve.port}/v1/completions -d '{text_completion_ex}'\"\n    )\n\n    chat_completion_ex = json.dumps(\n        {\n            \"messages\": [\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": \"Hello!\"},\n            ],\n            \"max_tokens\": 20,\n            \"temperature\": 0.8,\n            \"stream\": True,\n        }\n    )\n    logger.info(\n        f\"Chat Streaming Example:\\ncurl -X POST http://{self.cfg.serve.host}:{self.cfg.serve.port}/v1/chat/completions -d '{chat_completion_ex}'\"\n    )\n\n    try:\n        httpd.serve_forever()\n    except KeyboardInterrupt:\n        pass\n    httpd.server_close()\n    logger.info(\"Server stopped\")\n</code></pre>"},{"location":"reference/recipe/serve/#optimus_dl.recipe.serve.ServeRecipe.setup","title":"<code>setup()</code>","text":"<p>Load model weights and tokenizer, and configure the device.</p> Source code in <code>optimus_dl/recipe/serve/base.py</code> <pre><code>def setup(self):\n    \"\"\"Load model weights and tokenizer, and configure the device.\"\"\"\n    # Setup device\n    if self.cfg.common.device == \"auto\":\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        self.device = torch.device(self.cfg.common.device)\n\n    logger.info(f\"Using device: {self.device}\")\n\n    # Build collective for potential distributed init\n    collective = build_best_collective(\n        device=None if self.device.type == \"cuda\" else torch.device(\"cpu\"),\n        config=DistributedConfig(),\n    )\n\n    assert (self.cfg.common.checkpoint_path is not None) ^ (\n        self.cfg.common.model is not None\n    ), \"Either checkpoint_path or model must be specified, but not both\"\n\n    if self.cfg.common.checkpoint_path is not None:\n        logger.info(\n            f\"Loading model from checkpoint: {self.cfg.common.checkpoint_path}\"\n        )\n        self.model, _ = self.checkpoint_manager.build_model_from_checkpoint(\n            checkpoint_path=self.cfg.common.checkpoint_path, device=self.device\n        )\n    else:\n        logger.info(\"Building model from config\")\n        self.model = self.model_builder.build_model(\n            model_config=self.cfg.common.model,\n            collective=collective,\n        )\n\n    self.model.to(self.device)\n    self.model.eval()\n\n    # Build tokenizer\n    self.tokenizer = build_tokenizer(self.cfg.common.tokenizer)\n    logger.info(\"Model and tokenizer loaded\")\n</code></pre>"},{"location":"reference/recipe/serve/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>base</code>: Serving recipe for LLM Baselines models.</li> <li><code>config</code>: </li> <li><code>models</code>: A single message in a chat conversation.</li> </ul>"},{"location":"reference/recipe/serve/base/","title":"base","text":""},{"location":"reference/recipe/serve/base/#optimus_dl.recipe.serve.base","title":"<code>optimus_dl.recipe.serve.base</code>","text":"<p>Serving recipe for LLM Baselines models.</p>"},{"location":"reference/recipe/serve/base/#optimus_dl.recipe.serve.base.RequestHandler","title":"<code>RequestHandler</code>","text":"<p>               Bases: <code>BaseHTTPRequestHandler</code></p> <p>HTTP Request Handler for the model serving API.</p> <p>Handles POST requests for text completion and chat completion endpoints, parsing input JSON and formatting responses according to OpenAI-compatible schemas.</p> Source code in <code>optimus_dl/recipe/serve/base.py</code> <pre><code>class RequestHandler(BaseHTTPRequestHandler):\n    \"\"\"HTTP Request Handler for the model serving API.\n\n    Handles POST requests for text completion and chat completion endpoints,\n    parsing input JSON and formatting responses according to OpenAI-compatible schemas.\n    \"\"\"\n\n    def _send_response(self, response_model):\n        \"\"\"Send a successful JSON response from a Pydantic model.\"\"\"\n        self.send_response(200)\n        self.send_header(\"Content-type\", \"application/json\")\n        self.end_headers()\n        self.wfile.write(response_model.model_dump_json().encode(\"utf-8\"))\n\n    def _send_error(self, status_code: int, error_message: str):\n        \"\"\"Send an error response with a specific status code.\"\"\"\n        self.send_response(status_code)\n        self.send_header(\"Content-type\", \"application/json\")\n        self.end_headers()\n        self.wfile.write(json.dumps({\"error\": error_message}).encode(\"utf-8\"))\n\n    def _parse_request(self, model_class):\n        \"\"\"Parse and validate the request body using a Pydantic model.\"\"\"\n        content_length = int(self.headers.get(\"Content-Length\", 0))\n        post_data = self.rfile.read(content_length)\n        try:\n            data = json.loads(post_data)\n            return model_class(**data)\n        except json.JSONDecodeError as err:\n            self._send_error(400, \"Invalid JSON\")\n            raise ValueError from err\n        except ValidationError as err:\n            self._send_error(422, str(err))\n            raise ValueError from err\n\n    def do_POST(self):\n        \"\"\"Handle POST requests, routing to specific handlers.\"\"\"\n        routes = {\n            \"/v1/completions\": self.handle_completions,\n            \"/v1/chat/completions\": self.handle_chat_completions,\n        }\n\n        if self.path in routes:\n            try:\n                routes[self.path]()\n            except ValueError:\n                pass  # Handled in _parse_request\n            except Exception as e:\n                logger.error(f\"Internal Error: {e}\")\n                self._send_error(500, str(e))\n        else:\n            self._send_error(404, \"Not Found\")\n\n    def handle_completions(self):\n        \"\"\"Handle legacy text completion requests (/v1/completions).\"\"\"\n        request = self._parse_request(CompletionRequest)\n\n        # Non-streaming only for now for basic completions, or implement stream if needed\n        # Assuming request.stream is supported later or ignored.\n        # But generate_stream supports it.\n\n        response_text = self.server.recipe.generate(\n            request.prompt,\n            request.max_tokens,\n            request.temperature,\n            request.top_k,\n        )\n\n        response = CompletionResponse(\n            id=f\"cmpl-{int(time.time())}\",\n            object=\"text_completion\",\n            created=int(time.time()),\n            model=request.model,\n            choices=[\n                Choice(\n                    index=0,\n                    text=response_text,\n                    finish_reason=\"length\",  # Simplification\n                )\n            ],\n            usage={\n                \"prompt_tokens\": 0,\n                \"completion_tokens\": 0,\n                \"total_tokens\": 0,\n            },\n        )\n        self._send_response(response)\n\n    def handle_chat_completions(self):\n        \"\"\"Handle chat completion requests (/v1/chat/completions).\n\n        Supports both streaming (Server-Sent Events) and non-streaming responses.\n        \"\"\"\n        request = self._parse_request(ChatCompletionRequest)\n\n        # Convert pydantic messages to dict list for tokenizer\n        # request.messages is List[dict] already due to model definition flexibility\n        # but pydantic validates it. If it was List[ChatMessage], we would need dump.\n        # It is List[dict] in models.py now.\n        messages_dicts = request.messages\n\n        if request.stream:\n            self.send_response(200)\n            self.send_header(\"Content-Type\", \"text/event-stream\")\n            self.send_header(\"Cache-Control\", \"no-cache\")\n            self.send_header(\"Connection\", \"keep-alive\")\n            self.end_headers()\n\n            generator = self.server.recipe.generate_stream(\n                messages_dicts,\n                request.max_tokens,\n                request.temperature,\n                request.top_k,\n            )\n\n            id_ = f\"chatcmpl-{int(time.time())}\"\n            created = int(time.time())\n\n            for chunk_text in generator:\n                chunk_resp = ChatCompletionChunk(\n                    id=id_,\n                    object=\"chat.completion.chunk\",\n                    created=created,\n                    model=request.model,\n                    choices=[ChatChunkChoice(index=0, delta=Delta(content=chunk_text))],\n                )\n                self.wfile.write(f\"data: {chunk_resp.model_dump_json()}\\n\\n\".encode())\n                self.wfile.flush()\n\n            # Finish chunk\n            finish_resp = ChatCompletionChunk(\n                id=id_,\n                object=\"chat.completion.chunk\",\n                created=created,\n                model=request.model,\n                choices=[ChatChunkChoice(index=0, delta=Delta(), finish_reason=\"stop\")],\n            )\n            self.wfile.write(f\"data: {finish_resp.model_dump_json()}\\n\\n\".encode())\n            self.wfile.write(b\"data: [DONE]\\n\\n\")\n            self.wfile.flush()\n\n        else:\n            response_text = self.server.recipe.generate(\n                messages_dicts,\n                request.max_tokens,\n                request.temperature,\n                request.top_k,\n            )\n\n            response = ChatCompletionResponse(\n                id=f\"chatcmpl-{int(time.time())}\",\n                object=\"chat.completion\",\n                created=int(time.time()),\n                model=request.model,\n                choices=[\n                    ChatChoice(\n                        index=0,\n                        message=ChatMessage(role=\"assistant\", content=response_text),\n                        finish_reason=\"stop\",\n                    )\n                ],\n                usage={\n                    \"prompt_tokens\": 0,\n                    \"completion_tokens\": 0,\n                    \"total_tokens\": 0,\n                },\n            )\n            self._send_response(response)\n</code></pre>"},{"location":"reference/recipe/serve/base/#optimus_dl.recipe.serve.base.RequestHandler.do_POST","title":"<code>do_POST()</code>","text":"<p>Handle POST requests, routing to specific handlers.</p> Source code in <code>optimus_dl/recipe/serve/base.py</code> <pre><code>def do_POST(self):\n    \"\"\"Handle POST requests, routing to specific handlers.\"\"\"\n    routes = {\n        \"/v1/completions\": self.handle_completions,\n        \"/v1/chat/completions\": self.handle_chat_completions,\n    }\n\n    if self.path in routes:\n        try:\n            routes[self.path]()\n        except ValueError:\n            pass  # Handled in _parse_request\n        except Exception as e:\n            logger.error(f\"Internal Error: {e}\")\n            self._send_error(500, str(e))\n    else:\n        self._send_error(404, \"Not Found\")\n</code></pre>"},{"location":"reference/recipe/serve/base/#optimus_dl.recipe.serve.base.RequestHandler.handle_chat_completions","title":"<code>handle_chat_completions()</code>","text":"<p>Handle chat completion requests (/v1/chat/completions).</p> <p>Supports both streaming (Server-Sent Events) and non-streaming responses.</p> Source code in <code>optimus_dl/recipe/serve/base.py</code> <pre><code>def handle_chat_completions(self):\n    \"\"\"Handle chat completion requests (/v1/chat/completions).\n\n    Supports both streaming (Server-Sent Events) and non-streaming responses.\n    \"\"\"\n    request = self._parse_request(ChatCompletionRequest)\n\n    # Convert pydantic messages to dict list for tokenizer\n    # request.messages is List[dict] already due to model definition flexibility\n    # but pydantic validates it. If it was List[ChatMessage], we would need dump.\n    # It is List[dict] in models.py now.\n    messages_dicts = request.messages\n\n    if request.stream:\n        self.send_response(200)\n        self.send_header(\"Content-Type\", \"text/event-stream\")\n        self.send_header(\"Cache-Control\", \"no-cache\")\n        self.send_header(\"Connection\", \"keep-alive\")\n        self.end_headers()\n\n        generator = self.server.recipe.generate_stream(\n            messages_dicts,\n            request.max_tokens,\n            request.temperature,\n            request.top_k,\n        )\n\n        id_ = f\"chatcmpl-{int(time.time())}\"\n        created = int(time.time())\n\n        for chunk_text in generator:\n            chunk_resp = ChatCompletionChunk(\n                id=id_,\n                object=\"chat.completion.chunk\",\n                created=created,\n                model=request.model,\n                choices=[ChatChunkChoice(index=0, delta=Delta(content=chunk_text))],\n            )\n            self.wfile.write(f\"data: {chunk_resp.model_dump_json()}\\n\\n\".encode())\n            self.wfile.flush()\n\n        # Finish chunk\n        finish_resp = ChatCompletionChunk(\n            id=id_,\n            object=\"chat.completion.chunk\",\n            created=created,\n            model=request.model,\n            choices=[ChatChunkChoice(index=0, delta=Delta(), finish_reason=\"stop\")],\n        )\n        self.wfile.write(f\"data: {finish_resp.model_dump_json()}\\n\\n\".encode())\n        self.wfile.write(b\"data: [DONE]\\n\\n\")\n        self.wfile.flush()\n\n    else:\n        response_text = self.server.recipe.generate(\n            messages_dicts,\n            request.max_tokens,\n            request.temperature,\n            request.top_k,\n        )\n\n        response = ChatCompletionResponse(\n            id=f\"chatcmpl-{int(time.time())}\",\n            object=\"chat.completion\",\n            created=int(time.time()),\n            model=request.model,\n            choices=[\n                ChatChoice(\n                    index=0,\n                    message=ChatMessage(role=\"assistant\", content=response_text),\n                    finish_reason=\"stop\",\n                )\n            ],\n            usage={\n                \"prompt_tokens\": 0,\n                \"completion_tokens\": 0,\n                \"total_tokens\": 0,\n            },\n        )\n        self._send_response(response)\n</code></pre>"},{"location":"reference/recipe/serve/base/#optimus_dl.recipe.serve.base.RequestHandler.handle_completions","title":"<code>handle_completions()</code>","text":"<p>Handle legacy text completion requests (/v1/completions).</p> Source code in <code>optimus_dl/recipe/serve/base.py</code> <pre><code>def handle_completions(self):\n    \"\"\"Handle legacy text completion requests (/v1/completions).\"\"\"\n    request = self._parse_request(CompletionRequest)\n\n    # Non-streaming only for now for basic completions, or implement stream if needed\n    # Assuming request.stream is supported later or ignored.\n    # But generate_stream supports it.\n\n    response_text = self.server.recipe.generate(\n        request.prompt,\n        request.max_tokens,\n        request.temperature,\n        request.top_k,\n    )\n\n    response = CompletionResponse(\n        id=f\"cmpl-{int(time.time())}\",\n        object=\"text_completion\",\n        created=int(time.time()),\n        model=request.model,\n        choices=[\n            Choice(\n                index=0,\n                text=response_text,\n                finish_reason=\"length\",  # Simplification\n            )\n        ],\n        usage={\n            \"prompt_tokens\": 0,\n            \"completion_tokens\": 0,\n            \"total_tokens\": 0,\n        },\n    )\n    self._send_response(response)\n</code></pre>"},{"location":"reference/recipe/serve/base/#optimus_dl.recipe.serve.base.ServeRecipe","title":"<code>ServeRecipe</code>","text":"<p>Recipe for serving LLM Baselines models via simple HTTP API.</p> <p>This class loads a model from a checkpoint or config, initializes the tokenizer, and starts an HTTP server compatible with OpenAI clients.</p> Source code in <code>optimus_dl/recipe/serve/base.py</code> <pre><code>class ServeRecipe:\n    \"\"\"Recipe for serving LLM Baselines models via simple HTTP API.\n\n    This class loads a model from a checkpoint or config, initializes the\n    tokenizer, and starts an HTTP server compatible with OpenAI clients.\n    \"\"\"\n\n    def __init__(self, cfg: ServeConfig):\n        self.cfg = cfg\n        self.model = None\n        self.tokenizer = None\n        self.device = None\n\n        # Initialize builder with empty config as we load from checkpoint\n        chkp_cfg = CheckpointManagerConfig()\n        self.checkpoint_manager = CheckpointManager(chkp_cfg)\n\n        modelb_cfg = ModelBuilderConfig()\n        self.model_builder = ModelBuilder(modelb_cfg)\n\n    def setup(self):\n        \"\"\"Load model weights and tokenizer, and configure the device.\"\"\"\n        # Setup device\n        if self.cfg.common.device == \"auto\":\n            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = torch.device(self.cfg.common.device)\n\n        logger.info(f\"Using device: {self.device}\")\n\n        # Build collective for potential distributed init\n        collective = build_best_collective(\n            device=None if self.device.type == \"cuda\" else torch.device(\"cpu\"),\n            config=DistributedConfig(),\n        )\n\n        assert (self.cfg.common.checkpoint_path is not None) ^ (\n            self.cfg.common.model is not None\n        ), \"Either checkpoint_path or model must be specified, but not both\"\n\n        if self.cfg.common.checkpoint_path is not None:\n            logger.info(\n                f\"Loading model from checkpoint: {self.cfg.common.checkpoint_path}\"\n            )\n            self.model, _ = self.checkpoint_manager.build_model_from_checkpoint(\n                checkpoint_path=self.cfg.common.checkpoint_path, device=self.device\n            )\n        else:\n            logger.info(\"Building model from config\")\n            self.model = self.model_builder.build_model(\n                model_config=self.cfg.common.model,\n                collective=collective,\n            )\n\n        self.model.to(self.device)\n        self.model.eval()\n\n        # Build tokenizer\n        self.tokenizer = build_tokenizer(self.cfg.common.tokenizer)\n        logger.info(\"Model and tokenizer loaded\")\n\n    @torch.no_grad()\n    def _debug_tokens_log(self, input_ids):\n        \"\"\"Log tokens for debugging.\"\"\"\n        tokens = []\n        for token in input_ids.cpu().reshape(-1):\n            token = token.item()\n            tokens.append(f\"{token}:'{self.tokenizer.decode([token])}'\")\n        logger.debug(f\"Input tokens: {' '.join(tokens)}\")\n\n    @torch.no_grad()\n    def generate_stream(\n        self,\n        prompt_or_messages: str | list[dict],\n        max_new_tokens: int = 50,\n        temperature: float = 1.0,\n        top_k: int | None = None,\n    ):\n        \"\"\"Generate text continuation yielding chunks.\n\n        Handles tokenization (including chat templates), inference loop,\n        sampling, and detokenization delta logic for streaming.\n\n        Args:\n            prompt_or_messages: Input string or list of chat messages.\n            max_new_tokens: Maximum number of tokens to generate.\n            temperature: Sampling temperature (0.0 for greedy).\n            top_k: Optional top-k sampling.\n\n        Yields:\n            String chunks of generated text.\n        \"\"\"\n        if isinstance(prompt_or_messages, list):\n            # Apply chat template\n            input_ids_list = self.tokenizer.apply_chat_template(\n                prompt_or_messages, tokenize=True, add_generation_prompt=True\n            )\n            input_ids = torch.tensor(\n                input_ids_list, dtype=torch.long, device=self.device\n            ).unsqueeze(0)\n        else:\n            if isinstance(prompt_or_messages, list):\n                # Handle list of strings? Simple server assumes single string prompt\n                prompt_or_messages = prompt_or_messages[0]\n\n            input_ids = torch.tensor(\n                self.tokenizer.encode(prompt_or_messages),\n                dtype=torch.long,\n                device=self.device,\n            ).unsqueeze(0)\n\n        self._debug_tokens_log(input_ids)\n\n        generated_ids = []\n        last_text = \"\"\n\n        for _ in range(max_new_tokens):\n            # Crop context if needed\n            if input_ids.size(1) &gt; self.model.config.sequence_length:\n                input_cond = input_ids[:, -self.model.config.sequence_length :]\n            else:\n                input_cond = input_ids\n\n            outputs = self.model(input_cond)\n            logits = outputs[\"logits\"][:, -1, :]\n\n            if temperature &gt; 0:\n                logits = logits / temperature\n                if top_k is not None:\n                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                    logits[logits &lt; v[:, [-1]]] = -float(\"Inf\")\n                probs = F.softmax(logits, dim=-1)\n                next_token = torch.multinomial(probs, num_samples=1)\n            else:\n                next_token = torch.argmax(logits, dim=-1, keepdim=True)\n\n            input_ids = torch.cat([input_ids, next_token], dim=1)\n            generated_ids.append(next_token.item())\n\n            # Simple streaming: decode all and yield diff\n            # This is inefficient but safe for bytes/utf-8 boundaries\n            current_text = self.tokenizer.decode(generated_ids)\n            new_text = current_text[len(last_text) :]\n\n            if new_text:\n                yield new_text\n                last_text = current_text\n\n            if (\n                hasattr(self.cfg.common.tokenizer, \"eos_token_id\")\n                and next_token.item() == self.cfg.common.tokenizer.eos_token_id\n            ):\n                break\n\n    def generate(\n        self,\n        prompt_or_messages: str | list[dict],\n        max_new_tokens: int = 50,\n        temperature: float = 1.0,\n        top_k: int | None = None,\n    ) -&gt; str:\n        \"\"\"Generate full text continuation.\n\n        Wrapper around `generate_stream` that accumulates all chunks.\n        \"\"\"\n        return \"\".join(\n            list(\n                self.generate_stream(\n                    prompt_or_messages, max_new_tokens, temperature, top_k\n                )\n            )\n        )\n\n    def run(self):\n        \"\"\"Start the HTTP server.\"\"\"\n        self.setup()\n\n        server_address = (self.cfg.serve.host, self.cfg.serve.port)\n        httpd = HTTPServer(server_address, RequestHandler)\n        httpd.recipe = self\n\n        logger.info(f\"Serving at http://{self.cfg.serve.host}:{self.cfg.serve.port}\")\n\n        # Example payloads\n        text_completion_ex = json.dumps(\n            {\n                \"prompt\": \"Once upon a time\",\n                \"max_tokens\": 20,\n                \"temperature\": 0.8,\n            }\n        )\n        logger.info(\n            f\"Text Completion Example:\\ncurl -X POST http://{self.cfg.serve.host}:{self.cfg.serve.port}/v1/completions -d '{text_completion_ex}'\"\n        )\n\n        chat_completion_ex = json.dumps(\n            {\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                    {\"role\": \"user\", \"content\": \"Hello!\"},\n                ],\n                \"max_tokens\": 20,\n                \"temperature\": 0.8,\n                \"stream\": True,\n            }\n        )\n        logger.info(\n            f\"Chat Streaming Example:\\ncurl -X POST http://{self.cfg.serve.host}:{self.cfg.serve.port}/v1/chat/completions -d '{chat_completion_ex}'\"\n        )\n\n        try:\n            httpd.serve_forever()\n        except KeyboardInterrupt:\n            pass\n        httpd.server_close()\n        logger.info(\"Server stopped\")\n</code></pre>"},{"location":"reference/recipe/serve/base/#optimus_dl.recipe.serve.base.ServeRecipe.generate","title":"<code>generate(prompt_or_messages, max_new_tokens=50, temperature=1.0, top_k=None)</code>","text":"<p>Generate full text continuation.</p> <p>Wrapper around <code>generate_stream</code> that accumulates all chunks.</p> Source code in <code>optimus_dl/recipe/serve/base.py</code> <pre><code>def generate(\n    self,\n    prompt_or_messages: str | list[dict],\n    max_new_tokens: int = 50,\n    temperature: float = 1.0,\n    top_k: int | None = None,\n) -&gt; str:\n    \"\"\"Generate full text continuation.\n\n    Wrapper around `generate_stream` that accumulates all chunks.\n    \"\"\"\n    return \"\".join(\n        list(\n            self.generate_stream(\n                prompt_or_messages, max_new_tokens, temperature, top_k\n            )\n        )\n    )\n</code></pre>"},{"location":"reference/recipe/serve/base/#optimus_dl.recipe.serve.base.ServeRecipe.generate_stream","title":"<code>generate_stream(prompt_or_messages, max_new_tokens=50, temperature=1.0, top_k=None)</code>","text":"<p>Generate text continuation yielding chunks.</p> <p>Handles tokenization (including chat templates), inference loop, sampling, and detokenization delta logic for streaming.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_or_messages</code> <code>str | list[dict]</code> <p>Input string or list of chat messages.</p> required <code>max_new_tokens</code> <code>int</code> <p>Maximum number of tokens to generate.</p> <code>50</code> <code>temperature</code> <code>float</code> <p>Sampling temperature (0.0 for greedy).</p> <code>1.0</code> <code>top_k</code> <code>int | None</code> <p>Optional top-k sampling.</p> <code>None</code> <p>Yields:</p> Type Description <p>String chunks of generated text.</p> Source code in <code>optimus_dl/recipe/serve/base.py</code> <pre><code>@torch.no_grad()\ndef generate_stream(\n    self,\n    prompt_or_messages: str | list[dict],\n    max_new_tokens: int = 50,\n    temperature: float = 1.0,\n    top_k: int | None = None,\n):\n    \"\"\"Generate text continuation yielding chunks.\n\n    Handles tokenization (including chat templates), inference loop,\n    sampling, and detokenization delta logic for streaming.\n\n    Args:\n        prompt_or_messages: Input string or list of chat messages.\n        max_new_tokens: Maximum number of tokens to generate.\n        temperature: Sampling temperature (0.0 for greedy).\n        top_k: Optional top-k sampling.\n\n    Yields:\n        String chunks of generated text.\n    \"\"\"\n    if isinstance(prompt_or_messages, list):\n        # Apply chat template\n        input_ids_list = self.tokenizer.apply_chat_template(\n            prompt_or_messages, tokenize=True, add_generation_prompt=True\n        )\n        input_ids = torch.tensor(\n            input_ids_list, dtype=torch.long, device=self.device\n        ).unsqueeze(0)\n    else:\n        if isinstance(prompt_or_messages, list):\n            # Handle list of strings? Simple server assumes single string prompt\n            prompt_or_messages = prompt_or_messages[0]\n\n        input_ids = torch.tensor(\n            self.tokenizer.encode(prompt_or_messages),\n            dtype=torch.long,\n            device=self.device,\n        ).unsqueeze(0)\n\n    self._debug_tokens_log(input_ids)\n\n    generated_ids = []\n    last_text = \"\"\n\n    for _ in range(max_new_tokens):\n        # Crop context if needed\n        if input_ids.size(1) &gt; self.model.config.sequence_length:\n            input_cond = input_ids[:, -self.model.config.sequence_length :]\n        else:\n            input_cond = input_ids\n\n        outputs = self.model(input_cond)\n        logits = outputs[\"logits\"][:, -1, :]\n\n        if temperature &gt; 0:\n            logits = logits / temperature\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits &lt; v[:, [-1]]] = -float(\"Inf\")\n            probs = F.softmax(logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1)\n        else:\n            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n\n        input_ids = torch.cat([input_ids, next_token], dim=1)\n        generated_ids.append(next_token.item())\n\n        # Simple streaming: decode all and yield diff\n        # This is inefficient but safe for bytes/utf-8 boundaries\n        current_text = self.tokenizer.decode(generated_ids)\n        new_text = current_text[len(last_text) :]\n\n        if new_text:\n            yield new_text\n            last_text = current_text\n\n        if (\n            hasattr(self.cfg.common.tokenizer, \"eos_token_id\")\n            and next_token.item() == self.cfg.common.tokenizer.eos_token_id\n        ):\n            break\n</code></pre>"},{"location":"reference/recipe/serve/base/#optimus_dl.recipe.serve.base.ServeRecipe.run","title":"<code>run()</code>","text":"<p>Start the HTTP server.</p> Source code in <code>optimus_dl/recipe/serve/base.py</code> <pre><code>def run(self):\n    \"\"\"Start the HTTP server.\"\"\"\n    self.setup()\n\n    server_address = (self.cfg.serve.host, self.cfg.serve.port)\n    httpd = HTTPServer(server_address, RequestHandler)\n    httpd.recipe = self\n\n    logger.info(f\"Serving at http://{self.cfg.serve.host}:{self.cfg.serve.port}\")\n\n    # Example payloads\n    text_completion_ex = json.dumps(\n        {\n            \"prompt\": \"Once upon a time\",\n            \"max_tokens\": 20,\n            \"temperature\": 0.8,\n        }\n    )\n    logger.info(\n        f\"Text Completion Example:\\ncurl -X POST http://{self.cfg.serve.host}:{self.cfg.serve.port}/v1/completions -d '{text_completion_ex}'\"\n    )\n\n    chat_completion_ex = json.dumps(\n        {\n            \"messages\": [\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": \"Hello!\"},\n            ],\n            \"max_tokens\": 20,\n            \"temperature\": 0.8,\n            \"stream\": True,\n        }\n    )\n    logger.info(\n        f\"Chat Streaming Example:\\ncurl -X POST http://{self.cfg.serve.host}:{self.cfg.serve.port}/v1/chat/completions -d '{chat_completion_ex}'\"\n    )\n\n    try:\n        httpd.serve_forever()\n    except KeyboardInterrupt:\n        pass\n    httpd.server_close()\n    logger.info(\"Server stopped\")\n</code></pre>"},{"location":"reference/recipe/serve/base/#optimus_dl.recipe.serve.base.ServeRecipe.setup","title":"<code>setup()</code>","text":"<p>Load model weights and tokenizer, and configure the device.</p> Source code in <code>optimus_dl/recipe/serve/base.py</code> <pre><code>def setup(self):\n    \"\"\"Load model weights and tokenizer, and configure the device.\"\"\"\n    # Setup device\n    if self.cfg.common.device == \"auto\":\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        self.device = torch.device(self.cfg.common.device)\n\n    logger.info(f\"Using device: {self.device}\")\n\n    # Build collective for potential distributed init\n    collective = build_best_collective(\n        device=None if self.device.type == \"cuda\" else torch.device(\"cpu\"),\n        config=DistributedConfig(),\n    )\n\n    assert (self.cfg.common.checkpoint_path is not None) ^ (\n        self.cfg.common.model is not None\n    ), \"Either checkpoint_path or model must be specified, but not both\"\n\n    if self.cfg.common.checkpoint_path is not None:\n        logger.info(\n            f\"Loading model from checkpoint: {self.cfg.common.checkpoint_path}\"\n        )\n        self.model, _ = self.checkpoint_manager.build_model_from_checkpoint(\n            checkpoint_path=self.cfg.common.checkpoint_path, device=self.device\n        )\n    else:\n        logger.info(\"Building model from config\")\n        self.model = self.model_builder.build_model(\n            model_config=self.cfg.common.model,\n            collective=collective,\n        )\n\n    self.model.to(self.device)\n    self.model.eval()\n\n    # Build tokenizer\n    self.tokenizer = build_tokenizer(self.cfg.common.tokenizer)\n    logger.info(\"Model and tokenizer loaded\")\n</code></pre>"},{"location":"reference/recipe/serve/config/","title":"config","text":""},{"location":"reference/recipe/serve/config/#optimus_dl.recipe.serve.config","title":"<code>optimus_dl.recipe.serve.config</code>","text":""},{"location":"reference/recipe/serve/config/#optimus_dl.recipe.serve.config.ServeCommonConfig","title":"<code>ServeCommonConfig</code>  <code>dataclass</code>","text":"<p>ServeCommonConfig(checkpoint_path: str | None = None, model: Any = None, tokenizer: optimus_dl.modules.tokenizer.config.BaseTokenizerConfig = '???', device: str = 'auto')</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>str | None</code> <p>Path to model checkpoint</p> <code>None</code> <code>model</code> <code>Any</code> <p>Model to build (if you want to load model not from checkpoint)</p> <code>None</code> <code>tokenizer</code> <code>BaseTokenizerConfig</code> <code>'???'</code> <code>device</code> <code>str</code> <p>Device to use (cuda, cpu, auto)</p> <code>'auto'</code> Source code in <code>optimus_dl/recipe/serve/config.py</code> <pre><code>@dataclass\nclass ServeCommonConfig:\n    checkpoint_path: str | None = field(\n        default=None, metadata={\"description\": \"Path to model checkpoint\"}\n    )\n    model: Any = field(\n        default=None,\n        metadata={\n            \"description\": \"Model to build (if you want to load model not from checkpoint)\"\n        },\n    )\n    tokenizer: BaseTokenizerConfig = field(default=MISSING)\n    device: str = field(\n        default=\"auto\", metadata={\"description\": \"Device to use (cuda, cpu, auto)\"}\n    )\n</code></pre>"},{"location":"reference/recipe/serve/config/#optimus_dl.recipe.serve.config.ServeConfig","title":"<code>ServeConfig</code>  <code>dataclass</code>","text":"<p>ServeConfig(serve: optimus_dl.recipe.serve.config.ServeRecipeConfig = , common: optimus_dl.recipe.serve.config.ServeCommonConfig = ) <p>Parameters:</p> Name Type Description Default <code>serve</code> <code>ServeRecipeConfig</code> <p>ServeRecipeConfig(port: int = 8000, host: str = '0.0.0.0')</p> <code>&lt;dynamic&gt;</code> <code>common</code> <code>ServeCommonConfig</code> <p>ServeCommonConfig(checkpoint_path: str | None = None, model: Any = None, tokenizer: optimus_dl.modules.tokenizer.config.BaseTokenizerConfig = '???', device: str = 'auto')</p> <code>&lt;dynamic&gt;</code> Source code in <code>optimus_dl/recipe/serve/config.py</code> <pre><code>@dataclass\nclass ServeConfig:\n    serve: ServeRecipeConfig = field(default_factory=ServeRecipeConfig)\n    common: ServeCommonConfig = field(default_factory=ServeCommonConfig)\n</code></pre>"},{"location":"reference/recipe/serve/config/#optimus_dl.recipe.serve.config.ServeRecipeConfig","title":"<code>ServeRecipeConfig</code>  <code>dataclass</code>","text":"<p>ServeRecipeConfig(port: int = 8000, host: str = '0.0.0.0')</p> <p>Parameters:</p> Name Type Description Default <code>port</code> <code>int</code> <p>Port to serve on</p> <code>8000</code> <code>host</code> <code>str</code> <p>Host to serve on</p> <code>'0.0.0.0'</code> Source code in <code>optimus_dl/recipe/serve/config.py</code> <pre><code>@dataclass\nclass ServeRecipeConfig:\n    port: int = field(default=8000, metadata={\"description\": \"Port to serve on\"})\n    host: str = field(default=\"0.0.0.0\", metadata={\"description\": \"Host to serve on\"})\n</code></pre>"},{"location":"reference/recipe/serve/models/","title":"models","text":""},{"location":"reference/recipe/serve/models/#optimus_dl.recipe.serve.models","title":"<code>optimus_dl.recipe.serve.models</code>","text":""},{"location":"reference/recipe/serve/models/#optimus_dl.recipe.serve.models.ChatChoice","title":"<code>ChatChoice</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single chat completion choice.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> required <code>message</code> <code>ChatMessage</code> required <code>finish_reason</code> <code>str | None</code> <code>None</code> Source code in <code>optimus_dl/recipe/serve/models.py</code> <pre><code>class ChatChoice(BaseModel):\n    \"\"\"A single chat completion choice.\n\n    Attributes:\n        index: The index of the choice in the list of choices.\n        message: The generated message.\n        finish_reason: The reason the model stopped generating tokens.\n    \"\"\"\n\n    index: int\n    message: ChatMessage\n    finish_reason: str | None = None\n</code></pre>"},{"location":"reference/recipe/serve/models/#optimus_dl.recipe.serve.models.ChatChunkChoice","title":"<code>ChatChunkChoice</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single chat completion chunk choice.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> required <code>delta</code> <code>Delta</code> required <code>finish_reason</code> <code>str | None</code> <code>None</code> Source code in <code>optimus_dl/recipe/serve/models.py</code> <pre><code>class ChatChunkChoice(BaseModel):\n    \"\"\"A single chat completion chunk choice.\n\n    Attributes:\n        index: The index of the choice in the list of choices.\n        delta: The message delta.\n        finish_reason: The reason the model stopped generating tokens.\n    \"\"\"\n\n    index: int\n    delta: Delta\n    finish_reason: str | None = None\n</code></pre>"},{"location":"reference/recipe/serve/models/#optimus_dl.recipe.serve.models.ChatCompletionChunk","title":"<code>ChatCompletionChunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a streamed chunk of a chat completion response.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> required <code>object</code> <code>Literal['chat.completion.chunk']</code> required <code>created</code> <code>int</code> required <code>model</code> <code>str</code> required <code>choices</code> <code>list[ChatChunkChoice]</code> required Source code in <code>optimus_dl/recipe/serve/models.py</code> <pre><code>class ChatCompletionChunk(BaseModel):\n    \"\"\"Represents a streamed chunk of a chat completion response.\n\n    Attributes:\n        id: A unique identifier for the chat completion.\n        object: The object type, which is always \"chat.completion.chunk\".\n        created: The Unix timestamp (in seconds) of when the chat completion was created.\n        model: The model used for the chat completion.\n        choices: The list of chat completion choices.\n    \"\"\"\n\n    id: str\n    object: Literal[\"chat.completion.chunk\"]\n    created: int\n    model: str\n    choices: list[ChatChunkChoice]\n</code></pre>"},{"location":"reference/recipe/serve/models/#optimus_dl.recipe.serve.models.ChatCompletionRequest","title":"<code>ChatCompletionRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request body for the chat completion API.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <code>'optimus-dl-model'</code> <code>messages</code> <code>list[dict]</code> required <code>max_tokens</code> <code>int</code> <code>50</code> <code>temperature</code> <code>float</code> <code>1.0</code> <code>top_k</code> <code>int | None</code> <code>None</code> <code>stream</code> <code>bool</code> <code>False</code> Source code in <code>optimus_dl/recipe/serve/models.py</code> <pre><code>class ChatCompletionRequest(BaseModel):\n    \"\"\"Request body for the chat completion API.\n\n    Attributes:\n        model: ID of the model to use.\n        messages: A list of messages comprising the conversation so far.\n        max_tokens: The maximum number of tokens to generate in the chat completion.\n        temperature: What sampling temperature to use, between 0 and 2.\n        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering.\n        stream: If set, partial message deltas will be sent.\n    \"\"\"\n\n    model: str = \"optimus-dl-model\"\n    messages: list[dict]  # Use dict to allow flexibility or define strict message model\n    max_tokens: int = Field(default=50, ge=1)\n    temperature: float = Field(default=1.0, ge=0.0)\n    top_k: int | None = Field(default=None, ge=1)\n    stream: bool = False\n</code></pre>"},{"location":"reference/recipe/serve/models/#optimus_dl.recipe.serve.models.ChatCompletionResponse","title":"<code>ChatCompletionResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response object for the chat completion API.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> required <code>object</code> <code>Literal['chat.completion']</code> required <code>created</code> <code>int</code> required <code>model</code> <code>str</code> required <code>choices</code> <code>list[ChatChoice]</code> required <code>usage</code> <code>dict | None</code> <code>None</code> Source code in <code>optimus_dl/recipe/serve/models.py</code> <pre><code>class ChatCompletionResponse(BaseModel):\n    \"\"\"Response object for the chat completion API.\n\n    Attributes:\n        id: A unique identifier for the chat completion.\n        object: The object type, which is always \"chat.completion\".\n        created: The Unix timestamp (in seconds) of when the chat completion was created.\n        model: The model used for the chat completion.\n        choices: The list of chat completion choices.\n        usage: Usage statistics for the completion request.\n    \"\"\"\n\n    id: str\n    object: Literal[\"chat.completion\"]\n    created: int\n    model: str\n    choices: list[ChatChoice]\n    usage: dict | None = None\n</code></pre>"},{"location":"reference/recipe/serve/models/#optimus_dl.recipe.serve.models.ChatMessage","title":"<code>ChatMessage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single message in a chat conversation.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>role</code> <code>str | None</code> <code>None</code> <code>content</code> <code>str | None</code> <code>None</code> Source code in <code>optimus_dl/recipe/serve/models.py</code> <pre><code>class ChatMessage(BaseModel):\n    \"\"\"A single message in a chat conversation.\n\n    Attributes:\n        role: The role of the message sender (e.g., 'user', 'assistant', 'system').\n        content: The content of the message.\n    \"\"\"\n\n    role: str | None = None\n    content: str | None = None\n</code></pre>"},{"location":"reference/recipe/serve/models/#optimus_dl.recipe.serve.models.Choice","title":"<code>Choice</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single completion choice.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> required <code>text</code> <code>str</code> required <code>logprobs</code> <code>dict | None</code> <code>None</code> <code>finish_reason</code> <code>str | None</code> <code>None</code> Source code in <code>optimus_dl/recipe/serve/models.py</code> <pre><code>class Choice(BaseModel):\n    \"\"\"A single completion choice.\n\n    Attributes:\n        index: The index of the choice in the list of choices.\n        text: The generated text.\n        logprobs: Log probabilities of the token choices (optional).\n        finish_reason: The reason the model stopped generating tokens.\n    \"\"\"\n\n    index: int\n    text: str\n    logprobs: dict | None = None\n    finish_reason: str | None = None\n</code></pre>"},{"location":"reference/recipe/serve/models/#optimus_dl.recipe.serve.models.CompletionChunk","title":"<code>CompletionChunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a streamed chunk of a text completion response.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> required <code>object</code> <code>Literal['text_completion']</code> required <code>created</code> <code>int</code> required <code>model</code> <code>str</code> required <code>choices</code> <code>list[CompletionChunkChoice]</code> required Source code in <code>optimus_dl/recipe/serve/models.py</code> <pre><code>class CompletionChunk(BaseModel):\n    \"\"\"Represents a streamed chunk of a text completion response.\n\n    Attributes:\n        id: A unique identifier for the completion.\n        object: The object type, which is always \"text_completion\".\n        created: The Unix timestamp (in seconds) of when the completion was created.\n        model: The model used for the completion.\n        choices: The list of completion choices.\n    \"\"\"\n\n    id: str\n    object: Literal[\"text_completion\"]\n    created: int\n    model: str\n    choices: list[CompletionChunkChoice]\n</code></pre>"},{"location":"reference/recipe/serve/models/#optimus_dl.recipe.serve.models.CompletionChunkChoice","title":"<code>CompletionChunkChoice</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single text completion chunk choice.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> required <code>text</code> <code>str</code> required <code>logprobs</code> <code>dict | None</code> <code>None</code> <code>finish_reason</code> <code>str | None</code> <code>None</code> Source code in <code>optimus_dl/recipe/serve/models.py</code> <pre><code>class CompletionChunkChoice(BaseModel):\n    \"\"\"A single text completion chunk choice.\n\n    Attributes:\n        index: The index of the choice in the list of choices.\n        text: The text chunk.\n        logprobs: Log probabilities of the token choices (optional).\n        finish_reason: The reason the model stopped generating tokens.\n    \"\"\"\n\n    index: int\n    text: str\n    logprobs: dict | None = None\n    finish_reason: str | None = None\n</code></pre>"},{"location":"reference/recipe/serve/models/#optimus_dl.recipe.serve.models.CompletionRequest","title":"<code>CompletionRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request body for the text completion API.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <code>'optimus-dl-model'</code> <code>prompt</code> <code>str | list[str]</code> required <code>max_tokens</code> <code>int</code> <code>50</code> <code>temperature</code> <code>float</code> <code>1.0</code> <code>top_k</code> <code>int | None</code> <code>None</code> <code>stream</code> <code>bool</code> <code>False</code> Source code in <code>optimus_dl/recipe/serve/models.py</code> <pre><code>class CompletionRequest(BaseModel):\n    \"\"\"Request body for the text completion API.\n\n    Attributes:\n        model: ID of the model to use.\n        prompt: The prompt(s) to generate completions for.\n        max_tokens: The maximum number of tokens to generate in the completion.\n        temperature: What sampling temperature to use, between 0 and 2.\n        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering.\n        stream: If set, partial message deltas will be sent.\n    \"\"\"\n\n    model: str = \"optimus-dl-model\"\n    prompt: str | list[str]\n    max_tokens: int = Field(default=50, ge=1)\n    temperature: float = Field(default=1.0, ge=0.0)\n    top_k: int | None = Field(default=None, ge=1)\n    stream: bool = False\n</code></pre>"},{"location":"reference/recipe/serve/models/#optimus_dl.recipe.serve.models.CompletionResponse","title":"<code>CompletionResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response object for the text completion API.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> required <code>object</code> <code>Literal['text_completion']</code> required <code>created</code> <code>int</code> required <code>model</code> <code>str</code> required <code>choices</code> <code>list[Choice]</code> required <code>usage</code> <code>dict | None</code> <code>None</code> Source code in <code>optimus_dl/recipe/serve/models.py</code> <pre><code>class CompletionResponse(BaseModel):\n    \"\"\"Response object for the text completion API.\n\n    Attributes:\n        id: A unique identifier for the completion.\n        object: The object type, which is always \"text_completion\".\n        created: The Unix timestamp (in seconds) of when the completion was created.\n        model: The model used for completion.\n        choices: The list of completion choices.\n        usage: Usage statistics for the completion request.\n    \"\"\"\n\n    id: str\n    object: Literal[\"text_completion\"]\n    created: int\n    model: str\n    choices: list[Choice]\n    usage: dict | None = None\n</code></pre>"},{"location":"reference/recipe/serve/models/#optimus_dl.recipe.serve.models.Delta","title":"<code>Delta</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A partial message delta for streaming responses.</p> <p>Attributes:</p> Name Type Description <p>Parameters:</p> Name Type Description Default <code>role</code> <code>str | None</code> <code>None</code> <code>content</code> <code>str | None</code> <code>None</code> Source code in <code>optimus_dl/recipe/serve/models.py</code> <pre><code>class Delta(BaseModel):\n    \"\"\"A partial message delta for streaming responses.\n\n    Attributes:\n        role: The role of the message sender.\n        content: The content of the message delta.\n    \"\"\"\n\n    role: str | None = None\n    content: str | None = None\n</code></pre>"},{"location":"reference/recipe/train/","title":"Index","text":""},{"location":"reference/recipe/train/#optimus_dl.recipe.train","title":"<code>optimus_dl.recipe.train</code>","text":""},{"location":"reference/recipe/train/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>base</code>: Main training recipe that orchestrates all training components.</li> <li><code>builders</code>: </li> <li><code>config</code>: Training recipe configuration.</li> <li><code>mixins</code>: </li> </ul>"},{"location":"reference/recipe/train/base/","title":"base","text":""},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base","title":"<code>optimus_dl.recipe.train.base</code>","text":""},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe","title":"<code>TrainRecipe</code>","text":"<p>               Bases: <code>TrainingContextMixin</code>, <code>TrainingIterationMixin</code>, <code>TrainingInterruptionMixin</code></p> <p>Main training recipe that orchestrates all training components.</p> <p>This class uses composition to coordinate specialized builders and managers:</p> <ul> <li>ModelBuilder: Builds models and applies transforms</li> <li>OptimizerBuilder: Builds optimizers</li> <li>CriterionBuilder: Builds loss criteria</li> <li>DataBuilder: Builds train/eval data pipelines</li> <li>SchedulerBuilder: Builds learning rate schedulers</li> <li>LoggerManager: Handles metrics logging setup and operations</li> <li>CheckpointManager: Manages checkpoint saving and loading</li> <li>Evaluator: Handles evaluation runs and metrics</li> </ul> <p>It inherits from training logic mixins for the core loop execution:</p> <ul> <li>TrainingContextMixin: Sets up training context (AMP, scaler, etc.)</li> <li>TrainingIterationMixin: Orchestrates complete training iterations</li> <li>TrainingInterruptionMixin: Handles training interruptions and errors</li> </ul> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>@register_train_recipe(\"base\", TrainConfig)\nclass TrainRecipe(\n    TrainingContextMixin,\n    TrainingIterationMixin,\n    TrainingInterruptionMixin,\n):\n    \"\"\"Main training recipe that orchestrates all training components.\n\n    This class uses composition to coordinate specialized builders and managers:\n\n    - ModelBuilder: Builds models and applies transforms\n    - OptimizerBuilder: Builds optimizers\n    - CriterionBuilder: Builds loss criteria\n    - DataBuilder: Builds train/eval data pipelines\n    - SchedulerBuilder: Builds learning rate schedulers\n    - LoggerManager: Handles metrics logging setup and operations\n    - CheckpointManager: Manages checkpoint saving and loading\n    - Evaluator: Handles evaluation runs and metrics\n\n    It inherits from training logic mixins for the core loop execution:\n\n    - TrainingContextMixin: Sets up training context (AMP, scaler, etc.)\n    - TrainingIterationMixin: Orchestrates complete training iterations\n    - TrainingInterruptionMixin: Handles training interruptions and errors\n    \"\"\"\n\n    cfg: TrainConfig\n\n    def __init__(self, cfg: TrainConfig) -&gt; None:\n        self.cfg = cfg\n\n        # Initialize components via composition using registry for dependency injection\n        self.model_builder = build_component(\n            \"model_builder\",\n            cfg.model_builder,\n            cast_to=ModelBuilder,\n            model_transforms=cfg.model_transforms,\n        )\n        assert self.model_builder is not None, \"Model builder not initialized\"\n        self.optimizer_builder = build_component(\n            \"optimizer_builder\",\n            cfg.optimizer_builder,\n            cast_to=OptimizerBuilder,\n            optimization_config=cfg.optimization,\n        )\n        assert self.optimizer_builder is not None, \"Optimizer builder not initialized\"\n        self.criterion_builder = build_component(\n            \"criterion_builder\",\n            cfg.criterion_builder,\n            cast_to=CriterionBuilder,\n            criterion_config=cfg.criterion,\n        )\n        assert self.criterion_builder is not None, \"Criterion builder not initialized\"\n        self.data_builder = build_component(\n            \"data_builder\",\n            cfg.data_builder,\n            cast_to=DataBuilder,\n            data_config=cfg.data,\n            data_seed=cfg.common.data_seed,\n        )\n        assert self.data_builder is not None, \"Data builder not initialized\"\n        self.scheduler_builder = build_component(\n            \"scheduler_builder\",\n            cfg.scheduler_builder,\n            cast_to=SchedulerBuilder,\n            lr_scheduler_config=cfg.lr_scheduler,\n            optimization_config=cfg.optimization,\n        )\n        assert self.scheduler_builder is not None, \"Scheduler builder not initialized\"\n        self.logger_manager = build_component(\n            \"logger_manager\",\n            cfg.logger_manager,\n            cast_to=LoggerManager,\n            loggers_config=cfg.loggers,\n        )\n        assert self.logger_manager is not None, \"Logger manager not initialized\"\n        self.checkpoint_manager = build_component(\n            \"checkpoint_manager\",\n            cfg.checkpoint_manager,\n            cast_to=CheckpointManager,\n        )\n        assert self.checkpoint_manager is not None, \"Checkpoint manager not initialized\"\n        self.evaluator = build_component(\n            \"evaluator\",\n            cfg.evaluator,\n            cast_to=Evaluator,\n            eval_freq=cfg.common.eval_freq,\n            eval_iterations=cfg.common.eval_iterations,\n        )\n        assert self.evaluator is not None, \"Evaluator not initialized\"\n\n        # Initialize training logic mixins\n        TrainingContextMixin.__init__(self, cfg.optimization)\n        TrainingIterationMixin.__init__(self, cfg.optimization, cfg.common.log_freq)\n        TrainingInterruptionMixin.__init__(\n            self,\n            cfg.common.save_freq,\n            cfg.common.output_path,\n            self.save_checkpoint,  # Pass the checkpoint method as callback\n        )\n\n        self.set_exp_name()\n        self.validate_config()\n\n    # Delegate methods\n    def build_model(self, *args, **kwargs) -&gt; BaseModel:\n        \"\"\"Delegate to ModelBuilder.\"\"\"\n        return self.model_builder.build_model(*args, **kwargs)\n\n    def build_optimizer(self, *args, **kwargs) -&gt; Optimizer:\n        \"\"\"Delegate to OptimizerBuilder.\"\"\"\n        return self.optimizer_builder.build_optimizer(*args, **kwargs)\n\n    def build_lr_scheduler(self, *args, **kwargs):\n        \"\"\"Delegate to SchedulerBuilder.\"\"\"\n        return self.scheduler_builder.build_lr_scheduler(*args, **kwargs)\n\n    def build_criterion(self, *args, **kwargs) -&gt; BaseCriterion:\n        \"\"\"Delegate to CriterionBuilder.\"\"\"\n        return self.criterion_builder.build_criterion(*args, **kwargs)\n\n    def build_train_data(self, *args, **kwargs):\n        \"\"\"Delegate to DataBuilder for training data.\"\"\"\n        return self.data_builder.build_train_data(*args, **kwargs)\n\n    def build_eval_data(self, *args, **kwargs):\n        \"\"\"Delegate to DataBuilder for evaluation data.\"\"\"\n        return self.data_builder.build_eval_data(*args, **kwargs)\n\n    def build_loggers(self, *args, **kwargs):\n        \"\"\"Delegate to LoggerManager for building loggers.\"\"\"\n        return self.logger_manager.build_loggers(*args, **kwargs)\n\n    def setup_loggers(self, experiment_name: str, full_config: dict | None = None):\n        \"\"\"Setup logging with experiment configuration.\n\n        Args:\n            experiment_name: Name of the current experiment.\n            full_config: Full configuration dictionary. If None, uses self.cfg.\n        \"\"\"\n        if full_config is None:\n            full_config = self.cfg if hasattr(self.cfg, \"__dict__\") else dict(self.cfg)\n        return self.logger_manager.setup_loggers(experiment_name, full_config)\n\n    def log_metrics_to_loggers(self, *args, **kwargs):\n        \"\"\"Delegate metrics logging to LoggerManager.\"\"\"\n        return self.logger_manager.log_metrics_to_loggers(*args, **kwargs)\n\n    def close_loggers(self, *args, **kwargs):\n        \"\"\"Delegate logger cleanup to LoggerManager.\"\"\"\n        return self.logger_manager.close_loggers(*args, **kwargs)\n\n    def save_checkpoint_if_needed(self, *args, **kwargs):\n        \"\"\"Check save frequency and delegate to CheckpointManager.\"\"\"\n        config_dict = self.cfg if hasattr(self.cfg, \"__dict__\") else dict(self.cfg)\n        kwargs[\"full_config\"] = config_dict\n        kwargs[\"checkpoint_path\"] = self.cfg.common.output_path\n        kwargs[\"save_freq\"] = self.cfg.common.save_freq\n        kwargs[\"logger_manager\"] = self.logger_manager\n        return self.checkpoint_manager.save_checkpoint_if_needed(*args, **kwargs)\n\n    def save_checkpoint(self, *args, **kwargs):\n        \"\"\"Save a checkpoint via CheckpointManager.\"\"\"\n        config_dict = self.cfg if hasattr(self.cfg, \"__dict__\") else dict(self.cfg)\n        kwargs[\"full_config\"] = config_dict\n        if \"checkpoint_path\" not in kwargs:\n            kwargs[\"checkpoint_path\"] = self.cfg.common.output_path\n        if \"logger_manager\" not in kwargs:\n            kwargs[\"logger_manager\"] = self.logger_manager\n        return self.checkpoint_manager.save_checkpoint(*args, **kwargs)\n\n    def load_checkpoint_if_exists(self, *args, **kwargs):\n        \"\"\"Try to resume from latest checkpoint in output path.\"\"\"\n        if \"checkpoint_path\" not in kwargs:\n            kwargs[\"checkpoint_path\"] = self.cfg.common.output_path\n        if \"logger_manager\" not in kwargs:\n            kwargs[\"logger_manager\"] = self.logger_manager\n        return self.checkpoint_manager.load_checkpoint_if_exists(*args, **kwargs)\n\n    def load_checkpoint(self, *args, **kwargs):\n        \"\"\"Load a specific checkpoint.\"\"\"\n        if \"logger_manager\" not in kwargs:\n            kwargs[\"logger_manager\"] = self.logger_manager\n        return self.checkpoint_manager.load_checkpoint(*args, **kwargs)\n\n    def run_evaluation_if_needed(self, *args, **kwargs):\n        \"\"\"Check eval frequency and run evaluation via Evaluator.\"\"\"\n        return self.evaluator.run_evaluation_if_needed(*args, **kwargs)\n\n    def set_exp_name(self):\n        \"\"\"Set experiment name based on config or environment variables.\"\"\"\n        if not OmegaConf.is_missing(self.cfg.common, \"exp_name\"):\n            return\n        exp_name = f\"run-{self.cfg.model._name}-{time.strftime('%Y-%m-%d-%H-%M-%S')}\"\n        self.cfg.common.exp_name = exp_name\n        logger.info(f\"Experiment name set to: {self.cfg.common.exp_name}\")\n\n    def validate_config(self) -&gt; None:\n        \"\"\"Validate configuration before training starts.\"\"\"\n        # Required fields\n        assert self.cfg.model is not None, \"Model configuration is required\"\n        assert self.cfg.data is not None, \"Data configuration is required\"\n        assert self.cfg.criterion is not None, \"Criterion configuration is required\"\n        assert (\n            self.cfg.optimization is not None\n        ), \"Optimization configuration is required\"\n\n        # Training parameters\n        assert self.cfg.optimization.iterations &gt; 0, \"iterations must be positive\"\n        assert self.cfg.optimization.acc_steps &gt; 0, \"acc_steps must be positive\"\n        assert self.cfg.common.log_freq &gt; 0, \"log_freq must be positive\"\n\n        if self.cfg.common.save_freq &gt; 0:\n            assert (\n                self.cfg.common.output_path\n            ), \"output_path required when save_freq &gt; 0\"\n\n        logger.info(\"Configuration validation passed\")\n\n    def setup_context(self):\n        \"\"\"Setup global training context (precision, etc.).\"\"\"\n        set_seed(self.cfg.common.seed, deterministic=self.cfg.common.deterministic)\n        torch.set_float32_matmul_precision(\"highest\")\n        if torch.cuda.is_available():\n            torch.backends.cuda.matmul.fp32_precision = \"ieee\"\n            if hasattr(torch.backends.cudnn, \"conv\"):\n                torch.backends.cudnn.conv.fp32_precision = \"ieee\"\n\n    def run(self):\n        \"\"\"Run the complete training pipeline.\"\"\"\n        self.setup_context()\n        is_restart = self.checkpoint_manager.is_restart(self.cfg.common.output_path)\n\n        with meters_group(\"init\"):\n            log_event_start(\"perf/init\")\n            logger.info(f\"Using output path : {self.cfg.common.output_path}\")\n            logger.info(self.cfg)\n\n            # Setup device and distributed collective\n            device, collective = setup_device_and_collective(\n                use_gpu=self.cfg.common.use_gpu, config=self.cfg.common.distributed\n            )\n\n            logger.info(\n                \"Setting up console logging. Will log from master only from now.\"\n            )\n            if not collective.is_master:\n                setup_logging(logging.WARNING)\n\n            model: BaseModel = self.build_model(\n                model_config=self.cfg.model,\n                collective=collective,\n                is_restart=is_restart,\n                checkpoint_manager=self.checkpoint_manager,\n            )\n\n            optimizer: Optimizer = self.build_optimizer(model.make_parameter_groups())\n            lr_scheduler = self.build_lr_scheduler(optimizer)\n            criterion: BaseCriterion = self.build_criterion(collective=collective)\n\n            # Setup training context (AMP, scaler, etc.) using recipe mixin\n            training_context = self.setup_training_context(device)\n\n            try:\n                train_datapipeline = self.build_train_data(\n                    device=device, collective=collective\n                )\n                assert (\n                    train_datapipeline is not None\n                ), \"Train data pipeline not initialized\"\n                eval_datapipeline = self.build_eval_data(\n                    device=device, collective=collective\n                )\n                data_loaders = {\n                    \"train\": train_datapipeline.dataloader,\n                    # eval dataloader may be not restored\n                }\n            except Exception as e:\n                logger.error(f\"Failed to build data pipelines: {e}\")\n                raise\n\n            model = model.to(device)\n\n            # cannot be after checkpoint load as may erase the start event\n            log_event_end(\"perf/init\")\n\n            # Try to resume from checkpoint in output paths\n            start_iteration, metadata = self.load_checkpoint_if_exists(\n                model=model,\n                optimizer=optimizer,\n                lr_scheduler=lr_scheduler,\n                data_loaders=data_loaders,\n                collective=collective,\n                data_sources=train_datapipeline.datasets,\n            )\n            if is_restart:\n                # cases when training run but did not produce any artifacts is\n                # indistinguishable from the case when training is not started at all\n                assert metadata is not None, \"Misaligned is_restart flag\"\n\n            logger.info(f\"Considering this run as {is_restart = }\")\n            if not is_restart and self.cfg.common.load_checkpoint is not None:\n                # if checkpoint from output path was not loaded, we are sure that this launch is not\n                # re-scheduling / preemption re-start, so we can try loading model from load_checkpoint\n                metadata = self.load_checkpoint(\n                    model=model,\n                    optimizer=optimizer,\n                    lr_scheduler=lr_scheduler,\n                    data_loaders=data_loaders,\n                    data_sources=train_datapipeline.datasets,\n                    collective=collective,\n                    load_strategy=self.cfg.common.load_checkpoint_strategy,\n                    checkpoint_path=self.cfg.common.load_checkpoint,\n                )\n                start_iteration = metadata[\"iteration\"] + 1\n                logger.info(\n                    \"Loaded checkpoint from \"\n                    f\"checkpoint_path = {self.cfg.common.load_checkpoint} path with \"\n                    f\"load_strategy = {self.cfg.common.load_checkpoint_strategy} \"\n                    f\"with {start_iteration = }\"\n                )\n\n            if collective.is_master:\n                self.build_loggers()\n                self.setup_loggers(self.cfg.common.exp_name)\n\n        init_metrics = compute_meters(\n            group_name=\"init\",\n            aggregate=True,\n            collective=collective,\n        )\n        if collective.is_local_master:\n            self.log_metrics_to_loggers(init_metrics, start_iteration, \"init\")\n\n        common_chkp_kwargs = {\n            \"model\": model,\n            \"optimizer\": optimizer,\n            \"collective\": collective,\n            \"lr_scheduler\": lr_scheduler,\n            \"data_loaders\": data_loaders,\n            \"data_sources\": train_datapipeline.datasets,\n            \"grad_scaler\": training_context[\"scaler\"],\n        }\n\n        train_data_iter = iter(train_datapipeline.dataloader)\n\n        # Setup training metric engine if config exists\n        train_metric_engine = None\n        if self.cfg.metrics and \"train\" in self.cfg.metrics:\n            from optimus_dl.modules.metrics.engine import MetricEngine\n\n            train_metric_engine = MetricEngine(\"train\", self.cfg.metrics[\"train\"])\n\n        collective.barrier()\n        logger.info(\"All ranks are ready\")\n\n        pbar = trange(\n            start_iteration,\n            self.cfg.optimization.iterations + 1,\n            initial=start_iteration,\n            total=self.cfg.optimization.iterations,\n            miniters=self.cfg.common.log_freq,\n            maxinterval=1000000,\n            disable=not collective.is_local_master,\n            smoothing=0,\n        )\n        for iteration in pbar:\n            try:\n                # Execute one training iteration using recipe mixin\n                self.run_training_iteration(\n                    model=model,\n                    optimizer=optimizer,\n                    criterion=criterion,\n                    train_data_iter=train_data_iter,\n                    training_context=training_context,\n                    lr_scheduler=lr_scheduler,\n                    metric_engine=train_metric_engine,\n                )\n\n                with meters_group(\"train\") as should_log:\n                    if should_log:\n                        # Get aggregated metrics for progress bar\n                        current_metrics = compute_meters(\n                            \"train\",\n                            aggregate=True,\n                            collective=collective,\n                        )\n                        if train_metric_engine:\n                            current_metrics = train_metric_engine.compute(\n                                current_metrics\n                            )\n\n                        if collective.is_local_master:\n                            pbar.set_postfix(current_metrics, refresh=False)\n\n                        # Log metrics to all configured loggers\n                        if collective.is_master:\n                            self.log_metrics_to_loggers(\n                                current_metrics, iteration, \"train\"\n                            )\n\n                step_meters(\"train\")  # Step the metrics logging iteration counter\n                reset_meters(\n                    \"train\"\n                )  # Reset metrics after logging (keep metrics with reset=False)\n                with training_context[\"amp_ctx\"]:\n                    metrics = self.run_evaluation_if_needed(\n                        iteration=iteration,\n                        model=model,\n                        criterion=criterion,\n                        eval_data={\n                            k: v.dataloader\n                            for k, v in eval_datapipeline.items()\n                            if v is not None\n                        },\n                        collective=collective,\n                        all_metrics_configs=self.cfg.metrics,\n                    )\n                if metrics and collective.is_master:\n                    for eval_name, eval_metrics in metrics.items():\n                        self.log_metrics_to_loggers(eval_metrics, iteration, eval_name)\n\n                self.save_checkpoint_if_needed(\n                    iteration=iteration,\n                    **common_chkp_kwargs,\n                )\n\n            except KeyboardInterrupt:\n                self.handle_training_interruption(\n                    iteration=iteration,\n                    **common_chkp_kwargs,\n                )\n                break\n            except Exception as e:\n                logger.error(f\"Training step failed at iteration {iteration}: {e}\")\n                raise\n\n        # Close loggers at the end of training\n        if collective.is_master:\n            self.close_loggers()\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.build_criterion","title":"<code>build_criterion(*args, **kwargs)</code>","text":"<p>Delegate to CriterionBuilder.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def build_criterion(self, *args, **kwargs) -&gt; BaseCriterion:\n    \"\"\"Delegate to CriterionBuilder.\"\"\"\n    return self.criterion_builder.build_criterion(*args, **kwargs)\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.build_eval_data","title":"<code>build_eval_data(*args, **kwargs)</code>","text":"<p>Delegate to DataBuilder for evaluation data.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def build_eval_data(self, *args, **kwargs):\n    \"\"\"Delegate to DataBuilder for evaluation data.\"\"\"\n    return self.data_builder.build_eval_data(*args, **kwargs)\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.build_loggers","title":"<code>build_loggers(*args, **kwargs)</code>","text":"<p>Delegate to LoggerManager for building loggers.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def build_loggers(self, *args, **kwargs):\n    \"\"\"Delegate to LoggerManager for building loggers.\"\"\"\n    return self.logger_manager.build_loggers(*args, **kwargs)\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.build_lr_scheduler","title":"<code>build_lr_scheduler(*args, **kwargs)</code>","text":"<p>Delegate to SchedulerBuilder.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def build_lr_scheduler(self, *args, **kwargs):\n    \"\"\"Delegate to SchedulerBuilder.\"\"\"\n    return self.scheduler_builder.build_lr_scheduler(*args, **kwargs)\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.build_model","title":"<code>build_model(*args, **kwargs)</code>","text":"<p>Delegate to ModelBuilder.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def build_model(self, *args, **kwargs) -&gt; BaseModel:\n    \"\"\"Delegate to ModelBuilder.\"\"\"\n    return self.model_builder.build_model(*args, **kwargs)\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.build_optimizer","title":"<code>build_optimizer(*args, **kwargs)</code>","text":"<p>Delegate to OptimizerBuilder.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def build_optimizer(self, *args, **kwargs) -&gt; Optimizer:\n    \"\"\"Delegate to OptimizerBuilder.\"\"\"\n    return self.optimizer_builder.build_optimizer(*args, **kwargs)\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.build_train_data","title":"<code>build_train_data(*args, **kwargs)</code>","text":"<p>Delegate to DataBuilder for training data.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def build_train_data(self, *args, **kwargs):\n    \"\"\"Delegate to DataBuilder for training data.\"\"\"\n    return self.data_builder.build_train_data(*args, **kwargs)\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.close_loggers","title":"<code>close_loggers(*args, **kwargs)</code>","text":"<p>Delegate logger cleanup to LoggerManager.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def close_loggers(self, *args, **kwargs):\n    \"\"\"Delegate logger cleanup to LoggerManager.\"\"\"\n    return self.logger_manager.close_loggers(*args, **kwargs)\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.load_checkpoint","title":"<code>load_checkpoint(*args, **kwargs)</code>","text":"<p>Load a specific checkpoint.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def load_checkpoint(self, *args, **kwargs):\n    \"\"\"Load a specific checkpoint.\"\"\"\n    if \"logger_manager\" not in kwargs:\n        kwargs[\"logger_manager\"] = self.logger_manager\n    return self.checkpoint_manager.load_checkpoint(*args, **kwargs)\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.load_checkpoint_if_exists","title":"<code>load_checkpoint_if_exists(*args, **kwargs)</code>","text":"<p>Try to resume from latest checkpoint in output path.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def load_checkpoint_if_exists(self, *args, **kwargs):\n    \"\"\"Try to resume from latest checkpoint in output path.\"\"\"\n    if \"checkpoint_path\" not in kwargs:\n        kwargs[\"checkpoint_path\"] = self.cfg.common.output_path\n    if \"logger_manager\" not in kwargs:\n        kwargs[\"logger_manager\"] = self.logger_manager\n    return self.checkpoint_manager.load_checkpoint_if_exists(*args, **kwargs)\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.log_metrics_to_loggers","title":"<code>log_metrics_to_loggers(*args, **kwargs)</code>","text":"<p>Delegate metrics logging to LoggerManager.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def log_metrics_to_loggers(self, *args, **kwargs):\n    \"\"\"Delegate metrics logging to LoggerManager.\"\"\"\n    return self.logger_manager.log_metrics_to_loggers(*args, **kwargs)\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.run","title":"<code>run()</code>","text":"<p>Run the complete training pipeline.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def run(self):\n    \"\"\"Run the complete training pipeline.\"\"\"\n    self.setup_context()\n    is_restart = self.checkpoint_manager.is_restart(self.cfg.common.output_path)\n\n    with meters_group(\"init\"):\n        log_event_start(\"perf/init\")\n        logger.info(f\"Using output path : {self.cfg.common.output_path}\")\n        logger.info(self.cfg)\n\n        # Setup device and distributed collective\n        device, collective = setup_device_and_collective(\n            use_gpu=self.cfg.common.use_gpu, config=self.cfg.common.distributed\n        )\n\n        logger.info(\n            \"Setting up console logging. Will log from master only from now.\"\n        )\n        if not collective.is_master:\n            setup_logging(logging.WARNING)\n\n        model: BaseModel = self.build_model(\n            model_config=self.cfg.model,\n            collective=collective,\n            is_restart=is_restart,\n            checkpoint_manager=self.checkpoint_manager,\n        )\n\n        optimizer: Optimizer = self.build_optimizer(model.make_parameter_groups())\n        lr_scheduler = self.build_lr_scheduler(optimizer)\n        criterion: BaseCriterion = self.build_criterion(collective=collective)\n\n        # Setup training context (AMP, scaler, etc.) using recipe mixin\n        training_context = self.setup_training_context(device)\n\n        try:\n            train_datapipeline = self.build_train_data(\n                device=device, collective=collective\n            )\n            assert (\n                train_datapipeline is not None\n            ), \"Train data pipeline not initialized\"\n            eval_datapipeline = self.build_eval_data(\n                device=device, collective=collective\n            )\n            data_loaders = {\n                \"train\": train_datapipeline.dataloader,\n                # eval dataloader may be not restored\n            }\n        except Exception as e:\n            logger.error(f\"Failed to build data pipelines: {e}\")\n            raise\n\n        model = model.to(device)\n\n        # cannot be after checkpoint load as may erase the start event\n        log_event_end(\"perf/init\")\n\n        # Try to resume from checkpoint in output paths\n        start_iteration, metadata = self.load_checkpoint_if_exists(\n            model=model,\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            data_loaders=data_loaders,\n            collective=collective,\n            data_sources=train_datapipeline.datasets,\n        )\n        if is_restart:\n            # cases when training run but did not produce any artifacts is\n            # indistinguishable from the case when training is not started at all\n            assert metadata is not None, \"Misaligned is_restart flag\"\n\n        logger.info(f\"Considering this run as {is_restart = }\")\n        if not is_restart and self.cfg.common.load_checkpoint is not None:\n            # if checkpoint from output path was not loaded, we are sure that this launch is not\n            # re-scheduling / preemption re-start, so we can try loading model from load_checkpoint\n            metadata = self.load_checkpoint(\n                model=model,\n                optimizer=optimizer,\n                lr_scheduler=lr_scheduler,\n                data_loaders=data_loaders,\n                data_sources=train_datapipeline.datasets,\n                collective=collective,\n                load_strategy=self.cfg.common.load_checkpoint_strategy,\n                checkpoint_path=self.cfg.common.load_checkpoint,\n            )\n            start_iteration = metadata[\"iteration\"] + 1\n            logger.info(\n                \"Loaded checkpoint from \"\n                f\"checkpoint_path = {self.cfg.common.load_checkpoint} path with \"\n                f\"load_strategy = {self.cfg.common.load_checkpoint_strategy} \"\n                f\"with {start_iteration = }\"\n            )\n\n        if collective.is_master:\n            self.build_loggers()\n            self.setup_loggers(self.cfg.common.exp_name)\n\n    init_metrics = compute_meters(\n        group_name=\"init\",\n        aggregate=True,\n        collective=collective,\n    )\n    if collective.is_local_master:\n        self.log_metrics_to_loggers(init_metrics, start_iteration, \"init\")\n\n    common_chkp_kwargs = {\n        \"model\": model,\n        \"optimizer\": optimizer,\n        \"collective\": collective,\n        \"lr_scheduler\": lr_scheduler,\n        \"data_loaders\": data_loaders,\n        \"data_sources\": train_datapipeline.datasets,\n        \"grad_scaler\": training_context[\"scaler\"],\n    }\n\n    train_data_iter = iter(train_datapipeline.dataloader)\n\n    # Setup training metric engine if config exists\n    train_metric_engine = None\n    if self.cfg.metrics and \"train\" in self.cfg.metrics:\n        from optimus_dl.modules.metrics.engine import MetricEngine\n\n        train_metric_engine = MetricEngine(\"train\", self.cfg.metrics[\"train\"])\n\n    collective.barrier()\n    logger.info(\"All ranks are ready\")\n\n    pbar = trange(\n        start_iteration,\n        self.cfg.optimization.iterations + 1,\n        initial=start_iteration,\n        total=self.cfg.optimization.iterations,\n        miniters=self.cfg.common.log_freq,\n        maxinterval=1000000,\n        disable=not collective.is_local_master,\n        smoothing=0,\n    )\n    for iteration in pbar:\n        try:\n            # Execute one training iteration using recipe mixin\n            self.run_training_iteration(\n                model=model,\n                optimizer=optimizer,\n                criterion=criterion,\n                train_data_iter=train_data_iter,\n                training_context=training_context,\n                lr_scheduler=lr_scheduler,\n                metric_engine=train_metric_engine,\n            )\n\n            with meters_group(\"train\") as should_log:\n                if should_log:\n                    # Get aggregated metrics for progress bar\n                    current_metrics = compute_meters(\n                        \"train\",\n                        aggregate=True,\n                        collective=collective,\n                    )\n                    if train_metric_engine:\n                        current_metrics = train_metric_engine.compute(\n                            current_metrics\n                        )\n\n                    if collective.is_local_master:\n                        pbar.set_postfix(current_metrics, refresh=False)\n\n                    # Log metrics to all configured loggers\n                    if collective.is_master:\n                        self.log_metrics_to_loggers(\n                            current_metrics, iteration, \"train\"\n                        )\n\n            step_meters(\"train\")  # Step the metrics logging iteration counter\n            reset_meters(\n                \"train\"\n            )  # Reset metrics after logging (keep metrics with reset=False)\n            with training_context[\"amp_ctx\"]:\n                metrics = self.run_evaluation_if_needed(\n                    iteration=iteration,\n                    model=model,\n                    criterion=criterion,\n                    eval_data={\n                        k: v.dataloader\n                        for k, v in eval_datapipeline.items()\n                        if v is not None\n                    },\n                    collective=collective,\n                    all_metrics_configs=self.cfg.metrics,\n                )\n            if metrics and collective.is_master:\n                for eval_name, eval_metrics in metrics.items():\n                    self.log_metrics_to_loggers(eval_metrics, iteration, eval_name)\n\n            self.save_checkpoint_if_needed(\n                iteration=iteration,\n                **common_chkp_kwargs,\n            )\n\n        except KeyboardInterrupt:\n            self.handle_training_interruption(\n                iteration=iteration,\n                **common_chkp_kwargs,\n            )\n            break\n        except Exception as e:\n            logger.error(f\"Training step failed at iteration {iteration}: {e}\")\n            raise\n\n    # Close loggers at the end of training\n    if collective.is_master:\n        self.close_loggers()\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.run_evaluation_if_needed","title":"<code>run_evaluation_if_needed(*args, **kwargs)</code>","text":"<p>Check eval frequency and run evaluation via Evaluator.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def run_evaluation_if_needed(self, *args, **kwargs):\n    \"\"\"Check eval frequency and run evaluation via Evaluator.\"\"\"\n    return self.evaluator.run_evaluation_if_needed(*args, **kwargs)\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.save_checkpoint","title":"<code>save_checkpoint(*args, **kwargs)</code>","text":"<p>Save a checkpoint via CheckpointManager.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def save_checkpoint(self, *args, **kwargs):\n    \"\"\"Save a checkpoint via CheckpointManager.\"\"\"\n    config_dict = self.cfg if hasattr(self.cfg, \"__dict__\") else dict(self.cfg)\n    kwargs[\"full_config\"] = config_dict\n    if \"checkpoint_path\" not in kwargs:\n        kwargs[\"checkpoint_path\"] = self.cfg.common.output_path\n    if \"logger_manager\" not in kwargs:\n        kwargs[\"logger_manager\"] = self.logger_manager\n    return self.checkpoint_manager.save_checkpoint(*args, **kwargs)\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.save_checkpoint_if_needed","title":"<code>save_checkpoint_if_needed(*args, **kwargs)</code>","text":"<p>Check save frequency and delegate to CheckpointManager.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def save_checkpoint_if_needed(self, *args, **kwargs):\n    \"\"\"Check save frequency and delegate to CheckpointManager.\"\"\"\n    config_dict = self.cfg if hasattr(self.cfg, \"__dict__\") else dict(self.cfg)\n    kwargs[\"full_config\"] = config_dict\n    kwargs[\"checkpoint_path\"] = self.cfg.common.output_path\n    kwargs[\"save_freq\"] = self.cfg.common.save_freq\n    kwargs[\"logger_manager\"] = self.logger_manager\n    return self.checkpoint_manager.save_checkpoint_if_needed(*args, **kwargs)\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.set_exp_name","title":"<code>set_exp_name()</code>","text":"<p>Set experiment name based on config or environment variables.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def set_exp_name(self):\n    \"\"\"Set experiment name based on config or environment variables.\"\"\"\n    if not OmegaConf.is_missing(self.cfg.common, \"exp_name\"):\n        return\n    exp_name = f\"run-{self.cfg.model._name}-{time.strftime('%Y-%m-%d-%H-%M-%S')}\"\n    self.cfg.common.exp_name = exp_name\n    logger.info(f\"Experiment name set to: {self.cfg.common.exp_name}\")\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.setup_context","title":"<code>setup_context()</code>","text":"<p>Setup global training context (precision, etc.).</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def setup_context(self):\n    \"\"\"Setup global training context (precision, etc.).\"\"\"\n    set_seed(self.cfg.common.seed, deterministic=self.cfg.common.deterministic)\n    torch.set_float32_matmul_precision(\"highest\")\n    if torch.cuda.is_available():\n        torch.backends.cuda.matmul.fp32_precision = \"ieee\"\n        if hasattr(torch.backends.cudnn, \"conv\"):\n            torch.backends.cudnn.conv.fp32_precision = \"ieee\"\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.setup_loggers","title":"<code>setup_loggers(experiment_name, full_config=None)</code>","text":"<p>Setup logging with experiment configuration.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the current experiment.</p> required <code>full_config</code> <code>dict | None</code> <p>Full configuration dictionary. If None, uses self.cfg.</p> <code>None</code> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def setup_loggers(self, experiment_name: str, full_config: dict | None = None):\n    \"\"\"Setup logging with experiment configuration.\n\n    Args:\n        experiment_name: Name of the current experiment.\n        full_config: Full configuration dictionary. If None, uses self.cfg.\n    \"\"\"\n    if full_config is None:\n        full_config = self.cfg if hasattr(self.cfg, \"__dict__\") else dict(self.cfg)\n    return self.logger_manager.setup_loggers(experiment_name, full_config)\n</code></pre>"},{"location":"reference/recipe/train/base/#optimus_dl.recipe.train.base.TrainRecipe.validate_config","title":"<code>validate_config()</code>","text":"<p>Validate configuration before training starts.</p> Source code in <code>optimus_dl/recipe/train/base.py</code> <pre><code>def validate_config(self) -&gt; None:\n    \"\"\"Validate configuration before training starts.\"\"\"\n    # Required fields\n    assert self.cfg.model is not None, \"Model configuration is required\"\n    assert self.cfg.data is not None, \"Data configuration is required\"\n    assert self.cfg.criterion is not None, \"Criterion configuration is required\"\n    assert (\n        self.cfg.optimization is not None\n    ), \"Optimization configuration is required\"\n\n    # Training parameters\n    assert self.cfg.optimization.iterations &gt; 0, \"iterations must be positive\"\n    assert self.cfg.optimization.acc_steps &gt; 0, \"acc_steps must be positive\"\n    assert self.cfg.common.log_freq &gt; 0, \"log_freq must be positive\"\n\n    if self.cfg.common.save_freq &gt; 0:\n        assert (\n            self.cfg.common.output_path\n        ), \"output_path required when save_freq &gt; 0\"\n\n    logger.info(\"Configuration validation passed\")\n</code></pre>"},{"location":"reference/recipe/train/config/","title":"config","text":""},{"location":"reference/recipe/train/config/#optimus_dl.recipe.train.config","title":"<code>optimus_dl.recipe.train.config</code>","text":"<p>Training recipe configuration.</p> <p>This module defines the configuration classes for the training recipe, including all hyperparameters, component configurations, and training settings.</p>"},{"location":"reference/recipe/train/config/#optimus_dl.recipe.train.config.TrainConfig","title":"<code>TrainConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfigStrict</code></p> <p>Complete training configuration.</p> <p>This is the root configuration class for training. It contains all component configurations (model, data, optimizer, etc.) and uses the registry system for flexible component selection.</p> <p>The configuration is hierarchical and supports OmegaConf interpolation for sharing values across components. The <code>args</code> field serves as a \"scratch space\" for high-level variables that can be referenced throughout the config.</p> Example <pre><code>config = TrainConfig(\n    _name=\"base\",\n    args={\"batch_size\": 64, \"seq_len\": 1024},\n    model=ModelConfig(_name=\"llama\", n_embd=512),\n    optimization=OptimizationConfig(\n        batch_size=\"${args.batch_size}\",\n        lr=1e-4,\n    ),\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>dict</code> <p>dict() -&gt; new empty dictionary dict(mapping) -&gt; new dictionary initialized from a mapping object's     (key, value) pairs dict(iterable) -&gt; new dictionary initialized as if via:     d = {}     for k, v in iterable:         d[k] = v dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs     in the keyword argument list.  For example:  dict(one=1, two=2)</p> <code>&lt;class 'dict'&gt;</code> <code>common</code> <code>TrainRecipeConfig</code> <p>Configuration for training recipe common settings.</p> <p>This class contains all the common settings shared across training runs, including experiment metadata, logging frequency, checkpointing, evaluation, and distributed training settings.</p> <code>&lt;dynamic&gt;</code> <code>model</code> <code>ModelConfig</code> <code>'???'</code> <code>data</code> <code>DataConfig</code> <code>'???'</code> <code>criterion</code> <code>CriterionConfig</code> <code>'???'</code> <code>optimization</code> <code>OptimizationConfig</code> <code>'???'</code> <code>lr_scheduler</code> <code>RegistryConfig | None</code> <code>None</code> <code>loggers</code> <code>list[MetricsLoggerConfig] | None</code> <p>List of metrics logger configurations</p> <code>None</code> <code>metrics</code> <code>dict[str, list[dict]]</code> <p>Metric configurations mapped by dataset name (e.g., 'train', 'val_slice_1')</p> <code>&lt;class 'dict'&gt;</code> <code>model_transforms</code> <code>list[ModelTransformConfig]</code> <p>List of model transforms to apply after model building</p> <code>&lt;dynamic&gt;</code> <code>model_builder</code> <code>RegistryConfig</code> <code>ModelBuilderConfig(_name='base')</code> <code>optimizer_builder</code> <code>RegistryConfig</code> <code>OptimizerBuilderConfig(_name='base')</code> <code>criterion_builder</code> <code>RegistryConfig</code> <code>CriterionBuilderConfig(_name='base')</code> <code>data_builder</code> <code>RegistryConfig</code> <code>DataBuilderConfig(_name='base')</code> <code>scheduler_builder</code> <code>RegistryConfig</code> <code>SchedulerBuilderConfig(_name='base')</code> <code>logger_manager</code> <code>RegistryConfig</code> <code>LoggerManagerConfig(_name='base')</code> <code>checkpoint_manager</code> <code>RegistryConfig</code> <code>CheckpointManagerConfig(_name='base')</code> <code>evaluator</code> <code>RegistryConfig</code> <code>EvaluatorConfig(_name='base')</code> Source code in <code>optimus_dl/recipe/train/config.py</code> <pre><code>@dataclass\nclass TrainConfig(RegistryConfigStrict):\n    \"\"\"Complete training configuration.\n\n    This is the root configuration class for training. It contains all component\n    configurations (model, data, optimizer, etc.) and uses the registry system\n    for flexible component selection.\n\n    The configuration is hierarchical and supports OmegaConf interpolation for\n    sharing values across components. The `args` field serves as a \"scratch space\"\n    for high-level variables that can be referenced throughout the config.\n\n    Example:\n        ```python\n        config = TrainConfig(\n            _name=\"base\",\n            args={\"batch_size\": 64, \"seq_len\": 1024},\n            model=ModelConfig(_name=\"llama\", n_embd=512),\n            optimization=OptimizationConfig(\n                batch_size=\"${args.batch_size}\",\n                lr=1e-4,\n            ),\n        )\n\n        ```\"\"\"\n\n    args: dict = field(default_factory=dict)\n    common: TrainRecipeConfig = field(default_factory=TrainRecipeConfig)\n\n    model: ModelConfig = field(default=MISSING)\n    data: DataConfig = field(default=MISSING)\n    criterion: CriterionConfig = field(default=MISSING)\n    optimization: OptimizationConfig = field(default=MISSING)\n    lr_scheduler: RegistryConfig | None = field(default=None)\n\n    # Metrics logging configuration\n    loggers: list[MetricsLoggerConfig] | None = field(\n        default=None, metadata={\"description\": \"List of metrics logger configurations\"}\n    )\n\n    # Metrics configuration for MetricEngine, mapped by dataset name (e.g. 'train', 'val')\n    metrics: dict[str, list[dict]] = field(\n        default_factory=dict,\n        metadata={\n            \"description\": \"Metric configurations mapped by dataset name (e.g., 'train', 'val_slice_1')\"\n        },\n    )\n\n    # Model transforms configuration\n    model_transforms: list[ModelTransformConfig] = field(\n        default_factory=list,\n        metadata={\n            \"description\": \"List of model transforms to apply after model building\"\n        },\n    )\n\n    # Dependency Injection Configs\n    model_builder: RegistryConfig = field(\n        default_factory=lambda: ModelBuilderConfig(_name=\"base\")\n    )\n    optimizer_builder: RegistryConfig = field(\n        default_factory=lambda: OptimizerBuilderConfig(_name=\"base\")\n    )\n    criterion_builder: RegistryConfig = field(\n        default_factory=lambda: CriterionBuilderConfig(_name=\"base\")\n    )\n    data_builder: RegistryConfig = field(\n        default_factory=lambda: DataBuilderConfig(_name=\"base\")\n    )\n    scheduler_builder: RegistryConfig = field(\n        default_factory=lambda: SchedulerBuilderConfig(_name=\"base\")\n    )\n    logger_manager: RegistryConfig = field(\n        default_factory=lambda: LoggerManagerConfig(_name=\"base\")\n    )\n    checkpoint_manager: RegistryConfig = field(\n        default_factory=lambda: CheckpointManagerConfig(_name=\"base\")\n    )\n    evaluator: RegistryConfig = field(\n        default_factory=lambda: EvaluatorConfig(_name=\"base\")\n    )\n</code></pre>"},{"location":"reference/recipe/train/config/#optimus_dl.recipe.train.config.TrainRecipeConfig","title":"<code>TrainRecipeConfig</code>  <code>dataclass</code>","text":"<p>Configuration for training recipe common settings.</p> <p>This class contains all the common settings shared across training runs, including experiment metadata, logging frequency, checkpointing, evaluation, and distributed training settings.</p> <p>Parameters:</p> Name Type Description Default <code>exp_name</code> <code>str</code> <p>Experiment name</p> <code>'???'</code> <code>exp_description</code> <code>str | None</code> <p>Experiment description</p> <code>None</code> <code>exp_tags</code> <code>list[str]</code> <p>Experiment tags</p> <code>&lt;dynamic&gt;</code> <code>log_freq</code> <code>int</code> <p>Frequency of train metrics logging</p> <code>16</code> <code>seed</code> <code>int</code> <p>Seed to seed everything that's possible</p> <code>42</code> <code>data_seed</code> <code>int</code> <p>Seed to seed everything data-related. Will be different on each rank.</p> <code>42</code> <code>deterministic</code> <code>bool</code> <p>If True, force deterministic algorithms in PyTorch.</p> <code>True</code> <code>eval_iterations</code> <code>int | None</code> <p>Max number of iterations of validation data for every subset</p> <code>None</code> <code>eval_freq</code> <code>int</code> <p>Frequency of evaluations. Zero disables</p> <code>100</code> <code>save_freq</code> <code>int</code> <p>Frequency of checkpoint savings. As eval_freq by default</p> <code>'${.eval_freq}'</code> <code>output_path</code> <code>str</code> <p>Directory to dump checkpoints to</p> <code>\"${oc.env:PERSISTENT_PATH,'./outputs'}/${.exp_name}\"</code> <code>load_checkpoint</code> <code>str | None</code> <p>Path to checkpoint to load from, what to load from it is controlled by load_checkpoint_strategy</p> <code>None</code> <code>load_checkpoint_strategy</code> <code>LoadStrategy</code> <p>Strategy what to load from the checkpoint</p> <code>&lt;dynamic&gt;</code> <code>use_gpu</code> <code>bool</code> <code>True</code> <code>distributed</code> <code>DistributedConfig</code> <p>Distributed training configuration (GPU, TP, etc.)</p> <code>&lt;dynamic&gt;</code> Source code in <code>optimus_dl/recipe/train/config.py</code> <pre><code>@dataclass\nclass TrainRecipeConfig:\n    \"\"\"Configuration for training recipe common settings.\n\n    This class contains all the common settings shared across training runs,\n    including experiment metadata, logging frequency, checkpointing, evaluation,\n    and distributed training settings.\n    \"\"\"\n\n    # Exp metadata\n    exp_name: str = field(default=MISSING, metadata={\"description\": \"Experiment name\"})\n    exp_description: str | None = field(\n        default=None, metadata={\"description\": \"Experiment description\"}\n    )\n    exp_tags: list[str] = field(\n        default_factory=list, metadata={\"description\": \"Experiment tags\"}\n    )\n    log_freq: int = field(\n        default=16, metadata={\"description\": \"Frequency of train metrics logging\"}\n    )\n\n    # Reproducibility\n    seed: int = field(\n        default=42, metadata={\"description\": \"Seed to seed everything that's possible\"}\n    )\n    data_seed: int = field(\n        default=42,\n        metadata={\n            \"description\": \"Seed to seed everything data-related. Will be different on each rank.\"\n        },\n    )\n    deterministic: bool = field(\n        default=True,\n        metadata={\"description\": \"If True, force deterministic algorithms in PyTorch.\"},\n    )\n\n    # Evaluation\n    eval_iterations: int | None = field(\n        default=None,\n        metadata={\n            \"description\": \"Max number of iterations of validation data for every subset\"\n        },\n    )\n    eval_freq: int = field(\n        default=100, metadata={\"description\": \"Frequency of evaluations. Zero disables\"}\n    )\n    # Checkpointing\n    save_freq: int = field(\n        default=II(\".eval_freq\"),\n        metadata={\n            \"description\": \"Frequency of checkpoint savings. As eval_freq by default\"\n        },\n    )\n    output_path: str = field(\n        default=\"${oc.env:PERSISTENT_PATH,'./outputs'}/${.exp_name}\",\n        metadata={\"description\": \"Directory to dump checkpoints to\"},\n    )\n\n    load_checkpoint: str | None = field(\n        default=None,\n        metadata={\n            \"description\": \"Path to checkpoint to load from, what to load from it is controlled by load_checkpoint_strategy\"\n        },\n    )\n    load_checkpoint_strategy: LoadStrategy = field(\n        default_factory=LoadStrategy,\n        metadata={\"description\": \"Strategy what to load from the checkpoint\"},\n    )\n\n    # Distributed\n    use_gpu: bool = True\n    distributed: DistributedConfig = field(\n        default_factory=DistributedConfig,\n        metadata={\"description\": \"Distributed training configuration (GPU, TP, etc.)\"},\n    )\n</code></pre>"},{"location":"reference/recipe/train/builders/","title":"Index","text":""},{"location":"reference/recipe/train/builders/#optimus_dl.recipe.train.builders","title":"<code>optimus_dl.recipe.train.builders</code>","text":""},{"location":"reference/recipe/train/builders/#optimus_dl.recipe.train.builders.CriterionBuilder","title":"<code>CriterionBuilder</code>","text":"<p>Builder class responsible for creating the loss criterion instance.</p> <p>Uses the <code>criterion</code> registry to instantiate the specified loss function (e.g., CrossEntropy) based on the training configuration.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>CriterionBuilderConfig</code> <p>Builder configuration.</p> required <code>criterion_config</code> <code>CriterionConfig</code> <p>Configuration for the criterion itself.</p> required Source code in <code>optimus_dl/recipe/train/builders/criterion_builder.py</code> <pre><code>class CriterionBuilder:\n    \"\"\"Builder class responsible for creating the loss criterion instance.\n\n    Uses the `criterion` registry to instantiate the specified loss function\n    (e.g., CrossEntropy) based on the training configuration.\n\n    Args:\n        cfg: Builder configuration.\n        criterion_config: Configuration for the criterion itself.\n    \"\"\"\n\n    def __init__(\n        self, cfg: CriterionBuilderConfig, criterion_config: CriterionConfig, **kwargs\n    ):\n        self.criterion_config = criterion_config\n\n    def build_criterion(self, **kwargs) -&gt; BaseCriterion:\n        \"\"\"Instantiate and return the configured loss criterion.\"\"\"\n        criterion = build(\"criterion\", self.criterion_config, **kwargs)\n        assert isinstance(criterion, BaseCriterion)\n        logger.info(f\"Criterion \\n{criterion}\")\n        return criterion\n</code></pre>"},{"location":"reference/recipe/train/builders/#optimus_dl.recipe.train.builders.CriterionBuilder.build_criterion","title":"<code>build_criterion(**kwargs)</code>","text":"<p>Instantiate and return the configured loss criterion.</p> Source code in <code>optimus_dl/recipe/train/builders/criterion_builder.py</code> <pre><code>def build_criterion(self, **kwargs) -&gt; BaseCriterion:\n    \"\"\"Instantiate and return the configured loss criterion.\"\"\"\n    criterion = build(\"criterion\", self.criterion_config, **kwargs)\n    assert isinstance(criterion, BaseCriterion)\n    logger.info(f\"Criterion \\n{criterion}\")\n    return criterion\n</code></pre>"},{"location":"reference/recipe/train/builders/#optimus_dl.recipe.train.builders.DataBuilder","title":"<code>DataBuilder</code>","text":"<p>Builder class for constructing training and evaluation data pipelines.</p> <p>Manages the creation of <code>DataPipeline</code> objects, ensuring correct distributed sharding and iterator behavior (e.g., infinite loop for training, resettable for evaluation).</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DataBuilderConfig</code> <p>Builder configuration.</p> required <code>data_config</code> <code>DataConfig</code> <p>Configuration for datasets and transforms.</p> required Source code in <code>optimus_dl/recipe/train/builders/data_builder.py</code> <pre><code>class DataBuilder:\n    \"\"\"Builder class for constructing training and evaluation data pipelines.\n\n    Manages the creation of `DataPipeline` objects, ensuring correct distributed\n    sharding and iterator behavior (e.g., infinite loop for training, resettable\n    for evaluation).\n\n    Args:\n        cfg: Builder configuration.\n        data_config: Configuration for datasets and transforms.\n    \"\"\"\n\n    def __init__(\n        self, cfg: DataBuilderConfig, data_config: DataConfig, data_seed: int, **kwargs\n    ):\n        self.data_config = data_config\n        self.data_seed = data_seed\n\n    @staticmethod\n    def _get_rank_seed(seed: int, rank: int, world_size: int) -&gt; int:\n        \"\"\"\n        Generate a unique seed for each rank based on the base seed.\n        \"\"\"\n        rng = torch.Generator()\n        rng.manual_seed(seed + world_size * 10000 + rank)\n        return torch.randint(0, 2**32, (1,), generator=rng).item()\n\n    def build_train_data(self, collective: Collective, **kwargs) -&gt; DataPipeline | None:\n        \"\"\"Build the training data pipeline.\n\n        Automatically injects rank and world_size for sharding. The resulting\n        loader is configured to restart automatically on StopIteration, creating\n        an infinite stream.\n\n        Args:\n            collective: Distributed collective for sharding info.\n            **kwargs: Additional arguments passed to dataset builders.\n\n        Returns:\n            A DataPipeline containing the dataset and loader.\n        \"\"\"\n        kwargs[\"rank\"] = collective.dp_rank\n        kwargs[\"world_size\"] = collective.dp_world_size\n        kwargs[\"seed\"] = self._get_rank_seed(\n            self.data_seed, collective.dp_rank, collective.dp_world_size\n        )\n        train_data = build_data_pipeline(self.data_config.train_datasets, **kwargs)\n        if train_data is None:\n            return None\n        dataloader = torchdata.nodes.Loader(\n            root=train_data.dataloader,\n            restart_on_stop_iteration=True,\n        )\n        return DataPipeline(\n            datasets=train_data.datasets,\n            dataloader=dataloader,\n        )\n\n    def build_eval_data(\n        self, collective: Collective, **kwargs: Any\n    ) -&gt; dict[str, DataPipeline | None]:\n        \"\"\"Build evaluation data pipelines.\n\n        Constructs a dictionary of pipelines for multiple evaluation datasets.\n        Uses `LoaderIterResettable` to allow repeated iteration over the same\n        validation sets.\n\n        Args:\n            collective: Distributed collective.\n            **kwargs: Additional arguments.\n\n        Returns:\n            Dictionary mapping dataset names to DataPipelines.\n        \"\"\"\n        kwargs[\"rank\"] = collective.dp_rank\n        kwargs[\"world_size\"] = collective.dp_world_size\n        kwargs[\"seed\"] = self._get_rank_seed(\n            self.data_seed, collective.dp_rank, collective.dp_world_size\n        )\n        eval_data = build_data_pipeline_dict(self.data_config.eval_datasets, **kwargs)\n        eval_data = {\n            k: (\n                DataPipeline(\n                    datasets=v.datasets,\n                    dataloader=LoaderIterResettable(\n                        root=v.dataloader,\n                        restart_on_stop_iteration=False,\n                    ),\n                )\n                if v is not None\n                else None\n            )\n            for k, v in eval_data.items()\n        }\n        return eval_data\n</code></pre>"},{"location":"reference/recipe/train/builders/#optimus_dl.recipe.train.builders.DataBuilder.build_eval_data","title":"<code>build_eval_data(collective, **kwargs)</code>","text":"<p>Build evaluation data pipelines.</p> <p>Constructs a dictionary of pipelines for multiple evaluation datasets. Uses <code>LoaderIterResettable</code> to allow repeated iteration over the same validation sets.</p> <p>Parameters:</p> Name Type Description Default <code>collective</code> <code>Collective</code> <p>Distributed collective.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, DataPipeline | None]</code> <p>Dictionary mapping dataset names to DataPipelines.</p> Source code in <code>optimus_dl/recipe/train/builders/data_builder.py</code> <pre><code>def build_eval_data(\n    self, collective: Collective, **kwargs: Any\n) -&gt; dict[str, DataPipeline | None]:\n    \"\"\"Build evaluation data pipelines.\n\n    Constructs a dictionary of pipelines for multiple evaluation datasets.\n    Uses `LoaderIterResettable` to allow repeated iteration over the same\n    validation sets.\n\n    Args:\n        collective: Distributed collective.\n        **kwargs: Additional arguments.\n\n    Returns:\n        Dictionary mapping dataset names to DataPipelines.\n    \"\"\"\n    kwargs[\"rank\"] = collective.dp_rank\n    kwargs[\"world_size\"] = collective.dp_world_size\n    kwargs[\"seed\"] = self._get_rank_seed(\n        self.data_seed, collective.dp_rank, collective.dp_world_size\n    )\n    eval_data = build_data_pipeline_dict(self.data_config.eval_datasets, **kwargs)\n    eval_data = {\n        k: (\n            DataPipeline(\n                datasets=v.datasets,\n                dataloader=LoaderIterResettable(\n                    root=v.dataloader,\n                    restart_on_stop_iteration=False,\n                ),\n            )\n            if v is not None\n            else None\n        )\n        for k, v in eval_data.items()\n    }\n    return eval_data\n</code></pre>"},{"location":"reference/recipe/train/builders/#optimus_dl.recipe.train.builders.DataBuilder.build_train_data","title":"<code>build_train_data(collective, **kwargs)</code>","text":"<p>Build the training data pipeline.</p> <p>Automatically injects rank and world_size for sharding. The resulting loader is configured to restart automatically on StopIteration, creating an infinite stream.</p> <p>Parameters:</p> Name Type Description Default <code>collective</code> <code>Collective</code> <p>Distributed collective for sharding info.</p> required <code>**kwargs</code> <p>Additional arguments passed to dataset builders.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataPipeline | None</code> <p>A DataPipeline containing the dataset and loader.</p> Source code in <code>optimus_dl/recipe/train/builders/data_builder.py</code> <pre><code>def build_train_data(self, collective: Collective, **kwargs) -&gt; DataPipeline | None:\n    \"\"\"Build the training data pipeline.\n\n    Automatically injects rank and world_size for sharding. The resulting\n    loader is configured to restart automatically on StopIteration, creating\n    an infinite stream.\n\n    Args:\n        collective: Distributed collective for sharding info.\n        **kwargs: Additional arguments passed to dataset builders.\n\n    Returns:\n        A DataPipeline containing the dataset and loader.\n    \"\"\"\n    kwargs[\"rank\"] = collective.dp_rank\n    kwargs[\"world_size\"] = collective.dp_world_size\n    kwargs[\"seed\"] = self._get_rank_seed(\n        self.data_seed, collective.dp_rank, collective.dp_world_size\n    )\n    train_data = build_data_pipeline(self.data_config.train_datasets, **kwargs)\n    if train_data is None:\n        return None\n    dataloader = torchdata.nodes.Loader(\n        root=train_data.dataloader,\n        restart_on_stop_iteration=True,\n    )\n    return DataPipeline(\n        datasets=train_data.datasets,\n        dataloader=dataloader,\n    )\n</code></pre>"},{"location":"reference/recipe/train/builders/#optimus_dl.recipe.train.builders.ModelBuilder","title":"<code>ModelBuilder</code>","text":"<p>Mixin for building models and applying transformations.</p> <p>Encapsulates the logic for: 1.  Instantiating a <code>BaseModel</code> from a configuration object. 2.  Sequentially applying a list of <code>ModelTransforms</code> (e.g., FSDP, DDP, compile). 3.  Logging model statistics (parameter count).</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>ModelBuilderConfig</code> <p>Builder configuration.</p> required <code>model_transforms</code> <code>list[ModelTransformConfig] | None</code> <p>List of configurations for transforms to apply.</p> <code>None</code> Source code in <code>optimus_dl/recipe/mixins/model_builder.py</code> <pre><code>class ModelBuilder:\n    \"\"\"Mixin for building models and applying transformations.\n\n    Encapsulates the logic for:\n    1.  Instantiating a `BaseModel` from a configuration object.\n    2.  Sequentially applying a list of `ModelTransforms` (e.g., FSDP, DDP, compile).\n    3.  Logging model statistics (parameter count).\n\n    Args:\n        cfg: Builder configuration.\n        model_transforms: List of configurations for transforms to apply.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: ModelBuilderConfig,\n        model_transforms: list[ModelTransformConfig] | None = None,\n        **kwargs: Any,\n    ):\n        self.model_transforms = model_transforms or []\n\n    def build_model(\n        self, model_config: ModelConfig | None, collective: Collective, **kwargs\n    ) -&gt; BaseModel:\n        \"\"\"Build the model and apply all configured transforms.\n\n        Args:\n            model_config: Configuration for the model architecture.\n            collective: Distributed collective for transforms that need it.\n            **kwargs: Additional arguments passed to model constructor and transforms.\n\n        Returns:\n            The fully constructed and transformed model.\n        \"\"\"\n        if model_config is None:\n            raise ValueError(\n                \"model_config is None. Use build_model_from_checkpoint for evaluation.\"\n            )\n\n        model = build_model(model_config, **kwargs)\n        num_param_before = get_num_parameters(model)\n        logger.info(f\"Params num (before model transforms): {num_param_before:,}\")\n        log_averaged(\"model/num_params_before_transforms\", num_param_before)\n        assert isinstance(model, BaseModel)\n\n        # Apply model transforms (including distributed setup)\n        model = self._apply_model_transforms(\n            model, collective=collective, device=collective.default_device, **kwargs\n        )\n        num_param_after = get_num_parameters(model)\n        logger.info(f\"Model \\n{model}\")\n        logger.info(f\"Params num (after model transforms): {num_param_after:,}\")\n        log_averaged(\"model/num_params_after_transforms\", num_param_after)\n\n        return model\n\n    def _apply_model_transforms(self, model: BaseModel, **kwargs) -&gt; BaseModel:\n        \"\"\"Iteratively apply all configured model transforms.\n\n        Args:\n            model: The base model.\n            **kwargs: Context arguments (device, collective, etc.).\n\n        Returns:\n            The transformed model.\n        \"\"\"\n        for transform_cfg in self.model_transforms:\n            try:\n                transform = build_model_transform(transform_cfg, **kwargs)\n                if transform is not None:\n                    logger.info(f\"Applying model transform: {transform}\")\n                    model = transform.apply(model, **kwargs)\n                else:\n                    logger.warning(\n                        f\"Failed to build model transform from config: {transform_cfg}\"\n                    )\n            except Exception as e:\n                logger.error(f\"Failed to apply model transform {transform_cfg}: {e}\")\n                raise\n\n        return model\n</code></pre>"},{"location":"reference/recipe/train/builders/#optimus_dl.recipe.train.builders.ModelBuilder.build_model","title":"<code>build_model(model_config, collective, **kwargs)</code>","text":"<p>Build the model and apply all configured transforms.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>ModelConfig | None</code> <p>Configuration for the model architecture.</p> required <code>collective</code> <code>Collective</code> <p>Distributed collective for transforms that need it.</p> required <code>**kwargs</code> <p>Additional arguments passed to model constructor and transforms.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseModel</code> <p>The fully constructed and transformed model.</p> Source code in <code>optimus_dl/recipe/mixins/model_builder.py</code> <pre><code>def build_model(\n    self, model_config: ModelConfig | None, collective: Collective, **kwargs\n) -&gt; BaseModel:\n    \"\"\"Build the model and apply all configured transforms.\n\n    Args:\n        model_config: Configuration for the model architecture.\n        collective: Distributed collective for transforms that need it.\n        **kwargs: Additional arguments passed to model constructor and transforms.\n\n    Returns:\n        The fully constructed and transformed model.\n    \"\"\"\n    if model_config is None:\n        raise ValueError(\n            \"model_config is None. Use build_model_from_checkpoint for evaluation.\"\n        )\n\n    model = build_model(model_config, **kwargs)\n    num_param_before = get_num_parameters(model)\n    logger.info(f\"Params num (before model transforms): {num_param_before:,}\")\n    log_averaged(\"model/num_params_before_transforms\", num_param_before)\n    assert isinstance(model, BaseModel)\n\n    # Apply model transforms (including distributed setup)\n    model = self._apply_model_transforms(\n        model, collective=collective, device=collective.default_device, **kwargs\n    )\n    num_param_after = get_num_parameters(model)\n    logger.info(f\"Model \\n{model}\")\n    logger.info(f\"Params num (after model transforms): {num_param_after:,}\")\n    log_averaged(\"model/num_params_after_transforms\", num_param_after)\n\n    return model\n</code></pre>"},{"location":"reference/recipe/train/builders/#optimus_dl.recipe.train.builders.OptimizerBuilder","title":"<code>OptimizerBuilder</code>","text":"<p>Builder class responsible for creating the optimizer.</p> <p>Takes parameter groups from the model and instantiates the configured optimizer (e.g., AdamW). It also logs the total number of optimized parameters.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>OptimizerBuilderConfig</code> <p>Builder configuration.</p> required <code>optimization_config</code> <code>OptimizationConfig</code> <p>Optimization settings including the optimizer config.</p> required Source code in <code>optimus_dl/recipe/train/builders/optimizer_builder.py</code> <pre><code>class OptimizerBuilder:\n    \"\"\"Builder class responsible for creating the optimizer.\n\n    Takes parameter groups from the model and instantiates the configured\n    optimizer (e.g., AdamW). It also logs the total number of optimized\n    parameters.\n\n    Args:\n        cfg: Builder configuration.\n        optimization_config: Optimization settings including the optimizer config.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: OptimizerBuilderConfig,\n        optimization_config: OptimizationConfig,\n        **kwargs: Any,\n    ):\n        self.optimization_config = optimization_config\n\n    def build_optimizer(self, params, **kwargs) -&gt; Optimizer:\n        \"\"\"Build and validate the optimizer.\n\n        Args:\n            params: Iterable of parameters or dicts defining parameter groups\n                (typically from `model.make_parameter_groups()`).\n            **kwargs: Additional arguments passed to the optimizer constructor.\n\n        Returns:\n            Instantiated Optimizer.\n        \"\"\"\n        optimizer = build(\n            \"optimizer\", self.optimization_config.optimizer, params=params, **kwargs\n        )\n        assert isinstance(optimizer, Optimizer)\n        logger.info(f\"Optimizer \\n{optimizer}\")\n        optimized_params = []\n        for param_group in optimizer.param_groups:\n            optimized_params.append(\n                sum([p.numel() for p in param_group[\"params\"] if p.requires_grad])\n            )\n        optimized_params_num = sum(optimized_params)\n        logger.info(\n            f\"Optimized {optimized_params_num:,} parameters. Per group: {[f'{i:,}' for i in optimized_params]}\"\n        )\n        log_averaged(\"optimized_params\", optimized_params_num)\n\n        return optimizer\n</code></pre>"},{"location":"reference/recipe/train/builders/#optimus_dl.recipe.train.builders.OptimizerBuilder.build_optimizer","title":"<code>build_optimizer(params, **kwargs)</code>","text":"<p>Build and validate the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>Iterable of parameters or dicts defining parameter groups (typically from <code>model.make_parameter_groups()</code>).</p> required <code>**kwargs</code> <p>Additional arguments passed to the optimizer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optimizer</code> <p>Instantiated Optimizer.</p> Source code in <code>optimus_dl/recipe/train/builders/optimizer_builder.py</code> <pre><code>def build_optimizer(self, params, **kwargs) -&gt; Optimizer:\n    \"\"\"Build and validate the optimizer.\n\n    Args:\n        params: Iterable of parameters or dicts defining parameter groups\n            (typically from `model.make_parameter_groups()`).\n        **kwargs: Additional arguments passed to the optimizer constructor.\n\n    Returns:\n        Instantiated Optimizer.\n    \"\"\"\n    optimizer = build(\n        \"optimizer\", self.optimization_config.optimizer, params=params, **kwargs\n    )\n    assert isinstance(optimizer, Optimizer)\n    logger.info(f\"Optimizer \\n{optimizer}\")\n    optimized_params = []\n    for param_group in optimizer.param_groups:\n        optimized_params.append(\n            sum([p.numel() for p in param_group[\"params\"] if p.requires_grad])\n        )\n    optimized_params_num = sum(optimized_params)\n    logger.info(\n        f\"Optimized {optimized_params_num:,} parameters. Per group: {[f'{i:,}' for i in optimized_params]}\"\n    )\n    log_averaged(\"optimized_params\", optimized_params_num)\n\n    return optimizer\n</code></pre>"},{"location":"reference/recipe/train/builders/#optimus_dl.recipe.train.builders.SchedulerBuilder","title":"<code>SchedulerBuilder</code>","text":"<p>Builder class responsible for creating the learning rate scheduler.</p> <p>Instantiates a scheduler (e.g., CosineAnnealing, WSD) and associates it with the optimizer. It ensures the scheduler is aware of the total training iterations.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>SchedulerBuilderConfig</code> <p>Builder configuration.</p> required <code>lr_scheduler_config</code> <code>RegistryConfig | None</code> <p>Configuration for the scheduler itself (can be None).</p> required <code>optimization_config</code> <code>OptimizationConfig</code> <p>Optimization settings (needed for total iterations).</p> required Source code in <code>optimus_dl/recipe/train/builders/scheduler_builder.py</code> <pre><code>class SchedulerBuilder:\n    \"\"\"Builder class responsible for creating the learning rate scheduler.\n\n    Instantiates a scheduler (e.g., CosineAnnealing, WSD) and associates it\n    with the optimizer. It ensures the scheduler is aware of the total training\n    iterations.\n\n    Args:\n        cfg: Builder configuration.\n        lr_scheduler_config: Configuration for the scheduler itself (can be None).\n        optimization_config: Optimization settings (needed for total iterations).\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: SchedulerBuilderConfig,\n        lr_scheduler_config: RegistryConfig | None,\n        optimization_config: OptimizationConfig,\n        **kwargs: Any,\n    ):\n        self.lr_scheduler_config = lr_scheduler_config\n        self.optimization_config = optimization_config\n\n    def build_lr_scheduler(self, optimizer: Optimizer, **kwargs):\n        \"\"\"Build and validate the learning rate scheduler.\n\n        Args:\n            optimizer: The optimizer to schedule.\n            **kwargs: Additional arguments.\n\n        Returns:\n            Instantiated LR Scheduler or None if not configured.\n        \"\"\"\n        if self.lr_scheduler_config is None:\n            return None\n        lr_scheduler = build(\n            \"lr_scheduler\",\n            cfg=self.lr_scheduler_config,\n            optimizer=optimizer,\n            iterations=self.optimization_config.iterations,\n            **kwargs,\n        )\n        if lr_scheduler is not None:\n            logger.info(f\"LR Scheduler \\n{lr_scheduler}\")\n        return lr_scheduler\n</code></pre>"},{"location":"reference/recipe/train/builders/#optimus_dl.recipe.train.builders.SchedulerBuilder.build_lr_scheduler","title":"<code>build_lr_scheduler(optimizer, **kwargs)</code>","text":"<p>Build and validate the learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to schedule.</p> required <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Instantiated LR Scheduler or None if not configured.</p> Source code in <code>optimus_dl/recipe/train/builders/scheduler_builder.py</code> <pre><code>def build_lr_scheduler(self, optimizer: Optimizer, **kwargs):\n    \"\"\"Build and validate the learning rate scheduler.\n\n    Args:\n        optimizer: The optimizer to schedule.\n        **kwargs: Additional arguments.\n\n    Returns:\n        Instantiated LR Scheduler or None if not configured.\n    \"\"\"\n    if self.lr_scheduler_config is None:\n        return None\n    lr_scheduler = build(\n        \"lr_scheduler\",\n        cfg=self.lr_scheduler_config,\n        optimizer=optimizer,\n        iterations=self.optimization_config.iterations,\n        **kwargs,\n    )\n    if lr_scheduler is not None:\n        logger.info(f\"LR Scheduler \\n{lr_scheduler}\")\n    return lr_scheduler\n</code></pre>"},{"location":"reference/recipe/train/builders/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>criterion_builder</code>: Criterion builder mixin for building loss criteria.</li> <li><code>data_builder</code>: Data builder mixin for building data pipelines.</li> <li><code>optimizer_builder</code>: Optimizer builder mixin for building optimizers.</li> <li><code>scheduler_builder</code>: Scheduler builder mixin for building learning rate schedulers.</li> </ul>"},{"location":"reference/recipe/train/builders/criterion_builder/","title":"criterion_builder","text":""},{"location":"reference/recipe/train/builders/criterion_builder/#optimus_dl.recipe.train.builders.criterion_builder","title":"<code>optimus_dl.recipe.train.builders.criterion_builder</code>","text":"<p>Criterion builder mixin for building loss criteria.</p>"},{"location":"reference/recipe/train/builders/criterion_builder/#optimus_dl.recipe.train.builders.criterion_builder.CriterionBuilder","title":"<code>CriterionBuilder</code>","text":"<p>Builder class responsible for creating the loss criterion instance.</p> <p>Uses the <code>criterion</code> registry to instantiate the specified loss function (e.g., CrossEntropy) based on the training configuration.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>CriterionBuilderConfig</code> <p>Builder configuration.</p> required <code>criterion_config</code> <code>CriterionConfig</code> <p>Configuration for the criterion itself.</p> required Source code in <code>optimus_dl/recipe/train/builders/criterion_builder.py</code> <pre><code>class CriterionBuilder:\n    \"\"\"Builder class responsible for creating the loss criterion instance.\n\n    Uses the `criterion` registry to instantiate the specified loss function\n    (e.g., CrossEntropy) based on the training configuration.\n\n    Args:\n        cfg: Builder configuration.\n        criterion_config: Configuration for the criterion itself.\n    \"\"\"\n\n    def __init__(\n        self, cfg: CriterionBuilderConfig, criterion_config: CriterionConfig, **kwargs\n    ):\n        self.criterion_config = criterion_config\n\n    def build_criterion(self, **kwargs) -&gt; BaseCriterion:\n        \"\"\"Instantiate and return the configured loss criterion.\"\"\"\n        criterion = build(\"criterion\", self.criterion_config, **kwargs)\n        assert isinstance(criterion, BaseCriterion)\n        logger.info(f\"Criterion \\n{criterion}\")\n        return criterion\n</code></pre>"},{"location":"reference/recipe/train/builders/criterion_builder/#optimus_dl.recipe.train.builders.criterion_builder.CriterionBuilder.build_criterion","title":"<code>build_criterion(**kwargs)</code>","text":"<p>Instantiate and return the configured loss criterion.</p> Source code in <code>optimus_dl/recipe/train/builders/criterion_builder.py</code> <pre><code>def build_criterion(self, **kwargs) -&gt; BaseCriterion:\n    \"\"\"Instantiate and return the configured loss criterion.\"\"\"\n    criterion = build(\"criterion\", self.criterion_config, **kwargs)\n    assert isinstance(criterion, BaseCriterion)\n    logger.info(f\"Criterion \\n{criterion}\")\n    return criterion\n</code></pre>"},{"location":"reference/recipe/train/builders/criterion_builder/#optimus_dl.recipe.train.builders.criterion_builder.CriterionBuilderConfig","title":"<code>CriterionBuilderConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfig</code></p> <p>Configuration for CriterionBuilder (usually just a registry name).</p> Source code in <code>optimus_dl/recipe/train/builders/criterion_builder.py</code> <pre><code>@dataclass\nclass CriterionBuilderConfig(RegistryConfig):\n    \"\"\"Configuration for CriterionBuilder (usually just a registry name).\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/recipe/train/builders/data_builder/","title":"data_builder","text":""},{"location":"reference/recipe/train/builders/data_builder/#optimus_dl.recipe.train.builders.data_builder","title":"<code>optimus_dl.recipe.train.builders.data_builder</code>","text":"<p>Data builder mixin for building data pipelines.</p>"},{"location":"reference/recipe/train/builders/data_builder/#optimus_dl.recipe.train.builders.data_builder.DataBuilder","title":"<code>DataBuilder</code>","text":"<p>Builder class for constructing training and evaluation data pipelines.</p> <p>Manages the creation of <code>DataPipeline</code> objects, ensuring correct distributed sharding and iterator behavior (e.g., infinite loop for training, resettable for evaluation).</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DataBuilderConfig</code> <p>Builder configuration.</p> required <code>data_config</code> <code>DataConfig</code> <p>Configuration for datasets and transforms.</p> required Source code in <code>optimus_dl/recipe/train/builders/data_builder.py</code> <pre><code>class DataBuilder:\n    \"\"\"Builder class for constructing training and evaluation data pipelines.\n\n    Manages the creation of `DataPipeline` objects, ensuring correct distributed\n    sharding and iterator behavior (e.g., infinite loop for training, resettable\n    for evaluation).\n\n    Args:\n        cfg: Builder configuration.\n        data_config: Configuration for datasets and transforms.\n    \"\"\"\n\n    def __init__(\n        self, cfg: DataBuilderConfig, data_config: DataConfig, data_seed: int, **kwargs\n    ):\n        self.data_config = data_config\n        self.data_seed = data_seed\n\n    @staticmethod\n    def _get_rank_seed(seed: int, rank: int, world_size: int) -&gt; int:\n        \"\"\"\n        Generate a unique seed for each rank based on the base seed.\n        \"\"\"\n        rng = torch.Generator()\n        rng.manual_seed(seed + world_size * 10000 + rank)\n        return torch.randint(0, 2**32, (1,), generator=rng).item()\n\n    def build_train_data(self, collective: Collective, **kwargs) -&gt; DataPipeline | None:\n        \"\"\"Build the training data pipeline.\n\n        Automatically injects rank and world_size for sharding. The resulting\n        loader is configured to restart automatically on StopIteration, creating\n        an infinite stream.\n\n        Args:\n            collective: Distributed collective for sharding info.\n            **kwargs: Additional arguments passed to dataset builders.\n\n        Returns:\n            A DataPipeline containing the dataset and loader.\n        \"\"\"\n        kwargs[\"rank\"] = collective.dp_rank\n        kwargs[\"world_size\"] = collective.dp_world_size\n        kwargs[\"seed\"] = self._get_rank_seed(\n            self.data_seed, collective.dp_rank, collective.dp_world_size\n        )\n        train_data = build_data_pipeline(self.data_config.train_datasets, **kwargs)\n        if train_data is None:\n            return None\n        dataloader = torchdata.nodes.Loader(\n            root=train_data.dataloader,\n            restart_on_stop_iteration=True,\n        )\n        return DataPipeline(\n            datasets=train_data.datasets,\n            dataloader=dataloader,\n        )\n\n    def build_eval_data(\n        self, collective: Collective, **kwargs: Any\n    ) -&gt; dict[str, DataPipeline | None]:\n        \"\"\"Build evaluation data pipelines.\n\n        Constructs a dictionary of pipelines for multiple evaluation datasets.\n        Uses `LoaderIterResettable` to allow repeated iteration over the same\n        validation sets.\n\n        Args:\n            collective: Distributed collective.\n            **kwargs: Additional arguments.\n\n        Returns:\n            Dictionary mapping dataset names to DataPipelines.\n        \"\"\"\n        kwargs[\"rank\"] = collective.dp_rank\n        kwargs[\"world_size\"] = collective.dp_world_size\n        kwargs[\"seed\"] = self._get_rank_seed(\n            self.data_seed, collective.dp_rank, collective.dp_world_size\n        )\n        eval_data = build_data_pipeline_dict(self.data_config.eval_datasets, **kwargs)\n        eval_data = {\n            k: (\n                DataPipeline(\n                    datasets=v.datasets,\n                    dataloader=LoaderIterResettable(\n                        root=v.dataloader,\n                        restart_on_stop_iteration=False,\n                    ),\n                )\n                if v is not None\n                else None\n            )\n            for k, v in eval_data.items()\n        }\n        return eval_data\n</code></pre>"},{"location":"reference/recipe/train/builders/data_builder/#optimus_dl.recipe.train.builders.data_builder.DataBuilder.build_eval_data","title":"<code>build_eval_data(collective, **kwargs)</code>","text":"<p>Build evaluation data pipelines.</p> <p>Constructs a dictionary of pipelines for multiple evaluation datasets. Uses <code>LoaderIterResettable</code> to allow repeated iteration over the same validation sets.</p> <p>Parameters:</p> Name Type Description Default <code>collective</code> <code>Collective</code> <p>Distributed collective.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, DataPipeline | None]</code> <p>Dictionary mapping dataset names to DataPipelines.</p> Source code in <code>optimus_dl/recipe/train/builders/data_builder.py</code> <pre><code>def build_eval_data(\n    self, collective: Collective, **kwargs: Any\n) -&gt; dict[str, DataPipeline | None]:\n    \"\"\"Build evaluation data pipelines.\n\n    Constructs a dictionary of pipelines for multiple evaluation datasets.\n    Uses `LoaderIterResettable` to allow repeated iteration over the same\n    validation sets.\n\n    Args:\n        collective: Distributed collective.\n        **kwargs: Additional arguments.\n\n    Returns:\n        Dictionary mapping dataset names to DataPipelines.\n    \"\"\"\n    kwargs[\"rank\"] = collective.dp_rank\n    kwargs[\"world_size\"] = collective.dp_world_size\n    kwargs[\"seed\"] = self._get_rank_seed(\n        self.data_seed, collective.dp_rank, collective.dp_world_size\n    )\n    eval_data = build_data_pipeline_dict(self.data_config.eval_datasets, **kwargs)\n    eval_data = {\n        k: (\n            DataPipeline(\n                datasets=v.datasets,\n                dataloader=LoaderIterResettable(\n                    root=v.dataloader,\n                    restart_on_stop_iteration=False,\n                ),\n            )\n            if v is not None\n            else None\n        )\n        for k, v in eval_data.items()\n    }\n    return eval_data\n</code></pre>"},{"location":"reference/recipe/train/builders/data_builder/#optimus_dl.recipe.train.builders.data_builder.DataBuilder.build_train_data","title":"<code>build_train_data(collective, **kwargs)</code>","text":"<p>Build the training data pipeline.</p> <p>Automatically injects rank and world_size for sharding. The resulting loader is configured to restart automatically on StopIteration, creating an infinite stream.</p> <p>Parameters:</p> Name Type Description Default <code>collective</code> <code>Collective</code> <p>Distributed collective for sharding info.</p> required <code>**kwargs</code> <p>Additional arguments passed to dataset builders.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataPipeline | None</code> <p>A DataPipeline containing the dataset and loader.</p> Source code in <code>optimus_dl/recipe/train/builders/data_builder.py</code> <pre><code>def build_train_data(self, collective: Collective, **kwargs) -&gt; DataPipeline | None:\n    \"\"\"Build the training data pipeline.\n\n    Automatically injects rank and world_size for sharding. The resulting\n    loader is configured to restart automatically on StopIteration, creating\n    an infinite stream.\n\n    Args:\n        collective: Distributed collective for sharding info.\n        **kwargs: Additional arguments passed to dataset builders.\n\n    Returns:\n        A DataPipeline containing the dataset and loader.\n    \"\"\"\n    kwargs[\"rank\"] = collective.dp_rank\n    kwargs[\"world_size\"] = collective.dp_world_size\n    kwargs[\"seed\"] = self._get_rank_seed(\n        self.data_seed, collective.dp_rank, collective.dp_world_size\n    )\n    train_data = build_data_pipeline(self.data_config.train_datasets, **kwargs)\n    if train_data is None:\n        return None\n    dataloader = torchdata.nodes.Loader(\n        root=train_data.dataloader,\n        restart_on_stop_iteration=True,\n    )\n    return DataPipeline(\n        datasets=train_data.datasets,\n        dataloader=dataloader,\n    )\n</code></pre>"},{"location":"reference/recipe/train/builders/data_builder/#optimus_dl.recipe.train.builders.data_builder.DataBuilderConfig","title":"<code>DataBuilderConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfig</code></p> <p>Configuration for DataBuilder.</p> Source code in <code>optimus_dl/recipe/train/builders/data_builder.py</code> <pre><code>@dataclass\nclass DataBuilderConfig(RegistryConfig):\n    \"\"\"Configuration for DataBuilder.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/recipe/train/builders/data_builder/#optimus_dl.recipe.train.builders.data_builder.LoaderIterResettable","title":"<code>LoaderIterResettable</code>","text":"<p>               Bases: <code>Loader</code></p> <p>A Loader that automatically resets its iterator on <code>__iter__</code>.</p> <p>This is essential for evaluation loops where the dataloader is re-used multiple times.</p> Source code in <code>optimus_dl/recipe/train/builders/data_builder.py</code> <pre><code>class LoaderIterResettable(torchdata.nodes.Loader):\n    \"\"\"A Loader that automatically resets its iterator on `__iter__`.\n\n    This is essential for evaluation loops where the dataloader is re-used\n    multiple times.\n    \"\"\"\n\n    def __init__(self, root, restart_on_stop_iteration: bool = True):\n        super().__init__(root=root, restart_on_stop_iteration=restart_on_stop_iteration)\n\n    def __iter__(self):\n        \"\"\"Reset the iterator state and return a new iterator.\"\"\"\n        iter = super().__iter__()\n        iter.reset()\n        return iter\n</code></pre>"},{"location":"reference/recipe/train/builders/data_builder/#optimus_dl.recipe.train.builders.data_builder.LoaderIterResettable.__iter__","title":"<code>__iter__()</code>","text":"<p>Reset the iterator state and return a new iterator.</p> Source code in <code>optimus_dl/recipe/train/builders/data_builder.py</code> <pre><code>def __iter__(self):\n    \"\"\"Reset the iterator state and return a new iterator.\"\"\"\n    iter = super().__iter__()\n    iter.reset()\n    return iter\n</code></pre>"},{"location":"reference/recipe/train/builders/optimizer_builder/","title":"optimizer_builder","text":""},{"location":"reference/recipe/train/builders/optimizer_builder/#optimus_dl.recipe.train.builders.optimizer_builder","title":"<code>optimus_dl.recipe.train.builders.optimizer_builder</code>","text":"<p>Optimizer builder mixin for building optimizers.</p>"},{"location":"reference/recipe/train/builders/optimizer_builder/#optimus_dl.recipe.train.builders.optimizer_builder.OptimizerBuilder","title":"<code>OptimizerBuilder</code>","text":"<p>Builder class responsible for creating the optimizer.</p> <p>Takes parameter groups from the model and instantiates the configured optimizer (e.g., AdamW). It also logs the total number of optimized parameters.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>OptimizerBuilderConfig</code> <p>Builder configuration.</p> required <code>optimization_config</code> <code>OptimizationConfig</code> <p>Optimization settings including the optimizer config.</p> required Source code in <code>optimus_dl/recipe/train/builders/optimizer_builder.py</code> <pre><code>class OptimizerBuilder:\n    \"\"\"Builder class responsible for creating the optimizer.\n\n    Takes parameter groups from the model and instantiates the configured\n    optimizer (e.g., AdamW). It also logs the total number of optimized\n    parameters.\n\n    Args:\n        cfg: Builder configuration.\n        optimization_config: Optimization settings including the optimizer config.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: OptimizerBuilderConfig,\n        optimization_config: OptimizationConfig,\n        **kwargs: Any,\n    ):\n        self.optimization_config = optimization_config\n\n    def build_optimizer(self, params, **kwargs) -&gt; Optimizer:\n        \"\"\"Build and validate the optimizer.\n\n        Args:\n            params: Iterable of parameters or dicts defining parameter groups\n                (typically from `model.make_parameter_groups()`).\n            **kwargs: Additional arguments passed to the optimizer constructor.\n\n        Returns:\n            Instantiated Optimizer.\n        \"\"\"\n        optimizer = build(\n            \"optimizer\", self.optimization_config.optimizer, params=params, **kwargs\n        )\n        assert isinstance(optimizer, Optimizer)\n        logger.info(f\"Optimizer \\n{optimizer}\")\n        optimized_params = []\n        for param_group in optimizer.param_groups:\n            optimized_params.append(\n                sum([p.numel() for p in param_group[\"params\"] if p.requires_grad])\n            )\n        optimized_params_num = sum(optimized_params)\n        logger.info(\n            f\"Optimized {optimized_params_num:,} parameters. Per group: {[f'{i:,}' for i in optimized_params]}\"\n        )\n        log_averaged(\"optimized_params\", optimized_params_num)\n\n        return optimizer\n</code></pre>"},{"location":"reference/recipe/train/builders/optimizer_builder/#optimus_dl.recipe.train.builders.optimizer_builder.OptimizerBuilder.build_optimizer","title":"<code>build_optimizer(params, **kwargs)</code>","text":"<p>Build and validate the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <p>Iterable of parameters or dicts defining parameter groups (typically from <code>model.make_parameter_groups()</code>).</p> required <code>**kwargs</code> <p>Additional arguments passed to the optimizer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optimizer</code> <p>Instantiated Optimizer.</p> Source code in <code>optimus_dl/recipe/train/builders/optimizer_builder.py</code> <pre><code>def build_optimizer(self, params, **kwargs) -&gt; Optimizer:\n    \"\"\"Build and validate the optimizer.\n\n    Args:\n        params: Iterable of parameters or dicts defining parameter groups\n            (typically from `model.make_parameter_groups()`).\n        **kwargs: Additional arguments passed to the optimizer constructor.\n\n    Returns:\n        Instantiated Optimizer.\n    \"\"\"\n    optimizer = build(\n        \"optimizer\", self.optimization_config.optimizer, params=params, **kwargs\n    )\n    assert isinstance(optimizer, Optimizer)\n    logger.info(f\"Optimizer \\n{optimizer}\")\n    optimized_params = []\n    for param_group in optimizer.param_groups:\n        optimized_params.append(\n            sum([p.numel() for p in param_group[\"params\"] if p.requires_grad])\n        )\n    optimized_params_num = sum(optimized_params)\n    logger.info(\n        f\"Optimized {optimized_params_num:,} parameters. Per group: {[f'{i:,}' for i in optimized_params]}\"\n    )\n    log_averaged(\"optimized_params\", optimized_params_num)\n\n    return optimizer\n</code></pre>"},{"location":"reference/recipe/train/builders/optimizer_builder/#optimus_dl.recipe.train.builders.optimizer_builder.OptimizerBuilderConfig","title":"<code>OptimizerBuilderConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfig</code></p> <p>Configuration for OptimizerBuilder.</p> Source code in <code>optimus_dl/recipe/train/builders/optimizer_builder.py</code> <pre><code>@dataclass\nclass OptimizerBuilderConfig(RegistryConfig):\n    \"\"\"Configuration for OptimizerBuilder.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/recipe/train/builders/scheduler_builder/","title":"scheduler_builder","text":""},{"location":"reference/recipe/train/builders/scheduler_builder/#optimus_dl.recipe.train.builders.scheduler_builder","title":"<code>optimus_dl.recipe.train.builders.scheduler_builder</code>","text":"<p>Scheduler builder mixin for building learning rate schedulers.</p>"},{"location":"reference/recipe/train/builders/scheduler_builder/#optimus_dl.recipe.train.builders.scheduler_builder.SchedulerBuilder","title":"<code>SchedulerBuilder</code>","text":"<p>Builder class responsible for creating the learning rate scheduler.</p> <p>Instantiates a scheduler (e.g., CosineAnnealing, WSD) and associates it with the optimizer. It ensures the scheduler is aware of the total training iterations.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>SchedulerBuilderConfig</code> <p>Builder configuration.</p> required <code>lr_scheduler_config</code> <code>RegistryConfig | None</code> <p>Configuration for the scheduler itself (can be None).</p> required <code>optimization_config</code> <code>OptimizationConfig</code> <p>Optimization settings (needed for total iterations).</p> required Source code in <code>optimus_dl/recipe/train/builders/scheduler_builder.py</code> <pre><code>class SchedulerBuilder:\n    \"\"\"Builder class responsible for creating the learning rate scheduler.\n\n    Instantiates a scheduler (e.g., CosineAnnealing, WSD) and associates it\n    with the optimizer. It ensures the scheduler is aware of the total training\n    iterations.\n\n    Args:\n        cfg: Builder configuration.\n        lr_scheduler_config: Configuration for the scheduler itself (can be None).\n        optimization_config: Optimization settings (needed for total iterations).\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: SchedulerBuilderConfig,\n        lr_scheduler_config: RegistryConfig | None,\n        optimization_config: OptimizationConfig,\n        **kwargs: Any,\n    ):\n        self.lr_scheduler_config = lr_scheduler_config\n        self.optimization_config = optimization_config\n\n    def build_lr_scheduler(self, optimizer: Optimizer, **kwargs):\n        \"\"\"Build and validate the learning rate scheduler.\n\n        Args:\n            optimizer: The optimizer to schedule.\n            **kwargs: Additional arguments.\n\n        Returns:\n            Instantiated LR Scheduler or None if not configured.\n        \"\"\"\n        if self.lr_scheduler_config is None:\n            return None\n        lr_scheduler = build(\n            \"lr_scheduler\",\n            cfg=self.lr_scheduler_config,\n            optimizer=optimizer,\n            iterations=self.optimization_config.iterations,\n            **kwargs,\n        )\n        if lr_scheduler is not None:\n            logger.info(f\"LR Scheduler \\n{lr_scheduler}\")\n        return lr_scheduler\n</code></pre>"},{"location":"reference/recipe/train/builders/scheduler_builder/#optimus_dl.recipe.train.builders.scheduler_builder.SchedulerBuilder.build_lr_scheduler","title":"<code>build_lr_scheduler(optimizer, **kwargs)</code>","text":"<p>Build and validate the learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to schedule.</p> required <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Instantiated LR Scheduler or None if not configured.</p> Source code in <code>optimus_dl/recipe/train/builders/scheduler_builder.py</code> <pre><code>def build_lr_scheduler(self, optimizer: Optimizer, **kwargs):\n    \"\"\"Build and validate the learning rate scheduler.\n\n    Args:\n        optimizer: The optimizer to schedule.\n        **kwargs: Additional arguments.\n\n    Returns:\n        Instantiated LR Scheduler or None if not configured.\n    \"\"\"\n    if self.lr_scheduler_config is None:\n        return None\n    lr_scheduler = build(\n        \"lr_scheduler\",\n        cfg=self.lr_scheduler_config,\n        optimizer=optimizer,\n        iterations=self.optimization_config.iterations,\n        **kwargs,\n    )\n    if lr_scheduler is not None:\n        logger.info(f\"LR Scheduler \\n{lr_scheduler}\")\n    return lr_scheduler\n</code></pre>"},{"location":"reference/recipe/train/builders/scheduler_builder/#optimus_dl.recipe.train.builders.scheduler_builder.SchedulerBuilderConfig","title":"<code>SchedulerBuilderConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfig</code></p> <p>Configuration for SchedulerBuilder.</p> Source code in <code>optimus_dl/recipe/train/builders/scheduler_builder.py</code> <pre><code>@dataclass\nclass SchedulerBuilderConfig(RegistryConfig):\n    \"\"\"Configuration for SchedulerBuilder.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/recipe/train/mixins/","title":"Index","text":""},{"location":"reference/recipe/train/mixins/#optimus_dl.recipe.train.mixins","title":"<code>optimus_dl.recipe.train.mixins</code>","text":""},{"location":"reference/recipe/train/mixins/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>execution</code>: </li> <li><code>managers</code>: </li> </ul>"},{"location":"reference/recipe/train/mixins/execution/","title":"Index","text":""},{"location":"reference/recipe/train/mixins/execution/#optimus_dl.recipe.train.mixins.execution","title":"<code>optimus_dl.recipe.train.mixins.execution</code>","text":""},{"location":"reference/recipe/train/mixins/execution/#optimus_dl.recipe.train.mixins.execution.TrainingContextMixin","title":"<code>TrainingContextMixin</code>","text":"<p>Mixin for setting up the training context (precision, scaling, devices).</p> <p>Responsible for initializing PyTorch's AMP (Automatic Mixed Precision) and GradScaler based on the optimization configuration. This ensures consistent precision settings across the training loop.</p> <p>Parameters:</p> Name Type Description Default <code>optimization_config</code> <code>OptimizationConfig</code> <p>Configuration containing AMP settings.</p> required Source code in <code>optimus_dl/recipe/train/mixins/execution/context_mixin.py</code> <pre><code>class TrainingContextMixin:\n    \"\"\"Mixin for setting up the training context (precision, scaling, devices).\n\n    Responsible for initializing PyTorch's AMP (Automatic Mixed Precision) and\n    GradScaler based on the optimization configuration. This ensures consistent\n    precision settings across the training loop.\n\n    Args:\n        optimization_config: Configuration containing AMP settings.\n    \"\"\"\n\n    def __init__(self, optimization_config: OptimizationConfig):\n        self.optimization_config = optimization_config\n\n    def setup_training_context(self, device: torch.device) -&gt; dict[str, Any]:\n        \"\"\"Initialize AMP context and Gradient Scaler.\n\n        Args:\n            device: The target compute device.\n\n        Returns:\n            A dictionary containing:\n\n            - \"scaler\": The torch.cuda.amp.GradScaler instance.\n            - \"amp_ctx\": The torch.autocast context manager.\n            - \"amp_cfg\": The raw AMP configuration object.\n            - \"device\": The device being used.\n        \"\"\"\n        amp_cfg = self.optimization_config.amp\n        scaler = torch.GradScaler(\n            device=device.type,\n            enabled=amp_cfg.enabled and amp_cfg.enable_scaler,\n            init_scale=amp_cfg.init_scale,\n            growth_factor=amp_cfg.growth_factor,\n            backoff_factor=amp_cfg.backoff_factor,\n            growth_interval=amp_cfg.growth_interval,\n        )\n        logger.info(f\"Using grad scaler: {scaler.is_enabled()}\")\n        # Safe dtype conversion without eval()\n        dtype_map = {\n            \"torch.float16\": torch.float16,\n            \"torch.float32\": torch.float32,\n            \"torch.bfloat16\": torch.bfloat16,\n            \"float16\": torch.float16,\n            \"float32\": torch.float32,\n            \"bfloat16\": torch.bfloat16,\n        }\n\n        dtype = dtype_map.get(amp_cfg.dtype, torch.float16)\n        if amp_cfg.dtype not in dtype_map:\n            logger.warning(f\"Unknown dtype '{amp_cfg.dtype}', defaulting to float16\")\n\n        amp_ctx = torch.autocast(device.type, dtype=dtype, enabled=amp_cfg.enabled)\n\n        return {\n            \"scaler\": scaler,\n            \"amp_ctx\": amp_ctx,\n            \"amp_cfg\": amp_cfg,\n            \"device\": device,\n        }\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/#optimus_dl.recipe.train.mixins.execution.TrainingContextMixin.setup_training_context","title":"<code>setup_training_context(device)</code>","text":"<p>Initialize AMP context and Gradient Scaler.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>The target compute device.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing:</p> <code>dict[str, Any]</code> <ul> <li>\"scaler\": The torch.cuda.amp.GradScaler instance.</li> </ul> <code>dict[str, Any]</code> <ul> <li>\"amp_ctx\": The torch.autocast context manager.</li> </ul> <code>dict[str, Any]</code> <ul> <li>\"amp_cfg\": The raw AMP configuration object.</li> </ul> <code>dict[str, Any]</code> <ul> <li>\"device\": The device being used.</li> </ul> Source code in <code>optimus_dl/recipe/train/mixins/execution/context_mixin.py</code> <pre><code>def setup_training_context(self, device: torch.device) -&gt; dict[str, Any]:\n    \"\"\"Initialize AMP context and Gradient Scaler.\n\n    Args:\n        device: The target compute device.\n\n    Returns:\n        A dictionary containing:\n\n        - \"scaler\": The torch.cuda.amp.GradScaler instance.\n        - \"amp_ctx\": The torch.autocast context manager.\n        - \"amp_cfg\": The raw AMP configuration object.\n        - \"device\": The device being used.\n    \"\"\"\n    amp_cfg = self.optimization_config.amp\n    scaler = torch.GradScaler(\n        device=device.type,\n        enabled=amp_cfg.enabled and amp_cfg.enable_scaler,\n        init_scale=amp_cfg.init_scale,\n        growth_factor=amp_cfg.growth_factor,\n        backoff_factor=amp_cfg.backoff_factor,\n        growth_interval=amp_cfg.growth_interval,\n    )\n    logger.info(f\"Using grad scaler: {scaler.is_enabled()}\")\n    # Safe dtype conversion without eval()\n    dtype_map = {\n        \"torch.float16\": torch.float16,\n        \"torch.float32\": torch.float32,\n        \"torch.bfloat16\": torch.bfloat16,\n        \"float16\": torch.float16,\n        \"float32\": torch.float32,\n        \"bfloat16\": torch.bfloat16,\n    }\n\n    dtype = dtype_map.get(amp_cfg.dtype, torch.float16)\n    if amp_cfg.dtype not in dtype_map:\n        logger.warning(f\"Unknown dtype '{amp_cfg.dtype}', defaulting to float16\")\n\n    amp_ctx = torch.autocast(device.type, dtype=dtype, enabled=amp_cfg.enabled)\n\n    return {\n        \"scaler\": scaler,\n        \"amp_ctx\": amp_ctx,\n        \"amp_cfg\": amp_cfg,\n        \"device\": device,\n    }\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/#optimus_dl.recipe.train.mixins.execution.TrainingInterruptionMixin","title":"<code>TrainingInterruptionMixin</code>","text":"<p>Mixin for gracefully handling training interruptions.</p> <p>Provides a mechanism to catch <code>KeyboardInterrupt</code> (Ctrl+C) and trigger a safe shutdown sequence, which typically involves saving a final checkpoint to ensure progress is not lost.</p> <p>Parameters:</p> Name Type Description Default <code>save_freq</code> <code>int</code> <p>Frequency of regular checkpoints. If 0, saving is disabled.</p> <code>0</code> <code>output_path</code> <code>str | None</code> <p>Path where checkpoints are saved.</p> <code>None</code> <code>checkpoint_callback</code> <code>Callable[..., None] | None</code> <p>Callable to execute for saving the checkpoint.</p> <code>None</code> Source code in <code>optimus_dl/recipe/train/mixins/execution/interruption_mixin.py</code> <pre><code>class TrainingInterruptionMixin:\n    \"\"\"Mixin for gracefully handling training interruptions.\n\n    Provides a mechanism to catch `KeyboardInterrupt` (Ctrl+C) and trigger a\n    safe shutdown sequence, which typically involves saving a final checkpoint\n    to ensure progress is not lost.\n\n    Args:\n        save_freq: Frequency of regular checkpoints. If 0, saving is disabled.\n        output_path: Path where checkpoints are saved.\n        checkpoint_callback: Callable to execute for saving the checkpoint.\n    \"\"\"\n\n    def __init__(\n        self,\n        save_freq: int = 0,\n        output_path: str | None = None,\n        checkpoint_callback: Callable[..., None] | None = None,\n    ):\n        self.save_freq = save_freq\n        self.output_path = output_path\n        self.checkpoint_callback = checkpoint_callback\n\n    def handle_training_interruption(\n        self,\n        iteration: int,\n        collective: Collective | None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Handle interruption by saving a final checkpoint.\n\n        Args:\n            iteration: The current training iteration count.\n            collective: The distributed collective instance.\n            **kwargs: Additional arguments to pass to the checkpoint callback.\n        \"\"\"\n        logger.info(\"Training interrupted by user\")\n\n        # Check if we have checkpoint saving configured and callback available\n        if self.save_freq &gt; 0 and self.output_path and self.checkpoint_callback:\n            try:\n                logger.info(\"Saving final checkpoint...\")\n\n                # Call the checkpoint callback with the required parameters\n                self.checkpoint_callback(\n                    checkpoint_path=self.output_path,\n                    iteration=iteration,\n                    collective=collective,\n                    **kwargs,\n                )\n                logger.info(\"Final checkpoint saved\")\n\n            except Exception as e:\n                logger.error(f\"Failed to save final checkpoint: {e}\")\n                raise\n        elif self.save_freq &gt; 0:\n            logger.warning(\n                \"Checkpoint saving requested but no callback provided or output_path missing\"\n            )\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/#optimus_dl.recipe.train.mixins.execution.TrainingInterruptionMixin.handle_training_interruption","title":"<code>handle_training_interruption(iteration, collective, **kwargs)</code>","text":"<p>Handle interruption by saving a final checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>iteration</code> <code>int</code> <p>The current training iteration count.</p> required <code>collective</code> <code>Collective | None</code> <p>The distributed collective instance.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the checkpoint callback.</p> <code>{}</code> Source code in <code>optimus_dl/recipe/train/mixins/execution/interruption_mixin.py</code> <pre><code>def handle_training_interruption(\n    self,\n    iteration: int,\n    collective: Collective | None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Handle interruption by saving a final checkpoint.\n\n    Args:\n        iteration: The current training iteration count.\n        collective: The distributed collective instance.\n        **kwargs: Additional arguments to pass to the checkpoint callback.\n    \"\"\"\n    logger.info(\"Training interrupted by user\")\n\n    # Check if we have checkpoint saving configured and callback available\n    if self.save_freq &gt; 0 and self.output_path and self.checkpoint_callback:\n        try:\n            logger.info(\"Saving final checkpoint...\")\n\n            # Call the checkpoint callback with the required parameters\n            self.checkpoint_callback(\n                checkpoint_path=self.output_path,\n                iteration=iteration,\n                collective=collective,\n                **kwargs,\n            )\n            logger.info(\"Final checkpoint saved\")\n\n        except Exception as e:\n            logger.error(f\"Failed to save final checkpoint: {e}\")\n            raise\n    elif self.save_freq &gt; 0:\n        logger.warning(\n            \"Checkpoint saving requested but no callback provided or output_path missing\"\n        )\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/#optimus_dl.recipe.train.mixins.execution.TrainingIterationMixin","title":"<code>TrainingIterationMixin</code>","text":"<p>Mixin for executing a complete training step with gradient accumulation.</p> <p>Encapsulates the core training logic: 1.  Forward Pass: Runs the model and criterion, measuring time. 2.  Backward Pass: Scales gradients and backpropagates, handling     loss parallelism if applicable. 3.  Optimization: Unscales gradients, clips norms, and steps the optimizer. 4.  Logging: Records detailed performance metrics (forward/backward times,     grad norms, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>optimization_config</code> <code>OptimizationConfig</code> <p>Configuration for optimization (accumulation steps, clipping).</p> required <code>log_freq</code> <code>int</code> <p>Frequency of metric logging.</p> <code>1</code> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>class TrainingIterationMixin:\n    \"\"\"Mixin for executing a complete training step with gradient accumulation.\n\n    Encapsulates the core training logic:\n    1.  **Forward Pass**: Runs the model and criterion, measuring time.\n    2.  **Backward Pass**: Scales gradients and backpropagates, handling\n        loss parallelism if applicable.\n    3.  **Optimization**: Unscales gradients, clips norms, and steps the optimizer.\n    4.  **Logging**: Records detailed performance metrics (forward/backward times,\n        grad norms, etc.).\n\n    Args:\n        optimization_config: Configuration for optimization (accumulation steps, clipping).\n        log_freq: Frequency of metric logging.\n    \"\"\"\n\n    def __init__(self, optimization_config: OptimizationConfig, log_freq: int = 1):\n        self.optimization_config = optimization_config\n        self.log_freq = log_freq\n\n    def log_memory_usage(self):\n        \"\"\"Log GPU memory usage statistics.\"\"\"\n        if torch.cuda.is_available():\n            log_summed(\"gpu_gb_allocated\", torch.cuda.memory_allocated() / (1024**3))\n            log_summed(\"gpu_gb_used\", torch.cuda.max_memory_allocated() / (1024**3))\n\n    def execute_forward_pass(\n        self,\n        model: BaseModel,\n        criterion: BaseCriterion,\n        batch: Any,\n        amp_ctx: Any,\n        requested_protocols: set[str] | None = None,\n    ) -&gt; ForwardPassResult:\n        \"\"\"Run the forward pass inside an AMP context.\n\n        Args:\n            model: The model to run.\n            criterion: The loss function.\n            batch: The input data.\n            amp_ctx: The autocast context manager.\n            requested_protocols: Protocols requested by the metrics system.\n\n        Returns:\n            ForwardPassResult with the computed loss, exposed protocols, and execution time.\n        \"\"\"\n        with amp_ctx:\n            elapsed_forward, (loss, exposed) = measured_lambda(\n                lambda: criterion(model, batch, requested_protocols=requested_protocols)\n            )\n        return ForwardPassResult(\n            loss=loss, exposed_protocols=exposed, elapsed_time=elapsed_forward\n        )\n\n    def execute_backward_pass(self, loss: torch.Tensor, scaler: Any) -&gt; float:\n        \"\"\"Run the backward pass with gradient scaling.\n\n        Handles `loss_parallel` context if the loss is a DTensor.\n\n        Args:\n            loss: The computed loss tensor.\n            scaler: The gradient scaler.\n\n        Returns:\n            Execution time in milliseconds.\n        \"\"\"\n\n        def backward():\n            with loss_parallel() if isinstance(loss, DTensor) else nullcontext():\n                scaler.scale(loss).backward()\n\n        elapsed_backward, _ = measured_lambda(backward)\n        return elapsed_backward\n\n    def execute_optimizer_step(\n        self,\n        optimizer: Optimizer,\n        model: BaseModel,\n        scaler: Any,\n        clip_grad_norm: float | None = None,\n    ) -&gt; OptimizerStepResult:\n        \"\"\"Perform the optimization step.\n\n        Includes gradient unscaling, optional gradient clipping, and the\n        optimizer step itself. Updates the scaler state afterwards.\n\n        Args:\n            optimizer: The optimizer.\n            model: The model (needed for clipping gradients).\n            scaler: The gradient scaler.\n            clip_grad_norm: Maximum norm for gradient clipping.\n\n        Returns:\n            OptimizerStepResult with execution time and the computed gradient norm.\n        \"\"\"\n        scaler.unscale_(optimizer)\n\n        grad_norm = None\n        if clip_grad_norm is not None:\n            from torch.distributed.tensor.experimental import implicit_replication\n\n            with implicit_replication():\n                grad_norm = torch.nn.utils.clip_grad_norm_(\n                    model.parameters(), max_norm=clip_grad_norm\n                )\n\n        elapsed, _ = measured_lambda(lambda: scaler.step(optimizer))\n        scaler.update()\n\n        if scaler.is_enabled():\n            log_averaged(\"grad_scale\", scaler.get_scale())\n\n        return OptimizerStepResult(elapsed_time=elapsed, grad_norm=grad_norm)\n\n    def log_batch_metrics(\n        self,\n        elapsed_batch_get: float,\n        elapsed_forward: float,\n        elapsed_backward: float,\n        acc_steps: int,\n    ) -&gt; None:\n        \"\"\"Log timing metrics for data loading and forward/backward passes.\"\"\"\n        weight = 1 / acc_steps\n\n        log_averaged(\n            \"perf/batch_get\",\n            value=elapsed_batch_get,\n            weight=weight,\n            priority=999,\n        )\n        log_averaged(\n            \"perf/forward\",\n            value=elapsed_forward,\n            weight=weight,\n            priority=1000,\n        )\n        log_averaged(\n            \"perf/backward\",\n            value=elapsed_backward,\n            weight=weight,\n            priority=1001,\n        )\n\n    def log_optimizer_metrics(\n        self,\n        elapsed_optimizer: float,\n        grad_norm: torch.Tensor | None,\n        lr_scheduler: Any | None,\n        optimizer: Optimizer,\n    ) -&gt; None:\n        \"\"\"Log optimizer performance, gradient norms, and learning rates.\"\"\"\n        log_averaged(\"perf/optimizer\", value=elapsed_optimizer, priority=1002)\n\n        # Log gradient norm if clipping was performed\n        if grad_norm is not None:\n            log_averaged(\n                \"grad_norm\",\n                lambda: (float(grad_norm) if grad_norm is not None else 0.0),\n            )\n\n        # Learning rate (cheap but we only need it periodically)\n        if lr_scheduler is not None:\n            log_averaged(\"learning_rate\", lambda: lr_scheduler.get_last_lr()[0])\n        else:\n            log_averaged(\"learning_rate\", lambda: optimizer.param_groups[0][\"lr\"])\n\n    def run_training_iteration(\n        self,\n        model: BaseModel,\n        optimizer: Optimizer,\n        criterion: BaseCriterion,\n        train_data_iter: Iterator,\n        training_context: dict[str, Any],\n        lr_scheduler: Any | None = None,\n        metric_engine: Any | None = None,\n    ) -&gt; None:\n        \"\"\"Execute one full training iteration, including gradient accumulation.\n\n        This is the main driver for a training step. It loops `acc_steps` times\n        to accumulate gradients before performing a single optimizer update.\n\n        Args:\n            model: The model to train.\n            optimizer: The optimizer.\n            criterion: The loss function.\n            train_data_iter: Iterator yielding training batches.\n            training_context: Dict with scaler, amp_ctx, etc.\n            lr_scheduler: Optional learning rate scheduler.\n            metric_engine: Optional MetricEngine for training metrics.\n        \"\"\"\n        with meters_group(\"train\", log_freq=self.log_freq) as should_log:\n            optimizer.zero_grad()\n            model.train()\n\n            requested_protocols = None\n            if metric_engine and should_log:\n                requested_protocols = metric_engine.required_external_protocols\n\n            # Gradient accumulation loop\n            for microbatch_idx in range(self.optimization_config.acc_steps):\n                is_last_microbatch = (\n                    microbatch_idx == self.optimization_config.acc_steps - 1\n                )\n\n                try:\n                    elapsed_batch_get, batch = measured_next(train_data_iter)\n                except StopIteration:\n                    logger.error(\"Training data iterator exhausted unexpectedly\")\n                    break\n                except Exception as e:\n                    logger.error(f\"Error getting batch: {e}\")\n                    continue\n\n                with self.accumulation_context(model, is_last_microbatch):\n                    forward_result = self.execute_forward_pass(\n                        model,\n                        criterion,\n                        batch,\n                        training_context[\"amp_ctx\"],\n                        requested_protocols=requested_protocols,\n                    )\n                    loss = forward_result.loss / self.optimization_config.acc_steps\n\n                    if metric_engine and should_log:\n                        # Pass computed data (loss, logits, etc.) to avoid redundant work in engine\n                        computed_data = forward_result.exposed_protocols.copy()\n                        computed_data[\"loss\"] = forward_result.loss\n                        metric_engine.update(\n                            data=dict(model=model, batch=batch),\n                            computed_data=computed_data,\n                        )\n\n                    elapsed_backward = self.execute_backward_pass(\n                        loss, training_context[\"scaler\"]\n                    )\n\n                # Log performance metrics using the training metrics mixin\n                self.log_batch_metrics(\n                    elapsed_batch_get,\n                    forward_result.elapsed_time,\n                    elapsed_backward,\n                    self.optimization_config.acc_steps,\n                )\n\n            # Optimizer step\n            optimizer_result = self.execute_optimizer_step(\n                optimizer,\n                model,\n                training_context[\"scaler\"],\n                self.optimization_config.clip_grad_norm,\n            )\n\n            # Log optimizer metrics\n            self.log_optimizer_metrics(\n                optimizer_result.elapsed_time,\n                optimizer_result.grad_norm,\n                lr_scheduler,\n                optimizer,\n            )\n            self.log_memory_usage()\n            optimizer.zero_grad()\n\n            if lr_scheduler is not None:\n                lr_scheduler.step()\n\n    def accumulation_context(self, model, is_last_microbatch):\n        \"\"\"Get the appropriate context manager for gradient accumulation.\n\n        For FSDP/DDP models, this handles synchronization (e.g., disabling\n        all-reduce during accumulation steps).\n        \"\"\"\n        if hasattr(model, \"accumulation_context\"):\n            ctx = model.accumulation_context(is_last_microbatch=is_last_microbatch)\n            if not is_last_microbatch:\n                warn_once(logger, \"Using accumulation context\")\n            return ctx\n        else:\n            warn_once(logger, \"Model does not support accumulation context, skipping\")\n            return nullcontext()\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/#optimus_dl.recipe.train.mixins.execution.TrainingIterationMixin.accumulation_context","title":"<code>accumulation_context(model, is_last_microbatch)</code>","text":"<p>Get the appropriate context manager for gradient accumulation.</p> <p>For FSDP/DDP models, this handles synchronization (e.g., disabling all-reduce during accumulation steps).</p> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>def accumulation_context(self, model, is_last_microbatch):\n    \"\"\"Get the appropriate context manager for gradient accumulation.\n\n    For FSDP/DDP models, this handles synchronization (e.g., disabling\n    all-reduce during accumulation steps).\n    \"\"\"\n    if hasattr(model, \"accumulation_context\"):\n        ctx = model.accumulation_context(is_last_microbatch=is_last_microbatch)\n        if not is_last_microbatch:\n            warn_once(logger, \"Using accumulation context\")\n        return ctx\n    else:\n        warn_once(logger, \"Model does not support accumulation context, skipping\")\n        return nullcontext()\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/#optimus_dl.recipe.train.mixins.execution.TrainingIterationMixin.execute_backward_pass","title":"<code>execute_backward_pass(loss, scaler)</code>","text":"<p>Run the backward pass with gradient scaling.</p> <p>Handles <code>loss_parallel</code> context if the loss is a DTensor.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Tensor</code> <p>The computed loss tensor.</p> required <code>scaler</code> <code>Any</code> <p>The gradient scaler.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Execution time in milliseconds.</p> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>def execute_backward_pass(self, loss: torch.Tensor, scaler: Any) -&gt; float:\n    \"\"\"Run the backward pass with gradient scaling.\n\n    Handles `loss_parallel` context if the loss is a DTensor.\n\n    Args:\n        loss: The computed loss tensor.\n        scaler: The gradient scaler.\n\n    Returns:\n        Execution time in milliseconds.\n    \"\"\"\n\n    def backward():\n        with loss_parallel() if isinstance(loss, DTensor) else nullcontext():\n            scaler.scale(loss).backward()\n\n    elapsed_backward, _ = measured_lambda(backward)\n    return elapsed_backward\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/#optimus_dl.recipe.train.mixins.execution.TrainingIterationMixin.execute_forward_pass","title":"<code>execute_forward_pass(model, criterion, batch, amp_ctx, requested_protocols=None)</code>","text":"<p>Run the forward pass inside an AMP context.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel</code> <p>The model to run.</p> required <code>criterion</code> <code>BaseCriterion</code> <p>The loss function.</p> required <code>batch</code> <code>Any</code> <p>The input data.</p> required <code>amp_ctx</code> <code>Any</code> <p>The autocast context manager.</p> required <code>requested_protocols</code> <code>set[str] | None</code> <p>Protocols requested by the metrics system.</p> <code>None</code> <p>Returns:</p> Type Description <code>ForwardPassResult</code> <p>ForwardPassResult with the computed loss, exposed protocols, and execution time.</p> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>def execute_forward_pass(\n    self,\n    model: BaseModel,\n    criterion: BaseCriterion,\n    batch: Any,\n    amp_ctx: Any,\n    requested_protocols: set[str] | None = None,\n) -&gt; ForwardPassResult:\n    \"\"\"Run the forward pass inside an AMP context.\n\n    Args:\n        model: The model to run.\n        criterion: The loss function.\n        batch: The input data.\n        amp_ctx: The autocast context manager.\n        requested_protocols: Protocols requested by the metrics system.\n\n    Returns:\n        ForwardPassResult with the computed loss, exposed protocols, and execution time.\n    \"\"\"\n    with amp_ctx:\n        elapsed_forward, (loss, exposed) = measured_lambda(\n            lambda: criterion(model, batch, requested_protocols=requested_protocols)\n        )\n    return ForwardPassResult(\n        loss=loss, exposed_protocols=exposed, elapsed_time=elapsed_forward\n    )\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/#optimus_dl.recipe.train.mixins.execution.TrainingIterationMixin.execute_optimizer_step","title":"<code>execute_optimizer_step(optimizer, model, scaler, clip_grad_norm=None)</code>","text":"<p>Perform the optimization step.</p> <p>Includes gradient unscaling, optional gradient clipping, and the optimizer step itself. Updates the scaler state afterwards.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer.</p> required <code>model</code> <code>BaseModel</code> <p>The model (needed for clipping gradients).</p> required <code>scaler</code> <code>Any</code> <p>The gradient scaler.</p> required <code>clip_grad_norm</code> <code>float | None</code> <p>Maximum norm for gradient clipping.</p> <code>None</code> <p>Returns:</p> Type Description <code>OptimizerStepResult</code> <p>OptimizerStepResult with execution time and the computed gradient norm.</p> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>def execute_optimizer_step(\n    self,\n    optimizer: Optimizer,\n    model: BaseModel,\n    scaler: Any,\n    clip_grad_norm: float | None = None,\n) -&gt; OptimizerStepResult:\n    \"\"\"Perform the optimization step.\n\n    Includes gradient unscaling, optional gradient clipping, and the\n    optimizer step itself. Updates the scaler state afterwards.\n\n    Args:\n        optimizer: The optimizer.\n        model: The model (needed for clipping gradients).\n        scaler: The gradient scaler.\n        clip_grad_norm: Maximum norm for gradient clipping.\n\n    Returns:\n        OptimizerStepResult with execution time and the computed gradient norm.\n    \"\"\"\n    scaler.unscale_(optimizer)\n\n    grad_norm = None\n    if clip_grad_norm is not None:\n        from torch.distributed.tensor.experimental import implicit_replication\n\n        with implicit_replication():\n            grad_norm = torch.nn.utils.clip_grad_norm_(\n                model.parameters(), max_norm=clip_grad_norm\n            )\n\n    elapsed, _ = measured_lambda(lambda: scaler.step(optimizer))\n    scaler.update()\n\n    if scaler.is_enabled():\n        log_averaged(\"grad_scale\", scaler.get_scale())\n\n    return OptimizerStepResult(elapsed_time=elapsed, grad_norm=grad_norm)\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/#optimus_dl.recipe.train.mixins.execution.TrainingIterationMixin.log_batch_metrics","title":"<code>log_batch_metrics(elapsed_batch_get, elapsed_forward, elapsed_backward, acc_steps)</code>","text":"<p>Log timing metrics for data loading and forward/backward passes.</p> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>def log_batch_metrics(\n    self,\n    elapsed_batch_get: float,\n    elapsed_forward: float,\n    elapsed_backward: float,\n    acc_steps: int,\n) -&gt; None:\n    \"\"\"Log timing metrics for data loading and forward/backward passes.\"\"\"\n    weight = 1 / acc_steps\n\n    log_averaged(\n        \"perf/batch_get\",\n        value=elapsed_batch_get,\n        weight=weight,\n        priority=999,\n    )\n    log_averaged(\n        \"perf/forward\",\n        value=elapsed_forward,\n        weight=weight,\n        priority=1000,\n    )\n    log_averaged(\n        \"perf/backward\",\n        value=elapsed_backward,\n        weight=weight,\n        priority=1001,\n    )\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/#optimus_dl.recipe.train.mixins.execution.TrainingIterationMixin.log_memory_usage","title":"<code>log_memory_usage()</code>","text":"<p>Log GPU memory usage statistics.</p> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>def log_memory_usage(self):\n    \"\"\"Log GPU memory usage statistics.\"\"\"\n    if torch.cuda.is_available():\n        log_summed(\"gpu_gb_allocated\", torch.cuda.memory_allocated() / (1024**3))\n        log_summed(\"gpu_gb_used\", torch.cuda.max_memory_allocated() / (1024**3))\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/#optimus_dl.recipe.train.mixins.execution.TrainingIterationMixin.log_optimizer_metrics","title":"<code>log_optimizer_metrics(elapsed_optimizer, grad_norm, lr_scheduler, optimizer)</code>","text":"<p>Log optimizer performance, gradient norms, and learning rates.</p> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>def log_optimizer_metrics(\n    self,\n    elapsed_optimizer: float,\n    grad_norm: torch.Tensor | None,\n    lr_scheduler: Any | None,\n    optimizer: Optimizer,\n) -&gt; None:\n    \"\"\"Log optimizer performance, gradient norms, and learning rates.\"\"\"\n    log_averaged(\"perf/optimizer\", value=elapsed_optimizer, priority=1002)\n\n    # Log gradient norm if clipping was performed\n    if grad_norm is not None:\n        log_averaged(\n            \"grad_norm\",\n            lambda: (float(grad_norm) if grad_norm is not None else 0.0),\n        )\n\n    # Learning rate (cheap but we only need it periodically)\n    if lr_scheduler is not None:\n        log_averaged(\"learning_rate\", lambda: lr_scheduler.get_last_lr()[0])\n    else:\n        log_averaged(\"learning_rate\", lambda: optimizer.param_groups[0][\"lr\"])\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/#optimus_dl.recipe.train.mixins.execution.TrainingIterationMixin.run_training_iteration","title":"<code>run_training_iteration(model, optimizer, criterion, train_data_iter, training_context, lr_scheduler=None, metric_engine=None)</code>","text":"<p>Execute one full training iteration, including gradient accumulation.</p> <p>This is the main driver for a training step. It loops <code>acc_steps</code> times to accumulate gradients before performing a single optimizer update.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel</code> <p>The model to train.</p> required <code>optimizer</code> <code>Optimizer</code> <p>The optimizer.</p> required <code>criterion</code> <code>BaseCriterion</code> <p>The loss function.</p> required <code>train_data_iter</code> <code>Iterator</code> <p>Iterator yielding training batches.</p> required <code>training_context</code> <code>dict[str, Any]</code> <p>Dict with scaler, amp_ctx, etc.</p> required <code>lr_scheduler</code> <code>Any | None</code> <p>Optional learning rate scheduler.</p> <code>None</code> <code>metric_engine</code> <code>Any | None</code> <p>Optional MetricEngine for training metrics.</p> <code>None</code> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>def run_training_iteration(\n    self,\n    model: BaseModel,\n    optimizer: Optimizer,\n    criterion: BaseCriterion,\n    train_data_iter: Iterator,\n    training_context: dict[str, Any],\n    lr_scheduler: Any | None = None,\n    metric_engine: Any | None = None,\n) -&gt; None:\n    \"\"\"Execute one full training iteration, including gradient accumulation.\n\n    This is the main driver for a training step. It loops `acc_steps` times\n    to accumulate gradients before performing a single optimizer update.\n\n    Args:\n        model: The model to train.\n        optimizer: The optimizer.\n        criterion: The loss function.\n        train_data_iter: Iterator yielding training batches.\n        training_context: Dict with scaler, amp_ctx, etc.\n        lr_scheduler: Optional learning rate scheduler.\n        metric_engine: Optional MetricEngine for training metrics.\n    \"\"\"\n    with meters_group(\"train\", log_freq=self.log_freq) as should_log:\n        optimizer.zero_grad()\n        model.train()\n\n        requested_protocols = None\n        if metric_engine and should_log:\n            requested_protocols = metric_engine.required_external_protocols\n\n        # Gradient accumulation loop\n        for microbatch_idx in range(self.optimization_config.acc_steps):\n            is_last_microbatch = (\n                microbatch_idx == self.optimization_config.acc_steps - 1\n            )\n\n            try:\n                elapsed_batch_get, batch = measured_next(train_data_iter)\n            except StopIteration:\n                logger.error(\"Training data iterator exhausted unexpectedly\")\n                break\n            except Exception as e:\n                logger.error(f\"Error getting batch: {e}\")\n                continue\n\n            with self.accumulation_context(model, is_last_microbatch):\n                forward_result = self.execute_forward_pass(\n                    model,\n                    criterion,\n                    batch,\n                    training_context[\"amp_ctx\"],\n                    requested_protocols=requested_protocols,\n                )\n                loss = forward_result.loss / self.optimization_config.acc_steps\n\n                if metric_engine and should_log:\n                    # Pass computed data (loss, logits, etc.) to avoid redundant work in engine\n                    computed_data = forward_result.exposed_protocols.copy()\n                    computed_data[\"loss\"] = forward_result.loss\n                    metric_engine.update(\n                        data=dict(model=model, batch=batch),\n                        computed_data=computed_data,\n                    )\n\n                elapsed_backward = self.execute_backward_pass(\n                    loss, training_context[\"scaler\"]\n                )\n\n            # Log performance metrics using the training metrics mixin\n            self.log_batch_metrics(\n                elapsed_batch_get,\n                forward_result.elapsed_time,\n                elapsed_backward,\n                self.optimization_config.acc_steps,\n            )\n\n        # Optimizer step\n        optimizer_result = self.execute_optimizer_step(\n            optimizer,\n            model,\n            training_context[\"scaler\"],\n            self.optimization_config.clip_grad_norm,\n        )\n\n        # Log optimizer metrics\n        self.log_optimizer_metrics(\n            optimizer_result.elapsed_time,\n            optimizer_result.grad_norm,\n            lr_scheduler,\n            optimizer,\n        )\n        self.log_memory_usage()\n        optimizer.zero_grad()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>context_mixin</code>: Training context mixin for AMP and gradient scaler setup.</li> <li><code>interruption_mixin</code>: Training interruption mixin for handling errors and keyboard interrupts.</li> <li><code>iteration_mixin</code>: Training iteration mixin for orchestrating complete training iterations.</li> </ul>"},{"location":"reference/recipe/train/mixins/execution/context_mixin/","title":"context_mixin","text":""},{"location":"reference/recipe/train/mixins/execution/context_mixin/#optimus_dl.recipe.train.mixins.execution.context_mixin","title":"<code>optimus_dl.recipe.train.mixins.execution.context_mixin</code>","text":"<p>Training context mixin for AMP and gradient scaler setup.</p>"},{"location":"reference/recipe/train/mixins/execution/context_mixin/#optimus_dl.recipe.train.mixins.execution.context_mixin.TrainingContextMixin","title":"<code>TrainingContextMixin</code>","text":"<p>Mixin for setting up the training context (precision, scaling, devices).</p> <p>Responsible for initializing PyTorch's AMP (Automatic Mixed Precision) and GradScaler based on the optimization configuration. This ensures consistent precision settings across the training loop.</p> <p>Parameters:</p> Name Type Description Default <code>optimization_config</code> <code>OptimizationConfig</code> <p>Configuration containing AMP settings.</p> required Source code in <code>optimus_dl/recipe/train/mixins/execution/context_mixin.py</code> <pre><code>class TrainingContextMixin:\n    \"\"\"Mixin for setting up the training context (precision, scaling, devices).\n\n    Responsible for initializing PyTorch's AMP (Automatic Mixed Precision) and\n    GradScaler based on the optimization configuration. This ensures consistent\n    precision settings across the training loop.\n\n    Args:\n        optimization_config: Configuration containing AMP settings.\n    \"\"\"\n\n    def __init__(self, optimization_config: OptimizationConfig):\n        self.optimization_config = optimization_config\n\n    def setup_training_context(self, device: torch.device) -&gt; dict[str, Any]:\n        \"\"\"Initialize AMP context and Gradient Scaler.\n\n        Args:\n            device: The target compute device.\n\n        Returns:\n            A dictionary containing:\n\n            - \"scaler\": The torch.cuda.amp.GradScaler instance.\n            - \"amp_ctx\": The torch.autocast context manager.\n            - \"amp_cfg\": The raw AMP configuration object.\n            - \"device\": The device being used.\n        \"\"\"\n        amp_cfg = self.optimization_config.amp\n        scaler = torch.GradScaler(\n            device=device.type,\n            enabled=amp_cfg.enabled and amp_cfg.enable_scaler,\n            init_scale=amp_cfg.init_scale,\n            growth_factor=amp_cfg.growth_factor,\n            backoff_factor=amp_cfg.backoff_factor,\n            growth_interval=amp_cfg.growth_interval,\n        )\n        logger.info(f\"Using grad scaler: {scaler.is_enabled()}\")\n        # Safe dtype conversion without eval()\n        dtype_map = {\n            \"torch.float16\": torch.float16,\n            \"torch.float32\": torch.float32,\n            \"torch.bfloat16\": torch.bfloat16,\n            \"float16\": torch.float16,\n            \"float32\": torch.float32,\n            \"bfloat16\": torch.bfloat16,\n        }\n\n        dtype = dtype_map.get(amp_cfg.dtype, torch.float16)\n        if amp_cfg.dtype not in dtype_map:\n            logger.warning(f\"Unknown dtype '{amp_cfg.dtype}', defaulting to float16\")\n\n        amp_ctx = torch.autocast(device.type, dtype=dtype, enabled=amp_cfg.enabled)\n\n        return {\n            \"scaler\": scaler,\n            \"amp_ctx\": amp_ctx,\n            \"amp_cfg\": amp_cfg,\n            \"device\": device,\n        }\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/context_mixin/#optimus_dl.recipe.train.mixins.execution.context_mixin.TrainingContextMixin.setup_training_context","title":"<code>setup_training_context(device)</code>","text":"<p>Initialize AMP context and Gradient Scaler.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>The target compute device.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing:</p> <code>dict[str, Any]</code> <ul> <li>\"scaler\": The torch.cuda.amp.GradScaler instance.</li> </ul> <code>dict[str, Any]</code> <ul> <li>\"amp_ctx\": The torch.autocast context manager.</li> </ul> <code>dict[str, Any]</code> <ul> <li>\"amp_cfg\": The raw AMP configuration object.</li> </ul> <code>dict[str, Any]</code> <ul> <li>\"device\": The device being used.</li> </ul> Source code in <code>optimus_dl/recipe/train/mixins/execution/context_mixin.py</code> <pre><code>def setup_training_context(self, device: torch.device) -&gt; dict[str, Any]:\n    \"\"\"Initialize AMP context and Gradient Scaler.\n\n    Args:\n        device: The target compute device.\n\n    Returns:\n        A dictionary containing:\n\n        - \"scaler\": The torch.cuda.amp.GradScaler instance.\n        - \"amp_ctx\": The torch.autocast context manager.\n        - \"amp_cfg\": The raw AMP configuration object.\n        - \"device\": The device being used.\n    \"\"\"\n    amp_cfg = self.optimization_config.amp\n    scaler = torch.GradScaler(\n        device=device.type,\n        enabled=amp_cfg.enabled and amp_cfg.enable_scaler,\n        init_scale=amp_cfg.init_scale,\n        growth_factor=amp_cfg.growth_factor,\n        backoff_factor=amp_cfg.backoff_factor,\n        growth_interval=amp_cfg.growth_interval,\n    )\n    logger.info(f\"Using grad scaler: {scaler.is_enabled()}\")\n    # Safe dtype conversion without eval()\n    dtype_map = {\n        \"torch.float16\": torch.float16,\n        \"torch.float32\": torch.float32,\n        \"torch.bfloat16\": torch.bfloat16,\n        \"float16\": torch.float16,\n        \"float32\": torch.float32,\n        \"bfloat16\": torch.bfloat16,\n    }\n\n    dtype = dtype_map.get(amp_cfg.dtype, torch.float16)\n    if amp_cfg.dtype not in dtype_map:\n        logger.warning(f\"Unknown dtype '{amp_cfg.dtype}', defaulting to float16\")\n\n    amp_ctx = torch.autocast(device.type, dtype=dtype, enabled=amp_cfg.enabled)\n\n    return {\n        \"scaler\": scaler,\n        \"amp_ctx\": amp_ctx,\n        \"amp_cfg\": amp_cfg,\n        \"device\": device,\n    }\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/interruption_mixin/","title":"interruption_mixin","text":""},{"location":"reference/recipe/train/mixins/execution/interruption_mixin/#optimus_dl.recipe.train.mixins.execution.interruption_mixin","title":"<code>optimus_dl.recipe.train.mixins.execution.interruption_mixin</code>","text":"<p>Training interruption mixin for handling errors and keyboard interrupts.</p>"},{"location":"reference/recipe/train/mixins/execution/interruption_mixin/#optimus_dl.recipe.train.mixins.execution.interruption_mixin.TrainingInterruptionMixin","title":"<code>TrainingInterruptionMixin</code>","text":"<p>Mixin for gracefully handling training interruptions.</p> <p>Provides a mechanism to catch <code>KeyboardInterrupt</code> (Ctrl+C) and trigger a safe shutdown sequence, which typically involves saving a final checkpoint to ensure progress is not lost.</p> <p>Parameters:</p> Name Type Description Default <code>save_freq</code> <code>int</code> <p>Frequency of regular checkpoints. If 0, saving is disabled.</p> <code>0</code> <code>output_path</code> <code>str | None</code> <p>Path where checkpoints are saved.</p> <code>None</code> <code>checkpoint_callback</code> <code>Callable[..., None] | None</code> <p>Callable to execute for saving the checkpoint.</p> <code>None</code> Source code in <code>optimus_dl/recipe/train/mixins/execution/interruption_mixin.py</code> <pre><code>class TrainingInterruptionMixin:\n    \"\"\"Mixin for gracefully handling training interruptions.\n\n    Provides a mechanism to catch `KeyboardInterrupt` (Ctrl+C) and trigger a\n    safe shutdown sequence, which typically involves saving a final checkpoint\n    to ensure progress is not lost.\n\n    Args:\n        save_freq: Frequency of regular checkpoints. If 0, saving is disabled.\n        output_path: Path where checkpoints are saved.\n        checkpoint_callback: Callable to execute for saving the checkpoint.\n    \"\"\"\n\n    def __init__(\n        self,\n        save_freq: int = 0,\n        output_path: str | None = None,\n        checkpoint_callback: Callable[..., None] | None = None,\n    ):\n        self.save_freq = save_freq\n        self.output_path = output_path\n        self.checkpoint_callback = checkpoint_callback\n\n    def handle_training_interruption(\n        self,\n        iteration: int,\n        collective: Collective | None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Handle interruption by saving a final checkpoint.\n\n        Args:\n            iteration: The current training iteration count.\n            collective: The distributed collective instance.\n            **kwargs: Additional arguments to pass to the checkpoint callback.\n        \"\"\"\n        logger.info(\"Training interrupted by user\")\n\n        # Check if we have checkpoint saving configured and callback available\n        if self.save_freq &gt; 0 and self.output_path and self.checkpoint_callback:\n            try:\n                logger.info(\"Saving final checkpoint...\")\n\n                # Call the checkpoint callback with the required parameters\n                self.checkpoint_callback(\n                    checkpoint_path=self.output_path,\n                    iteration=iteration,\n                    collective=collective,\n                    **kwargs,\n                )\n                logger.info(\"Final checkpoint saved\")\n\n            except Exception as e:\n                logger.error(f\"Failed to save final checkpoint: {e}\")\n                raise\n        elif self.save_freq &gt; 0:\n            logger.warning(\n                \"Checkpoint saving requested but no callback provided or output_path missing\"\n            )\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/interruption_mixin/#optimus_dl.recipe.train.mixins.execution.interruption_mixin.TrainingInterruptionMixin.handle_training_interruption","title":"<code>handle_training_interruption(iteration, collective, **kwargs)</code>","text":"<p>Handle interruption by saving a final checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>iteration</code> <code>int</code> <p>The current training iteration count.</p> required <code>collective</code> <code>Collective | None</code> <p>The distributed collective instance.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the checkpoint callback.</p> <code>{}</code> Source code in <code>optimus_dl/recipe/train/mixins/execution/interruption_mixin.py</code> <pre><code>def handle_training_interruption(\n    self,\n    iteration: int,\n    collective: Collective | None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Handle interruption by saving a final checkpoint.\n\n    Args:\n        iteration: The current training iteration count.\n        collective: The distributed collective instance.\n        **kwargs: Additional arguments to pass to the checkpoint callback.\n    \"\"\"\n    logger.info(\"Training interrupted by user\")\n\n    # Check if we have checkpoint saving configured and callback available\n    if self.save_freq &gt; 0 and self.output_path and self.checkpoint_callback:\n        try:\n            logger.info(\"Saving final checkpoint...\")\n\n            # Call the checkpoint callback with the required parameters\n            self.checkpoint_callback(\n                checkpoint_path=self.output_path,\n                iteration=iteration,\n                collective=collective,\n                **kwargs,\n            )\n            logger.info(\"Final checkpoint saved\")\n\n        except Exception as e:\n            logger.error(f\"Failed to save final checkpoint: {e}\")\n            raise\n    elif self.save_freq &gt; 0:\n        logger.warning(\n            \"Checkpoint saving requested but no callback provided or output_path missing\"\n        )\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/iteration_mixin/","title":"iteration_mixin","text":""},{"location":"reference/recipe/train/mixins/execution/iteration_mixin/#optimus_dl.recipe.train.mixins.execution.iteration_mixin","title":"<code>optimus_dl.recipe.train.mixins.execution.iteration_mixin</code>","text":"<p>Training iteration mixin for orchestrating complete training iterations.</p>"},{"location":"reference/recipe/train/mixins/execution/iteration_mixin/#optimus_dl.recipe.train.mixins.execution.iteration_mixin.ForwardPassResult","title":"<code>ForwardPassResult</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>ForwardPassResult(loss, exposed_protocols, elapsed_time)</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Tensor</code> <code>None</code> <code>exposed_protocols</code> <code>dict[str, Any]</code> <code>None</code> <code>elapsed_time</code> <code>float</code> <code>None</code> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>class ForwardPassResult(NamedTuple):\n    loss: torch.Tensor\n    exposed_protocols: dict[str, Any]\n    elapsed_time: float\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/iteration_mixin/#optimus_dl.recipe.train.mixins.execution.iteration_mixin.OptimizerStepResult","title":"<code>OptimizerStepResult</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>OptimizerStepResult(elapsed_time, grad_norm)</p> <p>Parameters:</p> Name Type Description Default <code>elapsed_time</code> <code>float</code> <code>None</code> <code>grad_norm</code> <code>Tensor | None</code> <code>None</code> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>class OptimizerStepResult(NamedTuple):\n    elapsed_time: float\n    grad_norm: torch.Tensor | None\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/iteration_mixin/#optimus_dl.recipe.train.mixins.execution.iteration_mixin.TrainingIterationMixin","title":"<code>TrainingIterationMixin</code>","text":"<p>Mixin for executing a complete training step with gradient accumulation.</p> <p>Encapsulates the core training logic: 1.  Forward Pass: Runs the model and criterion, measuring time. 2.  Backward Pass: Scales gradients and backpropagates, handling     loss parallelism if applicable. 3.  Optimization: Unscales gradients, clips norms, and steps the optimizer. 4.  Logging: Records detailed performance metrics (forward/backward times,     grad norms, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>optimization_config</code> <code>OptimizationConfig</code> <p>Configuration for optimization (accumulation steps, clipping).</p> required <code>log_freq</code> <code>int</code> <p>Frequency of metric logging.</p> <code>1</code> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>class TrainingIterationMixin:\n    \"\"\"Mixin for executing a complete training step with gradient accumulation.\n\n    Encapsulates the core training logic:\n    1.  **Forward Pass**: Runs the model and criterion, measuring time.\n    2.  **Backward Pass**: Scales gradients and backpropagates, handling\n        loss parallelism if applicable.\n    3.  **Optimization**: Unscales gradients, clips norms, and steps the optimizer.\n    4.  **Logging**: Records detailed performance metrics (forward/backward times,\n        grad norms, etc.).\n\n    Args:\n        optimization_config: Configuration for optimization (accumulation steps, clipping).\n        log_freq: Frequency of metric logging.\n    \"\"\"\n\n    def __init__(self, optimization_config: OptimizationConfig, log_freq: int = 1):\n        self.optimization_config = optimization_config\n        self.log_freq = log_freq\n\n    def log_memory_usage(self):\n        \"\"\"Log GPU memory usage statistics.\"\"\"\n        if torch.cuda.is_available():\n            log_summed(\"gpu_gb_allocated\", torch.cuda.memory_allocated() / (1024**3))\n            log_summed(\"gpu_gb_used\", torch.cuda.max_memory_allocated() / (1024**3))\n\n    def execute_forward_pass(\n        self,\n        model: BaseModel,\n        criterion: BaseCriterion,\n        batch: Any,\n        amp_ctx: Any,\n        requested_protocols: set[str] | None = None,\n    ) -&gt; ForwardPassResult:\n        \"\"\"Run the forward pass inside an AMP context.\n\n        Args:\n            model: The model to run.\n            criterion: The loss function.\n            batch: The input data.\n            amp_ctx: The autocast context manager.\n            requested_protocols: Protocols requested by the metrics system.\n\n        Returns:\n            ForwardPassResult with the computed loss, exposed protocols, and execution time.\n        \"\"\"\n        with amp_ctx:\n            elapsed_forward, (loss, exposed) = measured_lambda(\n                lambda: criterion(model, batch, requested_protocols=requested_protocols)\n            )\n        return ForwardPassResult(\n            loss=loss, exposed_protocols=exposed, elapsed_time=elapsed_forward\n        )\n\n    def execute_backward_pass(self, loss: torch.Tensor, scaler: Any) -&gt; float:\n        \"\"\"Run the backward pass with gradient scaling.\n\n        Handles `loss_parallel` context if the loss is a DTensor.\n\n        Args:\n            loss: The computed loss tensor.\n            scaler: The gradient scaler.\n\n        Returns:\n            Execution time in milliseconds.\n        \"\"\"\n\n        def backward():\n            with loss_parallel() if isinstance(loss, DTensor) else nullcontext():\n                scaler.scale(loss).backward()\n\n        elapsed_backward, _ = measured_lambda(backward)\n        return elapsed_backward\n\n    def execute_optimizer_step(\n        self,\n        optimizer: Optimizer,\n        model: BaseModel,\n        scaler: Any,\n        clip_grad_norm: float | None = None,\n    ) -&gt; OptimizerStepResult:\n        \"\"\"Perform the optimization step.\n\n        Includes gradient unscaling, optional gradient clipping, and the\n        optimizer step itself. Updates the scaler state afterwards.\n\n        Args:\n            optimizer: The optimizer.\n            model: The model (needed for clipping gradients).\n            scaler: The gradient scaler.\n            clip_grad_norm: Maximum norm for gradient clipping.\n\n        Returns:\n            OptimizerStepResult with execution time and the computed gradient norm.\n        \"\"\"\n        scaler.unscale_(optimizer)\n\n        grad_norm = None\n        if clip_grad_norm is not None:\n            from torch.distributed.tensor.experimental import implicit_replication\n\n            with implicit_replication():\n                grad_norm = torch.nn.utils.clip_grad_norm_(\n                    model.parameters(), max_norm=clip_grad_norm\n                )\n\n        elapsed, _ = measured_lambda(lambda: scaler.step(optimizer))\n        scaler.update()\n\n        if scaler.is_enabled():\n            log_averaged(\"grad_scale\", scaler.get_scale())\n\n        return OptimizerStepResult(elapsed_time=elapsed, grad_norm=grad_norm)\n\n    def log_batch_metrics(\n        self,\n        elapsed_batch_get: float,\n        elapsed_forward: float,\n        elapsed_backward: float,\n        acc_steps: int,\n    ) -&gt; None:\n        \"\"\"Log timing metrics for data loading and forward/backward passes.\"\"\"\n        weight = 1 / acc_steps\n\n        log_averaged(\n            \"perf/batch_get\",\n            value=elapsed_batch_get,\n            weight=weight,\n            priority=999,\n        )\n        log_averaged(\n            \"perf/forward\",\n            value=elapsed_forward,\n            weight=weight,\n            priority=1000,\n        )\n        log_averaged(\n            \"perf/backward\",\n            value=elapsed_backward,\n            weight=weight,\n            priority=1001,\n        )\n\n    def log_optimizer_metrics(\n        self,\n        elapsed_optimizer: float,\n        grad_norm: torch.Tensor | None,\n        lr_scheduler: Any | None,\n        optimizer: Optimizer,\n    ) -&gt; None:\n        \"\"\"Log optimizer performance, gradient norms, and learning rates.\"\"\"\n        log_averaged(\"perf/optimizer\", value=elapsed_optimizer, priority=1002)\n\n        # Log gradient norm if clipping was performed\n        if grad_norm is not None:\n            log_averaged(\n                \"grad_norm\",\n                lambda: (float(grad_norm) if grad_norm is not None else 0.0),\n            )\n\n        # Learning rate (cheap but we only need it periodically)\n        if lr_scheduler is not None:\n            log_averaged(\"learning_rate\", lambda: lr_scheduler.get_last_lr()[0])\n        else:\n            log_averaged(\"learning_rate\", lambda: optimizer.param_groups[0][\"lr\"])\n\n    def run_training_iteration(\n        self,\n        model: BaseModel,\n        optimizer: Optimizer,\n        criterion: BaseCriterion,\n        train_data_iter: Iterator,\n        training_context: dict[str, Any],\n        lr_scheduler: Any | None = None,\n        metric_engine: Any | None = None,\n    ) -&gt; None:\n        \"\"\"Execute one full training iteration, including gradient accumulation.\n\n        This is the main driver for a training step. It loops `acc_steps` times\n        to accumulate gradients before performing a single optimizer update.\n\n        Args:\n            model: The model to train.\n            optimizer: The optimizer.\n            criterion: The loss function.\n            train_data_iter: Iterator yielding training batches.\n            training_context: Dict with scaler, amp_ctx, etc.\n            lr_scheduler: Optional learning rate scheduler.\n            metric_engine: Optional MetricEngine for training metrics.\n        \"\"\"\n        with meters_group(\"train\", log_freq=self.log_freq) as should_log:\n            optimizer.zero_grad()\n            model.train()\n\n            requested_protocols = None\n            if metric_engine and should_log:\n                requested_protocols = metric_engine.required_external_protocols\n\n            # Gradient accumulation loop\n            for microbatch_idx in range(self.optimization_config.acc_steps):\n                is_last_microbatch = (\n                    microbatch_idx == self.optimization_config.acc_steps - 1\n                )\n\n                try:\n                    elapsed_batch_get, batch = measured_next(train_data_iter)\n                except StopIteration:\n                    logger.error(\"Training data iterator exhausted unexpectedly\")\n                    break\n                except Exception as e:\n                    logger.error(f\"Error getting batch: {e}\")\n                    continue\n\n                with self.accumulation_context(model, is_last_microbatch):\n                    forward_result = self.execute_forward_pass(\n                        model,\n                        criterion,\n                        batch,\n                        training_context[\"amp_ctx\"],\n                        requested_protocols=requested_protocols,\n                    )\n                    loss = forward_result.loss / self.optimization_config.acc_steps\n\n                    if metric_engine and should_log:\n                        # Pass computed data (loss, logits, etc.) to avoid redundant work in engine\n                        computed_data = forward_result.exposed_protocols.copy()\n                        computed_data[\"loss\"] = forward_result.loss\n                        metric_engine.update(\n                            data=dict(model=model, batch=batch),\n                            computed_data=computed_data,\n                        )\n\n                    elapsed_backward = self.execute_backward_pass(\n                        loss, training_context[\"scaler\"]\n                    )\n\n                # Log performance metrics using the training metrics mixin\n                self.log_batch_metrics(\n                    elapsed_batch_get,\n                    forward_result.elapsed_time,\n                    elapsed_backward,\n                    self.optimization_config.acc_steps,\n                )\n\n            # Optimizer step\n            optimizer_result = self.execute_optimizer_step(\n                optimizer,\n                model,\n                training_context[\"scaler\"],\n                self.optimization_config.clip_grad_norm,\n            )\n\n            # Log optimizer metrics\n            self.log_optimizer_metrics(\n                optimizer_result.elapsed_time,\n                optimizer_result.grad_norm,\n                lr_scheduler,\n                optimizer,\n            )\n            self.log_memory_usage()\n            optimizer.zero_grad()\n\n            if lr_scheduler is not None:\n                lr_scheduler.step()\n\n    def accumulation_context(self, model, is_last_microbatch):\n        \"\"\"Get the appropriate context manager for gradient accumulation.\n\n        For FSDP/DDP models, this handles synchronization (e.g., disabling\n        all-reduce during accumulation steps).\n        \"\"\"\n        if hasattr(model, \"accumulation_context\"):\n            ctx = model.accumulation_context(is_last_microbatch=is_last_microbatch)\n            if not is_last_microbatch:\n                warn_once(logger, \"Using accumulation context\")\n            return ctx\n        else:\n            warn_once(logger, \"Model does not support accumulation context, skipping\")\n            return nullcontext()\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/iteration_mixin/#optimus_dl.recipe.train.mixins.execution.iteration_mixin.TrainingIterationMixin.accumulation_context","title":"<code>accumulation_context(model, is_last_microbatch)</code>","text":"<p>Get the appropriate context manager for gradient accumulation.</p> <p>For FSDP/DDP models, this handles synchronization (e.g., disabling all-reduce during accumulation steps).</p> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>def accumulation_context(self, model, is_last_microbatch):\n    \"\"\"Get the appropriate context manager for gradient accumulation.\n\n    For FSDP/DDP models, this handles synchronization (e.g., disabling\n    all-reduce during accumulation steps).\n    \"\"\"\n    if hasattr(model, \"accumulation_context\"):\n        ctx = model.accumulation_context(is_last_microbatch=is_last_microbatch)\n        if not is_last_microbatch:\n            warn_once(logger, \"Using accumulation context\")\n        return ctx\n    else:\n        warn_once(logger, \"Model does not support accumulation context, skipping\")\n        return nullcontext()\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/iteration_mixin/#optimus_dl.recipe.train.mixins.execution.iteration_mixin.TrainingIterationMixin.execute_backward_pass","title":"<code>execute_backward_pass(loss, scaler)</code>","text":"<p>Run the backward pass with gradient scaling.</p> <p>Handles <code>loss_parallel</code> context if the loss is a DTensor.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Tensor</code> <p>The computed loss tensor.</p> required <code>scaler</code> <code>Any</code> <p>The gradient scaler.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Execution time in milliseconds.</p> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>def execute_backward_pass(self, loss: torch.Tensor, scaler: Any) -&gt; float:\n    \"\"\"Run the backward pass with gradient scaling.\n\n    Handles `loss_parallel` context if the loss is a DTensor.\n\n    Args:\n        loss: The computed loss tensor.\n        scaler: The gradient scaler.\n\n    Returns:\n        Execution time in milliseconds.\n    \"\"\"\n\n    def backward():\n        with loss_parallel() if isinstance(loss, DTensor) else nullcontext():\n            scaler.scale(loss).backward()\n\n    elapsed_backward, _ = measured_lambda(backward)\n    return elapsed_backward\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/iteration_mixin/#optimus_dl.recipe.train.mixins.execution.iteration_mixin.TrainingIterationMixin.execute_forward_pass","title":"<code>execute_forward_pass(model, criterion, batch, amp_ctx, requested_protocols=None)</code>","text":"<p>Run the forward pass inside an AMP context.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel</code> <p>The model to run.</p> required <code>criterion</code> <code>BaseCriterion</code> <p>The loss function.</p> required <code>batch</code> <code>Any</code> <p>The input data.</p> required <code>amp_ctx</code> <code>Any</code> <p>The autocast context manager.</p> required <code>requested_protocols</code> <code>set[str] | None</code> <p>Protocols requested by the metrics system.</p> <code>None</code> <p>Returns:</p> Type Description <code>ForwardPassResult</code> <p>ForwardPassResult with the computed loss, exposed protocols, and execution time.</p> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>def execute_forward_pass(\n    self,\n    model: BaseModel,\n    criterion: BaseCriterion,\n    batch: Any,\n    amp_ctx: Any,\n    requested_protocols: set[str] | None = None,\n) -&gt; ForwardPassResult:\n    \"\"\"Run the forward pass inside an AMP context.\n\n    Args:\n        model: The model to run.\n        criterion: The loss function.\n        batch: The input data.\n        amp_ctx: The autocast context manager.\n        requested_protocols: Protocols requested by the metrics system.\n\n    Returns:\n        ForwardPassResult with the computed loss, exposed protocols, and execution time.\n    \"\"\"\n    with amp_ctx:\n        elapsed_forward, (loss, exposed) = measured_lambda(\n            lambda: criterion(model, batch, requested_protocols=requested_protocols)\n        )\n    return ForwardPassResult(\n        loss=loss, exposed_protocols=exposed, elapsed_time=elapsed_forward\n    )\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/iteration_mixin/#optimus_dl.recipe.train.mixins.execution.iteration_mixin.TrainingIterationMixin.execute_optimizer_step","title":"<code>execute_optimizer_step(optimizer, model, scaler, clip_grad_norm=None)</code>","text":"<p>Perform the optimization step.</p> <p>Includes gradient unscaling, optional gradient clipping, and the optimizer step itself. Updates the scaler state afterwards.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer.</p> required <code>model</code> <code>BaseModel</code> <p>The model (needed for clipping gradients).</p> required <code>scaler</code> <code>Any</code> <p>The gradient scaler.</p> required <code>clip_grad_norm</code> <code>float | None</code> <p>Maximum norm for gradient clipping.</p> <code>None</code> <p>Returns:</p> Type Description <code>OptimizerStepResult</code> <p>OptimizerStepResult with execution time and the computed gradient norm.</p> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>def execute_optimizer_step(\n    self,\n    optimizer: Optimizer,\n    model: BaseModel,\n    scaler: Any,\n    clip_grad_norm: float | None = None,\n) -&gt; OptimizerStepResult:\n    \"\"\"Perform the optimization step.\n\n    Includes gradient unscaling, optional gradient clipping, and the\n    optimizer step itself. Updates the scaler state afterwards.\n\n    Args:\n        optimizer: The optimizer.\n        model: The model (needed for clipping gradients).\n        scaler: The gradient scaler.\n        clip_grad_norm: Maximum norm for gradient clipping.\n\n    Returns:\n        OptimizerStepResult with execution time and the computed gradient norm.\n    \"\"\"\n    scaler.unscale_(optimizer)\n\n    grad_norm = None\n    if clip_grad_norm is not None:\n        from torch.distributed.tensor.experimental import implicit_replication\n\n        with implicit_replication():\n            grad_norm = torch.nn.utils.clip_grad_norm_(\n                model.parameters(), max_norm=clip_grad_norm\n            )\n\n    elapsed, _ = measured_lambda(lambda: scaler.step(optimizer))\n    scaler.update()\n\n    if scaler.is_enabled():\n        log_averaged(\"grad_scale\", scaler.get_scale())\n\n    return OptimizerStepResult(elapsed_time=elapsed, grad_norm=grad_norm)\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/iteration_mixin/#optimus_dl.recipe.train.mixins.execution.iteration_mixin.TrainingIterationMixin.log_batch_metrics","title":"<code>log_batch_metrics(elapsed_batch_get, elapsed_forward, elapsed_backward, acc_steps)</code>","text":"<p>Log timing metrics for data loading and forward/backward passes.</p> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>def log_batch_metrics(\n    self,\n    elapsed_batch_get: float,\n    elapsed_forward: float,\n    elapsed_backward: float,\n    acc_steps: int,\n) -&gt; None:\n    \"\"\"Log timing metrics for data loading and forward/backward passes.\"\"\"\n    weight = 1 / acc_steps\n\n    log_averaged(\n        \"perf/batch_get\",\n        value=elapsed_batch_get,\n        weight=weight,\n        priority=999,\n    )\n    log_averaged(\n        \"perf/forward\",\n        value=elapsed_forward,\n        weight=weight,\n        priority=1000,\n    )\n    log_averaged(\n        \"perf/backward\",\n        value=elapsed_backward,\n        weight=weight,\n        priority=1001,\n    )\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/iteration_mixin/#optimus_dl.recipe.train.mixins.execution.iteration_mixin.TrainingIterationMixin.log_memory_usage","title":"<code>log_memory_usage()</code>","text":"<p>Log GPU memory usage statistics.</p> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>def log_memory_usage(self):\n    \"\"\"Log GPU memory usage statistics.\"\"\"\n    if torch.cuda.is_available():\n        log_summed(\"gpu_gb_allocated\", torch.cuda.memory_allocated() / (1024**3))\n        log_summed(\"gpu_gb_used\", torch.cuda.max_memory_allocated() / (1024**3))\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/iteration_mixin/#optimus_dl.recipe.train.mixins.execution.iteration_mixin.TrainingIterationMixin.log_optimizer_metrics","title":"<code>log_optimizer_metrics(elapsed_optimizer, grad_norm, lr_scheduler, optimizer)</code>","text":"<p>Log optimizer performance, gradient norms, and learning rates.</p> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>def log_optimizer_metrics(\n    self,\n    elapsed_optimizer: float,\n    grad_norm: torch.Tensor | None,\n    lr_scheduler: Any | None,\n    optimizer: Optimizer,\n) -&gt; None:\n    \"\"\"Log optimizer performance, gradient norms, and learning rates.\"\"\"\n    log_averaged(\"perf/optimizer\", value=elapsed_optimizer, priority=1002)\n\n    # Log gradient norm if clipping was performed\n    if grad_norm is not None:\n        log_averaged(\n            \"grad_norm\",\n            lambda: (float(grad_norm) if grad_norm is not None else 0.0),\n        )\n\n    # Learning rate (cheap but we only need it periodically)\n    if lr_scheduler is not None:\n        log_averaged(\"learning_rate\", lambda: lr_scheduler.get_last_lr()[0])\n    else:\n        log_averaged(\"learning_rate\", lambda: optimizer.param_groups[0][\"lr\"])\n</code></pre>"},{"location":"reference/recipe/train/mixins/execution/iteration_mixin/#optimus_dl.recipe.train.mixins.execution.iteration_mixin.TrainingIterationMixin.run_training_iteration","title":"<code>run_training_iteration(model, optimizer, criterion, train_data_iter, training_context, lr_scheduler=None, metric_engine=None)</code>","text":"<p>Execute one full training iteration, including gradient accumulation.</p> <p>This is the main driver for a training step. It loops <code>acc_steps</code> times to accumulate gradients before performing a single optimizer update.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel</code> <p>The model to train.</p> required <code>optimizer</code> <code>Optimizer</code> <p>The optimizer.</p> required <code>criterion</code> <code>BaseCriterion</code> <p>The loss function.</p> required <code>train_data_iter</code> <code>Iterator</code> <p>Iterator yielding training batches.</p> required <code>training_context</code> <code>dict[str, Any]</code> <p>Dict with scaler, amp_ctx, etc.</p> required <code>lr_scheduler</code> <code>Any | None</code> <p>Optional learning rate scheduler.</p> <code>None</code> <code>metric_engine</code> <code>Any | None</code> <p>Optional MetricEngine for training metrics.</p> <code>None</code> Source code in <code>optimus_dl/recipe/train/mixins/execution/iteration_mixin.py</code> <pre><code>def run_training_iteration(\n    self,\n    model: BaseModel,\n    optimizer: Optimizer,\n    criterion: BaseCriterion,\n    train_data_iter: Iterator,\n    training_context: dict[str, Any],\n    lr_scheduler: Any | None = None,\n    metric_engine: Any | None = None,\n) -&gt; None:\n    \"\"\"Execute one full training iteration, including gradient accumulation.\n\n    This is the main driver for a training step. It loops `acc_steps` times\n    to accumulate gradients before performing a single optimizer update.\n\n    Args:\n        model: The model to train.\n        optimizer: The optimizer.\n        criterion: The loss function.\n        train_data_iter: Iterator yielding training batches.\n        training_context: Dict with scaler, amp_ctx, etc.\n        lr_scheduler: Optional learning rate scheduler.\n        metric_engine: Optional MetricEngine for training metrics.\n    \"\"\"\n    with meters_group(\"train\", log_freq=self.log_freq) as should_log:\n        optimizer.zero_grad()\n        model.train()\n\n        requested_protocols = None\n        if metric_engine and should_log:\n            requested_protocols = metric_engine.required_external_protocols\n\n        # Gradient accumulation loop\n        for microbatch_idx in range(self.optimization_config.acc_steps):\n            is_last_microbatch = (\n                microbatch_idx == self.optimization_config.acc_steps - 1\n            )\n\n            try:\n                elapsed_batch_get, batch = measured_next(train_data_iter)\n            except StopIteration:\n                logger.error(\"Training data iterator exhausted unexpectedly\")\n                break\n            except Exception as e:\n                logger.error(f\"Error getting batch: {e}\")\n                continue\n\n            with self.accumulation_context(model, is_last_microbatch):\n                forward_result = self.execute_forward_pass(\n                    model,\n                    criterion,\n                    batch,\n                    training_context[\"amp_ctx\"],\n                    requested_protocols=requested_protocols,\n                )\n                loss = forward_result.loss / self.optimization_config.acc_steps\n\n                if metric_engine and should_log:\n                    # Pass computed data (loss, logits, etc.) to avoid redundant work in engine\n                    computed_data = forward_result.exposed_protocols.copy()\n                    computed_data[\"loss\"] = forward_result.loss\n                    metric_engine.update(\n                        data=dict(model=model, batch=batch),\n                        computed_data=computed_data,\n                    )\n\n                elapsed_backward = self.execute_backward_pass(\n                    loss, training_context[\"scaler\"]\n                )\n\n            # Log performance metrics using the training metrics mixin\n            self.log_batch_metrics(\n                elapsed_batch_get,\n                forward_result.elapsed_time,\n                elapsed_backward,\n                self.optimization_config.acc_steps,\n            )\n\n        # Optimizer step\n        optimizer_result = self.execute_optimizer_step(\n            optimizer,\n            model,\n            training_context[\"scaler\"],\n            self.optimization_config.clip_grad_norm,\n        )\n\n        # Log optimizer metrics\n        self.log_optimizer_metrics(\n            optimizer_result.elapsed_time,\n            optimizer_result.grad_norm,\n            lr_scheduler,\n            optimizer,\n        )\n        self.log_memory_usage()\n        optimizer.zero_grad()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/","title":"Index","text":""},{"location":"reference/recipe/train/mixins/managers/#optimus_dl.recipe.train.mixins.managers","title":"<code>optimus_dl.recipe.train.mixins.managers</code>","text":""},{"location":"reference/recipe/train/mixins/managers/#optimus_dl.recipe.train.mixins.managers.Evaluator","title":"<code>Evaluator</code>","text":"<p>Manager for running periodic evaluations during training.</p> <p>Handles iterating over validation datasets, computing loss and other metrics, and aggregating results across distributed ranks.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>EvaluatorConfig</code> <p>Evaluator configuration.</p> required <code>eval_freq</code> <code>int</code> <p>Frequency of evaluation runs (in iterations).</p> <code>0</code> <code>eval_iterations</code> <code>int | None</code> <p>Max number of batches to process per evaluation dataset. If None, processes the entire dataset.</p> <code>None</code> Source code in <code>optimus_dl/recipe/train/mixins/managers/evaluation_manager.py</code> <pre><code>class Evaluator:\n    \"\"\"Manager for running periodic evaluations during training.\n\n    Handles iterating over validation datasets, computing loss and other metrics,\n    and aggregating results across distributed ranks.\n\n    Args:\n        cfg: Evaluator configuration.\n        eval_freq: Frequency of evaluation runs (in iterations).\n        eval_iterations: Max number of batches to process per evaluation dataset.\n            If None, processes the entire dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: EvaluatorConfig,\n        eval_freq: int = 0,\n        eval_iterations: int | None = None,\n        **kwargs: Any,\n    ):\n        self.cfg = cfg\n        self.eval_freq = eval_freq\n        self.eval_iterations = eval_iterations\n\n    def run_evaluation_if_needed(\n        self,\n        iteration: int,\n        model: BaseModel,\n        criterion: BaseCriterion,\n        eval_data: dict[str, Any],\n        collective: Any = None,\n        all_metrics_configs: dict[str, list[dict]] | None = None,\n    ) -&gt; None | dict:\n        \"\"\"Run evaluation if the current iteration matches the frequency.\n\n        Args:\n            iteration: Current training step.\n            model: The model to evaluate.\n            criterion: The loss function.\n            eval_data: Dictionary mapping dataset names to dataloaders.\n            collective: Distributed collective for metric aggregation.\n            all_metrics_configs: Root metrics configuration from TrainConfig.\n\n        Returns:\n            Dictionary of computed metrics if evaluation ran, else None.\n        \"\"\"\n        if self.eval_freq &lt;= 0 or iteration % self.eval_freq != 0:\n            return None\n\n        try:\n            return self.run_evaluation(\n                model=model,\n                criterion=criterion,\n                eval_data_dict=eval_data,\n                max_iterations=self.eval_iterations,\n                collective=collective,\n                all_metrics_configs=all_metrics_configs,\n            )\n        except Exception as e:\n            logger.error(f\"Evaluation failed: {e}\")\n            return None\n\n    def run_evaluation(\n        self,\n        model: BaseModel,\n        criterion: BaseCriterion,\n        eval_data_dict: dict,\n        max_iterations: int | None = None,\n        collective: Any = None,\n        all_metrics_configs: dict[str, list[dict]] | None = None,\n        metrics_prefix: str = \"eval\",\n        show_progress: bool = False,\n    ):\n        \"\"\"Execute the evaluation loop for all provided datasets.\n\n        Sets the model to eval mode, disables gradients, and runs the forward pass\n        for each batch. Metrics are aggregated globally.\n\n        Args:\n            model: Model to evaluate.\n            criterion: Loss function.\n            eval_data_dict: Dictionary of {name: dataloader/DataPipeline}.\n            max_iterations: Limit on number of batches.\n            collective: Distributed collective.\n            all_metrics_configs: Root metrics configuration mapping dataset names to configs.\n            metrics_prefix: Prefix for metric groups (e.g., \"eval\" or \"metrics\").\n            show_progress: Whether to show a progress bar.\n\n        Returns:\n            Nested dictionary of results: {dataset_name: {metric_name: value}}.\n        \"\"\"\n        model.eval()\n        total_metrics = {}\n        all_metrics_configs = all_metrics_configs or {}\n\n        for eval_name, eval_data in eval_data_dict.items():\n            logger.info(f\"Running evaluation {eval_name}\")\n\n            # Handle both raw dataloader and DataPipeline object\n            dataloader = getattr(eval_data, \"dataloader\", eval_data)\n\n            engine = None\n            requested_protocols = None\n            dataset_metrics = all_metrics_configs.get(eval_name)\n            if dataset_metrics:\n                from optimus_dl.modules.metrics.engine import MetricEngine\n\n                engine = MetricEngine(f\"{metrics_prefix}/{eval_name}\", dataset_metrics)\n                requested_protocols = engine.required_external_protocols\n\n            with (\n                torch.no_grad(),\n                meters_group(\n                    f\"{metrics_prefix}/{eval_name}\", log_freq=1, force_recreate=True\n                ),\n            ):\n                log_event_start(\"perf/total_run\")\n                start_time = time.perf_counter()\n\n                eval_iter = iter(dataloader)\n                iterations = 0\n\n                pbar = None\n                if show_progress:\n                    pbar = tqdm(\n                        desc=f\"Eval {eval_name}\",\n                        disable=collective is not None\n                        and not collective.is_local_master,\n                        unit=\"batch\",\n                        total=max_iterations,\n                    )\n\n                try:\n                    while max_iterations is None or iterations &lt; max_iterations:\n                        log_event_occurence(\"perf/full_iteration\")\n\n                        elapsed_batch_get, batch = measured_next(eval_iter)\n                        loss, exposed = criterion(\n                            model, batch, requested_protocols=requested_protocols\n                        )\n\n                        if engine:\n                            computed_data = exposed.copy()\n                            computed_data[\"loss\"] = loss\n                            engine.update(\n                                data=dict(model=model, batch=batch),\n                                computed_data=computed_data,\n                            )\n\n                        log_summed(\"num_batches\", lambda: 1)\n                        log_averaged(\n                            \"perf/batch_get\",\n                            elapsed_batch_get,\n                        )\n\n                        iterations += 1\n                        if pbar:\n                            pbar.update(1)\n\n                        # Step metrics for each evaluation iteration\n                        step_meters(f\"{metrics_prefix}/{eval_name}\")\n\n                except StopIteration:\n                    pass\n                finally:\n                    if pbar:\n                        pbar.close()\n\n                total_time = time.perf_counter() - start_time\n                log_event_end(\"perf/total_run\")\n\n            eval_metrics = compute_meters(\n                f\"{metrics_prefix}/{eval_name}\",\n                aggregate=True,\n                collective=collective,\n            )\n\n            if engine:\n                eval_metrics = engine.compute(eval_metrics)\n\n            # Add basic performance stats\n            eval_metrics[\"perf/total_run_ms\"] = total_time * 1000\n            if iterations &gt; 0:\n                eval_metrics[\"perf/ms_per_batch\"] = (total_time / iterations) * 1000\n\n            logger.info(f\"Finished eval {eval_name}: {eval_metrics}\")\n            total_metrics[f\"{metrics_prefix}/{eval_name}\"] = eval_metrics\n        return total_metrics\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/#optimus_dl.recipe.train.mixins.managers.Evaluator.run_evaluation","title":"<code>run_evaluation(model, criterion, eval_data_dict, max_iterations=None, collective=None, all_metrics_configs=None, metrics_prefix='eval', show_progress=False)</code>","text":"<p>Execute the evaluation loop for all provided datasets.</p> <p>Sets the model to eval mode, disables gradients, and runs the forward pass for each batch. Metrics are aggregated globally.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel</code> <p>Model to evaluate.</p> required <code>criterion</code> <code>BaseCriterion</code> <p>Loss function.</p> required <code>eval_data_dict</code> <code>dict</code> <p>Dictionary of {name: dataloader/DataPipeline}.</p> required <code>max_iterations</code> <code>int | None</code> <p>Limit on number of batches.</p> <code>None</code> <code>collective</code> <code>Any</code> <p>Distributed collective.</p> <code>None</code> <code>all_metrics_configs</code> <code>dict[str, list[dict]] | None</code> <p>Root metrics configuration mapping dataset names to configs.</p> <code>None</code> <code>metrics_prefix</code> <code>str</code> <p>Prefix for metric groups (e.g., \"eval\" or \"metrics\").</p> <code>'eval'</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>False</code> <p>Returns:</p> Type Description <p>Nested dictionary of results: {dataset_name: {metric_name: value}}.</p> Source code in <code>optimus_dl/recipe/train/mixins/managers/evaluation_manager.py</code> <pre><code>def run_evaluation(\n    self,\n    model: BaseModel,\n    criterion: BaseCriterion,\n    eval_data_dict: dict,\n    max_iterations: int | None = None,\n    collective: Any = None,\n    all_metrics_configs: dict[str, list[dict]] | None = None,\n    metrics_prefix: str = \"eval\",\n    show_progress: bool = False,\n):\n    \"\"\"Execute the evaluation loop for all provided datasets.\n\n    Sets the model to eval mode, disables gradients, and runs the forward pass\n    for each batch. Metrics are aggregated globally.\n\n    Args:\n        model: Model to evaluate.\n        criterion: Loss function.\n        eval_data_dict: Dictionary of {name: dataloader/DataPipeline}.\n        max_iterations: Limit on number of batches.\n        collective: Distributed collective.\n        all_metrics_configs: Root metrics configuration mapping dataset names to configs.\n        metrics_prefix: Prefix for metric groups (e.g., \"eval\" or \"metrics\").\n        show_progress: Whether to show a progress bar.\n\n    Returns:\n        Nested dictionary of results: {dataset_name: {metric_name: value}}.\n    \"\"\"\n    model.eval()\n    total_metrics = {}\n    all_metrics_configs = all_metrics_configs or {}\n\n    for eval_name, eval_data in eval_data_dict.items():\n        logger.info(f\"Running evaluation {eval_name}\")\n\n        # Handle both raw dataloader and DataPipeline object\n        dataloader = getattr(eval_data, \"dataloader\", eval_data)\n\n        engine = None\n        requested_protocols = None\n        dataset_metrics = all_metrics_configs.get(eval_name)\n        if dataset_metrics:\n            from optimus_dl.modules.metrics.engine import MetricEngine\n\n            engine = MetricEngine(f\"{metrics_prefix}/{eval_name}\", dataset_metrics)\n            requested_protocols = engine.required_external_protocols\n\n        with (\n            torch.no_grad(),\n            meters_group(\n                f\"{metrics_prefix}/{eval_name}\", log_freq=1, force_recreate=True\n            ),\n        ):\n            log_event_start(\"perf/total_run\")\n            start_time = time.perf_counter()\n\n            eval_iter = iter(dataloader)\n            iterations = 0\n\n            pbar = None\n            if show_progress:\n                pbar = tqdm(\n                    desc=f\"Eval {eval_name}\",\n                    disable=collective is not None\n                    and not collective.is_local_master,\n                    unit=\"batch\",\n                    total=max_iterations,\n                )\n\n            try:\n                while max_iterations is None or iterations &lt; max_iterations:\n                    log_event_occurence(\"perf/full_iteration\")\n\n                    elapsed_batch_get, batch = measured_next(eval_iter)\n                    loss, exposed = criterion(\n                        model, batch, requested_protocols=requested_protocols\n                    )\n\n                    if engine:\n                        computed_data = exposed.copy()\n                        computed_data[\"loss\"] = loss\n                        engine.update(\n                            data=dict(model=model, batch=batch),\n                            computed_data=computed_data,\n                        )\n\n                    log_summed(\"num_batches\", lambda: 1)\n                    log_averaged(\n                        \"perf/batch_get\",\n                        elapsed_batch_get,\n                    )\n\n                    iterations += 1\n                    if pbar:\n                        pbar.update(1)\n\n                    # Step metrics for each evaluation iteration\n                    step_meters(f\"{metrics_prefix}/{eval_name}\")\n\n            except StopIteration:\n                pass\n            finally:\n                if pbar:\n                    pbar.close()\n\n            total_time = time.perf_counter() - start_time\n            log_event_end(\"perf/total_run\")\n\n        eval_metrics = compute_meters(\n            f\"{metrics_prefix}/{eval_name}\",\n            aggregate=True,\n            collective=collective,\n        )\n\n        if engine:\n            eval_metrics = engine.compute(eval_metrics)\n\n        # Add basic performance stats\n        eval_metrics[\"perf/total_run_ms\"] = total_time * 1000\n        if iterations &gt; 0:\n            eval_metrics[\"perf/ms_per_batch\"] = (total_time / iterations) * 1000\n\n        logger.info(f\"Finished eval {eval_name}: {eval_metrics}\")\n        total_metrics[f\"{metrics_prefix}/{eval_name}\"] = eval_metrics\n    return total_metrics\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/#optimus_dl.recipe.train.mixins.managers.Evaluator.run_evaluation_if_needed","title":"<code>run_evaluation_if_needed(iteration, model, criterion, eval_data, collective=None, all_metrics_configs=None)</code>","text":"<p>Run evaluation if the current iteration matches the frequency.</p> <p>Parameters:</p> Name Type Description Default <code>iteration</code> <code>int</code> <p>Current training step.</p> required <code>model</code> <code>BaseModel</code> <p>The model to evaluate.</p> required <code>criterion</code> <code>BaseCriterion</code> <p>The loss function.</p> required <code>eval_data</code> <code>dict[str, Any]</code> <p>Dictionary mapping dataset names to dataloaders.</p> required <code>collective</code> <code>Any</code> <p>Distributed collective for metric aggregation.</p> <code>None</code> <code>all_metrics_configs</code> <code>dict[str, list[dict]] | None</code> <p>Root metrics configuration from TrainConfig.</p> <code>None</code> <p>Returns:</p> Type Description <code>None | dict</code> <p>Dictionary of computed metrics if evaluation ran, else None.</p> Source code in <code>optimus_dl/recipe/train/mixins/managers/evaluation_manager.py</code> <pre><code>def run_evaluation_if_needed(\n    self,\n    iteration: int,\n    model: BaseModel,\n    criterion: BaseCriterion,\n    eval_data: dict[str, Any],\n    collective: Any = None,\n    all_metrics_configs: dict[str, list[dict]] | None = None,\n) -&gt; None | dict:\n    \"\"\"Run evaluation if the current iteration matches the frequency.\n\n    Args:\n        iteration: Current training step.\n        model: The model to evaluate.\n        criterion: The loss function.\n        eval_data: Dictionary mapping dataset names to dataloaders.\n        collective: Distributed collective for metric aggregation.\n        all_metrics_configs: Root metrics configuration from TrainConfig.\n\n    Returns:\n        Dictionary of computed metrics if evaluation ran, else None.\n    \"\"\"\n    if self.eval_freq &lt;= 0 or iteration % self.eval_freq != 0:\n        return None\n\n    try:\n        return self.run_evaluation(\n            model=model,\n            criterion=criterion,\n            eval_data_dict=eval_data,\n            max_iterations=self.eval_iterations,\n            collective=collective,\n            all_metrics_configs=all_metrics_configs,\n        )\n    except Exception as e:\n        logger.error(f\"Evaluation failed: {e}\")\n        return None\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/#optimus_dl.recipe.train.mixins.managers.LoggerManager","title":"<code>LoggerManager</code>","text":"<p>Manager for multiple metrics loggers.</p> <p>This class instantiates and orchestrates a list of logging backends (e.g., JSONL, WandB). It provides a unified interface for setting up, logging to, and closing all configured loggers.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>LoggerManagerConfig</code> <p>Manager configuration.</p> required <code>loggers_config</code> <code>list[MetricsLoggerConfig] | None</code> <p>List of configurations for individual loggers.</p> required Source code in <code>optimus_dl/recipe/train/mixins/managers/logger_manager.py</code> <pre><code>class LoggerManager:\n    \"\"\"Manager for multiple metrics loggers.\n\n    This class instantiates and orchestrates a list of logging backends (e.g.,\n    JSONL, WandB). It provides a unified interface for setting up, logging to,\n    and closing all configured loggers.\n\n    Args:\n        cfg: Manager configuration.\n        loggers_config: List of configurations for individual loggers.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: LoggerManagerConfig,\n        loggers_config: list[MetricsLoggerConfig] | None,\n        **kwargs: Any,\n    ):\n        self.loggers_config = loggers_config\n        self.previous_state = {}\n        self.loggers: list[BaseMetricsLogger] | None = None\n\n    def build_loggers(self, **kwargs):\n        \"\"\"Instantiate all configured loggers.\n\n        Uses the registry to build logger instances. If previous state is available\n        (from a checkpoint), it is passed to the logger builders for resumption.\n\n        Returns:\n            List of active logger instances.\n        \"\"\"\n        if self.loggers_config is None:\n            logger.info(\"No loggers configuration found, metrics logging disabled\")\n            return\n        assert self.loggers is None, \"Loggers already built\"\n\n        loggers = []\n        for logger_config in self.loggers_config:\n            try:\n                logger_instance = build(\n                    \"metrics_logger\",\n                    logger_config,\n                    state_dict=self.previous_state.get(logger_config.id),\n                    **kwargs,\n                )\n                loggers.append(logger_instance)\n                logger.info(f\"Built logger: {logger_instance.__class__.__name__}\")\n            except Exception as e:\n                logger.error(f\"Failed to build logger from config {logger_config}: {e}\")\n                raise\n\n        self.loggers = loggers\n\n    def setup_loggers(self, experiment_name: str, full_config: dict):\n        \"\"\"Initialize all loggers with experiment context.\n\n        Args:\n            experiment_name: Name of the experiment.\n            full_config: Complete training configuration dictionary.\n        \"\"\"\n        for logger_instance in self.loggers or []:\n            try:\n                logger_instance.setup(experiment_name, full_config)\n            except Exception as e:\n                logger.error(\n                    f\"Failed to setup logger {logger_instance.__class__.__name__}: {e}\"\n                )\n\n    def log_metrics_to_loggers(self, metrics, step: int, group: str = \"train\"):\n        \"\"\"Dispatch metrics to all active loggers.\n\n        Args:\n            metrics: Dictionary of metric values.\n            step: Current iteration.\n            group: Metric group name.\n        \"\"\"\n        for logger_instance in self.loggers or []:\n            try:\n                logger_instance.log_metrics(metrics, step, group)\n            except Exception as e:\n                logger.error(\n                    f\"Failed to log metrics with {logger_instance.__class__.__name__}: {e}\"\n                )\n\n    def close_loggers(self):\n        \"\"\"Clean up all loggers.\"\"\"\n        for logger_instance in self.loggers or []:\n            try:\n                logger_instance.close()\n            except Exception as e:\n                logger.error(\n                    f\"Failed to close logger {logger_instance.__class__.__name__}: {e}\"\n                )\n\n    def state_dict(self):\n        \"\"\"Collect state from all loggers for checkpointing.\"\"\"\n        return {\n            logger_instance.cfg.id: logger_instance.state_dict()\n            for logger_instance in self.loggers or []\n        }\n\n    def load_state_dict(self, state_dict):\n        \"\"\"Load logger states from a checkpoint.\"\"\"\n        self.previous_state = state_dict\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/#optimus_dl.recipe.train.mixins.managers.LoggerManager.build_loggers","title":"<code>build_loggers(**kwargs)</code>","text":"<p>Instantiate all configured loggers.</p> <p>Uses the registry to build logger instances. If previous state is available (from a checkpoint), it is passed to the logger builders for resumption.</p> <p>Returns:</p> Type Description <p>List of active logger instances.</p> Source code in <code>optimus_dl/recipe/train/mixins/managers/logger_manager.py</code> <pre><code>def build_loggers(self, **kwargs):\n    \"\"\"Instantiate all configured loggers.\n\n    Uses the registry to build logger instances. If previous state is available\n    (from a checkpoint), it is passed to the logger builders for resumption.\n\n    Returns:\n        List of active logger instances.\n    \"\"\"\n    if self.loggers_config is None:\n        logger.info(\"No loggers configuration found, metrics logging disabled\")\n        return\n    assert self.loggers is None, \"Loggers already built\"\n\n    loggers = []\n    for logger_config in self.loggers_config:\n        try:\n            logger_instance = build(\n                \"metrics_logger\",\n                logger_config,\n                state_dict=self.previous_state.get(logger_config.id),\n                **kwargs,\n            )\n            loggers.append(logger_instance)\n            logger.info(f\"Built logger: {logger_instance.__class__.__name__}\")\n        except Exception as e:\n            logger.error(f\"Failed to build logger from config {logger_config}: {e}\")\n            raise\n\n    self.loggers = loggers\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/#optimus_dl.recipe.train.mixins.managers.LoggerManager.close_loggers","title":"<code>close_loggers()</code>","text":"<p>Clean up all loggers.</p> Source code in <code>optimus_dl/recipe/train/mixins/managers/logger_manager.py</code> <pre><code>def close_loggers(self):\n    \"\"\"Clean up all loggers.\"\"\"\n    for logger_instance in self.loggers or []:\n        try:\n            logger_instance.close()\n        except Exception as e:\n            logger.error(\n                f\"Failed to close logger {logger_instance.__class__.__name__}: {e}\"\n            )\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/#optimus_dl.recipe.train.mixins.managers.LoggerManager.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Load logger states from a checkpoint.</p> Source code in <code>optimus_dl/recipe/train/mixins/managers/logger_manager.py</code> <pre><code>def load_state_dict(self, state_dict):\n    \"\"\"Load logger states from a checkpoint.\"\"\"\n    self.previous_state = state_dict\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/#optimus_dl.recipe.train.mixins.managers.LoggerManager.log_metrics_to_loggers","title":"<code>log_metrics_to_loggers(metrics, step, group='train')</code>","text":"<p>Dispatch metrics to all active loggers.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <p>Dictionary of metric values.</p> required <code>step</code> <code>int</code> <p>Current iteration.</p> required <code>group</code> <code>str</code> <p>Metric group name.</p> <code>'train'</code> Source code in <code>optimus_dl/recipe/train/mixins/managers/logger_manager.py</code> <pre><code>def log_metrics_to_loggers(self, metrics, step: int, group: str = \"train\"):\n    \"\"\"Dispatch metrics to all active loggers.\n\n    Args:\n        metrics: Dictionary of metric values.\n        step: Current iteration.\n        group: Metric group name.\n    \"\"\"\n    for logger_instance in self.loggers or []:\n        try:\n            logger_instance.log_metrics(metrics, step, group)\n        except Exception as e:\n            logger.error(\n                f\"Failed to log metrics with {logger_instance.__class__.__name__}: {e}\"\n            )\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/#optimus_dl.recipe.train.mixins.managers.LoggerManager.setup_loggers","title":"<code>setup_loggers(experiment_name, full_config)</code>","text":"<p>Initialize all loggers with experiment context.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment.</p> required <code>full_config</code> <code>dict</code> <p>Complete training configuration dictionary.</p> required Source code in <code>optimus_dl/recipe/train/mixins/managers/logger_manager.py</code> <pre><code>def setup_loggers(self, experiment_name: str, full_config: dict):\n    \"\"\"Initialize all loggers with experiment context.\n\n    Args:\n        experiment_name: Name of the experiment.\n        full_config: Complete training configuration dictionary.\n    \"\"\"\n    for logger_instance in self.loggers or []:\n        try:\n            logger_instance.setup(experiment_name, full_config)\n        except Exception as e:\n            logger.error(\n                f\"Failed to setup logger {logger_instance.__class__.__name__}: {e}\"\n            )\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/#optimus_dl.recipe.train.mixins.managers.LoggerManager.state_dict","title":"<code>state_dict()</code>","text":"<p>Collect state from all loggers for checkpointing.</p> Source code in <code>optimus_dl/recipe/train/mixins/managers/logger_manager.py</code> <pre><code>def state_dict(self):\n    \"\"\"Collect state from all loggers for checkpointing.\"\"\"\n    return {\n        logger_instance.cfg.id: logger_instance.state_dict()\n        for logger_instance in self.loggers or []\n    }\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/#modules-and-sub-packages","title":"Modules and Sub-packages","text":"<ul> <li><code>evaluation_manager</code>: Evaluation mixin for evaluation functionality.</li> <li><code>logger_manager</code>: Logger mixin for handling metrics logging.</li> </ul>"},{"location":"reference/recipe/train/mixins/managers/evaluation_manager/","title":"evaluation_manager","text":""},{"location":"reference/recipe/train/mixins/managers/evaluation_manager/#optimus_dl.recipe.train.mixins.managers.evaluation_manager","title":"<code>optimus_dl.recipe.train.mixins.managers.evaluation_manager</code>","text":"<p>Evaluation mixin for evaluation functionality.</p>"},{"location":"reference/recipe/train/mixins/managers/evaluation_manager/#optimus_dl.recipe.train.mixins.managers.evaluation_manager.Evaluator","title":"<code>Evaluator</code>","text":"<p>Manager for running periodic evaluations during training.</p> <p>Handles iterating over validation datasets, computing loss and other metrics, and aggregating results across distributed ranks.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>EvaluatorConfig</code> <p>Evaluator configuration.</p> required <code>eval_freq</code> <code>int</code> <p>Frequency of evaluation runs (in iterations).</p> <code>0</code> <code>eval_iterations</code> <code>int | None</code> <p>Max number of batches to process per evaluation dataset. If None, processes the entire dataset.</p> <code>None</code> Source code in <code>optimus_dl/recipe/train/mixins/managers/evaluation_manager.py</code> <pre><code>class Evaluator:\n    \"\"\"Manager for running periodic evaluations during training.\n\n    Handles iterating over validation datasets, computing loss and other metrics,\n    and aggregating results across distributed ranks.\n\n    Args:\n        cfg: Evaluator configuration.\n        eval_freq: Frequency of evaluation runs (in iterations).\n        eval_iterations: Max number of batches to process per evaluation dataset.\n            If None, processes the entire dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: EvaluatorConfig,\n        eval_freq: int = 0,\n        eval_iterations: int | None = None,\n        **kwargs: Any,\n    ):\n        self.cfg = cfg\n        self.eval_freq = eval_freq\n        self.eval_iterations = eval_iterations\n\n    def run_evaluation_if_needed(\n        self,\n        iteration: int,\n        model: BaseModel,\n        criterion: BaseCriterion,\n        eval_data: dict[str, Any],\n        collective: Any = None,\n        all_metrics_configs: dict[str, list[dict]] | None = None,\n    ) -&gt; None | dict:\n        \"\"\"Run evaluation if the current iteration matches the frequency.\n\n        Args:\n            iteration: Current training step.\n            model: The model to evaluate.\n            criterion: The loss function.\n            eval_data: Dictionary mapping dataset names to dataloaders.\n            collective: Distributed collective for metric aggregation.\n            all_metrics_configs: Root metrics configuration from TrainConfig.\n\n        Returns:\n            Dictionary of computed metrics if evaluation ran, else None.\n        \"\"\"\n        if self.eval_freq &lt;= 0 or iteration % self.eval_freq != 0:\n            return None\n\n        try:\n            return self.run_evaluation(\n                model=model,\n                criterion=criterion,\n                eval_data_dict=eval_data,\n                max_iterations=self.eval_iterations,\n                collective=collective,\n                all_metrics_configs=all_metrics_configs,\n            )\n        except Exception as e:\n            logger.error(f\"Evaluation failed: {e}\")\n            return None\n\n    def run_evaluation(\n        self,\n        model: BaseModel,\n        criterion: BaseCriterion,\n        eval_data_dict: dict,\n        max_iterations: int | None = None,\n        collective: Any = None,\n        all_metrics_configs: dict[str, list[dict]] | None = None,\n        metrics_prefix: str = \"eval\",\n        show_progress: bool = False,\n    ):\n        \"\"\"Execute the evaluation loop for all provided datasets.\n\n        Sets the model to eval mode, disables gradients, and runs the forward pass\n        for each batch. Metrics are aggregated globally.\n\n        Args:\n            model: Model to evaluate.\n            criterion: Loss function.\n            eval_data_dict: Dictionary of {name: dataloader/DataPipeline}.\n            max_iterations: Limit on number of batches.\n            collective: Distributed collective.\n            all_metrics_configs: Root metrics configuration mapping dataset names to configs.\n            metrics_prefix: Prefix for metric groups (e.g., \"eval\" or \"metrics\").\n            show_progress: Whether to show a progress bar.\n\n        Returns:\n            Nested dictionary of results: {dataset_name: {metric_name: value}}.\n        \"\"\"\n        model.eval()\n        total_metrics = {}\n        all_metrics_configs = all_metrics_configs or {}\n\n        for eval_name, eval_data in eval_data_dict.items():\n            logger.info(f\"Running evaluation {eval_name}\")\n\n            # Handle both raw dataloader and DataPipeline object\n            dataloader = getattr(eval_data, \"dataloader\", eval_data)\n\n            engine = None\n            requested_protocols = None\n            dataset_metrics = all_metrics_configs.get(eval_name)\n            if dataset_metrics:\n                from optimus_dl.modules.metrics.engine import MetricEngine\n\n                engine = MetricEngine(f\"{metrics_prefix}/{eval_name}\", dataset_metrics)\n                requested_protocols = engine.required_external_protocols\n\n            with (\n                torch.no_grad(),\n                meters_group(\n                    f\"{metrics_prefix}/{eval_name}\", log_freq=1, force_recreate=True\n                ),\n            ):\n                log_event_start(\"perf/total_run\")\n                start_time = time.perf_counter()\n\n                eval_iter = iter(dataloader)\n                iterations = 0\n\n                pbar = None\n                if show_progress:\n                    pbar = tqdm(\n                        desc=f\"Eval {eval_name}\",\n                        disable=collective is not None\n                        and not collective.is_local_master,\n                        unit=\"batch\",\n                        total=max_iterations,\n                    )\n\n                try:\n                    while max_iterations is None or iterations &lt; max_iterations:\n                        log_event_occurence(\"perf/full_iteration\")\n\n                        elapsed_batch_get, batch = measured_next(eval_iter)\n                        loss, exposed = criterion(\n                            model, batch, requested_protocols=requested_protocols\n                        )\n\n                        if engine:\n                            computed_data = exposed.copy()\n                            computed_data[\"loss\"] = loss\n                            engine.update(\n                                data=dict(model=model, batch=batch),\n                                computed_data=computed_data,\n                            )\n\n                        log_summed(\"num_batches\", lambda: 1)\n                        log_averaged(\n                            \"perf/batch_get\",\n                            elapsed_batch_get,\n                        )\n\n                        iterations += 1\n                        if pbar:\n                            pbar.update(1)\n\n                        # Step metrics for each evaluation iteration\n                        step_meters(f\"{metrics_prefix}/{eval_name}\")\n\n                except StopIteration:\n                    pass\n                finally:\n                    if pbar:\n                        pbar.close()\n\n                total_time = time.perf_counter() - start_time\n                log_event_end(\"perf/total_run\")\n\n            eval_metrics = compute_meters(\n                f\"{metrics_prefix}/{eval_name}\",\n                aggregate=True,\n                collective=collective,\n            )\n\n            if engine:\n                eval_metrics = engine.compute(eval_metrics)\n\n            # Add basic performance stats\n            eval_metrics[\"perf/total_run_ms\"] = total_time * 1000\n            if iterations &gt; 0:\n                eval_metrics[\"perf/ms_per_batch\"] = (total_time / iterations) * 1000\n\n            logger.info(f\"Finished eval {eval_name}: {eval_metrics}\")\n            total_metrics[f\"{metrics_prefix}/{eval_name}\"] = eval_metrics\n        return total_metrics\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/evaluation_manager/#optimus_dl.recipe.train.mixins.managers.evaluation_manager.Evaluator.run_evaluation","title":"<code>run_evaluation(model, criterion, eval_data_dict, max_iterations=None, collective=None, all_metrics_configs=None, metrics_prefix='eval', show_progress=False)</code>","text":"<p>Execute the evaluation loop for all provided datasets.</p> <p>Sets the model to eval mode, disables gradients, and runs the forward pass for each batch. Metrics are aggregated globally.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel</code> <p>Model to evaluate.</p> required <code>criterion</code> <code>BaseCriterion</code> <p>Loss function.</p> required <code>eval_data_dict</code> <code>dict</code> <p>Dictionary of {name: dataloader/DataPipeline}.</p> required <code>max_iterations</code> <code>int | None</code> <p>Limit on number of batches.</p> <code>None</code> <code>collective</code> <code>Any</code> <p>Distributed collective.</p> <code>None</code> <code>all_metrics_configs</code> <code>dict[str, list[dict]] | None</code> <p>Root metrics configuration mapping dataset names to configs.</p> <code>None</code> <code>metrics_prefix</code> <code>str</code> <p>Prefix for metric groups (e.g., \"eval\" or \"metrics\").</p> <code>'eval'</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>False</code> <p>Returns:</p> Type Description <p>Nested dictionary of results: {dataset_name: {metric_name: value}}.</p> Source code in <code>optimus_dl/recipe/train/mixins/managers/evaluation_manager.py</code> <pre><code>def run_evaluation(\n    self,\n    model: BaseModel,\n    criterion: BaseCriterion,\n    eval_data_dict: dict,\n    max_iterations: int | None = None,\n    collective: Any = None,\n    all_metrics_configs: dict[str, list[dict]] | None = None,\n    metrics_prefix: str = \"eval\",\n    show_progress: bool = False,\n):\n    \"\"\"Execute the evaluation loop for all provided datasets.\n\n    Sets the model to eval mode, disables gradients, and runs the forward pass\n    for each batch. Metrics are aggregated globally.\n\n    Args:\n        model: Model to evaluate.\n        criterion: Loss function.\n        eval_data_dict: Dictionary of {name: dataloader/DataPipeline}.\n        max_iterations: Limit on number of batches.\n        collective: Distributed collective.\n        all_metrics_configs: Root metrics configuration mapping dataset names to configs.\n        metrics_prefix: Prefix for metric groups (e.g., \"eval\" or \"metrics\").\n        show_progress: Whether to show a progress bar.\n\n    Returns:\n        Nested dictionary of results: {dataset_name: {metric_name: value}}.\n    \"\"\"\n    model.eval()\n    total_metrics = {}\n    all_metrics_configs = all_metrics_configs or {}\n\n    for eval_name, eval_data in eval_data_dict.items():\n        logger.info(f\"Running evaluation {eval_name}\")\n\n        # Handle both raw dataloader and DataPipeline object\n        dataloader = getattr(eval_data, \"dataloader\", eval_data)\n\n        engine = None\n        requested_protocols = None\n        dataset_metrics = all_metrics_configs.get(eval_name)\n        if dataset_metrics:\n            from optimus_dl.modules.metrics.engine import MetricEngine\n\n            engine = MetricEngine(f\"{metrics_prefix}/{eval_name}\", dataset_metrics)\n            requested_protocols = engine.required_external_protocols\n\n        with (\n            torch.no_grad(),\n            meters_group(\n                f\"{metrics_prefix}/{eval_name}\", log_freq=1, force_recreate=True\n            ),\n        ):\n            log_event_start(\"perf/total_run\")\n            start_time = time.perf_counter()\n\n            eval_iter = iter(dataloader)\n            iterations = 0\n\n            pbar = None\n            if show_progress:\n                pbar = tqdm(\n                    desc=f\"Eval {eval_name}\",\n                    disable=collective is not None\n                    and not collective.is_local_master,\n                    unit=\"batch\",\n                    total=max_iterations,\n                )\n\n            try:\n                while max_iterations is None or iterations &lt; max_iterations:\n                    log_event_occurence(\"perf/full_iteration\")\n\n                    elapsed_batch_get, batch = measured_next(eval_iter)\n                    loss, exposed = criterion(\n                        model, batch, requested_protocols=requested_protocols\n                    )\n\n                    if engine:\n                        computed_data = exposed.copy()\n                        computed_data[\"loss\"] = loss\n                        engine.update(\n                            data=dict(model=model, batch=batch),\n                            computed_data=computed_data,\n                        )\n\n                    log_summed(\"num_batches\", lambda: 1)\n                    log_averaged(\n                        \"perf/batch_get\",\n                        elapsed_batch_get,\n                    )\n\n                    iterations += 1\n                    if pbar:\n                        pbar.update(1)\n\n                    # Step metrics for each evaluation iteration\n                    step_meters(f\"{metrics_prefix}/{eval_name}\")\n\n            except StopIteration:\n                pass\n            finally:\n                if pbar:\n                    pbar.close()\n\n            total_time = time.perf_counter() - start_time\n            log_event_end(\"perf/total_run\")\n\n        eval_metrics = compute_meters(\n            f\"{metrics_prefix}/{eval_name}\",\n            aggregate=True,\n            collective=collective,\n        )\n\n        if engine:\n            eval_metrics = engine.compute(eval_metrics)\n\n        # Add basic performance stats\n        eval_metrics[\"perf/total_run_ms\"] = total_time * 1000\n        if iterations &gt; 0:\n            eval_metrics[\"perf/ms_per_batch\"] = (total_time / iterations) * 1000\n\n        logger.info(f\"Finished eval {eval_name}: {eval_metrics}\")\n        total_metrics[f\"{metrics_prefix}/{eval_name}\"] = eval_metrics\n    return total_metrics\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/evaluation_manager/#optimus_dl.recipe.train.mixins.managers.evaluation_manager.Evaluator.run_evaluation_if_needed","title":"<code>run_evaluation_if_needed(iteration, model, criterion, eval_data, collective=None, all_metrics_configs=None)</code>","text":"<p>Run evaluation if the current iteration matches the frequency.</p> <p>Parameters:</p> Name Type Description Default <code>iteration</code> <code>int</code> <p>Current training step.</p> required <code>model</code> <code>BaseModel</code> <p>The model to evaluate.</p> required <code>criterion</code> <code>BaseCriterion</code> <p>The loss function.</p> required <code>eval_data</code> <code>dict[str, Any]</code> <p>Dictionary mapping dataset names to dataloaders.</p> required <code>collective</code> <code>Any</code> <p>Distributed collective for metric aggregation.</p> <code>None</code> <code>all_metrics_configs</code> <code>dict[str, list[dict]] | None</code> <p>Root metrics configuration from TrainConfig.</p> <code>None</code> <p>Returns:</p> Type Description <code>None | dict</code> <p>Dictionary of computed metrics if evaluation ran, else None.</p> Source code in <code>optimus_dl/recipe/train/mixins/managers/evaluation_manager.py</code> <pre><code>def run_evaluation_if_needed(\n    self,\n    iteration: int,\n    model: BaseModel,\n    criterion: BaseCriterion,\n    eval_data: dict[str, Any],\n    collective: Any = None,\n    all_metrics_configs: dict[str, list[dict]] | None = None,\n) -&gt; None | dict:\n    \"\"\"Run evaluation if the current iteration matches the frequency.\n\n    Args:\n        iteration: Current training step.\n        model: The model to evaluate.\n        criterion: The loss function.\n        eval_data: Dictionary mapping dataset names to dataloaders.\n        collective: Distributed collective for metric aggregation.\n        all_metrics_configs: Root metrics configuration from TrainConfig.\n\n    Returns:\n        Dictionary of computed metrics if evaluation ran, else None.\n    \"\"\"\n    if self.eval_freq &lt;= 0 or iteration % self.eval_freq != 0:\n        return None\n\n    try:\n        return self.run_evaluation(\n            model=model,\n            criterion=criterion,\n            eval_data_dict=eval_data,\n            max_iterations=self.eval_iterations,\n            collective=collective,\n            all_metrics_configs=all_metrics_configs,\n        )\n    except Exception as e:\n        logger.error(f\"Evaluation failed: {e}\")\n        return None\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/evaluation_manager/#optimus_dl.recipe.train.mixins.managers.evaluation_manager.EvaluatorConfig","title":"<code>EvaluatorConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfig</code></p> <p>Configuration for the Evaluator.</p> Source code in <code>optimus_dl/recipe/train/mixins/managers/evaluation_manager.py</code> <pre><code>@dataclass\nclass EvaluatorConfig(RegistryConfig):\n    \"\"\"Configuration for the Evaluator.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/logger_manager/","title":"logger_manager","text":""},{"location":"reference/recipe/train/mixins/managers/logger_manager/#optimus_dl.recipe.train.mixins.managers.logger_manager","title":"<code>optimus_dl.recipe.train.mixins.managers.logger_manager</code>","text":"<p>Logger mixin for handling metrics logging.</p>"},{"location":"reference/recipe/train/mixins/managers/logger_manager/#optimus_dl.recipe.train.mixins.managers.logger_manager.LoggerManager","title":"<code>LoggerManager</code>","text":"<p>Manager for multiple metrics loggers.</p> <p>This class instantiates and orchestrates a list of logging backends (e.g., JSONL, WandB). It provides a unified interface for setting up, logging to, and closing all configured loggers.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>LoggerManagerConfig</code> <p>Manager configuration.</p> required <code>loggers_config</code> <code>list[MetricsLoggerConfig] | None</code> <p>List of configurations for individual loggers.</p> required Source code in <code>optimus_dl/recipe/train/mixins/managers/logger_manager.py</code> <pre><code>class LoggerManager:\n    \"\"\"Manager for multiple metrics loggers.\n\n    This class instantiates and orchestrates a list of logging backends (e.g.,\n    JSONL, WandB). It provides a unified interface for setting up, logging to,\n    and closing all configured loggers.\n\n    Args:\n        cfg: Manager configuration.\n        loggers_config: List of configurations for individual loggers.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: LoggerManagerConfig,\n        loggers_config: list[MetricsLoggerConfig] | None,\n        **kwargs: Any,\n    ):\n        self.loggers_config = loggers_config\n        self.previous_state = {}\n        self.loggers: list[BaseMetricsLogger] | None = None\n\n    def build_loggers(self, **kwargs):\n        \"\"\"Instantiate all configured loggers.\n\n        Uses the registry to build logger instances. If previous state is available\n        (from a checkpoint), it is passed to the logger builders for resumption.\n\n        Returns:\n            List of active logger instances.\n        \"\"\"\n        if self.loggers_config is None:\n            logger.info(\"No loggers configuration found, metrics logging disabled\")\n            return\n        assert self.loggers is None, \"Loggers already built\"\n\n        loggers = []\n        for logger_config in self.loggers_config:\n            try:\n                logger_instance = build(\n                    \"metrics_logger\",\n                    logger_config,\n                    state_dict=self.previous_state.get(logger_config.id),\n                    **kwargs,\n                )\n                loggers.append(logger_instance)\n                logger.info(f\"Built logger: {logger_instance.__class__.__name__}\")\n            except Exception as e:\n                logger.error(f\"Failed to build logger from config {logger_config}: {e}\")\n                raise\n\n        self.loggers = loggers\n\n    def setup_loggers(self, experiment_name: str, full_config: dict):\n        \"\"\"Initialize all loggers with experiment context.\n\n        Args:\n            experiment_name: Name of the experiment.\n            full_config: Complete training configuration dictionary.\n        \"\"\"\n        for logger_instance in self.loggers or []:\n            try:\n                logger_instance.setup(experiment_name, full_config)\n            except Exception as e:\n                logger.error(\n                    f\"Failed to setup logger {logger_instance.__class__.__name__}: {e}\"\n                )\n\n    def log_metrics_to_loggers(self, metrics, step: int, group: str = \"train\"):\n        \"\"\"Dispatch metrics to all active loggers.\n\n        Args:\n            metrics: Dictionary of metric values.\n            step: Current iteration.\n            group: Metric group name.\n        \"\"\"\n        for logger_instance in self.loggers or []:\n            try:\n                logger_instance.log_metrics(metrics, step, group)\n            except Exception as e:\n                logger.error(\n                    f\"Failed to log metrics with {logger_instance.__class__.__name__}: {e}\"\n                )\n\n    def close_loggers(self):\n        \"\"\"Clean up all loggers.\"\"\"\n        for logger_instance in self.loggers or []:\n            try:\n                logger_instance.close()\n            except Exception as e:\n                logger.error(\n                    f\"Failed to close logger {logger_instance.__class__.__name__}: {e}\"\n                )\n\n    def state_dict(self):\n        \"\"\"Collect state from all loggers for checkpointing.\"\"\"\n        return {\n            logger_instance.cfg.id: logger_instance.state_dict()\n            for logger_instance in self.loggers or []\n        }\n\n    def load_state_dict(self, state_dict):\n        \"\"\"Load logger states from a checkpoint.\"\"\"\n        self.previous_state = state_dict\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/logger_manager/#optimus_dl.recipe.train.mixins.managers.logger_manager.LoggerManager.build_loggers","title":"<code>build_loggers(**kwargs)</code>","text":"<p>Instantiate all configured loggers.</p> <p>Uses the registry to build logger instances. If previous state is available (from a checkpoint), it is passed to the logger builders for resumption.</p> <p>Returns:</p> Type Description <p>List of active logger instances.</p> Source code in <code>optimus_dl/recipe/train/mixins/managers/logger_manager.py</code> <pre><code>def build_loggers(self, **kwargs):\n    \"\"\"Instantiate all configured loggers.\n\n    Uses the registry to build logger instances. If previous state is available\n    (from a checkpoint), it is passed to the logger builders for resumption.\n\n    Returns:\n        List of active logger instances.\n    \"\"\"\n    if self.loggers_config is None:\n        logger.info(\"No loggers configuration found, metrics logging disabled\")\n        return\n    assert self.loggers is None, \"Loggers already built\"\n\n    loggers = []\n    for logger_config in self.loggers_config:\n        try:\n            logger_instance = build(\n                \"metrics_logger\",\n                logger_config,\n                state_dict=self.previous_state.get(logger_config.id),\n                **kwargs,\n            )\n            loggers.append(logger_instance)\n            logger.info(f\"Built logger: {logger_instance.__class__.__name__}\")\n        except Exception as e:\n            logger.error(f\"Failed to build logger from config {logger_config}: {e}\")\n            raise\n\n    self.loggers = loggers\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/logger_manager/#optimus_dl.recipe.train.mixins.managers.logger_manager.LoggerManager.close_loggers","title":"<code>close_loggers()</code>","text":"<p>Clean up all loggers.</p> Source code in <code>optimus_dl/recipe/train/mixins/managers/logger_manager.py</code> <pre><code>def close_loggers(self):\n    \"\"\"Clean up all loggers.\"\"\"\n    for logger_instance in self.loggers or []:\n        try:\n            logger_instance.close()\n        except Exception as e:\n            logger.error(\n                f\"Failed to close logger {logger_instance.__class__.__name__}: {e}\"\n            )\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/logger_manager/#optimus_dl.recipe.train.mixins.managers.logger_manager.LoggerManager.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Load logger states from a checkpoint.</p> Source code in <code>optimus_dl/recipe/train/mixins/managers/logger_manager.py</code> <pre><code>def load_state_dict(self, state_dict):\n    \"\"\"Load logger states from a checkpoint.\"\"\"\n    self.previous_state = state_dict\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/logger_manager/#optimus_dl.recipe.train.mixins.managers.logger_manager.LoggerManager.log_metrics_to_loggers","title":"<code>log_metrics_to_loggers(metrics, step, group='train')</code>","text":"<p>Dispatch metrics to all active loggers.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <p>Dictionary of metric values.</p> required <code>step</code> <code>int</code> <p>Current iteration.</p> required <code>group</code> <code>str</code> <p>Metric group name.</p> <code>'train'</code> Source code in <code>optimus_dl/recipe/train/mixins/managers/logger_manager.py</code> <pre><code>def log_metrics_to_loggers(self, metrics, step: int, group: str = \"train\"):\n    \"\"\"Dispatch metrics to all active loggers.\n\n    Args:\n        metrics: Dictionary of metric values.\n        step: Current iteration.\n        group: Metric group name.\n    \"\"\"\n    for logger_instance in self.loggers or []:\n        try:\n            logger_instance.log_metrics(metrics, step, group)\n        except Exception as e:\n            logger.error(\n                f\"Failed to log metrics with {logger_instance.__class__.__name__}: {e}\"\n            )\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/logger_manager/#optimus_dl.recipe.train.mixins.managers.logger_manager.LoggerManager.setup_loggers","title":"<code>setup_loggers(experiment_name, full_config)</code>","text":"<p>Initialize all loggers with experiment context.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment.</p> required <code>full_config</code> <code>dict</code> <p>Complete training configuration dictionary.</p> required Source code in <code>optimus_dl/recipe/train/mixins/managers/logger_manager.py</code> <pre><code>def setup_loggers(self, experiment_name: str, full_config: dict):\n    \"\"\"Initialize all loggers with experiment context.\n\n    Args:\n        experiment_name: Name of the experiment.\n        full_config: Complete training configuration dictionary.\n    \"\"\"\n    for logger_instance in self.loggers or []:\n        try:\n            logger_instance.setup(experiment_name, full_config)\n        except Exception as e:\n            logger.error(\n                f\"Failed to setup logger {logger_instance.__class__.__name__}: {e}\"\n            )\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/logger_manager/#optimus_dl.recipe.train.mixins.managers.logger_manager.LoggerManager.state_dict","title":"<code>state_dict()</code>","text":"<p>Collect state from all loggers for checkpointing.</p> Source code in <code>optimus_dl/recipe/train/mixins/managers/logger_manager.py</code> <pre><code>def state_dict(self):\n    \"\"\"Collect state from all loggers for checkpointing.\"\"\"\n    return {\n        logger_instance.cfg.id: logger_instance.state_dict()\n        for logger_instance in self.loggers or []\n    }\n</code></pre>"},{"location":"reference/recipe/train/mixins/managers/logger_manager/#optimus_dl.recipe.train.mixins.managers.logger_manager.LoggerManagerConfig","title":"<code>LoggerManagerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegistryConfig</code></p> <p>Configuration for LoggerManager.</p> Source code in <code>optimus_dl/recipe/train/mixins/managers/logger_manager.py</code> <pre><code>@dataclass\nclass LoggerManagerConfig(RegistryConfig):\n    \"\"\"Configuration for LoggerManager.\"\"\"\n\n    pass\n</code></pre>"},{"location":"user-guide/configuration/","title":"Configuration Guide","text":"<p>Optimus-DL uses Hydra for its configuration system, enabling flexible, hierarchical, and composable setups. All training configurations are located in the <code>configs/</code> directory.</p>"},{"location":"user-guide/configuration/#core-concepts","title":"Core Concepts","text":""},{"location":"user-guide/configuration/#1-hierarchical-structure","title":"1. Hierarchical Structure","text":"<p>Configurations are built in layers. A main training config (e.g., <code>train_llama.yaml</code>) specifies defaults for different components like the model, optimizer, and data.</p> <p>Example from <code>train_llama.yaml</code>: <pre><code>defaults:\n  - defaults.yaml\n  - lr_scheduler: wsd\n  - model: llama2\n  - criterion: cross_entropy\n  - loggers: basic\n  - optimization/amp: bfloat16\n  - _self_\n</code></pre> Each item in the <code>defaults</code> list points to another YAML file, allowing you to mix and match components easily.</p>"},{"location":"user-guide/configuration/#2-the-args-section","title":"2. The <code>args</code> Section","text":"<p>We use a special <code>args</code> section as a \"scratch space\" for high-level variables that are reused throughout the configuration. This is the single source of truth for important parameters like batch size, sequence length, and vocabulary size.</p> <pre><code>args:\n  name: llama-finetune\n  batch_size: 64\n  seq_len: 1024\n  vocab_size: 32000\n</code></pre> <p>These values are then referenced in other parts of the config using OmegaConf's interpolation syntax (<code>${...}</code>).</p>"},{"location":"user-guide/configuration/#3-interpolation","title":"3. Interpolation","text":"<p>Interpolation is key to keeping the configuration DRY (Don't Repeat Yourself).</p> <pre><code>model:\n  vocab_size: ${args.vocab_size} # From args\n\ndata:\n  train_datasets:\n    transform:\n      _name: flat_batcher\n      batch_size: ${args.batch_size} # From args\n      seq_len: ${args.seq_len}       # From args\n</code></pre> <p>You can also evaluate simple expressions: <pre><code>args:\n  global_batch_size: 128\n  num_devices: 8\n\n# Calculate per-device batch size\nper_device_batch_size: ${eval:\"int(${args.global_batch_size} / ${args.num_devices})\"}\n</code></pre></p>"},{"location":"user-guide/configuration/#key-configuration-sections","title":"Key Configuration Sections","text":""},{"location":"user-guide/configuration/#model","title":"<code>model</code>","text":"<p>This section defines the model architecture. The <code>_name</code> key determines which model to build (e.g., <code>llama2</code>, <code>gpt2</code>). Other parameters are specific to the model, such as the number of layers, hidden size, and number of attention heads.</p> <pre><code>model:\n  _name: llama2\n  vocab_size: ${args.vocab_size}\n  n_layer: 12\n  n_head: 12\n  hidden_dim: 768\n</code></pre>"},{"location":"user-guide/configuration/#data","title":"<code>data</code>","text":"<p>This section defines the entire data pipeline, including training and evaluation datasets. It typically contains: - <code>train_datasets</code> and <code>eval_datasets</code>: Define the data sources and transforms. - <code>scratch</code>: A reusable space to define complex transform chains that can be referenced via interpolation.</p> <pre><code>data:\n  scratch:\n    # Define a reusable transform chain\n    my_transform:\n      _name: compose\n      transforms:\n        - _name: tokenize\n          tokenizer_config:\n            _name: tiktoken\n            name: gpt2\n        - _name: chunk_tokens\n          max_seq_len: ${args.seq_len}\n        - _name: shuffle\n          buffer_size: 8096\n        - _name: flat_batcher\n          batch_size: ${args.batch_size}\n          seq_len: ${args.seq_len}\n        - _name: prefetch\n        - _name: to_device\n\n  train_datasets:\n    source:\n      _name: loop\n      inner:\n        _name: preset_slimpajama6b\n        split: train\n    transform: ${data.scratch.my_transform} # Reference the chain\n</code></pre>"},{"location":"user-guide/configuration/#optimization","title":"<code>optimization</code>","text":"<p>This section controls the optimization process, including the optimizer, learning rate scheduler, and gradient clipping.</p> <pre><code># Optimization settings\noptimization:\n  iterations: ${args.iterations}\n  acc_steps: 1          # Gradient accumulation steps\n  clip_grad_norm: 5.0   # Gradient clipping norm\n\n  optimizer:\n    _name: adamw\n    lr: 5e-4            # Base learning rate\n    weight_decay: 1e-1\n    betas: [0.9, 0.99]  # Adam beta parameters\n    eps: 1e-8           # Adam epsilon\n</code></pre>"},{"location":"user-guide/configuration/#command-line-overrides","title":"Command-Line Overrides","text":"<p>One of Hydra's most powerful features is the ability to override any configuration value from the command line.</p> <pre><code># Override the learning rate and batch size\npython scripts/train.py \\\n  optimization.optimizer.lr=0.01 \\\n  args.batch_size=32\n\n# Swap out the entire model for GPT-2\npython scripts/train.py model=gpt2\n</code></pre> <p>This makes experimentation fast and easy without needing to modify the underlying configuration files.</p>"},{"location":"user-guide/data/","title":"Data Pipelines","text":"<p>Data handling in Optimus-DL is designed to be highly flexible and modular, allowing for complex data processing pipelines to be constructed from reusable components. The core components are located in <code>optimus_dl.modules.data</code>.</p> <p>The key components are: - Sources: Yield raw data items, like lines from a text file or examples from a Hugging Face dataset. - Transforms: A chain of operations applied to the data, such as tokenization, chunking, shuffling, and batching.</p> <p>For detailed information, see the Data API Reference.</p>"},{"location":"user-guide/data/#core-components","title":"Core Components","text":"<ul> <li><code>datasets</code>: Contains various dataset implementations, including tokenized datasets, and utilities for handling different data formats.</li> <li><code>presets</code>: Provides some predefined datasets for common use cases.</li> <li><code>transforms</code>: Includes a wide range of data transformations, from tokenization to batching and device placement.</li> </ul>"},{"location":"user-guide/models/","title":"Models","text":"<p>The <code>optimus_dl.modules.model</code> package contains the core model architectures implemented in the framework. These models are designed to be modular, configurable, and compatible with various distributed training strategies like Tensor Parallelism.</p> <p>See the full Model API Reference for detailed documentation on all classes and functions.</p>"},{"location":"user-guide/models/#available-models","title":"Available Models","text":"<p>Below is a list of the primary model implementations available in Optimus-DL.</p> <ul> <li><code>llama2</code>: A highly optimized implementation of the Llama 2 and Llama 3 architectures, including support for Grouped-Query Attention (GQA) and Rotary Position Embeddings (RoPE).</li> <li><code>gpt2</code>: A classic implementation of the GPT-2 architecture, often used for baselining and research.</li> <li><code>qwen</code>: An implementation of the Qwen architecture, featuring Q/K LayerNorm for improved attention mechanics.</li> <li><code>base</code>: Defines the base interface for all models, ensuring consistent API for training and evaluation.</li> <li><code>config</code>: Provides structured <code>dataclass</code> configurations for all supported models, validated with <code>hydra-zen</code>.</li> </ul> <p>Llama and Qwen support loading pretrained weights from HuggingFace.</p>"},{"location":"user-guide/optimizers/","title":"Optimizers","text":"<p>Optimus-DL leverages PyTorch's standard optimizers and provides additional customized implementations and configurations in the <code>optimus_dl.modules.optim</code> package. The framework is designed to easily integrate with different optimization strategies through the Hydra configuration system.</p> <p>For a complete list of available optimizers and their configurations, see the Optimizer API Reference.</p>"},{"location":"user-guide/optimizers/#key-optimizers","title":"Key Optimizers","text":"<p>While any PyTorch optimizer can be used, Optimus-DL provides convenient configs and wrappers for the following:</p> <ul> <li><code>adamw</code>: The AdamW optimizer, a standard choice for training transformer models, with options for weight decay and other hyperparameters. It is the default optimizer for most training recipes.</li> <li><code>muon</code>: The Muon optimizer, a momentum-based optimizer with Newton-Schulz iterations for improved convergence.</li> <li><code>soap</code>: The SOAP (Shampoo with Adam in the Preconditioner subspace) optimizer, a second-order optimizer that uses Shampoo preconditioning combined with Adam updates for improved training efficiency.</li> </ul>"},{"location":"user-guide/optimizers/#optimizer-configuration","title":"Optimizer Configuration","text":"<p>Optimizers are configured through the Hydra configuration system. Each optimizer has its own set of hyperparameters that can be tuned for your specific use case.</p>"},{"location":"user-guide/optimizers/#adamw-configuration-example","title":"AdamW Configuration Example","text":"<pre><code>optimization:\n  optimizer:\n    _name: adamw\n    lr: 5e-4\n    weight_decay: 1e-1\n    betas: [0.9, 0.99]\n    eps: 1e-8\n    fused: true\n</code></pre>"},{"location":"user-guide/optimizers/#muon-configuration-example","title":"Muon Configuration Example","text":"<pre><code>optimization:\n  optimizer:\n    _name: muon\n    lr: 1e-3\n    weight_decay: 0.1\n    momentum: 0.95\n    nesterov: true\n</code></pre>"},{"location":"user-guide/optimizers/#soap-configuration-example","title":"SOAP Configuration Example","text":"<pre><code>optimization:\n  optimizer:\n    _name: soap\n    lr: 3e-3\n    betas: [0.95, 0.95]\n    weight_decay: 0.01\n    precondition_frequency: 10\n    max_precond_dim: 10000\n</code></pre>"},{"location":"user-guide/quick-start/","title":"Quick Start Guide","text":"<p>This guide will walk you through the essential steps to get Optimus-DL up and running, from installation to starting your first training job.</p>"},{"location":"user-guide/quick-start/#1-installation","title":"1. Installation","text":"<p>First, clone the repository and install the framework in editable mode. This allows you to easily modify the code and have your changes reflected immediately.</p> <pre><code># Clone the repository\ngit clone https://github.com/alexdremov/optimus-dl\ncd optimus-dl\n\n# Install in editable mode with dependencies\npip install -e .\n</code></pre> <p>Alternatively, you can use docker: https://hub.docker.com/repository/docker/alexdremov/optimus-dl/general</p>"},{"location":"user-guide/quick-start/#2-running-a-training-job","title":"2. Running a Training Job","text":"<p>The easiest way to start is to run a training job with one of the provided default configurations. The <code>train_llama.yaml</code> config is a good starting point.</p> <p>Training is orchestrated by the <code>scripts/train.py</code> script.</p> <pre><code># Run with the default Llama configuration\npython scripts/train.py --config-name=train_llama\n\n# multi-gpu training\ntorchrun --nproc_per_node=gpu scripts/train.py --config-name=train_llama\n</code></pre> <p>This command will: 1. Load the <code>train_llama.yaml</code> configuration. 2. Build the model, data pipeline, optimizer, and other components. 3. Start the training loop, which will log progress and save checkpoints to the <code>outputs/</code> directory.</p>"},{"location":"user-guide/quick-start/#3-customizing-your-run","title":"3. Customizing Your Run","text":"<p>You can easily override any parameter from the configuration file directly on the command line. This is perfect for quick experiments.</p> <pre><code># Override the batch size and use a different model\npython scripts/train.py \\\n    --config-name=train_llama \\\n    model=gpt2 \\\n    args.batch_size=32\n</code></pre> <p>This will start a new training run using the <code>gpt2</code> model configuration and a batch size of 32, while keeping all other settings from <code>train_llama.yaml</code>.</p>"},{"location":"user-guide/quick-start/#4-evaluation","title":"4. Evaluation","text":"<p>Once you have a trained checkpoint, you can evaluate it on standard benchmarks using the integrated Language Model Evaluation Harness.</p> <pre><code># Evaluate a checkpoint on Hellaswag and MMLU\npython scripts/eval.py \\\n    common.checkpoint_path=outputs/my-run/checkpoint_00010000 \\\n    lm_eval.tasks=[hellaswag,mmlu] \\\n    lm_eval.batch_size=8\n</code></pre>"},{"location":"user-guide/quick-start/#5-serving-the-model","title":"5. Serving the Model","text":"<p>Optimus-DL also includes a simple server to deploy your trained models as an OpenAI-compatible API endpoint.</p> <pre><code># Serve a pre-configured TinyLlama model\npython scripts/serve.py --config-name=tinyllama\n</code></pre> <p>You can then send requests to the running server: <pre><code>curl -X POST http://localhost:8000/v1/completions \\\n  -d '{\"prompt\": \"The future of AI is\", \"max_tokens\": 50}'\n</code></pre></p>"},{"location":"user-guide/quick-start/#whats-next","title":"What's Next?","text":"<p>You've successfully run your first training job! Here's where to go next:</p> <ul> <li>Dive into Configuration: Learn how to create your own comprehensive training workflows in the Configuration Guide.</li> <li>Explore the Components: See what's available for Models, Data Pipelines, and Optimizers.</li> <li>Browse the API: For in-depth details, head to the API Reference.</li> </ul>"}]}